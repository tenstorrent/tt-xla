WARNING:root:Defaulting to PJRT_DEVICE=CPU
Using TT-Metal from the source tree: /localdev/hshah/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /localdev/hshah/tt-xla/venv/bin/python
cachedir: .pytest_cache
rootdir: /localdev/hshah/tt-xla
configfile: pytest.ini
plugins: forked-1.6.0, anyio-4.11.0, split-0.10.0, jaxtyping-0.3.3
collecting ... collected 1 item

tests/torch/multi_chip/n300/models/data_parallel/batch_sharded/mnist_inference.py::test_mnist_inference_tensor_parallel[32-784] 2025-10-20 15:39:39.322 (   0.000s) [        C1B56000]      dylib_platform.cc:47       1| DylibPlatform::SubclassInitialize
2025-10-20 15:39:39.333 (   0.011s) [        C1B56000]     client_instance.cc:44       1| ClientInstance::ClientInstance
2025-10-20 15:39:39.333 (   0.011s) [        C1B56000]              client.cc:18       1| TTClientInstance::TTClientInstance
2025-10-20 15:39:39.333 (   0.011s) [        C1B56000]     client_instance.cc:73       1| ClientInstance::Initialize
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]              stubs.inc:106   WARN| STUB: PJRT_Client_TopologyDescription
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     client_instance.cc:510      1| ClientInstance::PJRT_Client_PlatformVersion
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     client_instance.cc:490      1| ClientInstance::PJRT_Client_PlatformName
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     client_instance.cc:522      1| ClientInstance::PJRT_Client_Devices
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     client_instance.cc:535      1| ClientInstance::PJRT_Client_AddressableDevices
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     client_instance.cc:585      1| ClientInstance::PJRT_Client_AddressableMemories
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]        api_bindings.cc:76       1| PJRT_Plugin_Attributes
2025-10-20 15:40:21.984952: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:21.984 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:21.985 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:21.985 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:21.985 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:21.985 (  42.662s) [        C1B56000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-20 15:40:21.985 (  42.662s) [        C1B56000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-20 15:40:21.985 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:21.985 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:21.985 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:21.985 (  42.662s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.003 (  42.681s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.003 (  42.681s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.003 (  42.681s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.003 (  42.681s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.003 (  42.681s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.004 (  42.682s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.004 (  42.682s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.004 (  42.682s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.004 (  42.682s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.004 (  42.682s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.004 (  42.682s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.004 (  42.682s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.005 (  42.683s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.006 (  42.683s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.007 (  42.684s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.007 (  42.684s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.007 (  42.684s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.007 (  42.685s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.008 (  42.685s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.008 (  42.686s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.008 (  42.686s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.008 (  42.686s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.008 (  42.686s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.008 (  42.686s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.008 (  42.686s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.008 (  42.686s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.008 (  42.686s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.009 (  42.686s) [        C1B56000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.009 (  42.687s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.009 (  42.687s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.009 (  42.687s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.009 (  42.687s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.009 (  42.687s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.009 (  42.687s) [        C1B56000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-20 15:40:22.009 (  42.687s) [        C1B56000]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-20 15:40:22.009 (  42.687s) [        C1B56000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-20 15:40:22.009 (  42.687s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:22.010 (  42.687s) [        C1B56000]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:22.010 (  42.688s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.010 (  42.688s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.688s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.688s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.011 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.012 (  42.689s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.012 (  42.690s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.012 (  42.690s) [        C1B56000]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-20 15:40:22.021 (  42.699s) [        C1B56000]     client_instance.cc:598      1| ClientInstance::PJRT_Client_Compile
2025-10-20 15:40:22.021 (  42.699s) [        C1B56000]      module_builder.cc:221      1| ModuleBuilder::buildModule
2025-10-20 15:40:22.023 (  42.701s) [        C1B56000]      module_builder.cc:334      1| VHLO Module:
#loc1 = loc("xla__device_data")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<512x!vhlo.bf16_v1> loc("xla__device_data"), %arg1: !vhlo.tensor_v1<512x512x!vhlo.bf16_v1> loc("xla__device_data"), %arg2: !vhlo.tensor_v1<512x!vhlo.bf16_v1> loc("xla__device_data"), %arg3: !vhlo.tensor_v1<512x784x!vhlo.bf16_v1> loc("xla__device_data"), %arg4: !vhlo.tensor_v1<32x784x!vhlo.bf16_v1> loc("xla__device_data")) -> (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.transpose_v1"(%arg3) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[784,512]{0,1}">} : (!vhlo.tensor_v1<512x784x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<784x512x!vhlo.bf16_v1> loc(#loc2)
    %3 = "vhlo.dot_general_v2"(%arg4, %2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x784x!vhlo.bf16_v1>, !vhlo.tensor_v1<784x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1> loc(#loc3)
    %4 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1> loc(#loc3)
    %5 = "vhlo.add_v1"(%3, %4) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1> loc(#loc3)
    %6 = "vhlo.maximum_v1"(%5, %1) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1> loc(#loc4)
    %7 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[512,512]{0,1}">} : (!vhlo.tensor_v1<512x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x512x!vhlo.bf16_v1> loc(#loc2)
    %8 = "vhlo.dot_general_v2"(%6, %7) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<512x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1> loc(#loc3)
    %9 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1> loc(#loc3)
    %10 = "vhlo.add_v1"(%8, %9) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1> loc(#loc3)
    %11 = "vhlo.maximum_v1"(%10, %1) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1> loc(#loc4)
    "vhlo.return_v1"(%11) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("aten__permute")
#loc3 = loc("aten__addmm")
#loc4 = loc("aten__relu")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<512x512x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<512x784x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x784x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %2 = "vhlo.transpose_v1"(%arg3) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[784,512]{0,1}">} : (!vhlo.tensor_v1<512x784x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<784x512x!vhlo.bf16_v1>
    %3 = "vhlo.dot_general_v2"(%arg4, %2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x784x!vhlo.bf16_v1>, !vhlo.tensor_v1<784x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %5 = "vhlo.add_v1"(%3, %4) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %6 = "vhlo.maximum_v1"(%5, %1) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %7 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[512,512]{0,1}">} : (!vhlo.tensor_v1<512x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x512x!vhlo.bf16_v1>
    %8 = "vhlo.dot_general_v2"(%6, %7) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<512x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %9 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %10 = "vhlo.add_v1"(%8, %9) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %11 = "vhlo.maximum_v1"(%10, %1) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    "vhlo.return_v1"(%11) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<512x512x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<512x784x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x784x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %2 = "vhlo.transpose_v1"(%arg3) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[784,512]{0,1}">} : (!vhlo.tensor_v1<512x784x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<784x512x!vhlo.bf16_v1>
    %3 = "vhlo.dot_general_v2"(%arg4, %2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x784x!vhlo.bf16_v1>, !vhlo.tensor_v1<784x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %5 = "vhlo.add_v1"(%3, %4) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %6 = "vhlo.maximum_v1"(%5, %1) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %7 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[512,512]{0,1}">} : (!vhlo.tensor_v1<512x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x512x!vhlo.bf16_v1>
    %8 = "vhlo.dot_general_v2"(%6, %7) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<512x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %9 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %10 = "vhlo.add_v1"(%8, %9) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    %11 = "vhlo.maximum_v1"(%10, %1) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x!vhlo.bf16_v1>
    "vhlo.return_v1"(%11) : (!vhlo.tensor_v1<32x512x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,2]<=[2]}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<512x512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}"}, %arg2: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[2]<=[2]}"}, %arg3: tensor<512x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"}, %arg4: tensor<32x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}"}) -> tensor<32x512xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


2025-10-20 15:40:22.030 (  42.708s) [        C1B56000]      module_builder.cc:353      1| SHLO Module:
#loc1 = loc("xla__device_data")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("xla__device_data"), %arg1: tensor<512x512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}"} loc("xla__device_data"), %arg2: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[2]<=[2]}"} loc("xla__device_data"), %arg3: tensor<512x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}"} loc("xla__device_data"), %arg4: tensor<32x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}"} loc("xla__device_data")) -> tensor<32x512xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16> loc(#loc)
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16> loc(#loc2)
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16> loc(#loc3)
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16> loc(#loc4)
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16> loc(#loc2)
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16> loc(#loc3)
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16> loc(#loc4)
    return %10 : tensor<32x512xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("aten__permute")
#loc3 = loc("aten__addmm")
#loc4 = loc("aten__relu")
------------------ END OF MLIR MODULE ------------------
2025-10-20 15:40:22.031 (  42.709s) [        C1B56000]      module_builder.cc:365      1| SHLO Module after frontend StableHLO pipeline:
#loc1 = loc("xla__device_data")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>} loc("xla__device_data"), %arg1: tensor<512x512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("xla__device_data"), %arg2: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("xla__device_data"), %arg3: tensor<512x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("xla__device_data"), %arg4: tensor<32x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("xla__device_data")) -> tensor<32x512xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16> loc(#loc)
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16> loc(#loc2)
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16> loc(#loc3)
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16> loc(#loc4)
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16> loc(#loc2)
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16> loc(#loc3)
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16> loc(#loc4)
    return %10 : tensor<32x512xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("aten__permute")
#loc3 = loc("aten__addmm")
#loc4 = loc("aten__relu")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<512x512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<512x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<32x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<32x512xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<512x512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<512x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<32x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<32x512xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<512x512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<512x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<32x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<32x512xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<512x512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<512x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<32x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<32x512xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[2,1]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,2]<=[2]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %5 = stablehlo.maximum %4, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %9 = stablehlo.add %7, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %10 = stablehlo.maximum %9, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    return %10 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = sdy.reshard %arg4 <@mesh, [{?}, {?}]> : tensor<32x784xbf16>
    %3 = stablehlo.dot_general %2, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %5 = stablehlo.add %3, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %6 = stablehlo.maximum %5, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %7 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %9 = sdy.all_reduce {"_axis_0"} %8 out_sharding=<@mesh, [{?}, {?}]> : tensor<32x512xbf16>
    %10 = sdy.reshard %9 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<32x512xbf16>
    %11 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %12 = stablehlo.add %10, %11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %13 = stablehlo.maximum %12, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    return %13 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<32x512xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
    %2 = sdy.reshard %arg4 <@mesh, [{?}, {?}]> : tensor<32x784xbf16>
    %3 = stablehlo.dot_general %2, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
    %4 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %5 = stablehlo.add %3, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %6 = stablehlo.maximum %5, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %7 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
    %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
    %9 = sdy.all_reduce {"_axis_0"} %8 out_sharding=<@mesh, [{?}, {?}]> : tensor<32x512xbf16>
    %10 = sdy.reshard %9 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<32x512xbf16>
    %11 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
    %12 = stablehlo.add %10, %11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    %13 = stablehlo.maximum %12, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
    return %13 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={} (%arg5: tensor<512xbf16>, %arg6: tensor<512x512xbf16>, %arg7: tensor<512xbf16>, %arg8: tensor<512x784xbf16>, %arg9: tensor<32x784xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<32x512xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
      %3 = sdy.reshard %arg9 <@mesh, [{?}, {?}]> : tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
      %6 = stablehlo.add %4, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %7 = stablehlo.maximum %6, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
      %10 = sdy.all_reduce {"_axis_0"} %9 out_sharding=<@mesh, [{?}, {?}]> : tensor<32x512xbf16>
      %11 = sdy.reshard %10 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<32x512xbf16>
      %12 = stablehlo.broadcast_in_dim %arg5, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
      %13 = stablehlo.add %11, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %14 = stablehlo.maximum %13, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      sdy.return %14 : tensor<32x512xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={} (%arg5: tensor<512xbf16>, %arg6: tensor<512x512xbf16>, %arg7: tensor<512xbf16>, %arg8: tensor<512x784xbf16>, %arg9: tensor<32x784xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<32x512xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
      %3 = sdy.reshard %arg9 <@mesh, [{?}, {?}]> : tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
      %6 = stablehlo.add %4, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %7 = stablehlo.maximum %6, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
      %10 = sdy.all_reduce {"_axis_0"} %9 out_sharding=<@mesh, [{?}, {?}]> : tensor<32x512xbf16>
      %11 = sdy.reshard %10 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<32x512xbf16>
      %12 = stablehlo.broadcast_in_dim %arg5, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
      %13 = stablehlo.add %11, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %14 = stablehlo.maximum %13, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      sdy.return %14 : tensor<32x512xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={} (%arg5: tensor<512xbf16>, %arg6: tensor<512x512xbf16>, %arg7: tensor<512xbf16>, %arg8: tensor<512x784xbf16>, %arg9: tensor<32x784xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<32x512xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
      %3 = sdy.all_gather [{}, {"_axis_0"}] %arg9 out_sharding=<@mesh, [{}, {}]> : tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
      %6 = stablehlo.add %4, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %7 = stablehlo.maximum %6, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
      %10 = sdy.all_reduce {"_axis_0"} %9 out_sharding=<@mesh, [{?}, {?}]> : tensor<32x512xbf16>
      %11 = sdy.all_slice [{}, {"_axis_0"}] %10 out_sharding=<@mesh, [{}, {"_axis_0"}]> : tensor<32x512xbf16>
      %12 = stablehlo.broadcast_in_dim %arg5, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
      %13 = stablehlo.add %11, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %14 = stablehlo.maximum %13, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      sdy.return %14 : tensor<32x512xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={} (%arg5: tensor<512xbf16>, %arg6: tensor<512x512xbf16>, %arg7: tensor<512xbf16>, %arg8: tensor<512x784xbf16>, %arg9: tensor<32x784xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<32x512xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<512x784xbf16>) -> tensor<784x512xbf16>
      %3 = sdy.all_gather [{}, {"_axis_0"}] %arg9 out_sharding=<@mesh, [{}, {}]> : tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<32x784xbf16>, tensor<784x512xbf16>) -> tensor<32x512xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
      %6 = stablehlo.add %4, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %7 = stablehlo.maximum %6, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x512xbf16>) -> tensor<512x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<32x512xbf16>, tensor<512x512xbf16>) -> tensor<32x512xbf16>
      %10 = sdy.all_reduce {"_axis_0"} %9 out_sharding=<@mesh, [{?}, {?}]> : tensor<32x512xbf16>
      %11 = sdy.all_slice [{}, {"_axis_0"}] %10 out_sharding=<@mesh, [{}, {"_axis_0"}]> : tensor<32x512xbf16>
      %12 = stablehlo.broadcast_in_dim %arg5, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<32x512xbf16>
      %13 = stablehlo.add %11, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      %14 = stablehlo.maximum %13, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<32x512xbf16>
      sdy.return %14 : tensor<32x512xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


2025-10-20 15:40:22.052 (  42.730s) [        C1B56000]      module_builder.cc:682      1| SHLO Module after compiler StableHLO pipeline:
#loc1 = loc("xla__device_data")
#loc3 = loc("aten__addmm")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]> loc(#loc)
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("xla__device_data"), %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data"), %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data"), %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data"), %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data")) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16> loc("xla__device_data"), %arg6: tensor<512x256xbf16> loc("xla__device_data"), %arg7: tensor<256xbf16> loc("xla__device_data"), %arg8: tensor<256x784xbf16> loc("xla__device_data"), %arg9: tensor<32x392xbf16> loc("xla__device_data")) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16> loc(#loc)
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16> loc(#loc2)
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16> loc(#loc1)
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16> loc(#loc3)
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16> loc(#loc4)
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16> loc(#loc2)
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16> loc("aten__addmm"), %arg11: tensor<bf16> loc("aten__addmm")):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16> loc(#loc3)
        stablehlo.return %18 : tensor<bf16> loc(#loc3)
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16> loc(#loc3)
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16> loc(#loc3)
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16> loc(#loc3)
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16> loc(#loc3)
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16> loc(#loc4)
      sdy.return %17 : tensor<32x256xbf16> loc(#loc)
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16> loc(#loc)
    return %0 : tensor<32x512xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("aten__permute")
#loc4 = loc("aten__relu")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before LegalizeStableHLOCompositeToTTIR (legalize-stablehlo-composite-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<256xbf16>, %arg6: tensor<512x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x784xbf16>, %arg9: tensor<32x392xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<32x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[784,512]{0,1}"} : (tensor<256x784xbf16>) -> tensor<784x256xbf16>
      %3 = "stablehlo.all_gather"(%arg9) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> : (tensor<32x392xbf16>) -> tensor<32x784xbf16>
      %4 = stablehlo.dot_general %3, %2, contracting_dims = [1] x [0] : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
      %5 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %6 = stablehlo.add %4, %5 : tensor<32x256xbf16>
      %7 = stablehlo.maximum %6, %1 : tensor<32x256xbf16>
      %8 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,512]{0,1}"} : (tensor<512x256xbf16>) -> tensor<256x512xbf16>
      %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
      %10 = "stablehlo.all_reduce"(%9) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %18 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %18 : tensor<bf16>
      }) : (tensor<32x512xbf16>) -> tensor<32x512xbf16>
      %11 = stablehlo.reshape %10 : (tensor<32x512xbf16>) -> tensor<32x2x256xbf16>
      %12 = "stablehlo.all_to_all"(%11) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
      %13 = stablehlo.slice %12 [0:32, 0:1, 0:256] : (tensor<32x2x256xbf16>) -> tensor<32x1x256xbf16>
      %14 = stablehlo.reshape %13 : (tensor<32x1x256xbf16>) -> tensor<32x256xbf16>
      %15 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<256xbf16>) -> tensor<32x256xbf16>
      %16 = stablehlo.add %14, %15 : tensor<32x256xbf16>
      %17 = stablehlo.maximum %16, %1 : tensor<32x256xbf16>
      sdy.return %17 : tensor<32x256xbf16>
    } : (tensor<512xbf16>, tensor<512x512xbf16>, tensor<512xbf16>, tensor<512x784xbf16>, tensor<32x784xbf16>) -> tensor<32x512xbf16>
    return %0 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
    %4 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %6 = ttir.empty() : tensor<1x1xbf16>
    %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %8 = ttir.empty() : tensor<32x256xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %10 = ttir.empty() : tensor<784x256xbf16>
    %11 = "ttir.permute"(%3, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
    %12 = ttir.empty() : tensor<32x784xbf16>
    %13 = "ttir.all_gather"(%4, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
    %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
    %15 = ttir.empty() : tensor<1x256xbf16>
    %16 = "ttir.reshape"(%2, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %17 = ttir.empty() : tensor<32x256xbf16>
    %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %19 = ttir.empty() : tensor<32x256xbf16>
    %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %21 = ttir.empty() : tensor<32x256xbf16>
    %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %23 = ttir.empty() : tensor<256x512xbf16>
    %24 = "ttir.permute"(%1, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
    %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
    %26 = ttir.empty() : tensor<32x512xbf16>
    %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
    %28 = ttir.empty() : tensor<32x2x256xbf16>
    %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %30 = ttir.empty() : tensor<32x2x256xbf16>
    %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %32 = ttir.empty() : tensor<32x1x256xbf16>
    %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
    %34 = ttir.empty() : tensor<32x256xbf16>
    %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %36 = ttir.empty() : tensor<1x256xbf16>
    %37 = "ttir.reshape"(%0, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %38 = ttir.empty() : tensor<32x256xbf16>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %40 = ttir.empty() : tensor<32x256xbf16>
    %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %42 = ttir.empty() : tensor<32x256xbf16>
    %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
    return %44 : tensor<32x512xbf16>
  }
}


2025-10-20 15:40:22.065 (  42.743s) [        C1B56000]      module_builder.cc:715      1| TTIR Module:
#loc1 = loc("xla__device_data")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("xla__device_data"), %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data"), %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data"), %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data"), %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data")) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16> loc(#loc)
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16> loc(#loc)
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16> loc(#loc)
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16> loc(#loc)
    %4 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16> loc(#loc)
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
    %6 = ttir.empty() : tensor<1x1xbf16> loc(#loc)
    %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16> loc(#loc)
    %8 = ttir.empty() : tensor<32x256xbf16> loc(#loc)
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16> loc(#loc)
    %10 = ttir.empty() : tensor<784x256xbf16> loc(#loc2)
    %11 = "ttir.permute"(%3, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16> loc(#loc2)
    %12 = ttir.empty() : tensor<32x784xbf16> loc(#loc1)
    %13 = "ttir.all_gather"(%4, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16> loc(#loc1)
    %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
    %15 = ttir.empty() : tensor<1x256xbf16> loc(#loc3)
    %16 = "ttir.reshape"(%2, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16> loc(#loc3)
    %17 = ttir.empty() : tensor<32x256xbf16> loc(#loc3)
    %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
    %19 = ttir.empty() : tensor<32x256xbf16> loc(#loc3)
    %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
    %21 = ttir.empty() : tensor<32x256xbf16> loc(#loc4)
    %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16> loc(#loc4)
    %23 = ttir.empty() : tensor<256x512xbf16> loc(#loc2)
    %24 = "ttir.permute"(%1, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16> loc(#loc2)
    %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %26 = ttir.empty() : tensor<32x512xbf16> loc(#loc3)
    %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16> loc(#loc3)
    %28 = ttir.empty() : tensor<32x2x256xbf16> loc(#loc3)
    %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16> loc(#loc3)
    %30 = ttir.empty() : tensor<32x2x256xbf16> loc(#loc3)
    %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16> loc(#loc3)
    %32 = ttir.empty() : tensor<32x1x256xbf16> loc(#loc3)
    %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16> loc(#loc3)
    %34 = ttir.empty() : tensor<32x256xbf16> loc(#loc3)
    %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
    %36 = ttir.empty() : tensor<1x256xbf16> loc(#loc3)
    %37 = "ttir.reshape"(%0, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16> loc(#loc3)
    %38 = ttir.empty() : tensor<32x256xbf16> loc(#loc3)
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
    %40 = ttir.empty() : tensor<32x256xbf16> loc(#loc3)
    %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16> loc(#loc3)
    %42 = ttir.empty() : tensor<32x256xbf16> loc(#loc4)
    %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16> loc(#loc4)
    %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16> loc(#loc)
    return %44 : tensor<32x512xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("aten__permute")
#loc3 = loc("aten__addmm")
#loc4 = loc("aten__relu")
------------------ END OF MLIR MODULE ------------------
2025-10-20 15:40:22.067 (  42.744s) [        C1B56000]      module_builder.cc:769   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-10-20 15:40:22.067 (  42.744s) [        C1B56000]      module_builder.cc:783   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-10-20 15:40:22.067 (  42.744s) [        C1B56000]      module_builder.cc:795   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-10-20 15:40:22.067 (  42.745s) [        C1B56000]     client_instance.cc:373      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 2]
2025-10-20 15:40:22.067 (  42.745s) [        C1B56000]     client_instance.cc:471      1| ClientInstance::getOrCreateOptimizerSubmesh - creating optimizer submesh
// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
    %4 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %6 = ttir.empty() : tensor<1x1xbf16>
    %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %8 = ttir.empty() : tensor<32x256xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %10 = ttir.empty() : tensor<784x256xbf16>
    %11 = "ttir.permute"(%3, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
    %12 = ttir.empty() : tensor<32x784xbf16>
    %13 = "ttir.all_gather"(%4, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
    %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
    %15 = ttir.empty() : tensor<1x256xbf16>
    %16 = "ttir.reshape"(%2, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %17 = ttir.empty() : tensor<32x256xbf16>
    %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %19 = ttir.empty() : tensor<32x256xbf16>
    %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %21 = ttir.empty() : tensor<32x256xbf16>
    %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %23 = ttir.empty() : tensor<256x512xbf16>
    %24 = "ttir.permute"(%1, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
    %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
    %26 = ttir.empty() : tensor<32x512xbf16>
    %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
    %28 = ttir.empty() : tensor<32x2x256xbf16>
    %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %30 = ttir.empty() : tensor<32x2x256xbf16>
    %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %32 = ttir.empty() : tensor<32x1x256xbf16>
    %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
    %34 = ttir.empty() : tensor<32x256xbf16>
    %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %36 = ttir.empty() : tensor<1x256xbf16>
    %37 = "ttir.reshape"(%0, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %38 = ttir.empty() : tensor<32x256xbf16>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %40 = ttir.empty() : tensor<32x256xbf16>
    %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %42 = ttir.empty() : tensor<32x256xbf16>
    %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
    return %44 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
    %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
    %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
    %6 = ttir.empty() : tensor<1x1xbf16>
    %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %8 = ttir.empty() : tensor<32x256xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %10 = ttir.empty() : tensor<784x256xbf16>
    %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
    %12 = ttir.empty() : tensor<32x784xbf16>
    %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
    %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
    %15 = ttir.empty() : tensor<1x256xbf16>
    %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %17 = ttir.empty() : tensor<32x256xbf16>
    %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %19 = ttir.empty() : tensor<32x256xbf16>
    %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %21 = ttir.empty() : tensor<32x256xbf16>
    %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %23 = ttir.empty() : tensor<256x512xbf16>
    %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
    %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
    %26 = ttir.empty() : tensor<32x512xbf16>
    %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
    %28 = ttir.empty() : tensor<32x2x256xbf16>
    %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %30 = ttir.empty() : tensor<32x2x256xbf16>
    %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %32 = ttir.empty() : tensor<32x1x256xbf16>
    %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
    %34 = ttir.empty() : tensor<32x256xbf16>
    %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %36 = ttir.empty() : tensor<1x256xbf16>
    %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %38 = ttir.empty() : tensor<32x256xbf16>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %40 = ttir.empty() : tensor<32x256xbf16>
    %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %42 = ttir.empty() : tensor<32x256xbf16>
    %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
    return %44 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
    %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
    %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
    %6 = ttir.empty() : tensor<1x1xbf16>
    %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %8 = ttir.empty() : tensor<32x256xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %10 = ttir.empty() : tensor<784x256xbf16>
    %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
    %12 = ttir.empty() : tensor<32x784xbf16>
    %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
    %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
    %15 = ttir.empty() : tensor<1x256xbf16>
    %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %17 = ttir.empty() : tensor<32x256xbf16>
    %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %19 = ttir.empty() : tensor<32x256xbf16>
    %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %21 = ttir.empty() : tensor<32x256xbf16>
    %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %23 = ttir.empty() : tensor<256x512xbf16>
    %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
    %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
    %26 = ttir.empty() : tensor<32x512xbf16>
    %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
    %28 = ttir.empty() : tensor<32x2x256xbf16>
    %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %30 = ttir.empty() : tensor<32x2x256xbf16>
    %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %32 = ttir.empty() : tensor<32x1x256xbf16>
    %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
    %34 = ttir.empty() : tensor<32x256xbf16>
    %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %36 = ttir.empty() : tensor<1x256xbf16>
    %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %38 = ttir.empty() : tensor<32x256xbf16>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %40 = ttir.empty() : tensor<32x256xbf16>
    %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %42 = ttir.empty() : tensor<32x256xbf16>
    %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
    return %44 : tensor<32x512xbf16>
  }
}


// -----// IR Dump Before TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
    %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
    %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
    %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
    %6 = ttir.empty() : tensor<1x1xbf16>
    %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %8 = ttir.empty() : tensor<32x256xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %10 = ttir.empty() : tensor<784x256xbf16>
    %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
    %12 = ttir.empty() : tensor<32x784xbf16>
    %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
    %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
    %15 = ttir.empty() : tensor<1x256xbf16>
    %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %17 = ttir.empty() : tensor<32x256xbf16>
    %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %19 = ttir.empty() : tensor<32x256xbf16>
    %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %21 = ttir.empty() : tensor<32x256xbf16>
    %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %23 = ttir.empty() : tensor<256x512xbf16>
    %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
    %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
    %26 = ttir.empty() : tensor<32x512xbf16>
    %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
    %28 = ttir.empty() : tensor<32x2x256xbf16>
    %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %30 = ttir.empty() : tensor<32x2x256xbf16>
    %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
    %32 = ttir.empty() : tensor<32x1x256xbf16>
    %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
    %34 = ttir.empty() : tensor<32x256xbf16>
    %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %36 = ttir.empty() : tensor<1x256xbf16>
    %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %38 = ttir.empty() : tensor<32x256xbf16>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %40 = ttir.empty() : tensor<32x256xbf16>
    %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %42 = ttir.empty() : tensor<32x256xbf16>
    %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
    %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
    return %44 : tensor<32x512xbf16>
  }
}


// -----// IR Dump After TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
        %15 = ttir.empty() : tensor<1x256xbf16>
        %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %17 = ttir.empty() : tensor<32x256xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %19 = ttir.empty() : tensor<32x256xbf16>
        %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %21 = ttir.empty() : tensor<32x256xbf16>
        %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %23 = ttir.empty() : tensor<256x512xbf16>
        %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = ttir.empty() : tensor<1x256xbf16>
        %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %44 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRHoistTransform (ttir-cpu-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
        %15 = ttir.empty() : tensor<1x256xbf16>
        %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %17 = ttir.empty() : tensor<32x256xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %19 = ttir.empty() : tensor<32x256xbf16>
        %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %21 = ttir.empty() : tensor<32x256xbf16>
        %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %23 = ttir.empty() : tensor<256x512xbf16>
        %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = ttir.empty() : tensor<1x256xbf16>
        %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %44 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
        %15 = ttir.empty() : tensor<1x256xbf16>
        %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %17 = ttir.empty() : tensor<32x256xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %19 = ttir.empty() : tensor<32x256xbf16>
        %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %21 = ttir.empty() : tensor<32x256xbf16>
        %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %23 = ttir.empty() : tensor<256x512xbf16>
        %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = ttir.empty() : tensor<1x256xbf16>
        %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %44 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump After TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
        %15 = ttir.empty() : tensor<1x256xbf16>
        %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %17 = ttir.empty() : tensor<32x256xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %19 = ttir.empty() : tensor<32x256xbf16>
        %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %21 = ttir.empty() : tensor<32x256xbf16>
        %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %23 = ttir.empty() : tensor<256x512xbf16>
        %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = ttir.empty() : tensor<1x256xbf16>
        %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %44 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
        %15 = ttir.empty() : tensor<1x256xbf16>
        %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %17 = ttir.empty() : tensor<32x256xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %19 = ttir.empty() : tensor<32x256xbf16>
        %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %21 = ttir.empty() : tensor<32x256xbf16>
        %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %23 = ttir.empty() : tensor<256x512xbf16>
        %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = ttir.empty() : tensor<1x256xbf16>
        %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %44 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
        %15 = ttir.empty() : tensor<1x256xbf16>
        %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %17 = ttir.empty() : tensor<32x256xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %19 = ttir.empty() : tensor<32x256xbf16>
        %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %21 = ttir.empty() : tensor<32x256xbf16>
        %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %23 = ttir.empty() : tensor<256x512xbf16>
        %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = ttir.empty() : tensor<1x256xbf16>
        %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %44 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
        %15 = ttir.empty() : tensor<1x256xbf16>
        %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %17 = ttir.empty() : tensor<32x256xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %19 = ttir.empty() : tensor<32x256xbf16>
        %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %21 = ttir.empty() : tensor<32x256xbf16>
        %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %23 = ttir.empty() : tensor<256x512xbf16>
        %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = ttir.empty() : tensor<1x256xbf16>
        %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %44 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDequantConversion (ttir-quant-dequant-conversion) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
        %15 = ttir.empty() : tensor<1x256xbf16>
        %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %17 = ttir.empty() : tensor<32x256xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %19 = ttir.empty() : tensor<32x256xbf16>
        %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %21 = ttir.empty() : tensor<32x256xbf16>
        %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %23 = ttir.empty() : tensor<256x512xbf16>
        %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = ttir.empty() : tensor<1x256xbf16>
        %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %44 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = "ttir.dot_general"(%13, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x784xbf16>, tensor<784x256xbf16>) -> tensor<32x256xbf16>
        %15 = ttir.empty() : tensor<1x256xbf16>
        %16 = "ttir.reshape"(%3, %15) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %17 = ttir.empty() : tensor<32x256xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %19 = ttir.empty() : tensor<32x256xbf16>
        %20 = "ttir.add"(%14, %18, %19) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %21 = ttir.empty() : tensor<32x256xbf16>
        %22 = "ttir.maximum"(%20, %9, %21) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %23 = ttir.empty() : tensor<256x512xbf16>
        %24 = "ttir.permute"(%2, %23) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %25 = "ttir.dot_general"(%22, %24) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<32x256xbf16>, tensor<256x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.all_reduce"(%25, %26) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.reshape"(%33, %34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = ttir.empty() : tensor<1x256xbf16>
        %37 = "ttir.reshape"(%1, %36) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.add"(%35, %39, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.maximum"(%41, %9, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %44 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = ttir.empty() : tensor<32x784xbf16>
        %15 = "ttir.permute"(%13, %14) <{permutation = array<i64: 0, 1>}> : (tensor<32x784xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %16 = ttir.empty() : tensor<784x256xbf16>
        %17 = "ttir.permute"(%11, %16) <{permutation = array<i64: 0, 1>}> : (tensor<784x256xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %18 = ttir.empty() : tensor<32x784xbf16>
        %19 = "ttir.reshape"(%15, %18) <{shape = [32 : i32, 784 : i32]}> : (tensor<32x784xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %20 = ttir.empty() : tensor<784x256xbf16>
        %21 = "ttir.reshape"(%17, %20) <{shape = [784 : i32, 256 : i32]}> : (tensor<784x256xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %22 = ttir.empty() : tensor<32x256xbf16>
        %23 = "ttir.matmul"(%19, %21, %22) <{transpose_a = false, transpose_b = false}> : (tensor<32x784xbf16>, tensor<784x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %24 = ttir.empty() : tensor<32x256xbf16>
        %25 = "ttir.reshape"(%23, %24) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %26 = ttir.empty() : tensor<1x256xbf16>
        %27 = "ttir.reshape"(%3, %26) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %28 = ttir.empty() : tensor<32x256xbf16>
        %29 = "ttir.broadcast"(%27, %28) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %30 = ttir.empty() : tensor<32x256xbf16>
        %31 = "ttir.add"(%25, %29, %30) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.maximum"(%31, %9, %32) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<256x512xbf16>
        %35 = "ttir.permute"(%2, %34) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.permute"(%33, %36) <{permutation = array<i64: 0, 1>}> : (tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<256x512xbf16>
        %39 = "ttir.permute"(%35, %38) <{permutation = array<i64: 0, 1>}> : (tensor<256x512xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.reshape"(%37, %40) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<256x512xbf16>
        %43 = "ttir.reshape"(%39, %42) <{shape = [256 : i32, 512 : i32]}> : (tensor<256x512xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %44 = ttir.empty() : tensor<32x512xbf16>
        %45 = "ttir.matmul"(%41, %43, %44) <{transpose_a = false, transpose_b = false}> : (tensor<32x256xbf16>, tensor<256x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %46 = ttir.empty() : tensor<32x512xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [32 : i32, 512 : i32]}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %48 = ttir.empty() : tensor<32x512xbf16>
        %49 = "ttir.all_reduce"(%47, %48) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %50 = ttir.empty() : tensor<32x2x256xbf16>
        %51 = "ttir.reshape"(%49, %50) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %52 = ttir.empty() : tensor<32x2x256xbf16>
        %53 = "ttir.all_to_all"(%51, %52) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %54 = ttir.empty() : tensor<32x1x256xbf16>
        %55 = "ttir.slice_static"(%53, %54) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %56 = ttir.empty() : tensor<32x256xbf16>
        %57 = "ttir.reshape"(%55, %56) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %58 = ttir.empty() : tensor<1x256xbf16>
        %59 = "ttir.reshape"(%1, %58) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %60 = ttir.empty() : tensor<32x256xbf16>
        %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %62 = ttir.empty() : tensor<32x256xbf16>
        %63 = "ttir.add"(%57, %61, %62) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %64 = ttir.empty() : tensor<32x256xbf16>
        %65 = "ttir.maximum"(%63, %9, %64) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %66 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = ttir.empty() : tensor<32x784xbf16>
        %15 = "ttir.permute"(%13, %14) <{permutation = array<i64: 0, 1>}> : (tensor<32x784xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %16 = ttir.empty() : tensor<784x256xbf16>
        %17 = "ttir.permute"(%11, %16) <{permutation = array<i64: 0, 1>}> : (tensor<784x256xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %18 = ttir.empty() : tensor<32x784xbf16>
        %19 = "ttir.reshape"(%15, %18) <{shape = [32 : i32, 784 : i32]}> : (tensor<32x784xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %20 = ttir.empty() : tensor<784x256xbf16>
        %21 = "ttir.reshape"(%17, %20) <{shape = [784 : i32, 256 : i32]}> : (tensor<784x256xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %22 = ttir.empty() : tensor<32x256xbf16>
        %23 = "ttir.matmul"(%19, %21, %22) <{transpose_a = false, transpose_b = false}> : (tensor<32x784xbf16>, tensor<784x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %24 = ttir.empty() : tensor<32x256xbf16>
        %25 = "ttir.reshape"(%23, %24) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %26 = ttir.empty() : tensor<1x256xbf16>
        %27 = "ttir.reshape"(%3, %26) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %28 = ttir.empty() : tensor<32x256xbf16>
        %29 = "ttir.broadcast"(%27, %28) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %30 = ttir.empty() : tensor<32x256xbf16>
        %31 = "ttir.add"(%25, %29, %30) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.maximum"(%31, %9, %32) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<256x512xbf16>
        %35 = "ttir.permute"(%2, %34) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.permute"(%33, %36) <{permutation = array<i64: 0, 1>}> : (tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<256x512xbf16>
        %39 = "ttir.permute"(%35, %38) <{permutation = array<i64: 0, 1>}> : (tensor<256x512xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.reshape"(%37, %40) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<256x512xbf16>
        %43 = "ttir.reshape"(%39, %42) <{shape = [256 : i32, 512 : i32]}> : (tensor<256x512xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %44 = ttir.empty() : tensor<32x512xbf16>
        %45 = "ttir.matmul"(%41, %43, %44) <{transpose_a = false, transpose_b = false}> : (tensor<32x256xbf16>, tensor<256x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %46 = ttir.empty() : tensor<32x512xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [32 : i32, 512 : i32]}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %48 = ttir.empty() : tensor<32x512xbf16>
        %49 = "ttir.all_reduce"(%47, %48) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %50 = ttir.empty() : tensor<32x2x256xbf16>
        %51 = "ttir.reshape"(%49, %50) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %52 = ttir.empty() : tensor<32x2x256xbf16>
        %53 = "ttir.all_to_all"(%51, %52) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %54 = ttir.empty() : tensor<32x1x256xbf16>
        %55 = "ttir.slice_static"(%53, %54) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %56 = ttir.empty() : tensor<32x256xbf16>
        %57 = "ttir.reshape"(%55, %56) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %58 = ttir.empty() : tensor<1x256xbf16>
        %59 = "ttir.reshape"(%1, %58) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %60 = ttir.empty() : tensor<32x256xbf16>
        %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %62 = ttir.empty() : tensor<32x256xbf16>
        %63 = "ttir.add"(%57, %61, %62) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %64 = ttir.empty() : tensor<32x256xbf16>
        %65 = "ttir.maximum"(%63, %9, %64) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %66 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = ttir.empty() : tensor<32x256xbf16>
        %15 = "ttir.matmul"(%13, %11, %14) <{transpose_a = false, transpose_b = false}> : (tensor<32x784xbf16>, tensor<784x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %16 = ttir.empty() : tensor<1x256xbf16>
        %17 = "ttir.reshape"(%3, %16) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.add"(%15, %19, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x256xbf16>
        %23 = "ttir.maximum"(%21, %9, %22) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %24 = ttir.empty() : tensor<256x512xbf16>
        %25 = "ttir.permute"(%2, %24) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.matmul"(%23, %25, %26) <{transpose_a = false, transpose_b = false}> : (tensor<32x256xbf16>, tensor<256x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x512xbf16>
        %29 = "ttir.all_reduce"(%27, %28) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.reshape"(%29, %30) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x2x256xbf16>
        %33 = "ttir.all_to_all"(%31, %32) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %34 = ttir.empty() : tensor<32x1x256xbf16>
        %35 = "ttir.slice_static"(%33, %34) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.reshape"(%35, %36) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<1x256xbf16>
        %39 = "ttir.reshape"(%1, %38) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.broadcast"(%39, %40) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.add"(%37, %41, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = ttir.empty() : tensor<32x256xbf16>
        %45 = "ttir.maximum"(%43, %9, %44) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %46 = "ttir.mesh_shard"(%45) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %46 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<784x256xbf16>
        %11 = "ttir.permute"(%4, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x784xbf16>, tensor<784x256xbf16>) -> tensor<784x256xbf16>
        %12 = ttir.empty() : tensor<32x784xbf16>
        %13 = "ttir.all_gather"(%5, %12) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %14 = ttir.empty() : tensor<32x256xbf16>
        %15 = "ttir.matmul"(%13, %11, %14) <{transpose_a = false, transpose_b = false}> : (tensor<32x784xbf16>, tensor<784x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %16 = ttir.empty() : tensor<1x256xbf16>
        %17 = "ttir.reshape"(%3, %16) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.add"(%15, %19, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x256xbf16>
        %23 = "ttir.maximum"(%21, %9, %22) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %24 = ttir.empty() : tensor<256x512xbf16>
        %25 = "ttir.permute"(%2, %24) <{permutation = array<i64: 1, 0>}> : (tensor<512x256xbf16>, tensor<256x512xbf16>) -> tensor<256x512xbf16>
        %26 = ttir.empty() : tensor<32x512xbf16>
        %27 = "ttir.matmul"(%23, %25, %26) <{transpose_a = false, transpose_b = false}> : (tensor<32x256xbf16>, tensor<256x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %28 = ttir.empty() : tensor<32x512xbf16>
        %29 = "ttir.all_reduce"(%27, %28) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %30 = ttir.empty() : tensor<32x2x256xbf16>
        %31 = "ttir.reshape"(%29, %30) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %32 = ttir.empty() : tensor<32x2x256xbf16>
        %33 = "ttir.all_to_all"(%31, %32) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %34 = ttir.empty() : tensor<32x1x256xbf16>
        %35 = "ttir.slice_static"(%33, %34) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.reshape"(%35, %36) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<1x256xbf16>
        %39 = "ttir.reshape"(%1, %38) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.broadcast"(%39, %40) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = ttir.empty() : tensor<32x256xbf16>
        %43 = "ttir.add"(%37, %41, %42) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %44 = ttir.empty() : tensor<32x256xbf16>
        %45 = "ttir.maximum"(%43, %9, %44) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %46 = "ttir.mesh_shard"(%45) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %46 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<32x784xbf16>
        %11 = "ttir.all_gather"(%5, %10) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %12 = ttir.empty() : tensor<32x256xbf16>
        %13 = "ttir.matmul"(%11, %4, %12) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %14 = ttir.empty() : tensor<1x256xbf16>
        %15 = "ttir.reshape"(%3, %14) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.add"(%13, %17, %18) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.maximum"(%19, %9, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x512xbf16>
        %23 = "ttir.matmul"(%21, %2, %22) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %24 = ttir.empty() : tensor<32x512xbf16>
        %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x2x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.reshape"(%1, %34) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.add"(%33, %37, %38) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.maximum"(%39, %9, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %42 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<32x784xbf16>
        %11 = "ttir.all_gather"(%5, %10) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %12 = ttir.empty() : tensor<32x256xbf16>
        %13 = "ttir.matmul"(%11, %4, %12) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %14 = ttir.empty() : tensor<1x256xbf16>
        %15 = "ttir.reshape"(%3, %14) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.add"(%13, %17, %18) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.maximum"(%19, %9, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x512xbf16>
        %23 = "ttir.matmul"(%21, %2, %22) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %24 = ttir.empty() : tensor<32x512xbf16>
        %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x2x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.reshape"(%1, %34) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.add"(%33, %37, %38) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.maximum"(%39, %9, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %42 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<32x784xbf16>
        %11 = "ttir.all_gather"(%5, %10) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %12 = ttir.empty() : tensor<32x256xbf16>
        %13 = "ttir.matmul"(%11, %4, %12) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %14 = ttir.empty() : tensor<1x256xbf16>
        %15 = "ttir.reshape"(%3, %14) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.add"(%13, %17, %18) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.maximum"(%19, %9, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x512xbf16>
        %23 = "ttir.matmul"(%21, %2, %22) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %24 = ttir.empty() : tensor<32x512xbf16>
        %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x2x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.reshape"(%1, %34) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.add"(%33, %37, %38) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.maximum"(%39, %9, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %42 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<32x784xbf16>
        %11 = "ttir.all_gather"(%5, %10) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %12 = ttir.empty() : tensor<32x256xbf16>
        %13 = "ttir.matmul"(%11, %4, %12) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %14 = ttir.empty() : tensor<1x256xbf16>
        %15 = "ttir.reshape"(%3, %14) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.add"(%13, %17, %18) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.maximum"(%19, %9, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x512xbf16>
        %23 = "ttir.matmul"(%21, %2, %22) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %24 = ttir.empty() : tensor<32x512xbf16>
        %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x2x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.reshape"(%1, %34) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.add"(%33, %37, %38) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.maximum"(%39, %9, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %42 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRExplicateTMs (ttir-explicate-tms) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<32x784xbf16>
        %11 = "ttir.all_gather"(%5, %10) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %12 = ttir.empty() : tensor<32x256xbf16>
        %13 = "ttir.matmul"(%11, %4, %12) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %14 = ttir.empty() : tensor<1x256xbf16>
        %15 = "ttir.reshape"(%3, %14) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.add"(%13, %17, %18) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.maximum"(%19, %9, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x512xbf16>
        %23 = "ttir.matmul"(%21, %2, %22) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %24 = ttir.empty() : tensor<32x512xbf16>
        %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x2x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.reshape"(%1, %34) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.add"(%33, %37, %38) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.maximum"(%39, %9, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %42 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<32x784xbf16>
        %11 = "ttir.all_gather"(%5, %10) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %12 = ttir.empty() : tensor<32x256xbf16>
        %13 = "ttir.matmul"(%11, %4, %12) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %14 = ttir.empty() : tensor<1x256xbf16>
        %15 = "ttir.reshape"(%3, %14) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.add"(%13, %17, %18) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.maximum"(%19, %9, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x512xbf16>
        %23 = "ttir.matmul"(%21, %2, %22) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %24 = ttir.empty() : tensor<32x512xbf16>
        %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x2x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.reshape"(%1, %34) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.add"(%33, %37, %38) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.maximum"(%39, %9, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %42 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<32x784xbf16>
        %11 = "ttir.all_gather"(%5, %10) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %12 = ttir.empty() : tensor<32x256xbf16>
        %13 = "ttir.matmul"(%11, %4, %12) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %14 = ttir.empty() : tensor<1x256xbf16>
        %15 = "ttir.reshape"(%3, %14) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.add"(%13, %17, %18) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.maximum"(%19, %9, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x512xbf16>
        %23 = "ttir.matmul"(%21, %2, %22) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %24 = ttir.empty() : tensor<32x512xbf16>
        %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x2x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.reshape"(%1, %34) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.add"(%33, %37, %38) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.maximum"(%39, %9, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %42 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 32, 256>}> : (tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %10 = ttir.empty() : tensor<32x784xbf16>
        %11 = "ttir.all_gather"(%5, %10) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %12 = ttir.empty() : tensor<32x256xbf16>
        %13 = "ttir.matmul"(%11, %4, %12) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %14 = ttir.empty() : tensor<1x256xbf16>
        %15 = "ttir.reshape"(%3, %14) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x256xbf16>
        %19 = "ttir.add"(%13, %17, %18) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %20 = ttir.empty() : tensor<32x256xbf16>
        %21 = "ttir.maximum"(%19, %9, %20) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %22 = ttir.empty() : tensor<32x512xbf16>
        %23 = "ttir.matmul"(%21, %2, %22) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %24 = ttir.empty() : tensor<32x512xbf16>
        %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %26 = ttir.empty() : tensor<32x2x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %28 = ttir.empty() : tensor<32x2x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %30 = ttir.empty() : tensor<32x1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.reshape"(%1, %34) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<32x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 32, 1>}> : (tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %38 = ttir.empty() : tensor<32x256xbf16>
        %39 = "ttir.add"(%33, %37, %38) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %40 = ttir.empty() : tensor<32x256xbf16>
        %41 = "ttir.maximum"(%39, %9, %40) : (tensor<32x256xbf16>, tensor<32x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %42 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x784xbf16>
        %9 = "ttir.all_gather"(%5, %8) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %10 = ttir.empty() : tensor<32x256xbf16>
        %11 = "ttir.matmul"(%9, %4, %10) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %12 = ttir.empty() : tensor<1x256xbf16>
        %13 = "ttir.reshape"(%3, %12) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %14 = ttir.empty() : tensor<32x256xbf16>
        %15 = "ttir.add"(%11, %13, %14) : (tensor<32x256xbf16>, tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.maximum"(%15, %7, %16) : (tensor<32x256xbf16>, tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x512xbf16>
        %19 = "ttir.matmul"(%17, %2, %18) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %20 = ttir.empty() : tensor<32x512xbf16>
        %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %22 = ttir.empty() : tensor<32x2x256xbf16>
        %23 = "ttir.reshape"(%21, %22) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %24 = ttir.empty() : tensor<32x2x256xbf16>
        %25 = "ttir.all_to_all"(%23, %24) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %26 = ttir.empty() : tensor<32x1x256xbf16>
        %27 = "ttir.slice_static"(%25, %26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %28 = ttir.empty() : tensor<32x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %30 = ttir.empty() : tensor<1x256xbf16>
        %31 = "ttir.reshape"(%1, %30) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.add"(%29, %31, %32) : (tensor<32x256xbf16>, tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.maximum"(%33, %7, %34) : (tensor<32x256xbf16>, tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = "ttir.mesh_shard"(%35) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %36 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDataTypeConversionPass (ttir-quant-data-type-conversion) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x784xbf16>
        %9 = "ttir.all_gather"(%5, %8) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %10 = ttir.empty() : tensor<32x256xbf16>
        %11 = "ttir.matmul"(%9, %4, %10) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %12 = ttir.empty() : tensor<1x256xbf16>
        %13 = "ttir.reshape"(%3, %12) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %14 = ttir.empty() : tensor<32x256xbf16>
        %15 = "ttir.add"(%11, %13, %14) : (tensor<32x256xbf16>, tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.maximum"(%15, %7, %16) : (tensor<32x256xbf16>, tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x512xbf16>
        %19 = "ttir.matmul"(%17, %2, %18) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %20 = ttir.empty() : tensor<32x512xbf16>
        %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %22 = ttir.empty() : tensor<32x2x256xbf16>
        %23 = "ttir.reshape"(%21, %22) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %24 = ttir.empty() : tensor<32x2x256xbf16>
        %25 = "ttir.all_to_all"(%23, %24) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %26 = ttir.empty() : tensor<32x1x256xbf16>
        %27 = "ttir.slice_static"(%25, %26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %28 = ttir.empty() : tensor<32x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %30 = ttir.empty() : tensor<1x256xbf16>
        %31 = "ttir.reshape"(%1, %30) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.add"(%29, %31, %32) : (tensor<32x256xbf16>, tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.maximum"(%33, %7, %34) : (tensor<32x256xbf16>, tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = "ttir.mesh_shard"(%35) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %36 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16>) -> tensor<512x256xbf16>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16>) -> tensor<256xbf16>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16>) -> tensor<256x784xbf16>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16>) -> tensor<32x392xbf16>
        %6 = ttir.empty() : tensor<1x1xbf16>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %8 = ttir.empty() : tensor<32x784xbf16>
        %9 = "ttir.all_gather"(%5, %8) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16>, tensor<32x784xbf16>) -> tensor<32x784xbf16>
        %10 = ttir.empty() : tensor<32x256xbf16>
        %11 = "ttir.matmul"(%9, %4, %10) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16>, tensor<256x784xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %12 = ttir.empty() : tensor<1x256xbf16>
        %13 = "ttir.reshape"(%3, %12) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %14 = ttir.empty() : tensor<32x256xbf16>
        %15 = "ttir.add"(%11, %13, %14) : (tensor<32x256xbf16>, tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %16 = ttir.empty() : tensor<32x256xbf16>
        %17 = "ttir.maximum"(%15, %7, %16) : (tensor<32x256xbf16>, tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %18 = ttir.empty() : tensor<32x512xbf16>
        %19 = "ttir.matmul"(%17, %2, %18) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16>, tensor<512x256xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %20 = ttir.empty() : tensor<32x512xbf16>
        %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16>, tensor<32x512xbf16>) -> tensor<32x512xbf16>
        %22 = ttir.empty() : tensor<32x2x256xbf16>
        %23 = "ttir.reshape"(%21, %22) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %24 = ttir.empty() : tensor<32x2x256xbf16>
        %25 = "ttir.all_to_all"(%23, %24) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16>, tensor<32x2x256xbf16>) -> tensor<32x2x256xbf16>
        %26 = ttir.empty() : tensor<32x1x256xbf16>
        %27 = "ttir.slice_static"(%25, %26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16>, tensor<32x1x256xbf16>) -> tensor<32x1x256xbf16>
        %28 = ttir.empty() : tensor<32x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %30 = ttir.empty() : tensor<1x256xbf16>
        %31 = "ttir.reshape"(%1, %30) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %32 = ttir.empty() : tensor<32x256xbf16>
        %33 = "ttir.add"(%29, %31, %32) : (tensor<32x256xbf16>, tensor<1x256xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %34 = ttir.empty() : tensor<32x256xbf16>
        %35 = "ttir.maximum"(%33, %7, %34) : (tensor<32x256xbf16>, tensor<1x1xbf16>, tensor<32x256xbf16>) -> tensor<32x256xbf16>
        %36 = "ttir.mesh_shard"(%35) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16>) -> tensor<32x512xbf16>
        return %36 : tensor<32x512xbf16>
      }
    }
  }
}


// -----// IR Dump After TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout6>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout>) -> tensor<256xbf16, #ttnn_layout7>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout1>) -> tensor<512x256xbf16, #ttnn_layout8>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<256xbf16, #ttnn_layout9>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout3>) -> tensor<256x784xbf16, #ttnn_layout10>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout4>) -> tensor<32x392xbf16, #ttnn_layout11>
        %6 = ttir.empty() : tensor<1x1xbf16, #ttnn_layout12>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout6>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<1x1xbf16, #ttnn_layout12>
        %8 = ttir.empty() : tensor<32x784xbf16, #ttnn_layout4>
        %9 = "ttir.all_gather"(%5, %8) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16, #ttnn_layout11>, tensor<32x784xbf16, #ttnn_layout4>) -> tensor<32x784xbf16, #ttnn_layout4>
        %10 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %11 = "ttir.matmul"(%9, %4, %10) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout4>, tensor<256x784xbf16, #ttnn_layout10>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %12 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout13>
        %13 = "ttir.reshape"(%3, %12) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<1x256xbf16, #ttnn_layout13>
        %14 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %15 = "ttir.add"(%11, %13, %14) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %16 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %17 = "ttir.maximum"(%15, %7, %16) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %18 = ttir.empty() : tensor<32x512xbf16, #ttnn_layout14>
        %19 = "ttir.matmul"(%17, %2, %18) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<512x256xbf16, #ttnn_layout8>, tensor<32x512xbf16, #ttnn_layout14>) -> tensor<32x512xbf16, #ttnn_layout14>
        %20 = ttir.empty() : tensor<32x512xbf16, #ttnn_layout14>
        %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16, #ttnn_layout14>, tensor<32x512xbf16, #ttnn_layout14>) -> tensor<32x512xbf16, #ttnn_layout14>
        %22 = ttir.empty() : tensor<32x2x256xbf16, #ttnn_layout15>
        %23 = "ttir.reshape"(%21, %22) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16, #ttnn_layout14>, tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %24 = ttir.empty() : tensor<32x2x256xbf16, #ttnn_layout15>
        %25 = "ttir.all_to_all"(%23, %24) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16, #ttnn_layout15>, tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %26 = ttir.empty() : tensor<32x1x256xbf16, #ttnn_layout15>
        %27 = "ttir.slice_static"(%25, %26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %28 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %30 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout13>
        %31 = ttir.empty() : tensor<256xbf16, #ttnn_layout9>
        %32 = ttir.to_layout %1, %31 : tensor<256xbf16, #ttnn_layout7> into tensor<256xbf16, #ttnn_layout9> -> tensor<256xbf16, #ttnn_layout9>
        %33 = "ttir.reshape"(%32, %30) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<1x256xbf16, #ttnn_layout13>
        %34 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %35 = "ttir.add"(%29, %33, %34) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %36 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %37 = "ttir.maximum"(%35, %7, %36) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %38 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout16>
        %39 = ttir.to_layout %37, %38 : tensor<32x256xbf16, #ttnn_layout13> into tensor<32x256xbf16, #ttnn_layout16> -> tensor<32x256xbf16, #ttnn_layout16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout16>) -> tensor<32x512xbf16, #ttnn_layout5>
        return %40 : tensor<32x512xbf16, #ttnn_layout5>
      }
    }
  }
}


// -----// IR Dump Before ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout6>
        %1 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout>) -> tensor<256xbf16, #ttnn_layout7>
        %2 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout1>) -> tensor<512x256xbf16, #ttnn_layout8>
        %3 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<256xbf16, #ttnn_layout9>
        %4 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout3>) -> tensor<256x784xbf16, #ttnn_layout10>
        %5 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout4>) -> tensor<32x392xbf16, #ttnn_layout11>
        %6 = ttir.empty() : tensor<1x1xbf16, #ttnn_layout12>
        %7 = "ttir.reshape"(%0, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout6>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<1x1xbf16, #ttnn_layout12>
        %8 = ttir.empty() : tensor<32x784xbf16, #ttnn_layout4>
        %9 = "ttir.all_gather"(%5, %8) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32}> : (tensor<32x392xbf16, #ttnn_layout11>, tensor<32x784xbf16, #ttnn_layout4>) -> tensor<32x784xbf16, #ttnn_layout4>
        %10 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %11 = "ttir.matmul"(%9, %4, %10) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout4>, tensor<256x784xbf16, #ttnn_layout10>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %12 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout13>
        %13 = "ttir.reshape"(%3, %12) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<1x256xbf16, #ttnn_layout13>
        %14 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %15 = "ttir.add"(%11, %13, %14) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %16 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %17 = "ttir.maximum"(%15, %7, %16) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %18 = ttir.empty() : tensor<32x512xbf16, #ttnn_layout14>
        %19 = "ttir.matmul"(%17, %2, %18) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<512x256xbf16, #ttnn_layout8>, tensor<32x512xbf16, #ttnn_layout14>) -> tensor<32x512xbf16, #ttnn_layout14>
        %20 = ttir.empty() : tensor<32x512xbf16, #ttnn_layout14>
        %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16, #ttnn_layout14>, tensor<32x512xbf16, #ttnn_layout14>) -> tensor<32x512xbf16, #ttnn_layout14>
        %22 = ttir.empty() : tensor<32x2x256xbf16, #ttnn_layout15>
        %23 = "ttir.reshape"(%21, %22) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16, #ttnn_layout14>, tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %24 = ttir.empty() : tensor<32x2x256xbf16, #ttnn_layout15>
        %25 = "ttir.all_to_all"(%23, %24) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, split_count = 2 : si32, split_dim = 1 : si32}> : (tensor<32x2x256xbf16, #ttnn_layout15>, tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %26 = ttir.empty() : tensor<32x1x256xbf16, #ttnn_layout15>
        %27 = "ttir.slice_static"(%25, %26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %28 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %29 = "ttir.reshape"(%27, %28) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %30 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout13>
        %31 = ttir.empty() : tensor<256xbf16, #ttnn_layout9>
        %32 = ttir.to_layout %1, %31 : tensor<256xbf16, #ttnn_layout7> into tensor<256xbf16, #ttnn_layout9> -> tensor<256xbf16, #ttnn_layout9>
        %33 = "ttir.reshape"(%32, %30) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<1x256xbf16, #ttnn_layout13>
        %34 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %35 = "ttir.add"(%29, %33, %34) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %36 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout13>
        %37 = "ttir.maximum"(%35, %7, %36) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>, tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %38 = ttir.empty() : tensor<32x256xbf16, #ttnn_layout16>
        %39 = ttir.to_layout %37, %38 : tensor<32x256xbf16, #ttnn_layout13> into tensor<32x256xbf16, #ttnn_layout16> -> tensor<32x256xbf16, #ttnn_layout16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout16>) -> tensor<32x512xbf16, #ttnn_layout5>
        return %40 : tensor<32x512xbf16, #ttnn_layout5>
      }
    }
  }
}


// -----// IR Dump After ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout6>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout7>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout8>
        %4 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout9>
        %5 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout10>
        %6 = "ttnn.mesh_shard"(%arg4, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout11>
        %7 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xbf16, #ttnn_layout12>
        %8 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout6>) -> tensor<1x1xbf16, #ttnn_layout12>
        %9 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x784>}> : (!ttnn.device) -> tensor<32x784xbf16, #ttnn_layout4>
        %10 = "ttnn.all_gather"(%6, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<32x392xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<32x784xbf16, #ttnn_layout4>
        %11 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %12 = "ttnn.matmul"(%10, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout4>, tensor<256x784xbf16, #ttnn_layout10>) -> tensor<32x256xbf16, #ttnn_layout13>
        %13 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout13>
        %14 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout13>
        %15 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %16 = "ttnn.add"(%12, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %17 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %18 = "ttnn.maximum"(%16, %8) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout13>
        %19 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x512>}> : (!ttnn.device) -> tensor<32x512xbf16, #ttnn_layout14>
        %20 = "ttnn.matmul"(%18, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<512x256xbf16, #ttnn_layout8>) -> tensor<32x512xbf16, #ttnn_layout14>
        %21 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x512>}> : (!ttnn.device) -> tensor<32x512xbf16, #ttnn_layout14>
        %22 = "ttnn.all_reduce"(%20, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout14>
        %23 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x2x256>}> : (!ttnn.device) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %24 = "ttnn.reshape"(%22) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16, #ttnn_layout14>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %25 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x2x256>}> : (!ttnn.device) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %26 = "ttnn.slice_static"(%24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %27 = "ttnn.slice_static"(%24) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %28 = "ttnn.point_to_point"(%26) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %29 = "ttnn.point_to_point"(%27, %28) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %30 = "ttnn.point_to_point"(%26) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %31 = "ttnn.point_to_point"(%27, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %32 = "ttnn.concat"(%29, %31) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %33 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x1x256>}> : (!ttnn.device) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %34 = "ttnn.slice_static"(%32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %35 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %36 = "ttnn.reshape"(%34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout13>
        %37 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout13>
        %38 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout7>) -> tensor<256xbf16, #ttnn_layout9>
        %39 = "ttnn.reshape"(%38) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout13>
        %40 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %41 = "ttnn.add"(%36, %39) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %42 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %43 = "ttnn.maximum"(%41, %8) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout13>
        %44 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<32x256>}> : () -> tensor<32x256xbf16, #ttnn_layout16>
        %45 = "ttnn.to_layout"(%43) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout16>
        %46 = "ttnn.mesh_shard"(%45, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout5>
        return %46 : tensor<32x512xbf16, #ttnn_layout5>
      }
    }
  }
}


// -----// IR Dump Before TTNNFusing (ttnn-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout6>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout7>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout8>
        %4 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout9>
        %5 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout10>
        %6 = "ttnn.mesh_shard"(%arg4, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout11>
        %7 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xbf16, #ttnn_layout12>
        %8 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout6>) -> tensor<1x1xbf16, #ttnn_layout12>
        %9 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x784>}> : (!ttnn.device) -> tensor<32x784xbf16, #ttnn_layout4>
        %10 = "ttnn.all_gather"(%6, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<32x392xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<32x784xbf16, #ttnn_layout4>
        %11 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %12 = "ttnn.matmul"(%10, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout4>, tensor<256x784xbf16, #ttnn_layout10>) -> tensor<32x256xbf16, #ttnn_layout13>
        %13 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout13>
        %14 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout13>
        %15 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %16 = "ttnn.add"(%12, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %17 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %18 = "ttnn.maximum"(%16, %8) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout13>
        %19 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x512>}> : (!ttnn.device) -> tensor<32x512xbf16, #ttnn_layout14>
        %20 = "ttnn.matmul"(%18, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<512x256xbf16, #ttnn_layout8>) -> tensor<32x512xbf16, #ttnn_layout14>
        %21 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x512>}> : (!ttnn.device) -> tensor<32x512xbf16, #ttnn_layout14>
        %22 = "ttnn.all_reduce"(%20, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout14>
        %23 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x2x256>}> : (!ttnn.device) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %24 = "ttnn.reshape"(%22) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16, #ttnn_layout14>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %25 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x2x256>}> : (!ttnn.device) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %26 = "ttnn.slice_static"(%24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %27 = "ttnn.slice_static"(%24) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %28 = "ttnn.point_to_point"(%26) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %29 = "ttnn.point_to_point"(%27, %28) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %30 = "ttnn.point_to_point"(%26) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %31 = "ttnn.point_to_point"(%27, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %32 = "ttnn.concat"(%29, %31) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %33 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x1x256>}> : (!ttnn.device) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %34 = "ttnn.slice_static"(%32) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %35 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %36 = "ttnn.reshape"(%34) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout13>
        %37 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout13>
        %38 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout7>) -> tensor<256xbf16, #ttnn_layout9>
        %39 = "ttnn.reshape"(%38) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout13>
        %40 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %41 = "ttnn.add"(%36, %39) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %42 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<32x256>}> : (!ttnn.device) -> tensor<32x256xbf16, #ttnn_layout13>
        %43 = "ttnn.maximum"(%41, %8) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout13>
        %44 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<32x256>}> : () -> tensor<32x256xbf16, #ttnn_layout16>
        %45 = "ttnn.to_layout"(%43) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout16>
        %46 = "ttnn.mesh_shard"(%45, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout5>
        return %46 : tensor<32x512xbf16, #ttnn_layout5>
      }
    }
  }
}


// -----// IR Dump After TTNNFusing (ttnn-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout6>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout7>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout8>
        %4 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout9>
        %5 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout10>
        %6 = "ttnn.mesh_shard"(%arg4, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout11>
        %7 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout6>) -> tensor<1x1xbf16, #ttnn_layout12>
        %8 = "ttnn.all_gather"(%6, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<32x392xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<32x784xbf16, #ttnn_layout4>
        %9 = "ttnn.matmul"(%8, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout4>, tensor<256x784xbf16, #ttnn_layout10>) -> tensor<32x256xbf16, #ttnn_layout13>
        %10 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout13>
        %11 = "ttnn.add"(%9, %10) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %12 = "ttnn.maximum"(%11, %7) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout13>
        %13 = "ttnn.matmul"(%12, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<512x256xbf16, #ttnn_layout8>) -> tensor<32x512xbf16, #ttnn_layout14>
        %14 = "ttnn.all_reduce"(%13, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout14>
        %15 = "ttnn.reshape"(%14) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16, #ttnn_layout14>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %16 = "ttnn.slice_static"(%15) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %17 = "ttnn.slice_static"(%15) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %18 = "ttnn.point_to_point"(%16) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %19 = "ttnn.point_to_point"(%17, %18) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %20 = "ttnn.point_to_point"(%16) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %21 = "ttnn.point_to_point"(%17, %20) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %22 = "ttnn.concat"(%19, %21) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %23 = "ttnn.slice_static"(%22) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %24 = "ttnn.reshape"(%23) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout13>
        %25 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout7>) -> tensor<256xbf16, #ttnn_layout9>
        %26 = "ttnn.reshape"(%25) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout13>
        %27 = "ttnn.add"(%24, %26) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %28 = "ttnn.maximum"(%27, %7) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout13>
        %29 = "ttnn.to_layout"(%28) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout16>
        %30 = "ttnn.mesh_shard"(%29, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout5>
        return %30 : tensor<32x512xbf16, #ttnn_layout5>
      }
    }
  }
}


// -----// IR Dump Before TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout6>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout7>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout8>
        %4 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout9>
        %5 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout10>
        %6 = "ttnn.mesh_shard"(%arg4, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout11>
        %7 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout6>) -> tensor<1x1xbf16, #ttnn_layout12>
        %8 = "ttnn.all_gather"(%6, %0) <{all_gather_dim = 1 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<32x392xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<32x784xbf16, #ttnn_layout4>
        %9 = "ttnn.matmul"(%8, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout4>, tensor<256x784xbf16, #ttnn_layout10>) -> tensor<32x256xbf16, #ttnn_layout13>
        %10 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout13>
        %11 = "ttnn.add"(%9, %10) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %12 = "ttnn.maximum"(%11, %7) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout13>
        %13 = "ttnn.matmul"(%12, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<512x256xbf16, #ttnn_layout8>) -> tensor<32x512xbf16, #ttnn_layout14>
        %14 = "ttnn.all_reduce"(%13, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<32x512xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout14>
        %15 = "ttnn.reshape"(%14) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<32x512xbf16, #ttnn_layout14>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %16 = "ttnn.slice_static"(%15) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %17 = "ttnn.slice_static"(%15) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %18 = "ttnn.point_to_point"(%16) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %19 = "ttnn.point_to_point"(%17, %18) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %20 = "ttnn.point_to_point"(%16) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %21 = "ttnn.point_to_point"(%17, %20) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %22 = "ttnn.concat"(%19, %21) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout15>, tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x2x256xbf16, #ttnn_layout15>
        %23 = "ttnn.slice_static"(%22) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout15>) -> tensor<32x1x256xbf16, #ttnn_layout15>
        %24 = "ttnn.reshape"(%23) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout13>
        %25 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout7>) -> tensor<256xbf16, #ttnn_layout9>
        %26 = "ttnn.reshape"(%25) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout13>
        %27 = "ttnn.add"(%24, %26) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout13>
        %28 = "ttnn.maximum"(%27, %7) : (tensor<32x256xbf16, #ttnn_layout13>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout13>
        %29 = "ttnn.to_layout"(%28) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout13>) -> tensor<32x256xbf16, #ttnn_layout16>
        %30 = "ttnn.mesh_shard"(%29, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout5>
        return %30 : tensor<32x512xbf16, #ttnn_layout5>
      }
    }
  }
}


// -----// IR Dump After TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout6>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout7>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout8>
        %4 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout9>
        %5 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout10>
        %6 = "ttnn.mesh_shard"(%arg4, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout11>
        %7 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout6>) -> tensor<1x1xbf16, #ttnn_layout12>
        %8 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout11>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %9 = "ttnn.all_gather"(%8, %0) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %10 = "ttnn.reshape"(%9) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout4>
        %11 = "ttnn.matmul"(%10, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout4>, tensor<256x784xbf16, #ttnn_layout10>) -> tensor<32x256xbf16, #ttnn_layout15>
        %12 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout15>
        %13 = "ttnn.add"(%11, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.maximum"(%13, %7) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout15>
        %15 = "ttnn.matmul"(%14, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout8>) -> tensor<32x512xbf16, #ttnn_layout16>
        %16 = "ttnn.reshape"(%15) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %17 = "ttnn.reduce_scatter"(%16, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %18 = "ttnn.all_gather"(%17, %0) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %19 = "ttnn.reshape"(%18) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%19) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.slice_static"(%19) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%21, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.point_to_point"(%21, %24) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %26 = "ttnn.concat"(%23, %25) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %27 = "ttnn.slice_static"(%26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %28 = "ttnn.reshape"(%27) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %29 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout7>) -> tensor<256xbf16, #ttnn_layout9>
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout15>
        %31 = "ttnn.add"(%28, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.maximum"(%31, %7) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout15>
        %33 = "ttnn.to_layout"(%32) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout19>
        %34 = "ttnn.mesh_shard"(%33, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout5>
        return %34 : tensor<32x512xbf16, #ttnn_layout5>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout6>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout7>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout8>
        %4 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout9>
        %5 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout10>
        %6 = "ttnn.mesh_shard"(%arg4, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout11>
        %7 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout6>) -> tensor<1x1xbf16, #ttnn_layout12>
        %8 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout11>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %9 = "ttnn.all_gather"(%8, %0) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %10 = "ttnn.reshape"(%9) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout4>
        %11 = "ttnn.matmul"(%10, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout4>, tensor<256x784xbf16, #ttnn_layout10>) -> tensor<32x256xbf16, #ttnn_layout15>
        %12 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout15>
        %13 = "ttnn.add"(%11, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.maximum"(%13, %7) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout15>
        %15 = "ttnn.matmul"(%14, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout8>) -> tensor<32x512xbf16, #ttnn_layout16>
        %16 = "ttnn.reshape"(%15) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %17 = "ttnn.reduce_scatter"(%16, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %18 = "ttnn.all_gather"(%17, %0) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %19 = "ttnn.reshape"(%18) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%19) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.slice_static"(%19) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%21, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.point_to_point"(%21, %24) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %26 = "ttnn.concat"(%23, %25) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %27 = "ttnn.slice_static"(%26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %28 = "ttnn.reshape"(%27) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %29 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout7>) -> tensor<256xbf16, #ttnn_layout9>
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout15>
        %31 = "ttnn.add"(%28, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.maximum"(%31, %7) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout15>
        %33 = "ttnn.to_layout"(%32) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout19>
        %34 = "ttnn.mesh_shard"(%33, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout5>
        return %34 : tensor<32x512xbf16, #ttnn_layout5>
      }
    }
  }
}


// -----// IR Dump Before ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout6 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout6>
        %2 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout7>
        %3 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout8>
        %4 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout9>
        %5 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout10>
        %6 = "ttnn.mesh_shard"(%arg4, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout11>
        %7 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout6>) -> tensor<1x1xbf16, #ttnn_layout12>
        %8 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout11>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %9 = "ttnn.all_gather"(%8, %0) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %10 = "ttnn.reshape"(%9) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout4>
        %11 = "ttnn.matmul"(%10, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout4>, tensor<256x784xbf16, #ttnn_layout10>) -> tensor<32x256xbf16, #ttnn_layout15>
        %12 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout15>
        %13 = "ttnn.add"(%11, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.maximum"(%13, %7) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout15>
        %15 = "ttnn.matmul"(%14, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout8>) -> tensor<32x512xbf16, #ttnn_layout16>
        %16 = "ttnn.reshape"(%15) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %17 = "ttnn.reduce_scatter"(%16, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %18 = "ttnn.all_gather"(%17, %0) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %19 = "ttnn.reshape"(%18) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%19) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.slice_static"(%19) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%21, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.point_to_point"(%21, %24) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %26 = "ttnn.concat"(%23, %25) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %27 = "ttnn.slice_static"(%26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %28 = "ttnn.reshape"(%27) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %29 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout7>) -> tensor<256xbf16, #ttnn_layout9>
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout9>) -> tensor<1x256xbf16, #ttnn_layout15>
        %31 = "ttnn.add"(%28, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.maximum"(%31, %7) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout12>) -> tensor<32x256xbf16, #ttnn_layout15>
        %33 = "ttnn.to_layout"(%32) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout19>
        %34 = "ttnn.mesh_shard"(%33, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout5>
        return %34 : tensor<32x512xbf16, #ttnn_layout5>
      }
    }
  }
}


// -----// IR Dump After ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout8>
        %3 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout9>
        %4 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout10>
        %5 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout5>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout11>
        %6 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout12>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout12>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %8 = "ttnn.all_gather"(%7, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %9 = "ttnn.reshape"(%8) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout6>
        %10 = "ttnn.matmul"(%9, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout6>, tensor<256x784xbf16, #ttnn_layout11>) -> tensor<32x256xbf16, #ttnn_layout15>
        %11 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %13 = "ttnn.maximum"(%12, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.matmul"(%13, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout9>) -> tensor<32x512xbf16, #ttnn_layout16>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %16 = "ttnn.reduce_scatter"(%15, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %17 = "ttnn.all_gather"(%16, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %18 = "ttnn.reshape"(%17) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %19 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.concat"(%22, %24) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %26 = "ttnn.slice_static"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %27 = "ttnn.reshape"(%26) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %28 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout8>) -> tensor<256xbf16, #ttnn_layout10>
        %29 = "ttnn.reshape"(%28) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %30 = "ttnn.add"(%27, %29) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %31 = "ttnn.maximum"(%30, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.to_layout"(%31) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout19>
        %33 = "ttnn.mesh_shard"(%32, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout7>
        return %33 : tensor<32x512xbf16, #ttnn_layout7>
      }
    }
  }
}


// -----// IR Dump Before ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout8>
        %3 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout9>
        %4 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout10>
        %5 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout5>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout11>
        %6 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout12>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout12>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %8 = "ttnn.all_gather"(%7, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %9 = "ttnn.reshape"(%8) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout6>
        %10 = "ttnn.matmul"(%9, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout6>, tensor<256x784xbf16, #ttnn_layout11>) -> tensor<32x256xbf16, #ttnn_layout15>
        %11 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %13 = "ttnn.maximum"(%12, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.matmul"(%13, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout9>) -> tensor<32x512xbf16, #ttnn_layout16>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %16 = "ttnn.reduce_scatter"(%15, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %17 = "ttnn.all_gather"(%16, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %18 = "ttnn.reshape"(%17) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %19 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.concat"(%22, %24) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %26 = "ttnn.slice_static"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %27 = "ttnn.reshape"(%26) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %28 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout8>) -> tensor<256xbf16, #ttnn_layout10>
        %29 = "ttnn.reshape"(%28) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %30 = "ttnn.add"(%27, %29) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %31 = "ttnn.maximum"(%30, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.to_layout"(%31) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout19>
        %33 = "ttnn.mesh_shard"(%32, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout7>
        return %33 : tensor<32x512xbf16, #ttnn_layout7>
      }
    }
  }
}


// -----// IR Dump After ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout8>
        %3 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout9>
        %4 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout10>
        %5 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout5>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout11>
        %6 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout12>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout12>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %8 = "ttnn.all_gather"(%7, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %9 = "ttnn.reshape"(%8) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout6>
        %10 = "ttnn.matmul"(%9, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout6>, tensor<256x784xbf16, #ttnn_layout11>) -> tensor<32x256xbf16, #ttnn_layout15>
        %11 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %13 = "ttnn.maximum"(%12, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.matmul"(%13, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout9>) -> tensor<32x512xbf16, #ttnn_layout16>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %16 = "ttnn.reduce_scatter"(%15, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %17 = "ttnn.all_gather"(%16, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %18 = "ttnn.reshape"(%17) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %19 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.concat"(%22, %24) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %26 = "ttnn.slice_static"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %27 = "ttnn.reshape"(%26) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %28 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout8>) -> tensor<256xbf16, #ttnn_layout10>
        %29 = "ttnn.reshape"(%28) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %30 = "ttnn.add"(%27, %29) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %31 = "ttnn.maximum"(%30, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.to_layout"(%31) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout19>
        %33 = "ttnn.mesh_shard"(%32, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout7>
        return %33 : tensor<32x512xbf16, #ttnn_layout7>
      }
    }
  }
}


// -----// IR Dump Before TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout8>
        %3 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout9>
        %4 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout10>
        %5 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout5>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout11>
        %6 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout12>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout12>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %8 = "ttnn.all_gather"(%7, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %9 = "ttnn.reshape"(%8) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout6>
        %10 = "ttnn.matmul"(%9, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout6>, tensor<256x784xbf16, #ttnn_layout11>) -> tensor<32x256xbf16, #ttnn_layout15>
        %11 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %13 = "ttnn.maximum"(%12, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.matmul"(%13, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout9>) -> tensor<32x512xbf16, #ttnn_layout16>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %16 = "ttnn.reduce_scatter"(%15, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %17 = "ttnn.all_gather"(%16, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %18 = "ttnn.reshape"(%17) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %19 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.concat"(%22, %24) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %26 = "ttnn.slice_static"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %27 = "ttnn.reshape"(%26) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %28 = "ttnn.to_layout"(%2) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout8>) -> tensor<256xbf16, #ttnn_layout10>
        %29 = "ttnn.reshape"(%28) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %30 = "ttnn.add"(%27, %29) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %31 = "ttnn.maximum"(%30, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.to_layout"(%31) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout19>
        %33 = "ttnn.mesh_shard"(%32, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout7>
        return %33 : tensor<32x512xbf16, #ttnn_layout7>
      }
    }
  }
}


// -----// IR Dump After TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout8>
        %3 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout9>
        %4 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout10>
        %5 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout5>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout11>
        %6 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout12>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout12>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %8 = "ttnn.all_gather"(%7, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %9 = "ttnn.reshape"(%8) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout6>
        %10 = "ttnn.matmul"(%9, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout6>, tensor<256x784xbf16, #ttnn_layout11>) -> tensor<32x256xbf16, #ttnn_layout15>
        %11 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %13 = "ttnn.maximum"(%12, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.matmul"(%13, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout9>) -> tensor<32x512xbf16, #ttnn_layout16>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %16 = "ttnn.reduce_scatter"(%15, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %17 = "ttnn.all_gather"(%16, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %18 = "ttnn.reshape"(%17) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %19 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.concat"(%22, %24) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %26 = "ttnn.slice_static"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %27 = "ttnn.reshape"(%26) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %28 = "ttnn.to_device"(%2, %1) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout8>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout19>
        %29 = "ttnn.to_layout"(%28) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout19>) -> tensor<256xbf16, #ttnn_layout10>
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %31 = "ttnn.add"(%27, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.maximum"(%31, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %33 = "ttnn.to_layout"(%32) <{layout = #ttnn.layout<row_major>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout20>
        %34 = "ttnn.from_device"(%33) : (tensor<32x256xbf16, #ttnn_layout20>) -> tensor<32x256xbf16, #ttnn_layout21>
        %35 = "ttnn.mesh_shard"(%34, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout21>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout7>
        return %35 : tensor<32x512xbf16, #ttnn_layout7>
      }
    }
  }
}


// -----// IR Dump Before TTCoreOptimizationBarrierFold (ttcore-optimization-barrier-fold) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout8>
        %3 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout9>
        %4 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout10>
        %5 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout5>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout11>
        %6 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout12>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout12>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %8 = "ttnn.all_gather"(%7, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %9 = "ttnn.reshape"(%8) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout6>
        %10 = "ttnn.matmul"(%9, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout6>, tensor<256x784xbf16, #ttnn_layout11>) -> tensor<32x256xbf16, #ttnn_layout15>
        %11 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %13 = "ttnn.maximum"(%12, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.matmul"(%13, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout9>) -> tensor<32x512xbf16, #ttnn_layout16>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %16 = "ttnn.reduce_scatter"(%15, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %17 = "ttnn.all_gather"(%16, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %18 = "ttnn.reshape"(%17) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %19 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.concat"(%22, %24) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %26 = "ttnn.slice_static"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %27 = "ttnn.reshape"(%26) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %28 = "ttnn.to_device"(%2, %1) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout8>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout19>
        %29 = "ttnn.to_layout"(%28) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout19>) -> tensor<256xbf16, #ttnn_layout10>
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %31 = "ttnn.add"(%27, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.maximum"(%31, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %33 = "ttnn.to_layout"(%32) <{layout = #ttnn.layout<row_major>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout20>
        %34 = "ttnn.from_device"(%33) : (tensor<32x256xbf16, #ttnn_layout20>) -> tensor<32x256xbf16, #ttnn_layout21>
        %35 = "ttnn.mesh_shard"(%34, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout21>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout7>
        return %35 : tensor<32x512xbf16, #ttnn_layout7>
      }
    }
  }
}


// -----// IR Dump Before TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout8>
        %3 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout9>
        %4 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout10>
        %5 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout5>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout11>
        %6 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout12>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout12>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        %8 = "ttnn.all_gather"(%7, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        %9 = "ttnn.reshape"(%8) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout6>
        %10 = "ttnn.matmul"(%9, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout6>, tensor<256x784xbf16, #ttnn_layout11>) -> tensor<32x256xbf16, #ttnn_layout15>
        %11 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %13 = "ttnn.maximum"(%12, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %14 = "ttnn.matmul"(%13, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout9>) -> tensor<32x512xbf16, #ttnn_layout16>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %16 = "ttnn.reduce_scatter"(%15, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        %17 = "ttnn.all_gather"(%16, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        %18 = "ttnn.reshape"(%17) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %19 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %21 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %23 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %24 = "ttnn.point_to_point"(%20, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %25 = "ttnn.concat"(%22, %24) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        %26 = "ttnn.slice_static"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %27 = "ttnn.reshape"(%26) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        %28 = "ttnn.to_device"(%2, %1) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout8>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout19>
        %29 = "ttnn.to_layout"(%28) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout19>) -> tensor<256xbf16, #ttnn_layout10>
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        %31 = "ttnn.add"(%27, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        %32 = "ttnn.maximum"(%31, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        %33 = "ttnn.to_layout"(%32) <{layout = #ttnn.layout<row_major>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout20>
        %34 = "ttnn.from_device"(%33) : (tensor<32x256xbf16, #ttnn_layout20>) -> tensor<32x256xbf16, #ttnn_layout21>
        %35 = "ttnn.mesh_shard"(%34, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout21>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout7>
        return %35 : tensor<32x512xbf16, #ttnn_layout7>
      }
    }
  }
}


// -----// IR Dump After TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn_layout1>) -> ()
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, %arg1: tensor<512x512xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<512xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<512x784xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x784xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<32x512xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> ()
        %3 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout9>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<512x512xbf16, #ttnn_layout3>) -> ()
        %4 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout10>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<512xbf16, #ttnn_layout4>) -> ()
        %5 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout5>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<512x784xbf16, #ttnn_layout5>) -> ()
        %6 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout12>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<32x784xbf16, #ttnn_layout6>) -> ()
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout12>) -> tensor<1x1x32x392xbf16, #ttnn_layout13>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<32x392xbf16, #ttnn_layout12>) -> ()
        %8 = "ttnn.all_gather"(%7, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>) -> ()
        %9 = "ttnn.reshape"(%8) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout6>
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> ()
        %10 = "ttnn.matmul"(%9, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout6>, tensor<256x784xbf16, #ttnn_layout11>) -> tensor<32x256xbf16, #ttnn_layout15>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<32x784xbf16, #ttnn_layout6>) -> ()
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<256x784xbf16, #ttnn_layout11>) -> ()
        %11 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<256xbf16, #ttnn_layout10>) -> ()
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout15>) -> ()
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> ()
        %13 = "ttnn.maximum"(%12, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> ()
        %14 = "ttnn.matmul"(%13, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout9>) -> tensor<32x512xbf16, #ttnn_layout16>
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> ()
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<512x256xbf16, #ttnn_layout9>) -> ()
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<32x512xbf16, #ttnn_layout16>) -> ()
        %16 = "ttnn.reduce_scatter"(%15, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17>
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> ()
        %17 = "ttnn.all_gather"(%16, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>) -> ()
        %18 = "ttnn.reshape"(%17) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> ()
        %19 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %20 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> ()
        %21 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        %22 = "ttnn.point_to_point"(%20, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> ()
        %23 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> ()
        %24 = "ttnn.point_to_point"(%20, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> ()
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> ()
        %25 = "ttnn.concat"(%22, %24) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> ()
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> ()
        %26 = "ttnn.slice_static"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> ()
        %27 = "ttnn.reshape"(%26) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> ()
        %28 = "ttnn.to_device"(%2, %1) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout8>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout19>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout8>) -> ()
        %29 = "ttnn.to_layout"(%28) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout19>) -> tensor<256xbf16, #ttnn_layout10>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> ()
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<256xbf16, #ttnn_layout10>) -> ()
        %31 = "ttnn.add"(%27, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout15>) -> ()
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> ()
        %32 = "ttnn.maximum"(%31, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15>
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1xbf16, #ttnn_layout>) -> ()
        %33 = "ttnn.to_layout"(%32) <{layout = #ttnn.layout<row_major>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout20>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> ()
        %34 = "ttnn.from_device"(%33) : (tensor<32x256xbf16, #ttnn_layout20>) -> tensor<32x256xbf16, #ttnn_layout21>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout20>) -> ()
        %35 = "ttnn.mesh_shard"(%34, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout21>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout7>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout21>) -> ()
        return %35 : tensor<32x512xbf16, #ttnn_layout7>
      }
    }
  }
}


2025-10-20 15:40:22.382 (  43.060s) [        C1B56000]      module_builder.cc:859      1| TTNN Module:
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("xla__device_data")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073175424, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101440, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073183904, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x512xbf16, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x13x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x25x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]> loc(#loc)
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1> loc(#loc)
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn_layout1>) -> () loc(#loc)
        return %2 : tensor<1x1xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("xla__device_data"), %arg1: tensor<512x512xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data"), %arg2: tensor<512xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data"), %arg3: tensor<512x784xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data"), %arg4: tensor<32x784xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("xla__device_data")) -> (tensor<32x512xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout> loc(#loc)
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device loc(#loc)
        %2 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<512xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %3 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x512xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<512x256xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<512x512xbf16, #ttnn_layout3>) -> () loc(#loc)
        %4 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout10> loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<512xbf16, #ttnn_layout4>) -> () loc(#loc)
        %5 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<512x784xbf16, #ttnn_layout5>, !ttnn.device) -> tensor<256x784xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<512x784xbf16, #ttnn_layout5>) -> () loc(#loc)
        %6 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<identity>}> : (tensor<32x784xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<32x392xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<32x784xbf16, #ttnn_layout6>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 32 : i32, 392 : i32]}> : (tensor<32x392xbf16, #ttnn_layout12>) -> tensor<1x1x32x392xbf16, #ttnn_layout13> loc(#loc5)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<32x392xbf16, #ttnn_layout12>) -> () loc(#loc5)
        %8 = "ttnn.all_gather"(%7, %1) <{all_gather_dim = 3 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x32x784xbf16, #ttnn_layout14> loc(#loc6)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x32x392xbf16, #ttnn_layout13>) -> () loc(#loc6)
        %9 = "ttnn.reshape"(%8) <{shape = [32 : i32, 784 : i32]}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> tensor<32x784xbf16, #ttnn_layout6> loc(#loc1)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x1x32x784xbf16, #ttnn_layout14>) -> () loc(#loc1)
        %10 = "ttnn.matmul"(%9, %5) <{transpose_a = false, transpose_b = true}> : (tensor<32x784xbf16, #ttnn_layout6>, tensor<256x784xbf16, #ttnn_layout11>) -> tensor<32x256xbf16, #ttnn_layout15> loc(#loc2)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<32x784xbf16, #ttnn_layout6>) -> () loc(#loc2)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<256x784xbf16, #ttnn_layout11>) -> () loc(#loc2)
        %11 = "ttnn.reshape"(%4) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15> loc(#loc2)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<256xbf16, #ttnn_layout10>) -> () loc(#loc2)
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15> loc(#loc2)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout15>) -> () loc(#loc2)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> () loc(#loc2)
        %13 = "ttnn.maximum"(%12, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15> loc(#loc3)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> () loc(#loc3)
        %14 = "ttnn.matmul"(%13, %3) <{transpose_a = false, transpose_b = true}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<512x256xbf16, #ttnn_layout9>) -> tensor<32x512xbf16, #ttnn_layout16> loc(#loc2)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> () loc(#loc2)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<512x256xbf16, #ttnn_layout9>) -> () loc(#loc2)
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 32 : i32, 512 : i32]}> : (tensor<32x512xbf16, #ttnn_layout16>) -> tensor<1x1x32x512xbf16, #ttnn_layout17> loc(#loc9)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<32x512xbf16, #ttnn_layout16>) -> () loc(#loc9)
        %16 = "ttnn.reduce_scatter"(%15, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x16x512xbf16, #ttnn_layout17> loc(#loc10)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> () loc(#loc10)
        %17 = "ttnn.all_gather"(%16, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x32x512xbf16, #ttnn_layout17> loc(#loc8)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x16x512xbf16, #ttnn_layout17>) -> () loc(#loc8)
        %18 = "ttnn.reshape"(%17) <{shape = [32 : i32, 2 : i32, 256 : i32]}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> tensor<32x2x256xbf16, #ttnn_layout18> loc(#loc2)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1x32x512xbf16, #ttnn_layout17>) -> () loc(#loc2)
        %19 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18> loc(#loc2)
        %20 = "ttnn.slice_static"(%18) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [32 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18> loc(#loc2)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> () loc(#loc2)
        %21 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18> loc(#loc2)
        %22 = "ttnn.point_to_point"(%20, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18> loc(#loc2)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> () loc(#loc2)
        %23 = "ttnn.point_to_point"(%19) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18> loc(#loc2)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> () loc(#loc2)
        %24 = "ttnn.point_to_point"(%20, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18> loc(#loc2)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> () loc(#loc2)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> () loc(#loc2)
        %25 = "ttnn.concat"(%22, %24) <{dim = 1 : si32}> : (tensor<32x1x256xbf16, #ttnn_layout18>, tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x2x256xbf16, #ttnn_layout18> loc(#loc2)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> () loc(#loc2)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> () loc(#loc2)
        %26 = "ttnn.slice_static"(%25) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> tensor<32x1x256xbf16, #ttnn_layout18> loc(#loc2)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<32x2x256xbf16, #ttnn_layout18>) -> () loc(#loc2)
        %27 = "ttnn.reshape"(%26) <{shape = [32 : i32, 256 : i32]}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> tensor<32x256xbf16, #ttnn_layout15> loc(#loc2)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<32x1x256xbf16, #ttnn_layout18>) -> () loc(#loc2)
        %28 = "ttnn.to_device"(%2, %1) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout8>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout19> loc(#loc4)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout8>) -> () loc(#loc4)
        %29 = "ttnn.to_layout"(%28) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout19>) -> tensor<256xbf16, #ttnn_layout10> loc(#loc4)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc4)
        %30 = "ttnn.reshape"(%29) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout10>) -> tensor<1x256xbf16, #ttnn_layout15> loc(#loc2)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<256xbf16, #ttnn_layout10>) -> () loc(#loc2)
        %31 = "ttnn.add"(%27, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout15> loc(#loc2)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout15>) -> () loc(#loc2)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> () loc(#loc2)
        %32 = "ttnn.maximum"(%31, %0) : (tensor<32x256xbf16, #ttnn_layout15>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<32x256xbf16, #ttnn_layout15> loc(#loc3)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> () loc(#loc3)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1xbf16, #ttnn_layout>) -> () loc(#loc3)
        %33 = "ttnn.to_layout"(%32) <{layout = #ttnn.layout<row_major>}> : (tensor<32x256xbf16, #ttnn_layout15>) -> tensor<32x256xbf16, #ttnn_layout20> loc(#loc)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout15>) -> () loc(#loc)
        %34 = "ttnn.from_device"(%33) : (tensor<32x256xbf16, #ttnn_layout20>) -> tensor<32x256xbf16, #ttnn_layout21> loc(#loc)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout20>) -> () loc(#loc)
        %35 = "ttnn.mesh_shard"(%34, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 2>, shard_type = #ttcore.shard_type<devices>}> : (tensor<32x256xbf16, #ttnn_layout21>, !ttnn.device) -> tensor<32x512xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<32x256xbf16, #ttnn_layout21>) -> () loc(#loc)
        return %35 : tensor<32x512xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("aten__addmm")
#loc3 = loc("aten__relu")
#loc4 = loc("aten__addmm_in_0_layout")
#loc5 = loc("xla__device_data_reshape_to_4d"(#loc1))
#loc6 = loc("xla__device_data_all_gather_4d"(#loc1))
#loc7 = loc("aten__addmm_reduceScatter"(#loc2))
#loc8 = loc("aten__addmm_all_gather_4d"(#loc2))
#loc9 = loc("aten__addmm_reduceScatter_reshape_to_4d"(#loc7))
#loc10 = loc("aten__addmm_reduceScatter_reduce_scatter_4d"(#loc7))
------------------ END OF MLIR MODULE ------------------
2025-10-20 15:40:22.389 (  43.067s) [        C1B56000]loaded_executable_insta:69       1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-10-20 15:40:22.389 (  43.067s) [        C1B56000]loaded_executable_insta:88       1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-10-20 15:40:22.389 (  43.067s) [        C1B56000]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-10-20 15:40:22.389 (  43.067s) [        C1B56000]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-10-20 15:40:22.389 (  43.067s) [        C1B56000]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-10-20 15:40:22.389 (  43.067s) [        C1B56000]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-10-20 15:40:22.389 (  43.067s) [        C1B56000] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-20 15:40:22.390 (  43.067s) [        C1B56000] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-20 15:40:22.394 (  43.072s) [        C1B56000] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-20 15:40:22.394 (  43.072s) [        C1B56000] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640] executable_instance.cc:139      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]loaded_executable_insta:125      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-10-20 15:40:22.403 (  43.081s) [        AE7FC640]flatbuffer_loaded_execu:411      1| FlatbufferLoadedExecutableInstance::Execute
2025-10-20 15:40:22.404 (  43.081s) [        AE7FC640]     client_instance.cc:373      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 2]
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-20 15:40:47.005 (  67.683s) [        AE7FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-20 15:40:47.005 (  67.683s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.006 (  67.684s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.006 (  67.684s) [        C1B56000]     buffer_instance.cc:428      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-10-20 15:40:47.006 (  67.684s) [        C1B56000]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-20 15:40:47.006 (  67.684s) [        C1B56000]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-20 15:40:47.006 (  67.684s) [        C1B56000]     buffer_instance.cc:450      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-10-20 15:40:47.006 (  67.684s) [        C1B56000]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-20 15:40:47.006 (  67.684s) [        5FFFF640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-20 15:40:47.008 (  67.686s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.008 (  67.686s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.008 (  67.686s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.008 (  67.686s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.008 (  67.686s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.009 (  67.686s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.009 (  67.687s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.009 (  67.687s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.009 (  67.687s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-20 15:40:47.009 (  67.687s) [        C1B56000]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
PASSED

========================= 1 passed in 68.11s (0:01:08) =========================
