#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
#loc3 = loc("p2.18")
#loc4 = loc("p3.19")
#loc5 = loc("p4.21")
#loc6 = loc("p5.39")
module @SyncTensorsGraph.42 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.42 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
      func.func @main(%arg0: tensor<1x3840xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x3840xbf16>>} loc("p0.1"), %arg1: tensor<0x1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<0x1xi64>>} loc("p1.3"), %arg2: tensor<3840xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<3840xbf16>>} loc("p2.18"), %arg3: tensor<3840x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<3840x64xbf16>>} loc("p3.19"), %arg4: tensor<4096x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<4096x64xbf16>>} loc("p4.21"), %arg5: tensor<4096x3xi32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<4096x3xi32>>} loc("p5.39")) -> (tensor<4096x3840xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<4096x3840xbf16>>}, tensor<4096x3840xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<4096x3840xbf16>>}, tensor<4096x3xi32> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<4096x3xi32>>}) {
        %0 = "ttir.constant"() <{value = dense<4096> : tensor<0xi64>}> : () -> tensor<0xi64> loc(#loc)
        %1 = "ttir.constant"() <{value = dense<0> : tensor<0xi64>}> : () -> tensor<0xi64> loc(#loc)
        %2 = "ttir.permute"(%arg3) <{permutation = array<i64: 1, 0>}> : (tensor<3840x64xbf16>) -> tensor<64x3840xbf16> loc(#loc7)
        %3 = "ttir.dot_general"(%arg4, %2) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<4096x64xbf16>, tensor<64x3840xbf16>) -> tensor<4096x3840xbf16> loc(#loc8)
        %4 = "ttir.reshape"(%arg2) <{shape = [1 : i32, 3840 : i32]}> : (tensor<3840xbf16>) -> tensor<1x3840xbf16> loc(#loc9)
        %5 = "ttir.broadcast"(%4) <{broadcast_dimensions = array<i64: 4096, 1>}> : (tensor<1x3840xbf16>) -> tensor<4096x3840xbf16> loc(#loc9)
        %6 = "ttir.add"(%3, %5) : (tensor<4096x3840xbf16>, tensor<4096x3840xbf16>) -> tensor<4096x3840xbf16> loc(#loc10)
        %7 = "ttir.reshape"(%arg1) <{shape = [0 : i32]}> : (tensor<0x1xi64>) -> tensor<0xi64> loc(#loc11)
        %8 = "ttir.lt"(%7, %1) : (tensor<0xi64>, tensor<0xi64>) -> tensor<0xi1> loc(#loc12)
        %9 = "ttir.add"(%7, %0) : (tensor<0xi64>, tensor<0xi64>) -> tensor<0xi64> loc(#loc13)
        %10 = "ttir.where"(%8, %9, %7) : (tensor<0xi1>, tensor<0xi64>, tensor<0xi64>) -> tensor<0xi64> loc(#loc14)
        %11 = "ttir.reshape"(%10) <{shape = [0 : i32, 1 : i32]}> : (tensor<0xi64>) -> tensor<0x1xi64> loc(#loc15)
        %12 = "ttir.reshape"(%arg0) <{shape = [3840 : i32]}> : (tensor<1x3840xbf16>) -> tensor<3840xbf16> loc(#loc16)
        %13 = "ttir.reshape"(%12) <{shape = [1 : i32, 3840 : i32]}> : (tensor<3840xbf16>) -> tensor<1x3840xbf16> loc(#loc17)
        %14 = "ttir.broadcast"(%13) <{broadcast_dimensions = array<i64: 0, 1>}> : (tensor<1x3840xbf16>) -> tensor<0x3840xbf16> loc(#loc17)
        %15 = "ttir.repeat"(%11) <{repeat_dimensions = array<i64: 1, 3840>}> : (tensor<0x1xi64>) -> tensor<0x3840xi64> loc(#loc18)
        %16 = "ttir.scatter"(%6, %15, %14) <{dim = 0 : i32, scatter_reduce_type = #ttcore.reduce_type<invalid>}> : (tensor<4096x3840xbf16>, tensor<0x3840xi64>, tensor<0x3840xbf16>) -> tensor<4096x3840xbf16> loc(#loc18)
        return %16, %16, %arg5 : tensor<4096x3840xbf16>, tensor<4096x3840xbf16>, tensor<4096x3xi32> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc7 = loc("transpose.20")
#loc8 = loc("dot.23")
#loc9 = loc("broadcast.27")
#loc10 = loc("add.28")
#loc11 = loc("reshape.5")
#loc12 = loc("compare.14")
#loc13 = loc("add.11")
#loc14 = loc("select.15")
#loc15 = loc("reshape.16")
#loc16 = loc("reshape.2")
#loc17 = loc("broadcast.32")
#loc18 = loc("scatter.36")