name: Perf Test (call)

on:
  workflow_call:
    inputs:
      matrix:
        description: "JSON matrix of tests to run"
        required: true
        type: string
      docker_image:
        description: "Docker image to use"
        required: true
        type: string
      artifact_run_id:
        description: "Run ID to download wheel artifact from"
        required: true
        type: string
      wheel_artifact_name:
        description: "Name of the wheel artifact"
        required: true
        type: string
      skip-device-perf:
        description: "Skip device performance measurement"
        required: false
        type: boolean
        default: false
      perf_regression_check:
        description: "Enable perf metrics regression testing"
        required: false
        type: boolean
        default: false
      sh-runner:
        description: "Use shared runner"
        required: false
        type: boolean
        default: false

env:
  GH_TOKEN: ${{ github.token }}
  TTMLIR_TOOLCHAIN_DIR: /opt/ttmlir-toolchain
  TT_XLA_CI: 1

jobs:
  run-perf-test:
    strategy:
      fail-fast: false
      matrix:
        build: ${{ fromJson(inputs.matrix) }}

    runs-on: ${{ !inputs.sh-runner && fromJson(format('["{0}", "in-service"]', matrix.build.runs-on)) || matrix.build.runs-on }}

    name: "perf ${{ matrix.build.name }} (${{ matrix.build.runs-on }})"

    container:
      image: ${{ !inputs.sh-runner && inputs.docker_image || format('harbor.ci.tenstorrent.net/{0}', inputs.docker_image) }}
      options: --device /dev/tenstorrent
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache

    steps:
    - name: Mark repo as safe for git
      run: |
        git config --global --add safe.directory /__w/tt-xla/tt-xla

    - name: Set caching env variables
      shell: bash
      run: |
        if [[ "${{ inputs.sh-runner }}" == "false" || "${{ inputs.sh-runner }}" == "" ]]; then
          echo "DOCKER_CACHE_ROOT=/mnt/dockercache" >> $GITHUB_ENV
        fi

    - uses: actions/checkout@v4
      with:
        sparse-checkout: |
          .github
          tests/perf_benchmark
          third_party
          pytest.ini
          python_package/requirements.txt
          venv

    - name: Initialize submodules
      shell: bash
      run: |
        git submodule update --init --recursive third_party/tt_forge_models

    - name: Fetch job id
      id: fetch-job-id
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "perf ${{ matrix.build.name }} (${{ matrix.build.runs-on }})"

    - name: Set reusable strings
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}
      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "job_id=$JOB_ID" >> "$GITHUB_OUTPUT"
        echo "perf_report_path=$(pwd)/perf_reports" >> "$GITHUB_OUTPUT"
        echo "perf_report_json_file=$(pwd)/perf_reports/perf_report_$JOB_ID.json" >> "$GITHUB_OUTPUT"

    - name: Download and install wheel
      shell: bash
      run: |
        echo "=== Downloading wheel ==="
        gh run download ${{ inputs.artifact_run_id }} \
          --repo ${{ github.repository }} \
          --dir wheels \
          --name ${{ inputs.wheel_artifact_name }}

        echo "=== Installing wheel ==="
        pip install wheels/*.whl

    - name: Install system dependencies
      shell: bash
      run: |
        apt-get update -y -qq
        apt-get install -y -qq --no-install-recommends ${{ matrix.build.libreq || 'libgl1-mesa-glx libglib2.0-0' }}

    - name: Install Python dependencies
      shell: bash
      run: |
        pip install ${{ matrix.build.pyreq }}

    - name: Create perf report directory
      run: mkdir -p ${{ steps.strings.outputs.perf_report_path }}

    - name: Run benchmark
      id: run-benchmark
      shell: bash
      env:
        HF_HOME: /mnt/dockercache/huggingface
        TORCH_HOME: /mnt/dockercache/torchhub
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        cd ${{ steps.strings.outputs.work-dir }}
        export PYTHONPATH="${{ steps.strings.outputs.work-dir }}:$PYTHONPATH"

        # Run pytest or custom script
        if [ -n "${{ matrix.build.pytest }}" ]; then
          pytest -svv ${{ matrix.build.pytest }} \
            --output-file ${{ steps.strings.outputs.perf_report_json_file }}
        elif [ -n "${{ matrix.build.script }}" ]; then
          python ${{ matrix.build.script }} \
            --output-file ${{ steps.strings.outputs.perf_report_json_file }}
        else
          echo "No pytest or script specified for test ${{ matrix.build.name }}"
          exit 1
        fi

    - name: Upload TTIR MLIR
      id: upload-ttir-mlir
      uses: actions/upload-artifact@v4
      if: ${{ !matrix.build.skip-ttir-dump }}
      with:
        name: ttir-mlir-${{ steps.strings.outputs.job_id }}
        path: ./modules/irs/ttir_*.mlir
        if-no-files-found: warn

    - name: Upload TTNN MLIR
      id: upload-ttnn-mlir
      uses: actions/upload-artifact@v4
      if: ${{ !matrix.build.skip-ttnn-dump }}
      with:
        name: ttnn-mlir-${{ steps.strings.outputs.job_id }}
        path: ./modules/irs/ttnn_*.mlir
        if-no-files-found: warn

    # Device Performance Collection
    - name: Find tt-mlir workflow run
      id: find-run
      if: ${{ !matrix.build.skip-device-perf && !inputs.skip-device-perf }}
      shell: bash
      run: |
        mlir_sha=$(grep -oP 'set\(TT_MLIR_VERSION "\K[^"]+' third_party/CMakeLists.txt)
        echo "Found TT_MLIR_VERSION: $mlir_sha"

        RUN_ID=$(curl -s -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
          "https://api.github.com/repos/tenstorrent/tt-mlir/actions/workflows/on-push.yml/runs?head_sha=${mlir_sha}&per_page=1" \
          | jq -r '.workflow_runs[0].id')

        echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
        echo "mlir_sha=$mlir_sha" >> $GITHUB_OUTPUT
        echo "Workflow URL: https://github.com/tenstorrent/tt-mlir/actions/runs/$RUN_ID"
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Download ttrt wheel
      if: ${{ !matrix.build.skip-device-perf && !inputs.skip-device-perf }}
      uses: dawidd6/action-download-artifact@v11
      with:
        run_id: ${{ steps.find-run.outputs.run_id }}
        name: "ttrt-whl-tracy"
        repo: tenstorrent/tt-mlir
        check_artifacts: true
        path: ./
        skip_unpack: true

    - name: Extract ttrt wheel
      if: ${{ !matrix.build.skip-device-perf && !inputs.skip-device-perf }}
      shell: bash
      run: |
        echo "Extracting ttrt wheel"
        mkdir ttrt-whl-tracy
        mv ttrt-whl-tracy.zip ttrt-whl-tracy/ttrt-whl-tracy.zip
        unzip -q ttrt-whl-tracy/ttrt-whl-tracy.zip -d ttrt-whl-tracy

    - name: Install ttrt wheel
      if: ${{ !matrix.build.skip-device-perf && !inputs.skip-device-perf }}
      shell: bash
      run: |
        echo "Installing ttrt in separate venv"
        python3.11 -m venv ttrt-venv
        source ttrt-venv/bin/activate
        apt-get install -y -qq --no-install-recommends libtbb12 libcapstone4
        pip install ttrt-whl-tracy/ttrt*.whl --upgrade
        pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu

    - name: Run Device Perf
      if: ${{ !matrix.build.skip-device-perf && !inputs.skip-device-perf && matrix.build.runs-on != 'p150' }}
      shell: bash
      run: |
        source ttrt-venv/bin/activate
        pip install pandas==2.3.3

        echo "Save artifacts"
        ttrt query --save-artifacts

        echo "Copy binaries"
        mkdir -p flatbuffers
        cp ./modules/*.ttnn flatbuffers/ 2>/dev/null || echo "No .ttnn files found"

        echo "Run ttrt perf"
        ttrt perf flatbuffers --ignore-version || echo "ttrt perf failed, continuing..."

        echo "Write device perf to report"
        python3 ./tests/perf_benchmark/device_perf.py ttrt-artifacts ${{ steps.strings.outputs.perf_report_json_file }} || echo "device_perf.py failed, continuing..."

        echo "Copy device perf CSVs"
        mkdir -p ${{ steps.strings.outputs.perf_report_path }}/device_perf
        for dir in ttrt-artifacts/*/; do
          binary_name=$(basename "$dir")
          csv_file="$dir/perf/ops_perf_results_minus_const_eval_and_input_layout_conversions.csv"
          if [ -f "$csv_file" ]; then
            cp "$csv_file" "${{ steps.strings.outputs.perf_report_path }}/device_perf/${binary_name}.csv"
            echo "Copied $csv_file to ${binary_name}.csv"
          fi
        done

    - name: Upload Device Perf
      id: upload-device-perf
      uses: actions/upload-artifact@v4
      if: ${{ !matrix.build.skip-device-perf && !inputs.skip-device-perf && matrix.build.runs-on != 'p150' }}
      with:
        name: device-perf-${{ steps.strings.outputs.job_id }}
        path: ${{ steps.strings.outputs.perf_report_path }}/device_perf/
        compression-level: 0
        if-no-files-found: warn

    - name: Add config fields to perf report
      shell: bash
      run: |
        PERF_REPORT_FILE="${{ steps.strings.outputs.perf_report_json_file }}"
        TTIR_URL="${{ !matrix.build.skip-ttir-dump && steps.upload-ttir-mlir.outputs.artifact-url || '' }}"
        TTNN_URL="${{ !matrix.build.skip-ttnn-dump && steps.upload-ttnn-mlir.outputs.artifact-url || '' }}"
        DEVICE_PERF_URL="${{ !matrix.build.skip-device-perf && !inputs.skip-device-perf && matrix.build.runs-on != 'p150' && steps.upload-device-perf.outputs.artifact-url || '' }}"
        MLIR_SHA="${{ steps.find-run.outputs.mlir_sha || '' }}"
        JOB_ID_URL="https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/job/${{ steps.strings.outputs.job_id }}"

        if [ -f "$PERF_REPORT_FILE" ]; then
          python3 tests/perf_benchmark/extend_result_config.py "$PERF_REPORT_FILE" \
            ${TTIR_URL:+--ttir-url "$TTIR_URL"} \
            ${TTNN_URL:+--ttnn-url "$TTNN_URL"} \
            ${DEVICE_PERF_URL:+--device-perf-url "$DEVICE_PERF_URL"} \
            ${MLIR_SHA:+--mlir-sha "$MLIR_SHA"} \
            --job-id-url "$JOB_ID_URL"
        fi

    - name: Upload Perf Report
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: perf-reports-${{ steps.strings.outputs.job_id }}
        path: ${{ steps.strings.outputs.perf_report_path }}

    # Perf Regression Check
    - name: Set perf query strings
      id: set-query-strings
      if: inputs.perf_regression_check
      shell: bash
      run: |
        echo "model_name=$(cat ${{ steps.strings.outputs.perf_report_json_file }} | jq -r '.model')" >> $GITHUB_OUTPUT

        workflow_id=$(curl -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
          -H "Accept: application/vnd.github.v3+json" \
          "https://api.github.com/repos/${{ github.repository }}/actions/workflows/perf-benchmark.yml/runs?status=completed&branch=main&per_page=100" \
          | jq -r '.workflow_runs[0].id')
        echo "last_nightly_workflow_id=$workflow_id" >> $GITHUB_OUTPUT

        arch=""
        if [[ "${{ matrix.build.runs-on }}" == *"n150"* ]]; then
          arch="wormhole"
        elif [[ "${{ matrix.build.runs-on }}" == *"p150"* ]]; then
          arch="blackhole"
        elif [[ "${{ matrix.build.runs-on }}" == *"llmbox"* ]]; then
          arch="wormhole_llmbox"
        fi
        echo "arch=$arch" >> $GITHUB_OUTPUT

    - name: Fetch previous perf results
      id: prev-perf
      if: inputs.perf_regression_check
      uses: ./.github/actions/superset-api
      with:
        query: 'benchmarks/last_measurement'
        query_params: '{"project":"tt-xla","ml_model_name":"${{ steps.set-query-strings.outputs.model_name }}","batch_size":"${{ matrix.build.bs }}","precision":"${{ matrix.build.df }}","github_pipeline_id":"${{ steps.set-query-strings.outputs.last_nightly_workflow_id }}","arch":"${{ steps.set-query-strings.outputs.arch }}"}'

    - name: Check perf regression
      if: inputs.perf_regression_check
      shell: bash
      run: |
        if [[ "${{ steps.prev-perf.outputs.response }}" == "[]" ]]; then
          echo "Performance report not found in the last nightly"
          exit 0
        fi

        apt-get install -y -qq bc

        prev_perf_report='${{ steps.prev-perf.outputs.response }}'

        prev_total_samples=$(echo "$prev_perf_report" | jq -r '.[] | select(.name == "total_samples") | .last_value')
        prev_total_time=$(echo "$prev_perf_report" | jq -r '.[] | select(.name == "total_time") | .last_value')
        prev_samples_per_sec=$(echo "scale=6; $prev_total_samples / $prev_total_time" | bc -l)

        current_perf_report_file=${{ steps.strings.outputs.perf_report_json_file }}

        current_total_samples=$(jq -r '.measurements[] | select(.measurement_name == "total_samples") | .value' "$current_perf_report_file")
        current_total_time=$(jq -r '.measurements[] | select(.measurement_name == "total_time") | .value' "$current_perf_report_file")
        current_samples_per_sec=$(echo "scale=6; $current_total_samples / $current_total_time" | bc -l)

        perf_diff=$(echo "scale=4; (($prev_samples_per_sec - $current_samples_per_sec) / $prev_samples_per_sec) * 100" | bc -l)

        echo "Previous performance:"
        echo "  Total samples: $prev_total_samples"
        echo "  Total time: $prev_total_time"
        echo "  Samples per second: $prev_samples_per_sec"
        echo ""
        echo "Current performance:"
        echo "  Total samples: $current_total_samples"
        echo "  Total time: $current_total_time"
        echo "  Samples per second: $current_samples_per_sec"
        echo ""

        if (( $(echo "$perf_diff >= 5.0" | bc -l) )); then
          echo "Performance regression > 5% detected! Performance dropped by ${perf_diff}%"
          exit 1
        fi

        echo "Performance check passed. Diff: ${perf_diff}%"
