name: Run TT Forge Models Tests

on:
  workflow_call:
    inputs:
      docker_image:
        description: 'Docker image to use for build'
        required: true
        type: string
      tests-filter: # FIXME - Rename to test_mark eventually.
        description: 'Filter to apply to the test run'
        required: false
        type: string
        default: "expected_passing"
      num-splits:
        description: 'Number of splits for pytest'
        required: false
        type: number
        default: 4
      runners:
        description: 'JSON array of runners. Example: ["wormhole"] or ["wormhole","blackhole"]'
        required: false
        type: string
        default: '["wormhole","blackhole"]'
      run_id:
        description: 'Run ID to download artifacts from'
        required: false
        type: string
      codecov:
        description: 'Enable codecov upload'
        required: false
        type: boolean
        default: false
  workflow_run:
    workflows: [Build]
    types: [completed]

env:
  # Model files accessed via get_file() are cached here.
  DOCKER_CACHE_ROOT: /mnt/dockercache

permissions:
  packages: write
  checks: write

jobs:

  # Dynamically generate matrix based on number of groups
  generate-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - id: set-matrix
        run: |
          NUM_GROUPS=${{ inputs.num-splits }}
          MATRIX_JSON=$(jq -n --argjson n "$NUM_GROUPS" '
            [range(1; $n + 1)] |
            map({
              "wh-runner": "n150",
              "bh-runner": "p150b",
              "name": ("grp_" + tostring),
              "group": .
            })')
          printf "matrix=%s\n" "$(echo "$MATRIX_JSON" | jq -c)" >> "$GITHUB_OUTPUT"

  tests:
    timeout-minutes: 340
    needs: generate-matrix
    strategy:
      fail-fast: false
      matrix:
        build: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}
        runner: ${{ fromJson(inputs.runners) }}
    name: "run-models ${{ matrix.runner == 'wormhole' && 'wh' || matrix.runner == 'blackhole' && 'bh' || 'unk' }} (${{ matrix.build.name }}) - ${{ inputs.tests-filter }}"
    runs-on: ${{ matrix.runner == 'wormhole' && matrix.build.wh-runner || matrix.runner == 'blackhole' && matrix.build.bh-runner || 'n150'}}
    container:
      image: ${{ inputs.docker_image }}
      options: --user root --device /dev/tenstorrent --shm-size=2gb
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache

    steps:

    # Sets name of architecture
    # Check if we need to skip testing when a wh-runner is not set for wormhole run or a bh-runner on blackhole run
    - name: Check for runner
      run: |
        if [ "${{ matrix.runner }}" = "wormhole" ] && [ -n "${{ matrix.build.wh-runner }}" ]; then
          echo "RUN_TEST=wormhole" >> $GITHUB_ENV
        elif [ "${{ matrix.runner }}" = "blackhole" ] && [ -n "${{ matrix.build.bh-runner }}" ]; then
          echo "RUN_TEST=blackhole" >> $GITHUB_ENV
        else
          echo "RUN_TEST=skip" >> $GITHUB_ENV
        fi

    - name: Mark repo as safe for git
      run: |
        git config --global --add safe.directory /__w/tt-xla/tt-xla
        git config --global --add safe.directory /__w/tt-xla/tt-xla/third_party/tt_forge_models

    - uses: actions/checkout@v4
      with:
        submodules: recursive
        lfs: true

    - name: Fetch job id
      id: fetch-job-id
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "run-models ${{ matrix.runner == 'wormhole' && 'wh' || matrix.runner == 'blackhole' && 'bh' || 'unk' }} (${{ matrix.build.name }}) - ${{ inputs.tests-filter }}"

    - name: Set reusable strings
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}
      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "build-output-dir=$(pwd)/build" >> "$GITHUB_OUTPUT"
        echo "test_report_path=report_$JOB_ID.xml" >> "$GITHUB_OUTPUT"
        if [[ "${{ inputs.codecov }}" == "true" && "${{ matrix.build.codecov }}" == "true" ]]; then
          echo "do_codecov=true" >> "$GITHUB_OUTPUT"
          echo "build=codecov" >> "$GITHUB_OUTPUT"
        else
          echo "build=release" >> "$GITHUB_OUTPUT"
        fi

    - name: Download build artifacts
      if: steps.strings.outputs.do_codecov
      uses: tenstorrent/tt-forge/.github/actions/download-artifact@main
      with:
        name: build-artifacts
        path: build
        github_token: ${{ github.token }}

    - name: Download wheel
      if: ${{ inputs.run_id }}
      uses: tenstorrent/tt-forge/.github/actions/download-artifact@main
      with:
        name: xla-whl-${{ steps.strings.outputs.build }}
        run_id: ${{ inputs.run_id }}
        github_token: ${{ github.token }}

    - name: Find and download alternative wheel
      if: ${{ !inputs.run_id }}
      uses: dawidd6/action-download-artifact@v9
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        workflow_conclusion: success
        workflow_search: true
        workflow: on-push.yml
        name: xla-whl-codecov
        repo: tenstorrent/tt-xla
        check_artifacts: true
        search_artifacts: true

    - name: Install wheel
      shell: bash
      run: |
        source venv/activate
        pip install ${{ steps.strings.outputs.work-dir }}/pjrt_plugin_tt*.whl


    - name: Set pytest command
      if: ${{ env.RUN_TEST != 'skip' }}
      shell: bash
      run: |
        BASE_PYTEST_CMD="\
          tests/runner/test_models.py::test_all_models[phi1/causal_lm/pytorch-microsoft/phi-1-full-eval] \
          tests/runner/test_models.py::test_all_models[regnet/pytorch-regnet_y_064-full-eval] \
          tests/runner/test_models.py::test_all_models[regnet/pytorch-regnet_y_1_6gf-full-eval] \
          tests/runner/test_models.py::test_all_models[regnet/pytorch-regnet_y_32gf-full-eval] \
          tests/runner/test_models.py::test_all_models[regnet/pytorch-regnet_x_1_6gf-full-eval] \
          tests/runner/test_models.py::test_all_models[regnet/pytorch-regnet_x_8gf-full-eval] \
          tests/runner/test_models.py::test_all_models[bert/masked_lm/pytorch-bert-base-uncased-full-eval] \
          tests/runner/test_models.py::test_all_models[yoloworld/pytorch-full-eval] \
          tests/runner/test_models.py::test_all_models[monodepth2/pytorch-mono_640x192-full-eval] \
          tests/runner/test_models.py::test_all_models[monodepth2/pytorch-mono+stereo_no_pt_640x192-full-eval] \
          tests/runner/test_models.py::test_all_models[opt/causal_lm/pytorch-facebook/opt-1.3b-full-eval] \
          tests/runner/test_models.py::test_all_models[qwen_2_5/casual_lm/pytorch-0_5b_instruct-full-eval] \
          tests/runner/test_models.py::test_all_models[xglm/pytorch-xglm-1.7B-full-eval] \
          tests/runner/test_models.py::test_all_models[yolox/pytorch-yolox_m-full-eval] \
          tests/runner/test_models.py::test_all_models[yolox/pytorch-yolox_darknet-full-eval] \
          --log-memory \
          --arch ${{ env.RUN_TEST }}"
        echo "BASE_PYTEST_CMD=$BASE_PYTEST_CMD" >> $GITHUB_ENV

    - name: Collect Tests
      if: ${{ env.RUN_TEST != 'skip' }}
      shell: bash
      run: |
        source venv/activate
        echo "Collecting tests for group ${{ matrix.build.group }}..."
        pytest --collect-only -q ${{ env.BASE_PYTEST_CMD }}

    - name: Run Model Test
      if: ${{ env.RUN_TEST != 'skip' }}
      env:
        HF_HOME: ${{ env.DOCKER_CACHE_ROOT }}/huggingface
        TORCH_HOME: ${{ env.DOCKER_CACHE_ROOT }}/torch
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        TT_TORCH_REQS_DEBUG: 1
      shell: bash
      run: |
        source venv/activate
        pytest --durations=0 -vv -rA --capture=tee-sys --forked ${{ env.BASE_PYTEST_CMD }} --junit-xml=${{ steps.strings.outputs.test_report_path }} | tee pytest.log

    - name: Upload Test Log
      uses: actions/upload-artifact@v4
      if: ${{ (success() || failure()) && env.RUN_TEST != 'skip' }}
      with:
        name: test-log-${{ matrix.runner }}-${{ matrix.build.name }}-${{ steps.fetch-job-id.outputs.job_id }}-${{ inputs.tests-filter }}
        path: pytest.log

    - name: Upload Test Report Models
      uses: actions/upload-artifact@v4
      if: ${{ (success() || failure()) && env.RUN_TEST != 'skip' }}
      with:
        name: test-reports-models-${{ matrix.runner }}-${{ matrix.build.name }}-${{ steps.fetch-job-id.outputs.job_id }}-${{ inputs.tests-filter }}
        path: ${{ steps.strings.outputs.test_report_path }}
