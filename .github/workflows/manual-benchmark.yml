name: Performance Benchmark

on:
  workflow_dispatch:
    inputs:
      mlir_override:
        description: 'Git SHA of commit in tenstorrent/tt-mlir'
        required: false
        type: string
      docker_image:
        description: 'Docker image to use'
        required: false
        type: string
        default: 'ghcr.io/tenstorrent/tt-xla-slim:nightly-latest'
      test_filter:
        description: "Filter tests based on the name property. One or more strings which are contained (case insensitive) in the name (e.g. 'resn' or 'resn,mni,yol')"
        required: false
        type: string
      runs-on-filter:
        description: "Architecture you want to run the tests on"
        required: false
        type: choice
        options:
          - All
          - n150
          - p150
          - n300-llmbox
        default: n150
      skip-device-perf:
        description: "Skip device performance measurement"
        required: false
        type: boolean
        default: true
      perf_regression_check:
        description: "Enable perf metrics regression testing"
        required: false
        type: boolean
        default: false
      sh-runner:
        description: "Run on shared runner"
        required: false
        type: boolean
        default: true

permissions:
  packages: write
  checks: write
  contents: write
  id-token: write
  actions: write

jobs:
  docker-build:
    uses: ./.github/workflows/call-build-docker.yml
    secrets: inherit
    with:
      mlir_override: ${{ inputs.mlir_override }}

  build:
    needs: docker-build
    uses: ./.github/workflows/call-build.yml
    secrets: inherit
    with:
      mlir_override: ${{ inputs.mlir_override }}
      docker_image: ${{ needs.docker-build.outputs.docker-image }}

  filter-tests:
    runs-on: ubuntu-latest
    outputs:
      matrix_n150: ${{ steps.set-perf-benchmarks.outputs.matrix_n150 }}
      matrix_p150: ${{ steps.set-perf-benchmarks.outputs.matrix_p150 }}
      matrix_llmbox: ${{ steps.set-perf-benchmarks.outputs.matrix_llmbox }}
      matrix_n150_skip: ${{ steps.set-perf-benchmarks.outputs.matrix_n150_skip }}
      matrix_p150_skip: ${{ steps.set-perf-benchmarks.outputs.matrix_p150_skip }}
      matrix_llmbox_skip: ${{ steps.set-perf-benchmarks.outputs.matrix_llmbox_skip }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: Filter Matrix
      id: set-perf-benchmarks
      shell: bash
      run: |
        result=$(python .github/scripts/filter-test-matrix.py \
          .github/workflows/perf-bench-matrix.json \
          tt-xla \
          ${{ inputs.test_filter && format('--test-filter "{0}"', inputs.test_filter) || '' }} \
          ${{ inputs.sh-runner && '--sh-runner' || '' }})

        matrix=$(echo $result | jq -r -c '.matrix')
        matrix_n150=$(echo $result | jq -r -c '.matrix | map(select(."runs-on" | contains("n150") and (contains("llmbox") | not)))')
        matrix_p150=$(echo $result | jq -r -c '.matrix | map(select(."runs-on" | contains("p150")))')
        matrix_llmbox=$(echo $result | jq -r -c '.matrix | map(select(."runs-on" | contains("llmbox")))')

        matrix_n150_skip="false"
        matrix_p150_skip="false"
        matrix_llmbox_skip="false"

        if [ "$matrix_n150" == "[]" ]; then
          matrix_n150_skip="true"
        fi
        if [ "$matrix_p150" == "[]" ]; then
          matrix_p150_skip="true"
        fi
        if [ "$matrix_llmbox" == "[]" ]; then
          matrix_llmbox_skip="true"
        fi

        if [ "${{ inputs.runs-on-filter }}" == "n150" ]; then
          matrix_p150_skip="true"
          matrix_llmbox_skip="true"
        fi
        if [ "${{ inputs.runs-on-filter }}" == "p150" ]; then
          matrix_n150_skip="true"
          matrix_llmbox_skip="true"
        fi
        if [ "${{ inputs.runs-on-filter }}" == "n300-llmbox" ]; then
          matrix_n150_skip="true"
          matrix_p150_skip="true"
        fi

        echo "matrix_n150=$matrix_n150" >> $GITHUB_OUTPUT
        echo "matrix_p150=$matrix_p150" >> $GITHUB_OUTPUT
        echo "matrix_llmbox=$matrix_llmbox" >> $GITHUB_OUTPUT
        echo "matrix_n150_skip=$matrix_n150_skip" >> $GITHUB_OUTPUT
        echo "matrix_p150_skip=$matrix_p150_skip" >> $GITHUB_OUTPUT
        echo "matrix_llmbox_skip=$matrix_llmbox_skip" >> $GITHUB_OUTPUT

        # Set up the summary for the job
        echo "### Perf benchmarks inputs" >> $GITHUB_STEP_SUMMARY
        echo "Test filter: ${{ inputs.test_filter }}" >> $GITHUB_STEP_SUMMARY
        echo "Runs-on filter: ${{ inputs.runs-on-filter }}" >> $GITHUB_STEP_SUMMARY
        echo "Skip device perf: ${{ inputs.skip-device-perf }}" >> $GITHUB_STEP_SUMMARY
        echo "Perf regression check: ${{ inputs.perf_regression_check }}" >> $GITHUB_STEP_SUMMARY

  run-n150-perf-benchmarks:
    needs: [filter-tests, build]
    if: ${{ needs.filter-tests.outputs.matrix_n150_skip == 'false' }}
    secrets: inherit
    uses: ./.github/workflows/call-perf-test.yml
    with:
      matrix: ${{ needs.filter-tests.outputs.matrix_n150 }}
      docker_image: ${{ inputs.docker_image }}
      artifact_run_id: ${{ needs.build.outputs.artifacts_run_id }}
      wheel_artifact_name: ${{ needs.build.outputs.wheel_artifact_name }}
      skip-device-perf: ${{ inputs.skip-device-perf || false }}
      perf_regression_check: ${{ inputs.perf_regression_check || false }}
      sh-runner: ${{ inputs.sh-runner || false }}

  run-p150-perf-benchmarks:
    needs: [filter-tests, build]
    if: ${{ needs.filter-tests.outputs.matrix_p150_skip == 'false' }}
    secrets: inherit
    uses: ./.github/workflows/call-perf-test.yml
    with:
      matrix: ${{ needs.filter-tests.outputs.matrix_p150 }}
      docker_image: ${{ inputs.docker_image }}
      artifact_run_id: ${{ needs.build.outputs.artifacts_run_id }}
      wheel_artifact_name: ${{ needs.build.outputs.wheel_artifact_name }}
      skip-device-perf: ${{ inputs.skip-device-perf || false }}
      perf_regression_check: ${{ inputs.perf_regression_check || false }}
      sh-runner: ${{ inputs.sh-runner || false }}

  run-llmbox-perf-benchmarks:
    needs: [filter-tests, build]
    if: ${{ needs.filter-tests.outputs.matrix_llmbox_skip == 'false' }}
    secrets: inherit
    uses: ./.github/workflows/call-perf-test.yml
    with:
      matrix: ${{ needs.filter-tests.outputs.matrix_llmbox }}
      docker_image: ${{ inputs.docker_image }}
      artifact_run_id: ${{ needs.build.outputs.artifacts_run_id }}
      wheel_artifact_name: ${{ needs.build.outputs.wheel_artifact_name }}
      skip-device-perf: ${{ inputs.skip-device-perf || false }}
      perf_regression_check: ${{ inputs.perf_regression_check || false }}
      sh-runner: ${{ inputs.sh-runner || false }}

  fail-notify:
    if: always()
    needs:
      - run-n150-perf-benchmarks
      - run-p150-perf-benchmarks
      - run-llmbox-perf-benchmarks
    runs-on: ubuntu-latest
    outputs:
      failed: ${{ steps.check.outputs.failure }}
    steps:
      - name: Check if the needed jobs succeeded or failed
        id: check
        uses: re-actors/alls-green@release/v1
        with:
          jobs: ${{ toJSON(needs) }}
          allowed-skips: run-n150-perf-benchmarks, run-p150-perf-benchmarks, run-llmbox-perf-benchmarks
