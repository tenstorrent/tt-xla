name: Update Test Durations

# This workflow updates test durations based on the latest nightly, weekly, weekly training and push workflow runs.
# It processes the JUnit XML reports from the last successful nightly, weekly, weekly training and push workflows,
# calculates the test durations, and creates a pull request to update the `.test_durations` file.

on:
  workflow_dispatch:

permissions:
  packages: read
  checks: write
  contents: read

env:
  GH_TOKEN: ${{ github.token }}
  TTMLIR_TOOLCHAIN_DIR: /opt/ttmlir-toolchain
  TT_XLA_CI: 1
  # Define controller and all participating hostnames
  CONTROLLER_HOST: forge-qbae-01
  ALL_HOSTS: forge-qbae-01,forge-qbae-02
  # SSH user with keyless SSH configured between hosts
  SSH_USER: ttuser
  # Artifact download mode: 'specific' to use ARTIFACT_RUN_ID, 'latest' to use latest push-main
  ARTIFACT_MODE: specific
  ARTIFACT_RUN_ID: 21839981996

jobs:
  multihost-test:
    runs-on: ${{ matrix.hostname }}
    strategy:
      fail-fast: false
      matrix:
        hostname:
          - forge-qbae-01
          - forge-qbae-02
    container:
      image: ghcr.io/tenstorrent/tt-xla/tt-xla-ci-ubuntu-22-04:latest
      options: --device /dev/tenstorrent --name ubuntu-host-mapped
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache
        - /home/ttuser/.ssh:/root/.ssh:ro
    steps:
      - name: Mark repo as safe for git
        run: |
          git config --global --add safe.directory /__w/tt-xla/tt-xla
          git config --global --add safe.directory /__w/tt-xla/tt-xla/third_party/tt_forge_models
      - uses: actions/checkout@v4
        with:
          submodules: recursive
          sparse-checkout: |
            .github/scripts
            .gitmodules
            .test_durations
            pjrt_implementation
            third_party/pjrt_c_api
            pytest.ini
            python_package/requirements.txt
            integrations/vllm_plugin/requirements-vllm-plugin.txt
            tests
            venv
            examples

      - name: Download artifact and setup venv
        shell: bash
        env:
          CURRENT_HOST: ${{ matrix.hostname }}
          CONTROLLER_HOST: ${{ env.CONTROLLER_HOST }}
          ALL_HOSTS: ${{ env.ALL_HOSTS }}
          ARTIFACT_MODE: ${{ env.ARTIFACT_MODE }}
          ARTIFACT_RUN_ID: ${{ env.ARTIFACT_RUN_ID }}
        run: |
          echo "Current hostname: $CURRENT_HOST"
          echo "Controller hostname: $CONTROLLER_HOST"
          echo "All participating hosts: $ALL_HOSTS"
          hostname

          # Download the artifact based on mode
          ARTIFACT_DIR="xla_artifacts"
          if [ "$ARTIFACT_MODE" = "specific" ]; then
            echo "Downloading artifacts from specific run: $ARTIFACT_RUN_ID"
            python3 .github/scripts/download_artifacts.py --run-id $ARTIFACT_RUN_ID --filter xla-whl-release --output-folder $ARTIFACT_DIR
          else
            echo "Downloading artifacts from latest push-main"
            python3 .github/scripts/download_artifacts.py --workflow push-main.yml --branch main --filter xla-whl-release --output-folder $ARTIFACT_DIR
          fi

          # Create venv
          source venv/activate

          # Install the wheel
          WHEEL_PATH=$(find $ARTIFACT_DIR -name "*.whl" | head -n 1)
          echo "Installing wheel: $WHEEL_PATH"
          pip install "$WHEEL_PATH"

      - name: Reset hardware and run liveness check
        shell: bash
        run: |
          source venv/activate

          # Install tt-smi
          pip install tt-smi

          # Reset and test in a loop - retry if liveness check fails
          MAX_ATTEMPTS=5
          for i in $(seq 1 $MAX_ATTEMPTS); do
            echo "Attempt $i/$MAX_ATTEMPTS: Resetting hardware..."
            tt-smi -r
            sleep 1

            echo "Running liveness check (MNIST example)..."
            if python3 examples/pytorch/mnist.py; then
              echo "Liveness check passed!"
              break
            else
              echo "Liveness check failed, will retry..."
              if [ $i -eq $MAX_ATTEMPTS ]; then
                echo "Failed after $MAX_ATTEMPTS attempts"
                exit 1
              fi
            fi
          done

      - name: Setup SSH
        shell: bash
        run: |
          echo "Setting up SSH configuration for keyless access..."

          # Create SSH config in /tmp (since /root/.ssh is read-only)
          mkdir -p /tmp/ssh_config
          cat > /tmp/ssh_config/config <<EOF
          Host *
            StrictHostKeyChecking no
            UserKnownHostsFile=/dev/null
          EOF

          echo "SSH configuration complete"

      - name: Worker Pause
        if: matrix.hostname != env.CONTROLLER_HOST
        shell: bash
        env:
          CURRENT_HOST: ${{ matrix.hostname }}
          CONTROLLER_HOST: ${{ env.CONTROLLER_HOST }}
          ALL_HOSTS: ${{ env.ALL_HOSTS }}
        run: |
          echo "Worker ready and waiting for commands."

          # Write ready status file (we're already in the container)
          mkdir -p /tmp/worker_status
          echo "ready" > /tmp/worker_status/ready

          echo "Status file written, worker is ready"
          sleep 6000

      - name: Controller Barrier
        if: matrix.hostname == env.CONTROLLER_HOST
        shell: bash
        env:
          CURRENT_HOST: ${{ matrix.hostname }}
          CONTROLLER_HOST: ${{ env.CONTROLLER_HOST }}
          ALL_HOSTS: ${{ env.ALL_HOSTS }}
          SSH_USER: ${{ env.SSH_USER }}
        run: |
          echo "Controller waiting for all workers to be ready..."

          # Convert comma-separated hosts to array
          IFS=',' read -ra HOST_ARRAY <<< "$ALL_HOSTS"

          # Wait for each worker to be ready
          for remote_host in "${HOST_ARRAY[@]}"; do
            if [ "$remote_host" != "$CURRENT_HOST" ]; then
              echo "Checking if worker $remote_host is ready..."

              # Poll until the ready file exists in the worker container
              # SSH as the configured user, then docker exec into the container
              while true; do
                if ssh -F /tmp/ssh_config/config -o ConnectTimeout=5 ${SSH_USER}@${remote_host} "docker exec ubuntu-host-mapped test -f /tmp/worker_status/ready" 2>/dev/null; then
                  echo "Worker $remote_host is ready!"
                  break
                else
                  echo "Worker $remote_host not ready yet, waiting..."
                  sleep 5
                fi
              done
            fi
          done

          echo "All workers are ready! Controller barrier complete."

      - name: Controller Run Tests
        if: matrix.hostname == env.CONTROLLER_HOST
        shell: bash
        env:
          CURRENT_HOST: ${{ matrix.hostname }}
          CONTROLLER_HOST: ${{ env.CONTROLLER_HOST }}
          ALL_HOSTS: ${{ env.ALL_HOSTS }}
          SSH_USER: ${{ env.SSH_USER }}
        run: |
          echo "Running tests across all hosts..."
          source venv/activate

          echo "Sleeping for 6000 seconds"
          sleep 6000


          TTXLA_LOGGER_LEVEL=DEBUG pytest -svv tests/torch/multi_host/experimental/test_multihost_basic.py -k "dual_bh_quietbox"
