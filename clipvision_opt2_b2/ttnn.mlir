#dram = #ttnn.buffer_type<dram>
#l1 = #ttnn.buffer_type<l1>
#loc = loc(unknown)
#loc202 = loc("unknown|unknown|-1|unknownxla__device_data")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 9x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102208, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073060416, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 64 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 64 + d2, d3), <1x1>, memref<48x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 32 + d2, d3), <1x1>, memref<100x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<4x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 768 + d1, d2), <1x1>, memref<48x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 768 + d1, d2), <1x1>, memref<48x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!ttcore.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 32 + d2, d3), <1x1>, memref<73728x32xbf16, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 3072 + d2, d3), <1x1>, memref<96x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x96x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<4x96x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<24x96x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<24x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 672 + d1 * 224 + d2, d3), <1x1>, memref<42x7x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<1x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 224 + d2, d3), <1x1>, memref<3136x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 100352 + d1 * 100352 + d2, d3), <1x1>, memref<3136x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 100352 + d1 * 100352 + d2, d3), <1x1>, memref<100352x3xbf16, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<14x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<1536x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 768 + d1, d2), <48x1, (d0, d1) -> (0, d0 floordiv 8, d0 mod 8)>, memref<1x2x!ttcore.tile<32x32, bf16>, #l1>, <height_sharded>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 768 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x2, (d0, d1) -> (0, d1 floordiv 8, d1 mod 8)>, memref<1x1x!ttcore.tile<32x32, bf16>, #l1>, <width_sharded>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <4x8, (d0, d1) -> (0, d0, d1)>, memref<1x3x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <4x1, (d0, d1) -> (0, d0 floordiv 8, d0 mod 8)>, memref<1x1x!ttcore.tile<32x32, bf16>, #l1>, <height_sharded>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <4x8, (d0, d1) -> (0, d0, d1)>, memref<1x3x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<48x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <48x1, (d0, d1) -> (0, d0 floordiv 8, d0 mod 8)>, memref<1x2x!ttcore.tile<32x32, bf16>, #l1>, <height_sharded>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 64 + d2, d3), <1x1>, memref<48x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <4x8, (d0, d1) -> (0, d0, d1)>, memref<1x12x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x12x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x48, (d0, d1) -> (0, d1 floordiv 8, d1 mod 8)>, memref<4x2x!ttcore.tile<32x32, bf16>, #l1>, <width_sharded>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x24, (d0, d1) -> (0, d1 floordiv 8, d1 mod 8)>, memref<4x1x!ttcore.tile<32x32, bf16>, #l1>, <width_sharded>>
#ttnn_layout58 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1, (d0, d1) -> (0, d1 floordiv 8, d1 mod 8)>, memref<1x1x!ttcore.tile<32x32, bf16>, #l1>, <width_sharded>>
#ttnn_layout59 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x24, (d0, d1) -> (0, d1 floordiv 8, d1 mod 8)>, memref<1x1x!ttcore.tile<32x32, bf16>, #l1>, <width_sharded>>
#ttnn_layout60 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1, (d0, d1) -> (0, d1 floordiv 8, d1 mod 8)>, memref<1x1x!ttcore.tile<32x32, bf16>, #l1>, <width_sharded>>
module @SyncTensorsGraph.4178 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.4178 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<9x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main_const_eval_0() -> tensor<2x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 1.00135803E-5 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<2x1>}> : (!ttnn.device) -> tensor<2x1xbf16, #ttnn_layout> loc(#loc)
        return %1 : tensor<2x1xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_1(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc477)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc478)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc478)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc3)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc3)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc479)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc479)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc479)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc479)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_2(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc480)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc481)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc481)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc6)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc6)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc482)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc482)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc482)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc482)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_3(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc483)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_4(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1222)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_5(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1006)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_6(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc486)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_7(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc487)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_8(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1007)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_9(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc13)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc13)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc13)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc489)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc489)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_10(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<2x768x1xbf16, #ttnn_layout13> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc14)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x1x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x1x768xbf16, #ttnn_layout14> loc(#loc14)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc14)
        %2 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 1>}> : (tensor<2x1x768xbf16, #ttnn_layout14>) -> tensor<2x768x1xbf16, #ttnn_layout13> loc(#loc15)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x1x768xbf16, #ttnn_layout14>) -> () loc(#loc15)
        return %2 : tensor<2x768x1xbf16, #ttnn_layout13> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_11(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc490)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc491)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc491)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc18)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc18)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc492)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc492)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc492)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc492)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_12(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc493)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc494)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc494)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc21)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc21)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc495)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc495)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc495)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc495)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_13(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc496)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc497)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc497)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc24)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc24)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc498)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc498)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc498)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc498)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_14(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc25)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc25)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc25)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc499)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc499)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_15(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1223)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_16(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc27)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc27)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc27)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc501)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc501)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_17(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc502)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc503)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc503)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc30)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc30)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc504)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc504)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc504)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc504)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_18(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc505)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc506)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc506)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc33)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc33)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc507)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc507)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc507)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc507)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_19(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc34)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc34)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc34)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc508)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc508)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_20(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc509)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc510)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc510)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc37)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc37)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc511)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc511)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc511)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc511)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_21(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc38)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc38)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc38)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc512)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc512)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_22(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc39)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc39)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc39)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc513)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc513)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_23(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc514)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_24(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1224)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_25(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1010)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_26(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1011)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_27(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc44)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc44)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc44)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc518)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc518)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_28(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1225)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_29(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc520)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_30(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc521)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc522)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc522)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc49)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc49)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc523)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc523)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc523)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc523)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_31(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1013)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_32(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1014)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_33(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc50)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc50)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc50)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc526)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc526)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_34(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc527)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_35(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc51)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc51)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc51)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc528)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc528)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_36(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc529)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc530)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc530)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc54)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc54)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc531)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc531)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc531)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc531)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_37(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc532)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc533)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc533)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc57)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc57)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc534)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc534)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc534)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc534)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_38(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc58)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc58)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc58)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc535)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc535)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_39(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc536)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc537)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc537)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc61)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc61)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc538)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc538)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc538)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc538)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_40(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc539)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_41() -> tensor<2x50xbf16, #ttnn_layout15> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.00130462646 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<2x50>}> : (!ttnn.device) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc)
        return %1 : tensor<2x50xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_42(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1015)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_43(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc541)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc542)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc542)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc66)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc66)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc543)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc543)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc543)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc543)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_44(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc544)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_45(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1016)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_46(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc68)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_47(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc69)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc69)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc69)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc546)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc546)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_48(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc547)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_49(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1017)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_50(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc549)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_51() -> tensor<2xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.00130462646 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<2>}> : (!ttnn.device) -> tensor<2xbf16, #ttnn_layout16> loc(#loc)
        return %1 : tensor<2xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_52(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc550)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc551)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc551)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc74)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc74)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc552)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc552)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc552)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc552)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_53(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc75)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_54(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1226)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_55(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1019)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_56(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc555)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc556)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc556)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc79)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc79)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc557)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc557)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc557)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc557)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_57() -> (tensor<2x50x1xbf16, #ttnn_layout17>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 1.00135803E-5 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<2x50x1>}> : (!ttnn.device) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc80)
        %3 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc81)
        %4 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc82)
        %5 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc83)
        %6 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc84)
        %7 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc85)
        %8 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc86)
        %9 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc87)
        %10 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc88)
        %11 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc89)
        %12 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc90)
        %13 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc91)
        %14 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc92)
        %15 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc93)
        %16 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc94)
        %17 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc95)
        %18 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc96)
        %19 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc97)
        %20 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc98)
        %21 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc99)
        %22 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc100)
        %23 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc101)
        %24 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc102)
        %25 = "ttnn.reshape"(%1) <{shape = [2 : i32, 50 : i32]}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc103)
        return %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25 : tensor<2x50x1xbf16, #ttnn_layout17>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_58(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1020)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_59(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc559)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc560)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc560)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc107)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc107)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc561)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc561)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc561)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc561)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_60(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc562)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc563)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc563)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc110)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc110)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc564)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc564)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc564)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc564)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_61(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x1x768xbf16, #ttnn_layout12> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc111)
        return %0 : tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_62(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc565)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc566)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc566)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc114)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc114)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc567)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc567)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc567)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc567)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_63(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc568)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_64(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc569)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc570)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc570)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc117)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc117)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc571)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc571)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc571)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc571)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_65(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc572)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc573)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc573)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc120)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc120)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc574)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc574)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc574)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc574)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_66(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1021)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_67(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1227)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_68(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc577)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_69(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1023)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_70(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1024)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_71(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc126)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc126)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc126)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc580)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc580)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_72(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1025)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_73(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc582)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_74(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc583)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_75(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1026)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_76(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc585)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_77(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc586)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc587)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc587)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc132)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc132)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc588)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc588)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc588)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc588)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_78(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc133)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc133)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc133)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc589)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc589)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_79(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc590)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_80(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc591)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc592)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc592)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc136)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc136)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc593)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc593)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc593)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc593)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_81(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc594)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_82(%arg0: tensor<1x50xsi32, #ttnn_layout18> loc(unknown), %arg1: tensor<50x768xbf16, #ttnn_layout19> loc(unknown)) -> tensor<2x768x50xbf16, #ttnn_layout20> attributes {const_eval} {
        %0 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x50xsi32, #ttnn_layout18>) -> tensor<1x50xui32, #ttnn_layout21> loc(#loc137)
        %1 = "ttnn.embedding"(%0, %arg1) : (tensor<1x50xui32, #ttnn_layout21>, tensor<50x768xbf16, #ttnn_layout19>) -> tensor<1x50x768xbf16, #ttnn_layout22> loc(#loc137)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x50xui32, #ttnn_layout21>) -> () loc(#loc137)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1>}> : (tensor<1x50x768xbf16, #ttnn_layout22>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc138)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x50x768xbf16, #ttnn_layout22>) -> () loc(#loc138)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 1>}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x768x50xbf16, #ttnn_layout20> loc(#loc15)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc15)
        return %3 : tensor<2x768x50xbf16, #ttnn_layout20> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_83(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1027)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_84(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc596)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc597)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc597)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc142)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc142)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc598)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc598)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc598)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc598)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_85(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc599)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_86(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1228)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_87(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc601)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_88(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc602)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc603)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc603)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc146)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc146)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc604)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc604)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc604)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc604)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_89(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc147)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc147)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc147)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc605)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc605)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_90(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc148)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc148)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc148)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc606)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc606)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_91(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc149)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc149)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc149)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc607)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc607)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_92(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc150)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc150)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc150)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc608)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc608)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_93(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc609)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_94(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1029)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_95(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1030)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_96(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc612)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc613)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc613)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc153)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc153)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc614)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc614)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc614)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc614)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_97(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1229)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_98(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc155)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc155)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc155)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc616)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc616)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_99(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc617)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc618)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc618)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc158)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc158)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc619)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc619)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc619)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc619)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_100(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc620)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc621)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc621)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc161)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc161)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc622)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc622)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc622)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc622)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_101(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc623)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc624)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc624)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc164)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc164)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc625)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc625)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc625)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc625)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_102(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc165)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc165)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc165)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc626)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc626)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_103(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc627)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc628)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc628)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc168)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc168)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc629)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc629)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc629)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc629)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_104(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x1x768xbf16, #ttnn_layout12> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc169)
        return %0 : tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_105(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1032)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_106(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1033)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_107(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc632)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_108(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc633)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc634)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc634)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc173)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc173)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc635)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc635)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc635)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc635)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_109(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc636)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_110(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc174)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc174)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc174)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc637)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc637)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_111(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1230)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_112(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc639)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc640)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc640)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc178)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc178)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc641)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc641)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc641)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc641)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_113(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc179)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc179)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc179)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc642)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc642)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_114(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc643)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_115(%arg0: tensor<768x3x32x32xbf16, #ttnn_layout23> loc(unknown)) -> tensor<1x1x3072x768xbf16, #ttnn_layout24> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.prepare_conv2d_weights"(%arg0, %0) <{batch_size = 2 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 32, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = false, in_channels = 3 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 224 : i32, input_memory_config = #ttnn.memory_config<#dram, <interleaved>>, input_tensor_layout = #ttnn.layout<row_major>, input_width = 224 : i32, kernel_size = array<i32: 32, 32>, out_channels = 768 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 32, 32>, weights_format = "OIHW"}> : (tensor<768x3x32x32xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x3072x768xbf16, #ttnn_layout24> loc(#loc644)
        return %1 : tensor<1x1x3072x768xbf16, #ttnn_layout24> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_116(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc645)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_117(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc646)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc647)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc647)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc183)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc183)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc648)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc648)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc648)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc648)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_118(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1231)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_119() -> (tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 1.703125 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<2x50x3072>}> : (!ttnn.device) -> tensor<2x50x3072xbf16, #ttnn_layout26> loc(#loc)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1184)
        %3 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1185)
        %4 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1186)
        %5 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1187)
        %6 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1188)
        %7 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1189)
        %8 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1190)
        %9 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1191)
        %10 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1192)
        %11 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1193)
        %12 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1194)
        %13 = "ttnn.reshape"(%1) <{shape = [100 : i32, 3072 : i32]}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc1195)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x3072xbf16, #ttnn_layout26>) -> () loc(#loc1195)
        return %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13 : tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_120(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1232)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_121(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1038)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_122(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc653)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc654)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc654)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc189)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc189)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc655)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc655)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc655)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc655)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_123(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1039)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_124(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc1040)
        return %0 : tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_125(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc658)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc659)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc659)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc192)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc192)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc660)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc660)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc660)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc660)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_126(%arg0: tensor<3072xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3072xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc1233)
        return %0 : tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_127(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc193)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc193)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc193)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc661)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc661)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_128() -> tensor<2x12x50x50xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 1.250000e-01 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<2x12x50x50>}> : (!ttnn.device) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc)
        return %1 : tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_129(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc662)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x1x64xbf16, #ttnn_layout7> loc(#loc663)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc663)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x50x1>}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc196)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x1x64xbf16, #ttnn_layout7>) -> () loc(#loc196)
        %3 = "ttnn.concatenate_heads"(%2) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc664)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc664)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc664)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc664)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_130(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 12 : i32, 64 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x12x64xbf16, #ttnn_layout3> loc(#loc665)
        %1 = "ttnn.permute"(%0) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> tensor<1x12x64x1xbf16, #ttnn_layout4> loc(#loc666)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x12x64xbf16, #ttnn_layout3>) -> () loc(#loc666)
        %2 = "ttnn.repeat"(%1) <{repeat_dims = #ttnn.shape<2x1x1x50>}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc199)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x64x1xbf16, #ttnn_layout4>) -> () loc(#loc199)
        %3 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc667)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc667)
        %4 = "ttnn.reshape"(%3) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc667)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc667)
        return %4 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_131(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc200)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc200)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc200)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc668)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc668)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_132(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<100x768xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc201)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<2x50x1>}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc201)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc201)
        %2 = "ttnn.reshape"(%1) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc669)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc669)
        return %2 : tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<512x768xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___visual_projection_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg1: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_post_layernorm_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg2: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_post_layernorm_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg3: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg4: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg5: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg6: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg7: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg8: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg9: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg10: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg11: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg12: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg13: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg14: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg15: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg16: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg17: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg18: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg19: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg20: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg21: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg22: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg23: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg24: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg25: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg26: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg27: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg28: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg29: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg30: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg31: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg32: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg33: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg34: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg35: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg36: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg37: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg38: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg39: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg40: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg41: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg42: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg43: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg44: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg45: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg46: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg47: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg48: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg49: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg50: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg51: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg52: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg53: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg54: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg55: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg56: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg57: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg58: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg59: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg60: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg61: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg62: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg63: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg64: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg65: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg66: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg67: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg68: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg69: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg70: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg71: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg72: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg73: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg74: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg75: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg76: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg77: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg78: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg79: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg80: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg81: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg82: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg83: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg84: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg85: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg86: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg87: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg88: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg89: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg90: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg91: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg92: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg93: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg94: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg95: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg96: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg97: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg98: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg99: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg100: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg101: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg102: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg103: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg104: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg105: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg106: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg107: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg108: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg109: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg110: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg111: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg112: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg113: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg114: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg115: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg116: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg117: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg118: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg119: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg120: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg121: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg122: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg123: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg124: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg125: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg126: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg127: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg128: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg129: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg130: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg131: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg132: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg133: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg134: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg135: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg136: tensor<768x3072xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg137: tensor<3072xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg138: tensor<3072x768xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg139: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg140: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg141: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg142: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg143: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg144: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg145: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg146: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg147: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_pre_layrnorm_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg148: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_pre_layrnorm_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg149: tensor<1x50xsi32, #ttnn_layout18> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_embeddings_position_ids"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg150: tensor<50x768xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_embeddings_position_embedding_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg151: tensor<768x3x32x32xbf16, #ttnn_layout23> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___vision_model_embeddings_patch_embedding_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg152: tensor<2x3x224x224xbf16, #ttnn_layout31> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg153: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_embeddings_class_embedding"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg154: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg155: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg156: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg157: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg158: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg159: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg160: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg161: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg162: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg163: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg164: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg165: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg166: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg167: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg168: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg169: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg170: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg171: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg172: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg173: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg174: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg175: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg176: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg177: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg178: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg179: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg180: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg181: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg182: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg183: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg184: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg185: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg186: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg187: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg188: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg189: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg190: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg191: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg192: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg193: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg194: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg195: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg196: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg197: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg198: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg199: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg200: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg201: tensor<768x768xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data")) -> (tensor<2x512xbf16, #ttnn_layout32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<2x50x768xbf16, #ttnn_layout33> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<2x1xbf16, #ttnn_layout> loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg178]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg178) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg59]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg59) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %3 = ttcore.load_cached(@main_const_eval_3, [%arg145]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg145) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg137]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg137) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %5 = ttcore.load_cached(@main_const_eval_5, [%arg20]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %6 = ttcore.load_cached(@main_const_eval_6, [%arg127]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg127) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %7 = ttcore.load_cached(@main_const_eval_7, [%arg31]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg31) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %8 = ttcore.load_cached(@main_const_eval_8, [%arg116]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg116) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %9 = ttcore.load_cached(@main_const_eval_9, [%arg123]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg123) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %10 = ttcore.load_cached(@main_const_eval_10, [%arg153]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<2x768x1xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%arg153) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %11 = ttcore.load_cached(@main_const_eval_11, [%arg186]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg186) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %12 = ttcore.load_cached(@main_const_eval_12, [%arg190]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg190) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %13 = ttcore.load_cached(@main_const_eval_13, [%arg188]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg188) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %14 = ttcore.load_cached(@main_const_eval_14, [%arg3]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %15 = ttcore.load_cached(@main_const_eval_15, [%arg125]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg125) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %16 = ttcore.load_cached(@main_const_eval_16, [%arg21]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg21) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %17 = ttcore.load_cached(@main_const_eval_17, [%arg156]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg156) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %18 = ttcore.load_cached(@main_const_eval_18, [%arg194]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg194) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %19 = ttcore.load_cached(@main_const_eval_19, [%arg15]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %20 = ttcore.load_cached(@main_const_eval_20, [%arg131]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg131) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %21 = ttcore.load_cached(@main_const_eval_21, [%arg111]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg111) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %22 = ttcore.load_cached(@main_const_eval_22, [%arg27]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg27) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %23 = ttcore.load_cached(@main_const_eval_23, [%arg97]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg97) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %24 = ttcore.load_cached(@main_const_eval_24, [%arg77]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg77) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %25 = ttcore.load_cached(@main_const_eval_25, [%arg62]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg62) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %26 = ttcore.load_cached(@main_const_eval_26, [%arg110]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg110) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %27 = ttcore.load_cached(@main_const_eval_27, [%arg141]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg141) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %28 = ttcore.load_cached(@main_const_eval_28, [%arg65]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg65) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %29 = ttcore.load_cached(@main_const_eval_29, [%arg79]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg79) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %30 = ttcore.load_cached(@main_const_eval_30, [%arg192]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg192) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %31 = ttcore.load_cached(@main_const_eval_31, [%arg128]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg128) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %32 = ttcore.load_cached(@main_const_eval_32, [%arg80]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg80) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %33 = ttcore.load_cached(@main_const_eval_33, [%arg117]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg117) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %34 = ttcore.load_cached(@main_const_eval_34, [%arg115]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg115) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %35 = ttcore.load_cached(@main_const_eval_35, [%arg135]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg135) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %36 = ttcore.load_cached(@main_const_eval_36, [%arg168]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg168) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %37 = ttcore.load_cached(@main_const_eval_37, [%arg172]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg172) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %38 = ttcore.load_cached(@main_const_eval_38, [%arg75]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg75) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %39 = ttcore.load_cached(@main_const_eval_39, [%arg174]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg174) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %40 = ttcore.load_cached(@main_const_eval_40, [%arg13]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %41 = ttcore.load_cached(@main_const_eval_41, []) : () -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc)
        %42 = ttcore.load_cached(@main_const_eval_42, [%arg50]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg50) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %43 = ttcore.load_cached(@main_const_eval_43, [%arg23]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg23) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %44 = ttcore.load_cached(@main_const_eval_44, [%arg85]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg85) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %45 = ttcore.load_cached(@main_const_eval_45, [%arg98]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg98) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %46 = ttcore.load_cached(@main_const_eval_46, [%arg2]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %47 = ttcore.load_cached(@main_const_eval_47, [%arg45]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg45) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %48 = ttcore.load_cached(@main_const_eval_48, [%arg43]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg43) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %49 = ttcore.load_cached(@main_const_eval_49, [%arg8]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %50 = ttcore.load_cached(@main_const_eval_50, [%arg19]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %51 = ttcore.load_cached(@main_const_eval_51, []) : () -> tensor<2xbf16, #ttnn_layout16> loc(#loc)
        %52 = ttcore.load_cached(@main_const_eval_52, [%arg83]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg83) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %53 = ttcore.load_cached(@main_const_eval_53, [%arg1]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %54 = ttcore.load_cached(@main_const_eval_54, [%arg17]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %55 = ttcore.load_cached(@main_const_eval_55, [%arg14]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %56 = ttcore.load_cached(@main_const_eval_56, [%arg198]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg198) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %57:25 = ttcore.load_cached(@main_const_eval_57, []) : () -> (tensor<2x50x1xbf16, #ttnn_layout17>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) loc(#loc)
        %58 = ttcore.load_cached(@main_const_eval_58, [%arg122]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg122) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %59 = ttcore.load_cached(@main_const_eval_59, [%arg184]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg184) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %60 = ttcore.load_cached(@main_const_eval_60, [%arg35]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg35) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %61 = ttcore.load_cached(@main_const_eval_61, [%arg148]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg148) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %62 = ttcore.load_cached(@main_const_eval_62, [%arg166]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg166) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %63 = ttcore.load_cached(@main_const_eval_63, [%arg61]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg61) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %64 = ttcore.load_cached(@main_const_eval_64, [%arg160]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg160) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %65 = ttcore.load_cached(@main_const_eval_65, [%arg71]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg71) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %66 = ttcore.load_cached(@main_const_eval_66, [%arg92]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg92) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %67 = ttcore.load_cached(@main_const_eval_67, [%arg53]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg53) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %68 = ttcore.load_cached(@main_const_eval_68, [%arg55]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg55) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %69 = ttcore.load_cached(@main_const_eval_69, [%arg140]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg140) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %70 = ttcore.load_cached(@main_const_eval_70, [%arg68]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg68) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %71 = ttcore.load_cached(@main_const_eval_71, [%arg9]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %72 = ttcore.load_cached(@main_const_eval_72, [%arg104]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg104) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %73 = ttcore.load_cached(@main_const_eval_73, [%arg25]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg25) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %74 = ttcore.load_cached(@main_const_eval_74, [%arg133]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg133) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %75 = ttcore.load_cached(@main_const_eval_75, [%arg86]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg86) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %76 = ttcore.load_cached(@main_const_eval_76, [%arg103]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg103) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %77 = ttcore.load_cached(@main_const_eval_77, [%arg176]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg176) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %78 = ttcore.load_cached(@main_const_eval_78, [%arg57]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg57) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %79 = ttcore.load_cached(@main_const_eval_79, [%arg49]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg49) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %80 = ttcore.load_cached(@main_const_eval_80, [%arg119]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg119) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %81 = ttcore.load_cached(@main_const_eval_81, [%arg91]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg91) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %82 = ttcore.load_cached(@main_const_eval_82, [%arg149, %arg150]) : (tensor<1x50xsi32, #ttnn_layout18>, tensor<50x768xbf16, #ttnn_layout19>) -> tensor<2x768x50xbf16, #ttnn_layout20> loc(#loc)
        "ttnn.deallocate"(%arg150) <{force = false}> : (tensor<50x768xbf16, #ttnn_layout19>) -> () loc(#loc)
        "ttnn.deallocate"(%arg149) <{force = false}> : (tensor<1x50xsi32, #ttnn_layout18>) -> () loc(#loc)
        %83 = ttcore.load_cached(@main_const_eval_83, [%arg74]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg74) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %84 = ttcore.load_cached(@main_const_eval_84, [%arg170]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg170) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %85 = ttcore.load_cached(@main_const_eval_85, [%arg73]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg73) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %86 = ttcore.load_cached(@main_const_eval_86, [%arg5]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %87 = ttcore.load_cached(@main_const_eval_87, [%arg67]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg67) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %88 = ttcore.load_cached(@main_const_eval_88, [%arg180]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg180) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %89 = ttcore.load_cached(@main_const_eval_89, [%arg105]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg105) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %90 = ttcore.load_cached(@main_const_eval_90, [%arg93]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg93) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %91 = ttcore.load_cached(@main_const_eval_91, [%arg99]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg99) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %92 = ttcore.load_cached(@main_const_eval_92, [%arg63]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg63) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %93 = ttcore.load_cached(@main_const_eval_93, [%arg109]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg109) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %94 = ttcore.load_cached(@main_const_eval_94, [%arg26]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg26) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %95 = ttcore.load_cached(@main_const_eval_95, [%arg146]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg146) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %96 = ttcore.load_cached(@main_const_eval_96, [%arg200]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg200) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %97 = ttcore.load_cached(@main_const_eval_97, [%arg113]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg113) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %98 = ttcore.load_cached(@main_const_eval_98, [%arg129]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg129) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %99 = ttcore.load_cached(@main_const_eval_99, [%arg47]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg47) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %100 = ttcore.load_cached(@main_const_eval_100, [%arg164]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg164) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %101 = ttcore.load_cached(@main_const_eval_101, [%arg158]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg158) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %102 = ttcore.load_cached(@main_const_eval_102, [%arg33]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg33) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %103 = ttcore.load_cached(@main_const_eval_103, [%arg196]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg196) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %104 = ttcore.load_cached(@main_const_eval_104, [%arg147]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x1x768xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg147) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %105 = ttcore.load_cached(@main_const_eval_105, [%arg134]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg134) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %106 = ttcore.load_cached(@main_const_eval_106, [%arg44]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg44) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %107 = ttcore.load_cached(@main_const_eval_107, [%arg37]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg37) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %108 = ttcore.load_cached(@main_const_eval_108, [%arg162]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg162) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %109 = ttcore.load_cached(@main_const_eval_109, [%arg7]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %110 = ttcore.load_cached(@main_const_eval_110, [%arg39]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg39) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %111 = ttcore.load_cached(@main_const_eval_111, [%arg41]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg41) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %112 = ttcore.load_cached(@main_const_eval_112, [%arg143]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg143) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %113 = ttcore.load_cached(@main_const_eval_113, [%arg81]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg81) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %114 = ttcore.load_cached(@main_const_eval_114, [%arg121]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg121) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %115 = ttcore.load_cached(@main_const_eval_115, [%arg151]) : (tensor<768x3x32x32xbf16, #ttnn_layout23>) -> tensor<1x1x3072x768xbf16, #ttnn_layout24> loc(#loc)
        "ttnn.deallocate"(%arg151) <{force = false}> : (tensor<768x3x32x32xbf16, #ttnn_layout23>) -> () loc(#loc)
        %116 = ttcore.load_cached(@main_const_eval_116, [%arg139]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg139) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %117 = ttcore.load_cached(@main_const_eval_117, [%arg11]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %118 = ttcore.load_cached(@main_const_eval_118, [%arg89]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg89) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %119:12 = ttcore.load_cached(@main_const_eval_119, []) : () -> (tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>, tensor<100x3072xbf16, #ttnn_layout25>) loc(#loc)
        %120 = ttcore.load_cached(@main_const_eval_120, [%arg101]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg101) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %121 = ttcore.load_cached(@main_const_eval_121, [%arg56]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg56) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %122 = ttcore.load_cached(@main_const_eval_122, [%arg154]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg154) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %123 = ttcore.load_cached(@main_const_eval_123, [%arg32]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg32) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %124 = ttcore.load_cached(@main_const_eval_124, [%arg38]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<1x768xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg38) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %125 = ttcore.load_cached(@main_const_eval_125, [%arg107]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg107) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %126 = ttcore.load_cached(@main_const_eval_126, [%arg29]) : (tensor<3072xbf16, #ttnn_layout10>) -> tensor<1x3072xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg29) <{force = false}> : (tensor<3072xbf16, #ttnn_layout10>) -> () loc(#loc)
        %127 = ttcore.load_cached(@main_const_eval_127, [%arg87]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg87) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %128 = ttcore.load_cached(@main_const_eval_128, []) : () -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc)
        %129 = ttcore.load_cached(@main_const_eval_129, [%arg95]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg95) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %130 = ttcore.load_cached(@main_const_eval_130, [%arg182]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg182) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %131 = ttcore.load_cached(@main_const_eval_131, [%arg69]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg69) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %132 = ttcore.load_cached(@main_const_eval_132, [%arg51]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg51) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %133 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %134 = "ttnn.permute"(%arg152) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x3x224x224xbf16, #ttnn_layout31>) -> tensor<2x224x224x3xbf16, #ttnn_layout34> loc(#loc670)
        "ttnn.deallocate"(%arg152) <{force = false}> : (tensor<2x3x224x224xbf16, #ttnn_layout31>) -> () loc(#loc670)
        %135 = "ttnn.reshape"(%134) <{shape = [1 : i32, 1 : i32, 100352 : i32, 3 : i32]}> : (tensor<2x224x224x3xbf16, #ttnn_layout34>) -> tensor<1x1x100352x3xbf16, #ttnn_layout35> loc(#loc1041)
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<2x224x224x3xbf16, #ttnn_layout34>) -> () loc(#loc1041)
        %136 = "ttnn.to_layout"(%135) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x100352x3xbf16, #ttnn_layout35>) -> tensor<1x1x100352x3xbf16, #ttnn_layout36> loc(#loc180)
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x1x100352x3xbf16, #ttnn_layout35>) -> () loc(#loc180)
        %137 = "ttnn.conv2d"(%136, %115, %133) <{batch_size = 2 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 32, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 3 : i32, input_height = 224 : i32, input_width = 224 : i32, kernel_size = array<i32: 32, 32>, out_channels = 768 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 32, 32>}> : (tensor<1x1x100352x3xbf16, #ttnn_layout36>, tensor<1x1x3072x768xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x98x768xbf16, #ttnn_layout37> loc(#loc180)
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x1x100352x3xbf16, #ttnn_layout36>) -> () loc(#loc180)
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x1x3072x768xbf16, #ttnn_layout24>) -> () loc(#loc180)
        %138 = "ttnn.reshape"(%137) <{shape = [2 : i32, 7 : i32, 7 : i32, 768 : i32]}> : (tensor<1x1x98x768xbf16, #ttnn_layout37>) -> tensor<2x7x7x768xbf16, #ttnn_layout38> loc(#loc671)
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x1x98x768xbf16, #ttnn_layout37>) -> () loc(#loc671)
        %139 = "ttnn.permute"(%138) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<2x7x7x768xbf16, #ttnn_layout38>) -> tensor<2x768x7x7xbf16, #ttnn_layout39> loc(#loc180)
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<2x7x7x768xbf16, #ttnn_layout38>) -> () loc(#loc180)
        %140 = "ttnn.reshape"(%139) <{shape = [2 : i32, 768 : i32, 49 : i32]}> : (tensor<2x768x7x7xbf16, #ttnn_layout39>) -> tensor<2x768x49xbf16, #ttnn_layout20> loc(#loc203)
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<2x768x7x7xbf16, #ttnn_layout39>) -> () loc(#loc203)
        %141 = "ttnn.concat"(%10, %140) <{dim = 2 : si32}> : (tensor<2x768x1xbf16, #ttnn_layout13>, tensor<2x768x49xbf16, #ttnn_layout20>) -> tensor<2x768x50xbf16, #ttnn_layout20> loc(#loc15)
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<2x768x49xbf16, #ttnn_layout20>) -> () loc(#loc15)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<2x768x1xbf16, #ttnn_layout13>) -> () loc(#loc15)
        %142 = "ttnn.add"(%141, %82) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x768x50xbf16, #ttnn_layout20>, tensor<2x768x50xbf16, #ttnn_layout20>) -> tensor<2x768x50xbf16, #ttnn_layout40> loc(#loc138)
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<2x768x50xbf16, #ttnn_layout20>) -> () loc(#loc138)
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<2x768x50xbf16, #ttnn_layout20>) -> () loc(#loc138)
        %143 = "ttnn.to_memory_config"(%142) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x768x50xbf16, #ttnn_layout40>) -> tensor<2x768x50xbf16, #ttnn_layout41> loc(#loc672)
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<2x768x50xbf16, #ttnn_layout40>) -> () loc(#loc672)
        %144 = "ttnn.permute"(%143) <{permutation = array<i64: 0, 2, 1>}> : (tensor<2x768x50xbf16, #ttnn_layout41>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc138)
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<2x768x50xbf16, #ttnn_layout41>) -> () loc(#loc138)
        %145 = "ttnn.sum"(%144) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc204)
        %146 = "ttnn.multiply"(%145, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc204)
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc204)
        %147 = "ttnn.to_memory_config"(%146) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc673)
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc673)
        %148 = "ttnn.reshape"(%147) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc204)
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc204)
        %149 = "ttnn.neg"(%148) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc674)
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc674)
        %150 = "ttnn.add"(%144, %149) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout8>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc205)
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc205)
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc205)
        %151 = "ttnn.to_memory_config"(%150) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc675)
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc675)
        %152 = "ttnn.multiply"(%151, %151) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc204)
        %153 = "ttnn.to_memory_config"(%152) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc673)
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc673)
        %154 = "ttnn.sum"(%153) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc204)
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc204)
        %155 = "ttnn.multiply"(%154, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc204)
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc204)
        %156 = "ttnn.to_memory_config"(%155) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc673)
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc673)
        %157 = "ttnn.reshape"(%156) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc204)
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc204)
        %158 = "ttnn.add"(%157, %57#0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x1xbf16, #ttnn_layout17>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout44> loc(#loc169)
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc169)
        "ttnn.deallocate"(%57#0) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc169)
        %159 = "ttnn.to_memory_config"(%158) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x1xbf16, #ttnn_layout44>) -> tensor<2x50x1xbf16, #ttnn_layout45> loc(#loc676)
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout44>) -> () loc(#loc676)
        %160 = "ttnn.rsqrt"(%159) : (tensor<2x50x1xbf16, #ttnn_layout45>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc206)
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout45>) -> () loc(#loc206)
        %161 = "ttnn.multiply"(%151, %160) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc111)
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc111)
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc111)
        %162 = "ttnn.multiply"(%161, %61) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout43>, tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc111)
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc111)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc111)
        %163 = "ttnn.add"(%162, %104) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout43>, tensor<1x1x768xbf16, #ttnn_layout12>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc169)
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc169)
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x1x768xbf16, #ttnn_layout12>) -> () loc(#loc169)
        %164 = "ttnn.to_memory_config"(%163) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc676)
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc676)
        %165 = "ttnn.sum"(%164) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc80)
        %166 = "ttnn.multiply"(%165, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc80)
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc80)
        %167 = "ttnn.to_memory_config"(%166) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc677)
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc677)
        %168 = "ttnn.reshape"(%167) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc80)
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc80)
        %169 = "ttnn.neg"(%168) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc678)
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc678)
        %170 = "ttnn.add"(%164, %169) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc207)
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc207)
        %171 = "ttnn.to_memory_config"(%170) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc679)
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc679)
        %172 = "ttnn.reshape"(%171) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1198)
        %173 = "ttnn.multiply"(%171, %171) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc80)
        "ttnn.deallocate"(%171) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc80)
        %174 = "ttnn.to_memory_config"(%173) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc677)
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc677)
        %175 = "ttnn.sum"(%174) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc80)
        "ttnn.deallocate"(%174) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc80)
        %176 = "ttnn.multiply"(%175, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc80)
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc80)
        %177 = "ttnn.add"(%176, %57#1) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc208)
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc208)
        "ttnn.deallocate"(%57#1) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc208)
        %178 = "ttnn.to_memory_config"(%177) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc680)
        "ttnn.deallocate"(%177) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc680)
        %179 = "ttnn.rsqrt"(%178) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc209)
        "ttnn.deallocate"(%178) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc209)
        %180 = "ttnn.reshape"(%179) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc209)
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc209)
        %181 = "ttnn.multiply"(%172, %180) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc210)
        "ttnn.deallocate"(%180) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc210)
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc210)
        %182 = "ttnn.multiply"(%181, %95) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc210)
        "ttnn.deallocate"(%181) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc210)
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc210)
        %183 = "ttnn.add"(%182, %3) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc208)
        "ttnn.deallocate"(%182) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc208)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc208)
        %184 = "ttnn.to_memory_config"(%183) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc680)
        %185 = "ttnn.matmul"(%183, %arg157) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc681)
        "ttnn.deallocate"(%183) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc681)
        "ttnn.deallocate"(%arg157) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc681)
        %186 = "ttnn.add"(%185, %17) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc682)
        "ttnn.deallocate"(%185) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc682)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc682)
        %187 = "ttnn.to_memory_config"(%186) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1043)
        "ttnn.deallocate"(%186) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1043)
        %188 = "ttnn.reshape"(%187) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc30)
        "ttnn.deallocate"(%187) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc30)
        %189 = "ttnn.permute"(%188) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc30)
        "ttnn.deallocate"(%188) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc30)
        %190 = "ttnn.reshape"(%189) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc30)
        "ttnn.deallocate"(%189) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc30)
        %191 = "ttnn.to_memory_config"(%184) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1044)
        %192 = "ttnn.matmul"(%191, %arg155) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc683)
        "ttnn.deallocate"(%191) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc683)
        "ttnn.deallocate"(%arg155) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc683)
        %193 = "ttnn.add"(%192, %122) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc684)
        "ttnn.deallocate"(%192) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc684)
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc684)
        %194 = "ttnn.to_memory_config"(%193) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1045)
        "ttnn.deallocate"(%193) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1045)
        %195 = "ttnn.reshape"(%194) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc189)
        "ttnn.deallocate"(%194) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc189)
        %196 = "ttnn.permute"(%195) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc189)
        "ttnn.deallocate"(%195) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc189)
        %197 = "ttnn.reshape"(%196) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc189)
        "ttnn.deallocate"(%196) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc189)
        %198 = "ttnn.to_memory_config"(%190) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc685)
        "ttnn.deallocate"(%190) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc685)
        %199 = "ttnn.matmul"(%198, %197) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc211)
        "ttnn.deallocate"(%198) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc211)
        "ttnn.deallocate"(%197) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc211)
        %200 = "ttnn.to_memory_config"(%199) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc686)
        "ttnn.deallocate"(%199) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc686)
        %201 = "ttnn.reshape"(%200) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc212)
        "ttnn.deallocate"(%200) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc212)
        %202 = "ttnn.multiply"(%201, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc213)
        "ttnn.deallocate"(%201) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc213)
        %203 = "ttnn.typecast"(%202) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc214)
        "ttnn.deallocate"(%202) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc214)
        %204 = "ttnn.softmax"(%203) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc215)
        "ttnn.deallocate"(%203) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc215)
        %205 = "ttnn.typecast"(%204) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc216)
        "ttnn.deallocate"(%204) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc216)
        %206 = "ttnn.reshape"(%205) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc216)
        "ttnn.deallocate"(%205) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc216)
        %207 = "ttnn.to_memory_config"(%184) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1046)
        "ttnn.deallocate"(%184) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1046)
        %208 = "ttnn.matmul"(%207, %arg144) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc687)
        "ttnn.deallocate"(%207) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc687)
        "ttnn.deallocate"(%arg144) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc687)
        %209 = "ttnn.add"(%208, %112) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc688)
        "ttnn.deallocate"(%208) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc688)
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc688)
        %210 = "ttnn.to_memory_config"(%209) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1047)
        "ttnn.deallocate"(%209) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1047)
        %211 = "ttnn.reshape"(%210) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc178)
        "ttnn.deallocate"(%210) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc178)
        %212 = "ttnn.permute"(%211) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc178)
        "ttnn.deallocate"(%211) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc178)
        %213 = "ttnn.reshape"(%212) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc178)
        "ttnn.deallocate"(%212) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc178)
        %214 = "ttnn.to_memory_config"(%206) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc689)
        "ttnn.deallocate"(%206) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc689)
        %215 = "ttnn.matmul"(%214, %213) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc217)
        "ttnn.deallocate"(%214) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc217)
        "ttnn.deallocate"(%213) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc217)
        %216 = "ttnn.to_memory_config"(%215) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc690)
        "ttnn.deallocate"(%215) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc690)
        %217 = "ttnn.reshape"(%216) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc218)
        "ttnn.deallocate"(%216) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc218)
        %218 = "ttnn.concatenate_heads"(%217) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc219)
        "ttnn.deallocate"(%217) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc219)
        %219 = "ttnn.reshape"(%218) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc219)
        "ttnn.deallocate"(%218) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc219)
        %220 = "ttnn.to_memory_config"(%219) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1048)
        "ttnn.deallocate"(%219) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1048)
        %221 = "ttnn.matmul"(%220, %arg142) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc691)
        "ttnn.deallocate"(%220) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc691)
        "ttnn.deallocate"(%arg142) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc691)
        %222 = "ttnn.add"(%221, %27) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc692)
        "ttnn.deallocate"(%221) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc692)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc692)
        %223 = "ttnn.to_memory_config"(%222) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1049)
        "ttnn.deallocate"(%222) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1049)
        %224 = "ttnn.reshape"(%223) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc44)
        "ttnn.deallocate"(%223) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc44)
        %225 = "ttnn.add"(%164, %224) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc220)
        "ttnn.deallocate"(%224) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc220)
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc220)
        %226 = "ttnn.to_memory_config"(%225) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc693)
        "ttnn.deallocate"(%225) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc693)
        %227 = "ttnn.sum"(%226) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc81)
        %228 = "ttnn.multiply"(%227, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc81)
        "ttnn.deallocate"(%227) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc81)
        %229 = "ttnn.to_memory_config"(%228) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc694)
        "ttnn.deallocate"(%228) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc694)
        %230 = "ttnn.reshape"(%229) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc81)
        "ttnn.deallocate"(%229) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc81)
        %231 = "ttnn.neg"(%230) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc695)
        "ttnn.deallocate"(%230) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc695)
        %232 = "ttnn.add"(%226, %231) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc221)
        "ttnn.deallocate"(%231) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc221)
        %233 = "ttnn.to_memory_config"(%232) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc696)
        "ttnn.deallocate"(%232) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc696)
        %234 = "ttnn.reshape"(%233) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1199)
        %235 = "ttnn.multiply"(%233, %233) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc81)
        "ttnn.deallocate"(%233) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc81)
        %236 = "ttnn.to_memory_config"(%235) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc694)
        "ttnn.deallocate"(%235) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc694)
        %237 = "ttnn.sum"(%236) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc81)
        "ttnn.deallocate"(%236) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc81)
        %238 = "ttnn.multiply"(%237, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc81)
        "ttnn.deallocate"(%237) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc81)
        %239 = "ttnn.add"(%238, %57#2) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc222)
        "ttnn.deallocate"(%238) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc222)
        "ttnn.deallocate"(%57#2) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc222)
        %240 = "ttnn.to_memory_config"(%239) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc697)
        "ttnn.deallocate"(%239) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc697)
        %241 = "ttnn.rsqrt"(%240) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc223)
        "ttnn.deallocate"(%240) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc223)
        %242 = "ttnn.reshape"(%241) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc223)
        "ttnn.deallocate"(%241) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc223)
        %243 = "ttnn.multiply"(%234, %242) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc224)
        "ttnn.deallocate"(%242) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc224)
        "ttnn.deallocate"(%234) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc224)
        %244 = "ttnn.multiply"(%243, %69) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc224)
        "ttnn.deallocate"(%243) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc224)
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc224)
        %245 = "ttnn.add"(%244, %116) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc222)
        "ttnn.deallocate"(%244) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc222)
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc222)
        %246 = "ttnn.matmul"(%245, %arg138) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc698)
        "ttnn.deallocate"(%245) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc698)
        "ttnn.deallocate"(%arg138) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc698)
        %247 = "ttnn.add"(%246, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc699)
        "ttnn.deallocate"(%246) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc699)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc699)
        %248 = "ttnn.to_memory_config"(%247) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1051)
        "ttnn.deallocate"(%247) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1051)
        %249 = "ttnn.multiply"(%248, %119#0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc226)
        "ttnn.deallocate"(%119#0) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc226)
        %250 = "ttnn.to_memory_config"(%249) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc700)
        "ttnn.deallocate"(%249) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc700)
        %251 = "ttnn.sigmoid"(%250) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc227)
        "ttnn.deallocate"(%250) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc227)
        %252 = "ttnn.multiply"(%248, %251) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc226)
        "ttnn.deallocate"(%251) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc226)
        "ttnn.deallocate"(%248) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc226)
        %253 = "ttnn.matmul"(%252, %arg136) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc701)
        "ttnn.deallocate"(%252) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc701)
        "ttnn.deallocate"(%arg136) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc701)
        %254 = "ttnn.add"(%253, %35) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc702)
        "ttnn.deallocate"(%253) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc702)
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc702)
        %255 = "ttnn.to_memory_config"(%254) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1052)
        "ttnn.deallocate"(%254) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1052)
        %256 = "ttnn.reshape"(%255) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc51)
        "ttnn.deallocate"(%255) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc51)
        %257 = "ttnn.add"(%226, %256) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc228)
        "ttnn.deallocate"(%256) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc228)
        "ttnn.deallocate"(%226) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc228)
        %258 = "ttnn.to_memory_config"(%257) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc703)
        "ttnn.deallocate"(%257) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc703)
        %259 = "ttnn.sum"(%258) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc82)
        %260 = "ttnn.multiply"(%259, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc82)
        "ttnn.deallocate"(%259) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc82)
        %261 = "ttnn.to_memory_config"(%260) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc704)
        "ttnn.deallocate"(%260) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc704)
        %262 = "ttnn.reshape"(%261) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc82)
        "ttnn.deallocate"(%261) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc82)
        %263 = "ttnn.neg"(%262) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc705)
        "ttnn.deallocate"(%262) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc705)
        %264 = "ttnn.add"(%258, %263) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc229)
        "ttnn.deallocate"(%263) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc229)
        %265 = "ttnn.to_memory_config"(%264) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc706)
        "ttnn.deallocate"(%264) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc706)
        %266 = "ttnn.reshape"(%265) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1200)
        %267 = "ttnn.multiply"(%265, %265) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc82)
        "ttnn.deallocate"(%265) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc82)
        %268 = "ttnn.to_memory_config"(%267) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc704)
        "ttnn.deallocate"(%267) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc704)
        %269 = "ttnn.sum"(%268) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc82)
        "ttnn.deallocate"(%268) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc82)
        %270 = "ttnn.multiply"(%269, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc82)
        "ttnn.deallocate"(%269) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc82)
        %271 = "ttnn.add"(%270, %57#3) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc230)
        "ttnn.deallocate"(%270) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc230)
        "ttnn.deallocate"(%57#3) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc230)
        %272 = "ttnn.to_memory_config"(%271) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc707)
        "ttnn.deallocate"(%271) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc707)
        %273 = "ttnn.rsqrt"(%272) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc231)
        "ttnn.deallocate"(%272) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc231)
        %274 = "ttnn.reshape"(%273) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc231)
        "ttnn.deallocate"(%273) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc231)
        %275 = "ttnn.multiply"(%266, %274) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc232)
        "ttnn.deallocate"(%274) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc232)
        "ttnn.deallocate"(%266) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc232)
        %276 = "ttnn.multiply"(%275, %105) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc232)
        "ttnn.deallocate"(%275) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc232)
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc232)
        %277 = "ttnn.add"(%276, %74) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc230)
        "ttnn.deallocate"(%276) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc230)
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc230)
        %278 = "ttnn.to_memory_config"(%277) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc707)
        %279 = "ttnn.matmul"(%277, %arg161) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc708)
        "ttnn.deallocate"(%277) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc708)
        "ttnn.deallocate"(%arg161) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc708)
        %280 = "ttnn.add"(%279, %64) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc709)
        "ttnn.deallocate"(%279) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc709)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc709)
        %281 = "ttnn.to_memory_config"(%280) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1054)
        "ttnn.deallocate"(%280) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1054)
        %282 = "ttnn.reshape"(%281) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc117)
        "ttnn.deallocate"(%281) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc117)
        %283 = "ttnn.permute"(%282) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc117)
        "ttnn.deallocate"(%282) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc117)
        %284 = "ttnn.reshape"(%283) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc117)
        "ttnn.deallocate"(%283) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc117)
        %285 = "ttnn.to_memory_config"(%278) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1055)
        %286 = "ttnn.matmul"(%285, %arg159) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc710)
        "ttnn.deallocate"(%285) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc710)
        "ttnn.deallocate"(%arg159) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc710)
        %287 = "ttnn.add"(%286, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc711)
        "ttnn.deallocate"(%286) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc711)
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc711)
        %288 = "ttnn.to_memory_config"(%287) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1056)
        "ttnn.deallocate"(%287) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1056)
        %289 = "ttnn.reshape"(%288) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc164)
        "ttnn.deallocate"(%288) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc164)
        %290 = "ttnn.permute"(%289) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc164)
        "ttnn.deallocate"(%289) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc164)
        %291 = "ttnn.reshape"(%290) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc164)
        "ttnn.deallocate"(%290) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc164)
        %292 = "ttnn.to_memory_config"(%284) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc712)
        "ttnn.deallocate"(%284) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc712)
        %293 = "ttnn.matmul"(%292, %291) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc233)
        "ttnn.deallocate"(%292) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc233)
        "ttnn.deallocate"(%291) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc233)
        %294 = "ttnn.to_memory_config"(%293) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc713)
        "ttnn.deallocate"(%293) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc713)
        %295 = "ttnn.reshape"(%294) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc234)
        "ttnn.deallocate"(%294) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc234)
        %296 = "ttnn.multiply"(%295, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc235)
        "ttnn.deallocate"(%295) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc235)
        %297 = "ttnn.typecast"(%296) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc236)
        "ttnn.deallocate"(%296) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc236)
        %298 = "ttnn.softmax"(%297) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc237)
        "ttnn.deallocate"(%297) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc237)
        %299 = "ttnn.typecast"(%298) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc238)
        "ttnn.deallocate"(%298) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc238)
        %300 = "ttnn.reshape"(%299) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc238)
        "ttnn.deallocate"(%299) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc238)
        %301 = "ttnn.to_memory_config"(%278) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1057)
        "ttnn.deallocate"(%278) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1057)
        %302 = "ttnn.matmul"(%301, %arg132) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc714)
        "ttnn.deallocate"(%301) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc714)
        "ttnn.deallocate"(%arg132) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc714)
        %303 = "ttnn.add"(%302, %20) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc715)
        "ttnn.deallocate"(%302) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc715)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc715)
        %304 = "ttnn.to_memory_config"(%303) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1058)
        "ttnn.deallocate"(%303) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1058)
        %305 = "ttnn.reshape"(%304) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc37)
        "ttnn.deallocate"(%304) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc37)
        %306 = "ttnn.permute"(%305) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc37)
        "ttnn.deallocate"(%305) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc37)
        %307 = "ttnn.reshape"(%306) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc37)
        "ttnn.deallocate"(%306) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc37)
        %308 = "ttnn.to_memory_config"(%300) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc716)
        "ttnn.deallocate"(%300) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc716)
        %309 = "ttnn.matmul"(%308, %307) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc239)
        "ttnn.deallocate"(%308) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc239)
        "ttnn.deallocate"(%307) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc239)
        %310 = "ttnn.to_memory_config"(%309) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc717)
        "ttnn.deallocate"(%309) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc717)
        %311 = "ttnn.reshape"(%310) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc240)
        "ttnn.deallocate"(%310) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc240)
        %312 = "ttnn.concatenate_heads"(%311) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc241)
        "ttnn.deallocate"(%311) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc241)
        %313 = "ttnn.reshape"(%312) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc241)
        "ttnn.deallocate"(%312) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc241)
        %314 = "ttnn.to_memory_config"(%313) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1059)
        "ttnn.deallocate"(%313) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1059)
        %315 = "ttnn.matmul"(%314, %arg130) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc718)
        "ttnn.deallocate"(%314) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc718)
        "ttnn.deallocate"(%arg130) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc718)
        %316 = "ttnn.add"(%315, %98) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc719)
        "ttnn.deallocate"(%315) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc719)
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc719)
        %317 = "ttnn.to_memory_config"(%316) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1060)
        "ttnn.deallocate"(%316) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1060)
        %318 = "ttnn.reshape"(%317) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc155)
        "ttnn.deallocate"(%317) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc155)
        %319 = "ttnn.add"(%258, %318) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc242)
        "ttnn.deallocate"(%318) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc242)
        "ttnn.deallocate"(%258) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc242)
        %320 = "ttnn.to_memory_config"(%319) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc720)
        "ttnn.deallocate"(%319) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc720)
        %321 = "ttnn.sum"(%320) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc83)
        %322 = "ttnn.multiply"(%321, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc83)
        "ttnn.deallocate"(%321) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc83)
        %323 = "ttnn.to_memory_config"(%322) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc721)
        "ttnn.deallocate"(%322) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc721)
        %324 = "ttnn.reshape"(%323) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc83)
        "ttnn.deallocate"(%323) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc83)
        %325 = "ttnn.neg"(%324) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc722)
        "ttnn.deallocate"(%324) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc722)
        %326 = "ttnn.add"(%320, %325) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc243)
        "ttnn.deallocate"(%325) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc243)
        %327 = "ttnn.to_memory_config"(%326) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc723)
        "ttnn.deallocate"(%326) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc723)
        %328 = "ttnn.reshape"(%327) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1201)
        %329 = "ttnn.multiply"(%327, %327) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc83)
        "ttnn.deallocate"(%327) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc83)
        %330 = "ttnn.to_memory_config"(%329) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc721)
        "ttnn.deallocate"(%329) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc721)
        %331 = "ttnn.sum"(%330) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc83)
        "ttnn.deallocate"(%330) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc83)
        %332 = "ttnn.multiply"(%331, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc83)
        "ttnn.deallocate"(%331) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc83)
        %333 = "ttnn.add"(%332, %57#4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc244)
        "ttnn.deallocate"(%332) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc244)
        "ttnn.deallocate"(%57#4) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc244)
        %334 = "ttnn.to_memory_config"(%333) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc724)
        "ttnn.deallocate"(%333) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc724)
        %335 = "ttnn.rsqrt"(%334) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc245)
        "ttnn.deallocate"(%334) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc245)
        %336 = "ttnn.reshape"(%335) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc245)
        "ttnn.deallocate"(%335) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc245)
        %337 = "ttnn.multiply"(%328, %336) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc246)
        "ttnn.deallocate"(%336) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc246)
        "ttnn.deallocate"(%328) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc246)
        %338 = "ttnn.multiply"(%337, %31) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc246)
        "ttnn.deallocate"(%337) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc246)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc246)
        %339 = "ttnn.add"(%338, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc244)
        "ttnn.deallocate"(%338) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc244)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc244)
        %340 = "ttnn.matmul"(%339, %arg126) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc725)
        "ttnn.deallocate"(%339) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc725)
        "ttnn.deallocate"(%arg126) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc725)
        %341 = "ttnn.add"(%340, %15) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc726)
        "ttnn.deallocate"(%340) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc726)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc726)
        %342 = "ttnn.to_memory_config"(%341) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1062)
        "ttnn.deallocate"(%341) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1062)
        %343 = "ttnn.multiply"(%342, %119#1) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc248)
        "ttnn.deallocate"(%119#1) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc248)
        %344 = "ttnn.to_memory_config"(%343) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc727)
        "ttnn.deallocate"(%343) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc727)
        %345 = "ttnn.sigmoid"(%344) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc249)
        "ttnn.deallocate"(%344) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc249)
        %346 = "ttnn.multiply"(%342, %345) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc248)
        "ttnn.deallocate"(%345) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc248)
        "ttnn.deallocate"(%342) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc248)
        %347 = "ttnn.matmul"(%346, %arg124) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc728)
        "ttnn.deallocate"(%346) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc728)
        "ttnn.deallocate"(%arg124) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc728)
        %348 = "ttnn.add"(%347, %9) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc729)
        "ttnn.deallocate"(%347) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc729)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc729)
        %349 = "ttnn.to_memory_config"(%348) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1063)
        "ttnn.deallocate"(%348) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1063)
        %350 = "ttnn.reshape"(%349) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc13)
        "ttnn.deallocate"(%349) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc13)
        %351 = "ttnn.add"(%320, %350) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc250)
        "ttnn.deallocate"(%350) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc250)
        "ttnn.deallocate"(%320) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc250)
        %352 = "ttnn.to_memory_config"(%351) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc730)
        "ttnn.deallocate"(%351) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc730)
        %353 = "ttnn.sum"(%352) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc84)
        %354 = "ttnn.multiply"(%353, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc84)
        "ttnn.deallocate"(%353) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc84)
        %355 = "ttnn.to_memory_config"(%354) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc731)
        "ttnn.deallocate"(%354) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc731)
        %356 = "ttnn.reshape"(%355) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc84)
        "ttnn.deallocate"(%355) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc84)
        %357 = "ttnn.neg"(%356) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc732)
        "ttnn.deallocate"(%356) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc732)
        %358 = "ttnn.add"(%352, %357) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc251)
        "ttnn.deallocate"(%357) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc251)
        %359 = "ttnn.to_memory_config"(%358) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc733)
        "ttnn.deallocate"(%358) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc733)
        %360 = "ttnn.reshape"(%359) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1202)
        %361 = "ttnn.multiply"(%359, %359) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc84)
        "ttnn.deallocate"(%359) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc84)
        %362 = "ttnn.to_memory_config"(%361) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc731)
        "ttnn.deallocate"(%361) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc731)
        %363 = "ttnn.sum"(%362) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc84)
        "ttnn.deallocate"(%362) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc84)
        %364 = "ttnn.multiply"(%363, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc84)
        "ttnn.deallocate"(%363) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc84)
        %365 = "ttnn.add"(%364, %57#5) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc252)
        "ttnn.deallocate"(%364) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc252)
        "ttnn.deallocate"(%57#5) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc252)
        %366 = "ttnn.to_memory_config"(%365) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc734)
        "ttnn.deallocate"(%365) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc734)
        %367 = "ttnn.rsqrt"(%366) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc253)
        "ttnn.deallocate"(%366) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc253)
        %368 = "ttnn.reshape"(%367) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc253)
        "ttnn.deallocate"(%367) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc253)
        %369 = "ttnn.multiply"(%360, %368) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc254)
        "ttnn.deallocate"(%368) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc254)
        "ttnn.deallocate"(%360) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc254)
        %370 = "ttnn.multiply"(%369, %58) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc254)
        "ttnn.deallocate"(%369) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc254)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc254)
        %371 = "ttnn.add"(%370, %114) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc252)
        "ttnn.deallocate"(%370) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc252)
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc252)
        %372 = "ttnn.to_memory_config"(%371) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc734)
        %373 = "ttnn.matmul"(%371, %arg165) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc735)
        "ttnn.deallocate"(%371) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc735)
        "ttnn.deallocate"(%arg165) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc735)
        %374 = "ttnn.add"(%373, %100) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc736)
        "ttnn.deallocate"(%373) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc736)
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc736)
        %375 = "ttnn.to_memory_config"(%374) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1065)
        "ttnn.deallocate"(%374) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1065)
        %376 = "ttnn.reshape"(%375) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc161)
        "ttnn.deallocate"(%375) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc161)
        %377 = "ttnn.permute"(%376) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc161)
        "ttnn.deallocate"(%376) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc161)
        %378 = "ttnn.reshape"(%377) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc161)
        "ttnn.deallocate"(%377) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc161)
        %379 = "ttnn.to_memory_config"(%372) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1066)
        %380 = "ttnn.matmul"(%379, %arg163) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc737)
        "ttnn.deallocate"(%379) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc737)
        "ttnn.deallocate"(%arg163) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc737)
        %381 = "ttnn.add"(%380, %108) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc738)
        "ttnn.deallocate"(%380) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc738)
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc738)
        %382 = "ttnn.to_memory_config"(%381) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1067)
        "ttnn.deallocate"(%381) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1067)
        %383 = "ttnn.reshape"(%382) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc173)
        "ttnn.deallocate"(%382) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc173)
        %384 = "ttnn.permute"(%383) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc173)
        "ttnn.deallocate"(%383) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc173)
        %385 = "ttnn.reshape"(%384) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc173)
        "ttnn.deallocate"(%384) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc173)
        %386 = "ttnn.to_memory_config"(%378) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc739)
        "ttnn.deallocate"(%378) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc739)
        %387 = "ttnn.matmul"(%386, %385) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc255)
        "ttnn.deallocate"(%386) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc255)
        "ttnn.deallocate"(%385) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc255)
        %388 = "ttnn.to_memory_config"(%387) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc740)
        "ttnn.deallocate"(%387) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc740)
        %389 = "ttnn.reshape"(%388) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc256)
        "ttnn.deallocate"(%388) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc256)
        %390 = "ttnn.multiply"(%389, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc257)
        "ttnn.deallocate"(%389) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc257)
        %391 = "ttnn.typecast"(%390) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc258)
        "ttnn.deallocate"(%390) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc258)
        %392 = "ttnn.softmax"(%391) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc259)
        "ttnn.deallocate"(%391) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc259)
        %393 = "ttnn.typecast"(%392) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc260)
        "ttnn.deallocate"(%392) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc260)
        %394 = "ttnn.reshape"(%393) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc260)
        "ttnn.deallocate"(%393) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc260)
        %395 = "ttnn.to_memory_config"(%372) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1068)
        "ttnn.deallocate"(%372) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1068)
        %396 = "ttnn.matmul"(%395, %arg120) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc741)
        "ttnn.deallocate"(%395) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc741)
        "ttnn.deallocate"(%arg120) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc741)
        %397 = "ttnn.add"(%396, %80) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc742)
        "ttnn.deallocate"(%396) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc742)
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc742)
        %398 = "ttnn.to_memory_config"(%397) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1069)
        "ttnn.deallocate"(%397) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1069)
        %399 = "ttnn.reshape"(%398) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc136)
        "ttnn.deallocate"(%398) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc136)
        %400 = "ttnn.permute"(%399) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc136)
        "ttnn.deallocate"(%399) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc136)
        %401 = "ttnn.reshape"(%400) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc136)
        "ttnn.deallocate"(%400) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc136)
        %402 = "ttnn.to_memory_config"(%394) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc743)
        "ttnn.deallocate"(%394) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc743)
        %403 = "ttnn.matmul"(%402, %401) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc261)
        "ttnn.deallocate"(%402) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc261)
        "ttnn.deallocate"(%401) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc261)
        %404 = "ttnn.to_memory_config"(%403) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc744)
        "ttnn.deallocate"(%403) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc744)
        %405 = "ttnn.reshape"(%404) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc262)
        "ttnn.deallocate"(%404) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc262)
        %406 = "ttnn.concatenate_heads"(%405) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc263)
        "ttnn.deallocate"(%405) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc263)
        %407 = "ttnn.reshape"(%406) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc263)
        "ttnn.deallocate"(%406) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc263)
        %408 = "ttnn.to_memory_config"(%407) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1070)
        "ttnn.deallocate"(%407) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1070)
        %409 = "ttnn.matmul"(%408, %arg118) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc745)
        "ttnn.deallocate"(%408) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc745)
        "ttnn.deallocate"(%arg118) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc745)
        %410 = "ttnn.add"(%409, %33) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc746)
        "ttnn.deallocate"(%409) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc746)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc746)
        %411 = "ttnn.to_memory_config"(%410) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1071)
        "ttnn.deallocate"(%410) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1071)
        %412 = "ttnn.reshape"(%411) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc50)
        "ttnn.deallocate"(%411) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc50)
        %413 = "ttnn.add"(%352, %412) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc264)
        "ttnn.deallocate"(%412) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc264)
        "ttnn.deallocate"(%352) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc264)
        %414 = "ttnn.to_memory_config"(%413) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc747)
        "ttnn.deallocate"(%413) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc747)
        %415 = "ttnn.sum"(%414) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc85)
        %416 = "ttnn.multiply"(%415, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc85)
        "ttnn.deallocate"(%415) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc85)
        %417 = "ttnn.to_memory_config"(%416) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc748)
        "ttnn.deallocate"(%416) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc748)
        %418 = "ttnn.reshape"(%417) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc85)
        "ttnn.deallocate"(%417) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc85)
        %419 = "ttnn.neg"(%418) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc749)
        "ttnn.deallocate"(%418) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc749)
        %420 = "ttnn.add"(%414, %419) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc265)
        "ttnn.deallocate"(%419) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc265)
        %421 = "ttnn.to_memory_config"(%420) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc750)
        "ttnn.deallocate"(%420) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc750)
        %422 = "ttnn.reshape"(%421) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1203)
        %423 = "ttnn.multiply"(%421, %421) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc85)
        "ttnn.deallocate"(%421) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc85)
        %424 = "ttnn.to_memory_config"(%423) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc748)
        "ttnn.deallocate"(%423) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc748)
        %425 = "ttnn.sum"(%424) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc85)
        "ttnn.deallocate"(%424) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc85)
        %426 = "ttnn.multiply"(%425, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc85)
        "ttnn.deallocate"(%425) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc85)
        %427 = "ttnn.add"(%426, %57#6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc266)
        "ttnn.deallocate"(%426) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc266)
        "ttnn.deallocate"(%57#6) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc266)
        %428 = "ttnn.to_memory_config"(%427) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc751)
        "ttnn.deallocate"(%427) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc751)
        %429 = "ttnn.rsqrt"(%428) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc267)
        "ttnn.deallocate"(%428) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc267)
        %430 = "ttnn.reshape"(%429) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc267)
        "ttnn.deallocate"(%429) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc267)
        %431 = "ttnn.multiply"(%422, %430) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc268)
        "ttnn.deallocate"(%430) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc268)
        "ttnn.deallocate"(%422) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc268)
        %432 = "ttnn.multiply"(%431, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc268)
        "ttnn.deallocate"(%431) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc268)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc268)
        %433 = "ttnn.add"(%432, %34) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc266)
        "ttnn.deallocate"(%432) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc266)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc266)
        %434 = "ttnn.matmul"(%433, %arg114) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc752)
        "ttnn.deallocate"(%433) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc752)
        "ttnn.deallocate"(%arg114) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc752)
        %435 = "ttnn.add"(%434, %97) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc753)
        "ttnn.deallocate"(%434) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc753)
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc753)
        %436 = "ttnn.to_memory_config"(%435) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1073)
        "ttnn.deallocate"(%435) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1073)
        %437 = "ttnn.multiply"(%436, %119#2) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc270)
        "ttnn.deallocate"(%119#2) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc270)
        %438 = "ttnn.to_memory_config"(%437) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc754)
        "ttnn.deallocate"(%437) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc754)
        %439 = "ttnn.sigmoid"(%438) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc271)
        "ttnn.deallocate"(%438) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc271)
        %440 = "ttnn.multiply"(%436, %439) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc270)
        "ttnn.deallocate"(%439) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc270)
        "ttnn.deallocate"(%436) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc270)
        %441 = "ttnn.matmul"(%440, %arg112) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc755)
        "ttnn.deallocate"(%440) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc755)
        "ttnn.deallocate"(%arg112) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc755)
        %442 = "ttnn.add"(%441, %21) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc756)
        "ttnn.deallocate"(%441) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc756)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc756)
        %443 = "ttnn.to_memory_config"(%442) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1074)
        "ttnn.deallocate"(%442) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1074)
        %444 = "ttnn.reshape"(%443) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc38)
        "ttnn.deallocate"(%443) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc38)
        %445 = "ttnn.add"(%414, %444) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc272)
        "ttnn.deallocate"(%444) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc272)
        "ttnn.deallocate"(%414) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc272)
        %446 = "ttnn.to_memory_config"(%445) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc757)
        "ttnn.deallocate"(%445) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc757)
        %447 = "ttnn.sum"(%446) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc86)
        %448 = "ttnn.multiply"(%447, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc86)
        "ttnn.deallocate"(%447) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc86)
        %449 = "ttnn.to_memory_config"(%448) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc758)
        "ttnn.deallocate"(%448) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc758)
        %450 = "ttnn.reshape"(%449) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc86)
        "ttnn.deallocate"(%449) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc86)
        %451 = "ttnn.neg"(%450) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc759)
        "ttnn.deallocate"(%450) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc759)
        %452 = "ttnn.add"(%446, %451) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc273)
        "ttnn.deallocate"(%451) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc273)
        %453 = "ttnn.to_memory_config"(%452) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc760)
        "ttnn.deallocate"(%452) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc760)
        %454 = "ttnn.reshape"(%453) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1204)
        %455 = "ttnn.multiply"(%453, %453) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc86)
        "ttnn.deallocate"(%453) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc86)
        %456 = "ttnn.to_memory_config"(%455) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc758)
        "ttnn.deallocate"(%455) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc758)
        %457 = "ttnn.sum"(%456) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc86)
        "ttnn.deallocate"(%456) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc86)
        %458 = "ttnn.multiply"(%457, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc86)
        "ttnn.deallocate"(%457) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc86)
        %459 = "ttnn.add"(%458, %57#7) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc274)
        "ttnn.deallocate"(%458) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc274)
        "ttnn.deallocate"(%57#7) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc274)
        %460 = "ttnn.to_memory_config"(%459) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc761)
        "ttnn.deallocate"(%459) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc761)
        %461 = "ttnn.rsqrt"(%460) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc275)
        "ttnn.deallocate"(%460) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc275)
        %462 = "ttnn.reshape"(%461) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc275)
        "ttnn.deallocate"(%461) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc275)
        %463 = "ttnn.multiply"(%454, %462) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc276)
        "ttnn.deallocate"(%462) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc276)
        "ttnn.deallocate"(%454) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc276)
        %464 = "ttnn.multiply"(%463, %26) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc276)
        "ttnn.deallocate"(%463) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc276)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc276)
        %465 = "ttnn.add"(%464, %93) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc274)
        "ttnn.deallocate"(%464) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc274)
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc274)
        %466 = "ttnn.to_memory_config"(%465) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc761)
        %467 = "ttnn.matmul"(%465, %arg169) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc762)
        "ttnn.deallocate"(%465) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc762)
        "ttnn.deallocate"(%arg169) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc762)
        %468 = "ttnn.add"(%467, %36) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc763)
        "ttnn.deallocate"(%467) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc763)
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc763)
        %469 = "ttnn.to_memory_config"(%468) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1076)
        "ttnn.deallocate"(%468) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1076)
        %470 = "ttnn.reshape"(%469) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc54)
        "ttnn.deallocate"(%469) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc54)
        %471 = "ttnn.permute"(%470) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc54)
        "ttnn.deallocate"(%470) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc54)
        %472 = "ttnn.reshape"(%471) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc54)
        "ttnn.deallocate"(%471) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc54)
        %473 = "ttnn.to_memory_config"(%466) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1077)
        %474 = "ttnn.matmul"(%473, %arg167) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc764)
        "ttnn.deallocate"(%473) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc764)
        "ttnn.deallocate"(%arg167) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc764)
        %475 = "ttnn.add"(%474, %62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc765)
        "ttnn.deallocate"(%474) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc765)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc765)
        %476 = "ttnn.to_memory_config"(%475) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1078)
        "ttnn.deallocate"(%475) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1078)
        %477 = "ttnn.reshape"(%476) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc114)
        "ttnn.deallocate"(%476) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc114)
        %478 = "ttnn.permute"(%477) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc114)
        "ttnn.deallocate"(%477) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc114)
        %479 = "ttnn.reshape"(%478) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc114)
        "ttnn.deallocate"(%478) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc114)
        %480 = "ttnn.to_memory_config"(%472) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc766)
        "ttnn.deallocate"(%472) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc766)
        %481 = "ttnn.matmul"(%480, %479) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc277)
        "ttnn.deallocate"(%480) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc277)
        "ttnn.deallocate"(%479) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc277)
        %482 = "ttnn.to_memory_config"(%481) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc767)
        "ttnn.deallocate"(%481) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc767)
        %483 = "ttnn.reshape"(%482) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc278)
        "ttnn.deallocate"(%482) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc278)
        %484 = "ttnn.multiply"(%483, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc279)
        "ttnn.deallocate"(%483) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc279)
        %485 = "ttnn.typecast"(%484) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc280)
        "ttnn.deallocate"(%484) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc280)
        %486 = "ttnn.softmax"(%485) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc281)
        "ttnn.deallocate"(%485) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc281)
        %487 = "ttnn.typecast"(%486) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc282)
        "ttnn.deallocate"(%486) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc282)
        %488 = "ttnn.reshape"(%487) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc282)
        "ttnn.deallocate"(%487) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc282)
        %489 = "ttnn.to_memory_config"(%466) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1079)
        "ttnn.deallocate"(%466) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1079)
        %490 = "ttnn.matmul"(%489, %arg108) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc768)
        "ttnn.deallocate"(%489) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc768)
        "ttnn.deallocate"(%arg108) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc768)
        %491 = "ttnn.add"(%490, %125) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc769)
        "ttnn.deallocate"(%490) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc769)
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc769)
        %492 = "ttnn.to_memory_config"(%491) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1080)
        "ttnn.deallocate"(%491) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1080)
        %493 = "ttnn.reshape"(%492) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc192)
        "ttnn.deallocate"(%492) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc192)
        %494 = "ttnn.permute"(%493) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc192)
        "ttnn.deallocate"(%493) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc192)
        %495 = "ttnn.reshape"(%494) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc192)
        "ttnn.deallocate"(%494) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc192)
        %496 = "ttnn.to_memory_config"(%488) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc770)
        "ttnn.deallocate"(%488) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc770)
        %497 = "ttnn.matmul"(%496, %495) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc283)
        "ttnn.deallocate"(%496) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc283)
        "ttnn.deallocate"(%495) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc283)
        %498 = "ttnn.to_memory_config"(%497) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc771)
        "ttnn.deallocate"(%497) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc771)
        %499 = "ttnn.reshape"(%498) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc284)
        "ttnn.deallocate"(%498) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc284)
        %500 = "ttnn.concatenate_heads"(%499) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc285)
        "ttnn.deallocate"(%499) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc285)
        %501 = "ttnn.reshape"(%500) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc285)
        "ttnn.deallocate"(%500) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc285)
        %502 = "ttnn.to_memory_config"(%501) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1081)
        "ttnn.deallocate"(%501) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1081)
        %503 = "ttnn.matmul"(%502, %arg106) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc772)
        "ttnn.deallocate"(%502) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc772)
        "ttnn.deallocate"(%arg106) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc772)
        %504 = "ttnn.add"(%503, %89) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc773)
        "ttnn.deallocate"(%503) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc773)
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc773)
        %505 = "ttnn.to_memory_config"(%504) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1082)
        "ttnn.deallocate"(%504) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1082)
        %506 = "ttnn.reshape"(%505) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc147)
        "ttnn.deallocate"(%505) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc147)
        %507 = "ttnn.add"(%446, %506) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc286)
        "ttnn.deallocate"(%506) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc286)
        "ttnn.deallocate"(%446) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc286)
        %508 = "ttnn.to_memory_config"(%507) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc774)
        "ttnn.deallocate"(%507) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc774)
        %509 = "ttnn.sum"(%508) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc87)
        %510 = "ttnn.multiply"(%509, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc87)
        "ttnn.deallocate"(%509) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc87)
        %511 = "ttnn.to_memory_config"(%510) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc775)
        "ttnn.deallocate"(%510) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc775)
        %512 = "ttnn.reshape"(%511) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc87)
        "ttnn.deallocate"(%511) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc87)
        %513 = "ttnn.neg"(%512) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc776)
        "ttnn.deallocate"(%512) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc776)
        %514 = "ttnn.add"(%508, %513) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc287)
        "ttnn.deallocate"(%513) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc287)
        %515 = "ttnn.to_memory_config"(%514) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc777)
        "ttnn.deallocate"(%514) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc777)
        %516 = "ttnn.reshape"(%515) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1205)
        %517 = "ttnn.multiply"(%515, %515) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc87)
        "ttnn.deallocate"(%515) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc87)
        %518 = "ttnn.to_memory_config"(%517) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc775)
        "ttnn.deallocate"(%517) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc775)
        %519 = "ttnn.sum"(%518) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc87)
        "ttnn.deallocate"(%518) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc87)
        %520 = "ttnn.multiply"(%519, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc87)
        "ttnn.deallocate"(%519) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc87)
        %521 = "ttnn.add"(%520, %57#8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc288)
        "ttnn.deallocate"(%520) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc288)
        "ttnn.deallocate"(%57#8) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc288)
        %522 = "ttnn.to_memory_config"(%521) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc778)
        "ttnn.deallocate"(%521) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc778)
        %523 = "ttnn.rsqrt"(%522) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc289)
        "ttnn.deallocate"(%522) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc289)
        %524 = "ttnn.reshape"(%523) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc289)
        "ttnn.deallocate"(%523) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc289)
        %525 = "ttnn.multiply"(%516, %524) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc290)
        "ttnn.deallocate"(%524) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc290)
        "ttnn.deallocate"(%516) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc290)
        %526 = "ttnn.multiply"(%525, %72) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc290)
        "ttnn.deallocate"(%525) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc290)
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc290)
        %527 = "ttnn.add"(%526, %76) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc288)
        "ttnn.deallocate"(%526) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc288)
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc288)
        %528 = "ttnn.matmul"(%527, %arg102) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc779)
        "ttnn.deallocate"(%527) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc779)
        "ttnn.deallocate"(%arg102) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc779)
        %529 = "ttnn.add"(%528, %120) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc780)
        "ttnn.deallocate"(%528) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc780)
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc780)
        %530 = "ttnn.to_memory_config"(%529) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1084)
        "ttnn.deallocate"(%529) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1084)
        %531 = "ttnn.multiply"(%530, %119#3) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc292)
        "ttnn.deallocate"(%119#3) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc292)
        %532 = "ttnn.to_memory_config"(%531) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc781)
        "ttnn.deallocate"(%531) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc781)
        %533 = "ttnn.sigmoid"(%532) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc293)
        "ttnn.deallocate"(%532) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc293)
        %534 = "ttnn.multiply"(%530, %533) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc292)
        "ttnn.deallocate"(%533) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc292)
        "ttnn.deallocate"(%530) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc292)
        %535 = "ttnn.matmul"(%534, %arg100) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc782)
        "ttnn.deallocate"(%534) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc782)
        "ttnn.deallocate"(%arg100) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc782)
        %536 = "ttnn.add"(%535, %91) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc783)
        "ttnn.deallocate"(%535) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc783)
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc783)
        %537 = "ttnn.to_memory_config"(%536) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1085)
        "ttnn.deallocate"(%536) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1085)
        %538 = "ttnn.reshape"(%537) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc149)
        "ttnn.deallocate"(%537) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc149)
        %539 = "ttnn.add"(%508, %538) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc294)
        "ttnn.deallocate"(%538) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc294)
        "ttnn.deallocate"(%508) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc294)
        %540 = "ttnn.to_memory_config"(%539) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc784)
        "ttnn.deallocate"(%539) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc784)
        %541 = "ttnn.sum"(%540) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc88)
        %542 = "ttnn.multiply"(%541, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc88)
        "ttnn.deallocate"(%541) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc88)
        %543 = "ttnn.to_memory_config"(%542) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc785)
        "ttnn.deallocate"(%542) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc785)
        %544 = "ttnn.reshape"(%543) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc88)
        "ttnn.deallocate"(%543) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc88)
        %545 = "ttnn.neg"(%544) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc786)
        "ttnn.deallocate"(%544) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc786)
        %546 = "ttnn.add"(%540, %545) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc295)
        "ttnn.deallocate"(%545) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc295)
        %547 = "ttnn.to_memory_config"(%546) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc787)
        "ttnn.deallocate"(%546) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc787)
        %548 = "ttnn.reshape"(%547) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1206)
        %549 = "ttnn.multiply"(%547, %547) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc88)
        "ttnn.deallocate"(%547) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc88)
        %550 = "ttnn.to_memory_config"(%549) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc785)
        "ttnn.deallocate"(%549) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc785)
        %551 = "ttnn.sum"(%550) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc88)
        "ttnn.deallocate"(%550) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc88)
        %552 = "ttnn.multiply"(%551, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc88)
        "ttnn.deallocate"(%551) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc88)
        %553 = "ttnn.add"(%552, %57#9) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc296)
        "ttnn.deallocate"(%552) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc296)
        "ttnn.deallocate"(%57#9) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc296)
        %554 = "ttnn.to_memory_config"(%553) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc788)
        "ttnn.deallocate"(%553) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc788)
        %555 = "ttnn.rsqrt"(%554) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc297)
        "ttnn.deallocate"(%554) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc297)
        %556 = "ttnn.reshape"(%555) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc297)
        "ttnn.deallocate"(%555) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc297)
        %557 = "ttnn.multiply"(%548, %556) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc298)
        "ttnn.deallocate"(%556) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc298)
        "ttnn.deallocate"(%548) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc298)
        %558 = "ttnn.multiply"(%557, %45) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc298)
        "ttnn.deallocate"(%557) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc298)
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc298)
        %559 = "ttnn.add"(%558, %23) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc296)
        "ttnn.deallocate"(%558) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc296)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc296)
        %560 = "ttnn.to_memory_config"(%559) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc788)
        %561 = "ttnn.matmul"(%559, %arg173) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc789)
        "ttnn.deallocate"(%559) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc789)
        "ttnn.deallocate"(%arg173) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc789)
        %562 = "ttnn.add"(%561, %37) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc790)
        "ttnn.deallocate"(%561) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc790)
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc790)
        %563 = "ttnn.to_memory_config"(%562) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1087)
        "ttnn.deallocate"(%562) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1087)
        %564 = "ttnn.reshape"(%563) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc57)
        "ttnn.deallocate"(%563) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc57)
        %565 = "ttnn.permute"(%564) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc57)
        "ttnn.deallocate"(%564) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc57)
        %566 = "ttnn.reshape"(%565) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc57)
        "ttnn.deallocate"(%565) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc57)
        %567 = "ttnn.to_memory_config"(%560) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1088)
        %568 = "ttnn.matmul"(%567, %arg171) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc791)
        "ttnn.deallocate"(%567) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc791)
        "ttnn.deallocate"(%arg171) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc791)
        %569 = "ttnn.add"(%568, %84) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc792)
        "ttnn.deallocate"(%568) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc792)
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc792)
        %570 = "ttnn.to_memory_config"(%569) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1089)
        "ttnn.deallocate"(%569) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1089)
        %571 = "ttnn.reshape"(%570) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc142)
        "ttnn.deallocate"(%570) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc142)
        %572 = "ttnn.permute"(%571) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc142)
        "ttnn.deallocate"(%571) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc142)
        %573 = "ttnn.reshape"(%572) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc142)
        "ttnn.deallocate"(%572) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc142)
        %574 = "ttnn.to_memory_config"(%566) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc793)
        "ttnn.deallocate"(%566) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc793)
        %575 = "ttnn.matmul"(%574, %573) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc299)
        "ttnn.deallocate"(%574) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc299)
        "ttnn.deallocate"(%573) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc299)
        %576 = "ttnn.to_memory_config"(%575) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc794)
        "ttnn.deallocate"(%575) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc794)
        %577 = "ttnn.reshape"(%576) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc300)
        "ttnn.deallocate"(%576) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc300)
        %578 = "ttnn.multiply"(%577, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc301)
        "ttnn.deallocate"(%577) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc301)
        %579 = "ttnn.typecast"(%578) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc302)
        "ttnn.deallocate"(%578) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc302)
        %580 = "ttnn.softmax"(%579) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc303)
        "ttnn.deallocate"(%579) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc303)
        %581 = "ttnn.typecast"(%580) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc304)
        "ttnn.deallocate"(%580) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc304)
        %582 = "ttnn.reshape"(%581) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc304)
        "ttnn.deallocate"(%581) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc304)
        %583 = "ttnn.to_memory_config"(%560) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1090)
        "ttnn.deallocate"(%560) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1090)
        %584 = "ttnn.matmul"(%583, %arg96) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc795)
        "ttnn.deallocate"(%583) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc795)
        "ttnn.deallocate"(%arg96) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc795)
        %585 = "ttnn.add"(%584, %129) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc796)
        "ttnn.deallocate"(%584) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc796)
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc796)
        %586 = "ttnn.to_memory_config"(%585) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1091)
        "ttnn.deallocate"(%585) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1091)
        %587 = "ttnn.reshape"(%586) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc196)
        "ttnn.deallocate"(%586) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc196)
        %588 = "ttnn.permute"(%587) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc196)
        "ttnn.deallocate"(%587) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc196)
        %589 = "ttnn.reshape"(%588) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc196)
        "ttnn.deallocate"(%588) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc196)
        %590 = "ttnn.to_memory_config"(%582) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc797)
        "ttnn.deallocate"(%582) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc797)
        %591 = "ttnn.matmul"(%590, %589) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc305)
        "ttnn.deallocate"(%590) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc305)
        "ttnn.deallocate"(%589) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc305)
        %592 = "ttnn.to_memory_config"(%591) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc798)
        "ttnn.deallocate"(%591) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc798)
        %593 = "ttnn.reshape"(%592) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc306)
        "ttnn.deallocate"(%592) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc306)
        %594 = "ttnn.concatenate_heads"(%593) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc307)
        "ttnn.deallocate"(%593) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc307)
        %595 = "ttnn.reshape"(%594) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc307)
        "ttnn.deallocate"(%594) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc307)
        %596 = "ttnn.to_memory_config"(%595) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1092)
        "ttnn.deallocate"(%595) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1092)
        %597 = "ttnn.matmul"(%596, %arg94) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc799)
        "ttnn.deallocate"(%596) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc799)
        "ttnn.deallocate"(%arg94) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc799)
        %598 = "ttnn.add"(%597, %90) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc800)
        "ttnn.deallocate"(%597) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc800)
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc800)
        %599 = "ttnn.to_memory_config"(%598) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1093)
        "ttnn.deallocate"(%598) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1093)
        %600 = "ttnn.reshape"(%599) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc148)
        "ttnn.deallocate"(%599) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc148)
        %601 = "ttnn.add"(%540, %600) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc308)
        "ttnn.deallocate"(%600) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc308)
        "ttnn.deallocate"(%540) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc308)
        %602 = "ttnn.to_memory_config"(%601) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc801)
        "ttnn.deallocate"(%601) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc801)
        %603 = "ttnn.sum"(%602) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc89)
        %604 = "ttnn.multiply"(%603, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc89)
        "ttnn.deallocate"(%603) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc89)
        %605 = "ttnn.to_memory_config"(%604) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc802)
        "ttnn.deallocate"(%604) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc802)
        %606 = "ttnn.reshape"(%605) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc89)
        "ttnn.deallocate"(%605) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc89)
        %607 = "ttnn.neg"(%606) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc803)
        "ttnn.deallocate"(%606) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc803)
        %608 = "ttnn.add"(%602, %607) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc309)
        "ttnn.deallocate"(%607) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc309)
        %609 = "ttnn.to_memory_config"(%608) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc804)
        "ttnn.deallocate"(%608) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc804)
        %610 = "ttnn.reshape"(%609) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1207)
        %611 = "ttnn.multiply"(%609, %609) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc89)
        "ttnn.deallocate"(%609) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc89)
        %612 = "ttnn.to_memory_config"(%611) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc802)
        "ttnn.deallocate"(%611) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc802)
        %613 = "ttnn.sum"(%612) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc89)
        "ttnn.deallocate"(%612) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc89)
        %614 = "ttnn.multiply"(%613, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc89)
        "ttnn.deallocate"(%613) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc89)
        %615 = "ttnn.add"(%614, %57#10) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc310)
        "ttnn.deallocate"(%614) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc310)
        "ttnn.deallocate"(%57#10) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc310)
        %616 = "ttnn.to_memory_config"(%615) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc805)
        "ttnn.deallocate"(%615) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc805)
        %617 = "ttnn.rsqrt"(%616) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc311)
        "ttnn.deallocate"(%616) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc311)
        %618 = "ttnn.reshape"(%617) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc311)
        "ttnn.deallocate"(%617) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc311)
        %619 = "ttnn.multiply"(%610, %618) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc312)
        "ttnn.deallocate"(%618) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc312)
        "ttnn.deallocate"(%610) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc312)
        %620 = "ttnn.multiply"(%619, %66) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc312)
        "ttnn.deallocate"(%619) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc312)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc312)
        %621 = "ttnn.add"(%620, %81) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc310)
        "ttnn.deallocate"(%620) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc310)
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc310)
        %622 = "ttnn.matmul"(%621, %arg90) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc806)
        "ttnn.deallocate"(%621) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc806)
        "ttnn.deallocate"(%arg90) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc806)
        %623 = "ttnn.add"(%622, %118) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc807)
        "ttnn.deallocate"(%622) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc807)
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc807)
        %624 = "ttnn.to_memory_config"(%623) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1095)
        "ttnn.deallocate"(%623) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1095)
        %625 = "ttnn.multiply"(%624, %119#4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc314)
        "ttnn.deallocate"(%119#4) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc314)
        %626 = "ttnn.to_memory_config"(%625) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc808)
        "ttnn.deallocate"(%625) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc808)
        %627 = "ttnn.sigmoid"(%626) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc315)
        "ttnn.deallocate"(%626) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc315)
        %628 = "ttnn.multiply"(%624, %627) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc314)
        "ttnn.deallocate"(%627) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc314)
        "ttnn.deallocate"(%624) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc314)
        %629 = "ttnn.matmul"(%628, %arg88) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc809)
        "ttnn.deallocate"(%628) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc809)
        "ttnn.deallocate"(%arg88) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc809)
        %630 = "ttnn.add"(%629, %127) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc810)
        "ttnn.deallocate"(%629) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc810)
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc810)
        %631 = "ttnn.to_memory_config"(%630) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1096)
        "ttnn.deallocate"(%630) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1096)
        %632 = "ttnn.reshape"(%631) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc193)
        "ttnn.deallocate"(%631) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc193)
        %633 = "ttnn.add"(%602, %632) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc316)
        "ttnn.deallocate"(%632) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc316)
        "ttnn.deallocate"(%602) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc316)
        %634 = "ttnn.to_memory_config"(%633) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc811)
        "ttnn.deallocate"(%633) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc811)
        %635 = "ttnn.sum"(%634) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc90)
        %636 = "ttnn.multiply"(%635, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc90)
        "ttnn.deallocate"(%635) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc90)
        %637 = "ttnn.to_memory_config"(%636) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc812)
        "ttnn.deallocate"(%636) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc812)
        %638 = "ttnn.reshape"(%637) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc90)
        "ttnn.deallocate"(%637) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc90)
        %639 = "ttnn.neg"(%638) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc813)
        "ttnn.deallocate"(%638) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc813)
        %640 = "ttnn.add"(%634, %639) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc317)
        "ttnn.deallocate"(%639) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc317)
        %641 = "ttnn.to_memory_config"(%640) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc814)
        "ttnn.deallocate"(%640) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc814)
        %642 = "ttnn.reshape"(%641) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1208)
        %643 = "ttnn.multiply"(%641, %641) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc90)
        "ttnn.deallocate"(%641) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc90)
        %644 = "ttnn.to_memory_config"(%643) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc812)
        "ttnn.deallocate"(%643) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc812)
        %645 = "ttnn.sum"(%644) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc90)
        "ttnn.deallocate"(%644) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc90)
        %646 = "ttnn.multiply"(%645, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc90)
        "ttnn.deallocate"(%645) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc90)
        %647 = "ttnn.add"(%646, %57#11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc318)
        "ttnn.deallocate"(%646) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc318)
        "ttnn.deallocate"(%57#11) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc318)
        %648 = "ttnn.to_memory_config"(%647) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc815)
        "ttnn.deallocate"(%647) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc815)
        %649 = "ttnn.rsqrt"(%648) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc319)
        "ttnn.deallocate"(%648) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc319)
        %650 = "ttnn.reshape"(%649) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc319)
        "ttnn.deallocate"(%649) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc319)
        %651 = "ttnn.multiply"(%642, %650) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc320)
        "ttnn.deallocate"(%650) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc320)
        "ttnn.deallocate"(%642) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc320)
        %652 = "ttnn.multiply"(%651, %75) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc320)
        "ttnn.deallocate"(%651) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc320)
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc320)
        %653 = "ttnn.add"(%652, %44) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc318)
        "ttnn.deallocate"(%652) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc318)
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc318)
        %654 = "ttnn.to_memory_config"(%653) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc815)
        %655 = "ttnn.matmul"(%653, %arg177) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc816)
        "ttnn.deallocate"(%653) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc816)
        "ttnn.deallocate"(%arg177) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc816)
        %656 = "ttnn.add"(%655, %77) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc817)
        "ttnn.deallocate"(%655) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc817)
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc817)
        %657 = "ttnn.to_memory_config"(%656) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1098)
        "ttnn.deallocate"(%656) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1098)
        %658 = "ttnn.reshape"(%657) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc132)
        "ttnn.deallocate"(%657) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc132)
        %659 = "ttnn.permute"(%658) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc132)
        "ttnn.deallocate"(%658) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc132)
        %660 = "ttnn.reshape"(%659) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc132)
        "ttnn.deallocate"(%659) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc132)
        %661 = "ttnn.to_memory_config"(%654) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1099)
        %662 = "ttnn.matmul"(%661, %arg175) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc818)
        "ttnn.deallocate"(%661) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc818)
        "ttnn.deallocate"(%arg175) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc818)
        %663 = "ttnn.add"(%662, %39) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc819)
        "ttnn.deallocate"(%662) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc819)
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc819)
        %664 = "ttnn.to_memory_config"(%663) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1100)
        "ttnn.deallocate"(%663) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1100)
        %665 = "ttnn.reshape"(%664) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc61)
        "ttnn.deallocate"(%664) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc61)
        %666 = "ttnn.permute"(%665) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc61)
        "ttnn.deallocate"(%665) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc61)
        %667 = "ttnn.reshape"(%666) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc61)
        "ttnn.deallocate"(%666) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc61)
        %668 = "ttnn.to_memory_config"(%660) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc820)
        "ttnn.deallocate"(%660) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc820)
        %669 = "ttnn.matmul"(%668, %667) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc321)
        "ttnn.deallocate"(%668) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc321)
        "ttnn.deallocate"(%667) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc321)
        %670 = "ttnn.to_memory_config"(%669) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc821)
        "ttnn.deallocate"(%669) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc821)
        %671 = "ttnn.reshape"(%670) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc322)
        "ttnn.deallocate"(%670) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc322)
        %672 = "ttnn.multiply"(%671, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc323)
        "ttnn.deallocate"(%671) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc323)
        %673 = "ttnn.typecast"(%672) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc324)
        "ttnn.deallocate"(%672) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc324)
        %674 = "ttnn.softmax"(%673) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc325)
        "ttnn.deallocate"(%673) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc325)
        %675 = "ttnn.typecast"(%674) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc326)
        "ttnn.deallocate"(%674) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc326)
        %676 = "ttnn.reshape"(%675) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc326)
        "ttnn.deallocate"(%675) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc326)
        %677 = "ttnn.to_memory_config"(%654) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1101)
        "ttnn.deallocate"(%654) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1101)
        %678 = "ttnn.matmul"(%677, %arg84) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc822)
        "ttnn.deallocate"(%677) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc822)
        "ttnn.deallocate"(%arg84) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc822)
        %679 = "ttnn.add"(%678, %52) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc823)
        "ttnn.deallocate"(%678) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc823)
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc823)
        %680 = "ttnn.to_memory_config"(%679) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1102)
        "ttnn.deallocate"(%679) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1102)
        %681 = "ttnn.reshape"(%680) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc74)
        "ttnn.deallocate"(%680) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc74)
        %682 = "ttnn.permute"(%681) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc74)
        "ttnn.deallocate"(%681) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc74)
        %683 = "ttnn.reshape"(%682) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc74)
        "ttnn.deallocate"(%682) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc74)
        %684 = "ttnn.to_memory_config"(%676) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc824)
        "ttnn.deallocate"(%676) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc824)
        %685 = "ttnn.matmul"(%684, %683) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc327)
        "ttnn.deallocate"(%684) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc327)
        "ttnn.deallocate"(%683) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc327)
        %686 = "ttnn.to_memory_config"(%685) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc825)
        "ttnn.deallocate"(%685) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc825)
        %687 = "ttnn.reshape"(%686) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc328)
        "ttnn.deallocate"(%686) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc328)
        %688 = "ttnn.concatenate_heads"(%687) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc329)
        "ttnn.deallocate"(%687) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc329)
        %689 = "ttnn.reshape"(%688) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc329)
        "ttnn.deallocate"(%688) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc329)
        %690 = "ttnn.to_memory_config"(%689) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1103)
        "ttnn.deallocate"(%689) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1103)
        %691 = "ttnn.matmul"(%690, %arg82) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc826)
        "ttnn.deallocate"(%690) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc826)
        "ttnn.deallocate"(%arg82) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc826)
        %692 = "ttnn.add"(%691, %113) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc827)
        "ttnn.deallocate"(%691) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc827)
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc827)
        %693 = "ttnn.to_memory_config"(%692) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1104)
        "ttnn.deallocate"(%692) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1104)
        %694 = "ttnn.reshape"(%693) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc179)
        "ttnn.deallocate"(%693) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc179)
        %695 = "ttnn.add"(%634, %694) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc330)
        "ttnn.deallocate"(%694) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc330)
        "ttnn.deallocate"(%634) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc330)
        %696 = "ttnn.to_memory_config"(%695) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc828)
        "ttnn.deallocate"(%695) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc828)
        %697 = "ttnn.sum"(%696) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc91)
        %698 = "ttnn.multiply"(%697, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc91)
        "ttnn.deallocate"(%697) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc91)
        %699 = "ttnn.to_memory_config"(%698) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc829)
        "ttnn.deallocate"(%698) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc829)
        %700 = "ttnn.reshape"(%699) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc91)
        "ttnn.deallocate"(%699) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc91)
        %701 = "ttnn.neg"(%700) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc830)
        "ttnn.deallocate"(%700) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc830)
        %702 = "ttnn.add"(%696, %701) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc331)
        "ttnn.deallocate"(%701) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc331)
        %703 = "ttnn.to_memory_config"(%702) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc831)
        "ttnn.deallocate"(%702) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc831)
        %704 = "ttnn.reshape"(%703) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1209)
        %705 = "ttnn.multiply"(%703, %703) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc91)
        "ttnn.deallocate"(%703) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc91)
        %706 = "ttnn.to_memory_config"(%705) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc829)
        "ttnn.deallocate"(%705) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc829)
        %707 = "ttnn.sum"(%706) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc91)
        "ttnn.deallocate"(%706) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc91)
        %708 = "ttnn.multiply"(%707, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc91)
        "ttnn.deallocate"(%707) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc91)
        %709 = "ttnn.add"(%708, %57#12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc332)
        "ttnn.deallocate"(%708) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc332)
        "ttnn.deallocate"(%57#12) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc332)
        %710 = "ttnn.to_memory_config"(%709) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc832)
        "ttnn.deallocate"(%709) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc832)
        %711 = "ttnn.rsqrt"(%710) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc333)
        "ttnn.deallocate"(%710) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc333)
        %712 = "ttnn.reshape"(%711) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc333)
        "ttnn.deallocate"(%711) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc333)
        %713 = "ttnn.multiply"(%704, %712) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc334)
        "ttnn.deallocate"(%712) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc334)
        "ttnn.deallocate"(%704) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc334)
        %714 = "ttnn.multiply"(%713, %32) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc334)
        "ttnn.deallocate"(%713) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc334)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc334)
        %715 = "ttnn.add"(%714, %29) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc332)
        "ttnn.deallocate"(%714) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc332)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc332)
        %716 = "ttnn.matmul"(%715, %arg78) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc833)
        "ttnn.deallocate"(%715) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc833)
        "ttnn.deallocate"(%arg78) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc833)
        %717 = "ttnn.add"(%716, %24) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc834)
        "ttnn.deallocate"(%716) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc834)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc834)
        %718 = "ttnn.to_memory_config"(%717) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1106)
        "ttnn.deallocate"(%717) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1106)
        %719 = "ttnn.multiply"(%718, %119#5) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc336)
        "ttnn.deallocate"(%119#5) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc336)
        %720 = "ttnn.to_memory_config"(%719) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc835)
        "ttnn.deallocate"(%719) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc835)
        %721 = "ttnn.sigmoid"(%720) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc337)
        "ttnn.deallocate"(%720) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc337)
        %722 = "ttnn.multiply"(%718, %721) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc336)
        "ttnn.deallocate"(%721) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc336)
        "ttnn.deallocate"(%718) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc336)
        %723 = "ttnn.matmul"(%722, %arg76) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc836)
        "ttnn.deallocate"(%722) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc836)
        "ttnn.deallocate"(%arg76) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc836)
        %724 = "ttnn.add"(%723, %38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc837)
        "ttnn.deallocate"(%723) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc837)
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc837)
        %725 = "ttnn.to_memory_config"(%724) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1107)
        "ttnn.deallocate"(%724) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1107)
        %726 = "ttnn.reshape"(%725) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc58)
        "ttnn.deallocate"(%725) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc58)
        %727 = "ttnn.add"(%696, %726) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc338)
        "ttnn.deallocate"(%726) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc338)
        "ttnn.deallocate"(%696) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc338)
        %728 = "ttnn.to_memory_config"(%727) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc838)
        "ttnn.deallocate"(%727) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc838)
        %729 = "ttnn.sum"(%728) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc92)
        %730 = "ttnn.multiply"(%729, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc92)
        "ttnn.deallocate"(%729) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc92)
        %731 = "ttnn.to_memory_config"(%730) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc839)
        "ttnn.deallocate"(%730) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc839)
        %732 = "ttnn.reshape"(%731) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc92)
        "ttnn.deallocate"(%731) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc92)
        %733 = "ttnn.neg"(%732) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc840)
        "ttnn.deallocate"(%732) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc840)
        %734 = "ttnn.add"(%728, %733) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc339)
        "ttnn.deallocate"(%733) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc339)
        %735 = "ttnn.to_memory_config"(%734) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc841)
        "ttnn.deallocate"(%734) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc841)
        %736 = "ttnn.reshape"(%735) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1210)
        %737 = "ttnn.multiply"(%735, %735) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc92)
        "ttnn.deallocate"(%735) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc92)
        %738 = "ttnn.to_memory_config"(%737) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc839)
        "ttnn.deallocate"(%737) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc839)
        %739 = "ttnn.sum"(%738) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc92)
        "ttnn.deallocate"(%738) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc92)
        %740 = "ttnn.multiply"(%739, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc92)
        "ttnn.deallocate"(%739) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc92)
        %741 = "ttnn.add"(%740, %57#13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc340)
        "ttnn.deallocate"(%740) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc340)
        "ttnn.deallocate"(%57#13) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc340)
        %742 = "ttnn.to_memory_config"(%741) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc842)
        "ttnn.deallocate"(%741) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc842)
        %743 = "ttnn.rsqrt"(%742) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc341)
        "ttnn.deallocate"(%742) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc341)
        %744 = "ttnn.reshape"(%743) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc341)
        "ttnn.deallocate"(%743) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc341)
        %745 = "ttnn.multiply"(%736, %744) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc342)
        "ttnn.deallocate"(%744) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc342)
        "ttnn.deallocate"(%736) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc342)
        %746 = "ttnn.multiply"(%745, %83) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc342)
        "ttnn.deallocate"(%745) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc342)
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc342)
        %747 = "ttnn.add"(%746, %85) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc340)
        "ttnn.deallocate"(%746) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc340)
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc340)
        %748 = "ttnn.to_memory_config"(%747) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc842)
        %749 = "ttnn.matmul"(%747, %arg181) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc843)
        "ttnn.deallocate"(%747) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc843)
        "ttnn.deallocate"(%arg181) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc843)
        %750 = "ttnn.add"(%749, %88) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc844)
        "ttnn.deallocate"(%749) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc844)
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc844)
        %751 = "ttnn.to_memory_config"(%750) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1109)
        "ttnn.deallocate"(%750) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1109)
        %752 = "ttnn.reshape"(%751) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc146)
        "ttnn.deallocate"(%751) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc146)
        %753 = "ttnn.permute"(%752) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc146)
        "ttnn.deallocate"(%752) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc146)
        %754 = "ttnn.reshape"(%753) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc146)
        "ttnn.deallocate"(%753) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc146)
        %755 = "ttnn.to_memory_config"(%748) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1110)
        %756 = "ttnn.matmul"(%755, %arg179) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc845)
        "ttnn.deallocate"(%755) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc845)
        "ttnn.deallocate"(%arg179) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc845)
        %757 = "ttnn.add"(%756, %1) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc846)
        "ttnn.deallocate"(%756) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc846)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc846)
        %758 = "ttnn.to_memory_config"(%757) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1111)
        "ttnn.deallocate"(%757) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1111)
        %759 = "ttnn.reshape"(%758) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc3)
        "ttnn.deallocate"(%758) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc3)
        %760 = "ttnn.permute"(%759) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc3)
        "ttnn.deallocate"(%759) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc3)
        %761 = "ttnn.reshape"(%760) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc3)
        "ttnn.deallocate"(%760) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc3)
        %762 = "ttnn.to_memory_config"(%754) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc847)
        "ttnn.deallocate"(%754) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc847)
        %763 = "ttnn.matmul"(%762, %761) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc343)
        "ttnn.deallocate"(%762) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc343)
        "ttnn.deallocate"(%761) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc343)
        %764 = "ttnn.to_memory_config"(%763) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc848)
        "ttnn.deallocate"(%763) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc848)
        %765 = "ttnn.reshape"(%764) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc344)
        "ttnn.deallocate"(%764) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc344)
        %766 = "ttnn.multiply"(%765, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc345)
        "ttnn.deallocate"(%765) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc345)
        %767 = "ttnn.typecast"(%766) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc346)
        "ttnn.deallocate"(%766) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc346)
        %768 = "ttnn.softmax"(%767) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc347)
        "ttnn.deallocate"(%767) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc347)
        %769 = "ttnn.typecast"(%768) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc348)
        "ttnn.deallocate"(%768) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc348)
        %770 = "ttnn.reshape"(%769) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc348)
        "ttnn.deallocate"(%769) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc348)
        %771 = "ttnn.to_memory_config"(%748) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1112)
        "ttnn.deallocate"(%748) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1112)
        %772 = "ttnn.matmul"(%771, %arg72) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc849)
        "ttnn.deallocate"(%771) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc849)
        "ttnn.deallocate"(%arg72) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc849)
        %773 = "ttnn.add"(%772, %65) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc850)
        "ttnn.deallocate"(%772) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc850)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc850)
        %774 = "ttnn.to_memory_config"(%773) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1113)
        "ttnn.deallocate"(%773) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1113)
        %775 = "ttnn.reshape"(%774) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc120)
        "ttnn.deallocate"(%774) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc120)
        %776 = "ttnn.permute"(%775) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc120)
        "ttnn.deallocate"(%775) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc120)
        %777 = "ttnn.reshape"(%776) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc120)
        "ttnn.deallocate"(%776) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc120)
        %778 = "ttnn.to_memory_config"(%770) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc851)
        "ttnn.deallocate"(%770) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc851)
        %779 = "ttnn.matmul"(%778, %777) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc349)
        "ttnn.deallocate"(%778) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc349)
        "ttnn.deallocate"(%777) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc349)
        %780 = "ttnn.to_memory_config"(%779) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc852)
        "ttnn.deallocate"(%779) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc852)
        %781 = "ttnn.reshape"(%780) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc350)
        "ttnn.deallocate"(%780) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc350)
        %782 = "ttnn.concatenate_heads"(%781) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc351)
        "ttnn.deallocate"(%781) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc351)
        %783 = "ttnn.reshape"(%782) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc351)
        "ttnn.deallocate"(%782) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc351)
        %784 = "ttnn.to_memory_config"(%783) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1114)
        "ttnn.deallocate"(%783) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1114)
        %785 = "ttnn.matmul"(%784, %arg70) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc853)
        "ttnn.deallocate"(%784) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc853)
        "ttnn.deallocate"(%arg70) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc853)
        %786 = "ttnn.add"(%785, %131) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc854)
        "ttnn.deallocate"(%785) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc854)
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc854)
        %787 = "ttnn.to_memory_config"(%786) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1115)
        "ttnn.deallocate"(%786) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1115)
        %788 = "ttnn.reshape"(%787) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc200)
        "ttnn.deallocate"(%787) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc200)
        %789 = "ttnn.add"(%728, %788) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc352)
        "ttnn.deallocate"(%788) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc352)
        "ttnn.deallocate"(%728) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc352)
        %790 = "ttnn.to_memory_config"(%789) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc855)
        "ttnn.deallocate"(%789) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc855)
        %791 = "ttnn.sum"(%790) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc93)
        %792 = "ttnn.multiply"(%791, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc93)
        "ttnn.deallocate"(%791) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc93)
        %793 = "ttnn.to_memory_config"(%792) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc856)
        "ttnn.deallocate"(%792) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc856)
        %794 = "ttnn.reshape"(%793) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc93)
        "ttnn.deallocate"(%793) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc93)
        %795 = "ttnn.neg"(%794) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc857)
        "ttnn.deallocate"(%794) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc857)
        %796 = "ttnn.add"(%790, %795) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc353)
        "ttnn.deallocate"(%795) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc353)
        %797 = "ttnn.to_memory_config"(%796) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc858)
        "ttnn.deallocate"(%796) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc858)
        %798 = "ttnn.reshape"(%797) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1211)
        %799 = "ttnn.multiply"(%797, %797) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc93)
        "ttnn.deallocate"(%797) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc93)
        %800 = "ttnn.to_memory_config"(%799) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc856)
        "ttnn.deallocate"(%799) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc856)
        %801 = "ttnn.sum"(%800) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc93)
        "ttnn.deallocate"(%800) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc93)
        %802 = "ttnn.multiply"(%801, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc93)
        "ttnn.deallocate"(%801) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc93)
        %803 = "ttnn.add"(%802, %57#14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc354)
        "ttnn.deallocate"(%802) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc354)
        "ttnn.deallocate"(%57#14) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc354)
        %804 = "ttnn.to_memory_config"(%803) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc859)
        "ttnn.deallocate"(%803) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc859)
        %805 = "ttnn.rsqrt"(%804) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc355)
        "ttnn.deallocate"(%804) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc355)
        %806 = "ttnn.reshape"(%805) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc355)
        "ttnn.deallocate"(%805) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc355)
        %807 = "ttnn.multiply"(%798, %806) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc356)
        "ttnn.deallocate"(%806) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc356)
        "ttnn.deallocate"(%798) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc356)
        %808 = "ttnn.multiply"(%807, %70) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc356)
        "ttnn.deallocate"(%807) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc356)
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc356)
        %809 = "ttnn.add"(%808, %87) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc354)
        "ttnn.deallocate"(%808) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc354)
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc354)
        %810 = "ttnn.matmul"(%809, %arg66) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc860)
        "ttnn.deallocate"(%809) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc860)
        "ttnn.deallocate"(%arg66) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc860)
        %811 = "ttnn.add"(%810, %28) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc861)
        "ttnn.deallocate"(%810) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc861)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc861)
        %812 = "ttnn.to_memory_config"(%811) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1117)
        "ttnn.deallocate"(%811) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1117)
        %813 = "ttnn.multiply"(%812, %119#6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc358)
        "ttnn.deallocate"(%119#6) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc358)
        %814 = "ttnn.to_memory_config"(%813) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc862)
        "ttnn.deallocate"(%813) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc862)
        %815 = "ttnn.sigmoid"(%814) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc359)
        "ttnn.deallocate"(%814) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc359)
        %816 = "ttnn.multiply"(%812, %815) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc358)
        "ttnn.deallocate"(%815) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc358)
        "ttnn.deallocate"(%812) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc358)
        %817 = "ttnn.matmul"(%816, %arg64) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc863)
        "ttnn.deallocate"(%816) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc863)
        "ttnn.deallocate"(%arg64) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc863)
        %818 = "ttnn.add"(%817, %92) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc864)
        "ttnn.deallocate"(%817) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc864)
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc864)
        %819 = "ttnn.to_memory_config"(%818) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1118)
        "ttnn.deallocate"(%818) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1118)
        %820 = "ttnn.reshape"(%819) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc150)
        "ttnn.deallocate"(%819) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc150)
        %821 = "ttnn.add"(%790, %820) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc360)
        "ttnn.deallocate"(%820) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc360)
        "ttnn.deallocate"(%790) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc360)
        %822 = "ttnn.to_memory_config"(%821) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc865)
        "ttnn.deallocate"(%821) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc865)
        %823 = "ttnn.sum"(%822) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc94)
        %824 = "ttnn.multiply"(%823, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc94)
        "ttnn.deallocate"(%823) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc94)
        %825 = "ttnn.to_memory_config"(%824) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc866)
        "ttnn.deallocate"(%824) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc866)
        %826 = "ttnn.reshape"(%825) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc94)
        "ttnn.deallocate"(%825) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc94)
        %827 = "ttnn.neg"(%826) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc867)
        "ttnn.deallocate"(%826) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc867)
        %828 = "ttnn.add"(%822, %827) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc361)
        "ttnn.deallocate"(%827) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc361)
        %829 = "ttnn.to_memory_config"(%828) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc868)
        "ttnn.deallocate"(%828) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc868)
        %830 = "ttnn.reshape"(%829) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1212)
        %831 = "ttnn.multiply"(%829, %829) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc94)
        "ttnn.deallocate"(%829) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc94)
        %832 = "ttnn.to_memory_config"(%831) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc866)
        "ttnn.deallocate"(%831) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc866)
        %833 = "ttnn.sum"(%832) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc94)
        "ttnn.deallocate"(%832) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc94)
        %834 = "ttnn.multiply"(%833, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc94)
        "ttnn.deallocate"(%833) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc94)
        %835 = "ttnn.add"(%834, %57#15) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc362)
        "ttnn.deallocate"(%834) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc362)
        "ttnn.deallocate"(%57#15) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc362)
        %836 = "ttnn.to_memory_config"(%835) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc869)
        "ttnn.deallocate"(%835) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc869)
        %837 = "ttnn.rsqrt"(%836) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc363)
        "ttnn.deallocate"(%836) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc363)
        %838 = "ttnn.reshape"(%837) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc363)
        "ttnn.deallocate"(%837) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc363)
        %839 = "ttnn.multiply"(%830, %838) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc364)
        "ttnn.deallocate"(%838) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc364)
        "ttnn.deallocate"(%830) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc364)
        %840 = "ttnn.multiply"(%839, %25) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc364)
        "ttnn.deallocate"(%839) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc364)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc364)
        %841 = "ttnn.add"(%840, %63) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc362)
        "ttnn.deallocate"(%840) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc362)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc362)
        %842 = "ttnn.to_memory_config"(%841) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc869)
        %843 = "ttnn.matmul"(%841, %arg185) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc870)
        "ttnn.deallocate"(%841) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc870)
        "ttnn.deallocate"(%arg185) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc870)
        %844 = "ttnn.add"(%843, %59) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc871)
        "ttnn.deallocate"(%843) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc871)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc871)
        %845 = "ttnn.to_memory_config"(%844) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1120)
        "ttnn.deallocate"(%844) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1120)
        %846 = "ttnn.reshape"(%845) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc107)
        "ttnn.deallocate"(%845) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc107)
        %847 = "ttnn.permute"(%846) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc107)
        "ttnn.deallocate"(%846) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc107)
        %848 = "ttnn.reshape"(%847) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc107)
        "ttnn.deallocate"(%847) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc107)
        %849 = "ttnn.to_memory_config"(%842) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1121)
        %850 = "ttnn.matmul"(%849, %arg183) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc872)
        "ttnn.deallocate"(%849) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc872)
        "ttnn.deallocate"(%arg183) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc872)
        %851 = "ttnn.add"(%850, %130) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc873)
        "ttnn.deallocate"(%850) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc873)
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc873)
        %852 = "ttnn.to_memory_config"(%851) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1122)
        "ttnn.deallocate"(%851) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1122)
        %853 = "ttnn.reshape"(%852) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc199)
        "ttnn.deallocate"(%852) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc199)
        %854 = "ttnn.permute"(%853) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc199)
        "ttnn.deallocate"(%853) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc199)
        %855 = "ttnn.reshape"(%854) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc199)
        "ttnn.deallocate"(%854) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc199)
        %856 = "ttnn.to_memory_config"(%848) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc874)
        "ttnn.deallocate"(%848) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc874)
        %857 = "ttnn.matmul"(%856, %855) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc365)
        "ttnn.deallocate"(%856) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc365)
        "ttnn.deallocate"(%855) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc365)
        %858 = "ttnn.to_memory_config"(%857) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc875)
        "ttnn.deallocate"(%857) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc875)
        %859 = "ttnn.reshape"(%858) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc366)
        "ttnn.deallocate"(%858) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc366)
        %860 = "ttnn.multiply"(%859, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc367)
        "ttnn.deallocate"(%859) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc367)
        %861 = "ttnn.typecast"(%860) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc368)
        "ttnn.deallocate"(%860) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc368)
        %862 = "ttnn.softmax"(%861) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc369)
        "ttnn.deallocate"(%861) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc369)
        %863 = "ttnn.typecast"(%862) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc370)
        "ttnn.deallocate"(%862) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc370)
        %864 = "ttnn.reshape"(%863) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc370)
        "ttnn.deallocate"(%863) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc370)
        %865 = "ttnn.to_memory_config"(%842) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1123)
        "ttnn.deallocate"(%842) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1123)
        %866 = "ttnn.matmul"(%865, %arg60) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc876)
        "ttnn.deallocate"(%865) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc876)
        "ttnn.deallocate"(%arg60) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc876)
        %867 = "ttnn.add"(%866, %2) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc877)
        "ttnn.deallocate"(%866) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc877)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc877)
        %868 = "ttnn.to_memory_config"(%867) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1124)
        "ttnn.deallocate"(%867) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1124)
        %869 = "ttnn.reshape"(%868) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc6)
        "ttnn.deallocate"(%868) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc6)
        %870 = "ttnn.permute"(%869) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc6)
        "ttnn.deallocate"(%869) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc6)
        %871 = "ttnn.reshape"(%870) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc6)
        "ttnn.deallocate"(%870) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc6)
        %872 = "ttnn.to_memory_config"(%864) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc878)
        "ttnn.deallocate"(%864) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc878)
        %873 = "ttnn.matmul"(%872, %871) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc371)
        "ttnn.deallocate"(%872) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc371)
        "ttnn.deallocate"(%871) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc371)
        %874 = "ttnn.to_memory_config"(%873) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc879)
        "ttnn.deallocate"(%873) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc879)
        %875 = "ttnn.reshape"(%874) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc372)
        "ttnn.deallocate"(%874) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc372)
        %876 = "ttnn.concatenate_heads"(%875) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc373)
        "ttnn.deallocate"(%875) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc373)
        %877 = "ttnn.reshape"(%876) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc373)
        "ttnn.deallocate"(%876) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc373)
        %878 = "ttnn.to_memory_config"(%877) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1125)
        "ttnn.deallocate"(%877) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1125)
        %879 = "ttnn.matmul"(%878, %arg58) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc880)
        "ttnn.deallocate"(%878) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc880)
        "ttnn.deallocate"(%arg58) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc880)
        %880 = "ttnn.add"(%879, %78) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc881)
        "ttnn.deallocate"(%879) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc881)
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc881)
        %881 = "ttnn.to_memory_config"(%880) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1126)
        "ttnn.deallocate"(%880) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1126)
        %882 = "ttnn.reshape"(%881) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc133)
        "ttnn.deallocate"(%881) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc133)
        %883 = "ttnn.add"(%822, %882) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc374)
        "ttnn.deallocate"(%882) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc374)
        "ttnn.deallocate"(%822) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc374)
        %884 = "ttnn.to_memory_config"(%883) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc882)
        "ttnn.deallocate"(%883) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc882)
        %885 = "ttnn.sum"(%884) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc95)
        %886 = "ttnn.multiply"(%885, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc95)
        "ttnn.deallocate"(%885) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc95)
        %887 = "ttnn.to_memory_config"(%886) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc883)
        "ttnn.deallocate"(%886) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc883)
        %888 = "ttnn.reshape"(%887) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc95)
        "ttnn.deallocate"(%887) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc95)
        %889 = "ttnn.neg"(%888) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc884)
        "ttnn.deallocate"(%888) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc884)
        %890 = "ttnn.add"(%884, %889) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc375)
        "ttnn.deallocate"(%889) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc375)
        %891 = "ttnn.to_memory_config"(%890) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc885)
        "ttnn.deallocate"(%890) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc885)
        %892 = "ttnn.reshape"(%891) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1213)
        %893 = "ttnn.multiply"(%891, %891) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc95)
        "ttnn.deallocate"(%891) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc95)
        %894 = "ttnn.to_memory_config"(%893) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc883)
        "ttnn.deallocate"(%893) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc883)
        %895 = "ttnn.sum"(%894) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc95)
        "ttnn.deallocate"(%894) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc95)
        %896 = "ttnn.multiply"(%895, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc95)
        "ttnn.deallocate"(%895) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc95)
        %897 = "ttnn.add"(%896, %57#16) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc376)
        "ttnn.deallocate"(%896) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc376)
        "ttnn.deallocate"(%57#16) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc376)
        %898 = "ttnn.to_memory_config"(%897) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc886)
        "ttnn.deallocate"(%897) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc886)
        %899 = "ttnn.rsqrt"(%898) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc377)
        "ttnn.deallocate"(%898) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc377)
        %900 = "ttnn.reshape"(%899) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc377)
        "ttnn.deallocate"(%899) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc377)
        %901 = "ttnn.multiply"(%892, %900) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc378)
        "ttnn.deallocate"(%900) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc378)
        "ttnn.deallocate"(%892) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc378)
        %902 = "ttnn.multiply"(%901, %121) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc378)
        "ttnn.deallocate"(%901) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc378)
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc378)
        %903 = "ttnn.add"(%902, %68) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc376)
        "ttnn.deallocate"(%902) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc376)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc376)
        %904 = "ttnn.matmul"(%903, %arg54) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc887)
        "ttnn.deallocate"(%903) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc887)
        "ttnn.deallocate"(%arg54) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc887)
        %905 = "ttnn.add"(%904, %67) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc888)
        "ttnn.deallocate"(%904) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc888)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc888)
        %906 = "ttnn.to_memory_config"(%905) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1128)
        "ttnn.deallocate"(%905) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1128)
        %907 = "ttnn.multiply"(%906, %119#7) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc380)
        "ttnn.deallocate"(%119#7) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc380)
        %908 = "ttnn.to_memory_config"(%907) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc889)
        "ttnn.deallocate"(%907) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc889)
        %909 = "ttnn.sigmoid"(%908) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc381)
        "ttnn.deallocate"(%908) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc381)
        %910 = "ttnn.multiply"(%906, %909) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc380)
        "ttnn.deallocate"(%909) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc380)
        "ttnn.deallocate"(%906) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc380)
        %911 = "ttnn.matmul"(%910, %arg52) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc890)
        "ttnn.deallocate"(%910) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc890)
        "ttnn.deallocate"(%arg52) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc890)
        %912 = "ttnn.add"(%911, %132) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc891)
        "ttnn.deallocate"(%911) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc891)
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc891)
        %913 = "ttnn.to_memory_config"(%912) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1129)
        "ttnn.deallocate"(%912) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1129)
        %914 = "ttnn.reshape"(%913) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc201)
        "ttnn.deallocate"(%913) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc201)
        %915 = "ttnn.add"(%884, %914) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc382)
        "ttnn.deallocate"(%914) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc382)
        "ttnn.deallocate"(%884) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc382)
        %916 = "ttnn.to_memory_config"(%915) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc892)
        "ttnn.deallocate"(%915) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc892)
        %917 = "ttnn.sum"(%916) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc96)
        %918 = "ttnn.multiply"(%917, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc96)
        "ttnn.deallocate"(%917) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc96)
        %919 = "ttnn.to_memory_config"(%918) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc893)
        "ttnn.deallocate"(%918) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc893)
        %920 = "ttnn.reshape"(%919) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc96)
        "ttnn.deallocate"(%919) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc96)
        %921 = "ttnn.neg"(%920) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc894)
        "ttnn.deallocate"(%920) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc894)
        %922 = "ttnn.add"(%916, %921) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc383)
        "ttnn.deallocate"(%921) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc383)
        %923 = "ttnn.to_memory_config"(%922) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc895)
        "ttnn.deallocate"(%922) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc895)
        %924 = "ttnn.reshape"(%923) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1214)
        %925 = "ttnn.multiply"(%923, %923) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc96)
        "ttnn.deallocate"(%923) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc96)
        %926 = "ttnn.to_memory_config"(%925) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc893)
        "ttnn.deallocate"(%925) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc893)
        %927 = "ttnn.sum"(%926) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc96)
        "ttnn.deallocate"(%926) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc96)
        %928 = "ttnn.multiply"(%927, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc96)
        "ttnn.deallocate"(%927) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc96)
        %929 = "ttnn.add"(%928, %57#17) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc384)
        "ttnn.deallocate"(%928) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc384)
        "ttnn.deallocate"(%57#17) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc384)
        %930 = "ttnn.to_memory_config"(%929) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc896)
        "ttnn.deallocate"(%929) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc896)
        %931 = "ttnn.rsqrt"(%930) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc385)
        "ttnn.deallocate"(%930) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc385)
        %932 = "ttnn.reshape"(%931) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc385)
        "ttnn.deallocate"(%931) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc385)
        %933 = "ttnn.multiply"(%924, %932) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc386)
        "ttnn.deallocate"(%932) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc386)
        "ttnn.deallocate"(%924) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc386)
        %934 = "ttnn.multiply"(%933, %42) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc386)
        "ttnn.deallocate"(%933) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc386)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc386)
        %935 = "ttnn.add"(%934, %79) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc384)
        "ttnn.deallocate"(%934) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc384)
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc384)
        %936 = "ttnn.to_memory_config"(%935) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc896)
        %937 = "ttnn.matmul"(%935, %arg189) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc897)
        "ttnn.deallocate"(%935) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc897)
        "ttnn.deallocate"(%arg189) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc897)
        %938 = "ttnn.add"(%937, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc898)
        "ttnn.deallocate"(%937) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc898)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc898)
        %939 = "ttnn.to_memory_config"(%938) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1131)
        "ttnn.deallocate"(%938) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1131)
        %940 = "ttnn.reshape"(%939) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc24)
        "ttnn.deallocate"(%939) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc24)
        %941 = "ttnn.permute"(%940) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc24)
        "ttnn.deallocate"(%940) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc24)
        %942 = "ttnn.reshape"(%941) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc24)
        "ttnn.deallocate"(%941) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc24)
        %943 = "ttnn.to_memory_config"(%936) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1132)
        %944 = "ttnn.matmul"(%943, %arg187) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc899)
        "ttnn.deallocate"(%943) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc899)
        "ttnn.deallocate"(%arg187) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc899)
        %945 = "ttnn.add"(%944, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc900)
        "ttnn.deallocate"(%944) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc900)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc900)
        %946 = "ttnn.to_memory_config"(%945) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1133)
        "ttnn.deallocate"(%945) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1133)
        %947 = "ttnn.reshape"(%946) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc18)
        "ttnn.deallocate"(%946) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc18)
        %948 = "ttnn.permute"(%947) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc18)
        "ttnn.deallocate"(%947) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc18)
        %949 = "ttnn.reshape"(%948) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc18)
        "ttnn.deallocate"(%948) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc18)
        %950 = "ttnn.to_memory_config"(%942) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc901)
        "ttnn.deallocate"(%942) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc901)
        %951 = "ttnn.matmul"(%950, %949) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc387)
        "ttnn.deallocate"(%950) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc387)
        "ttnn.deallocate"(%949) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc387)
        %952 = "ttnn.to_memory_config"(%951) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc902)
        "ttnn.deallocate"(%951) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc902)
        %953 = "ttnn.reshape"(%952) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc388)
        "ttnn.deallocate"(%952) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc388)
        %954 = "ttnn.multiply"(%953, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc389)
        "ttnn.deallocate"(%953) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc389)
        %955 = "ttnn.typecast"(%954) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc390)
        "ttnn.deallocate"(%954) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc390)
        %956 = "ttnn.softmax"(%955) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc391)
        "ttnn.deallocate"(%955) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc391)
        %957 = "ttnn.typecast"(%956) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc392)
        "ttnn.deallocate"(%956) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc392)
        %958 = "ttnn.reshape"(%957) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc392)
        "ttnn.deallocate"(%957) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc392)
        %959 = "ttnn.to_memory_config"(%936) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1134)
        "ttnn.deallocate"(%936) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1134)
        %960 = "ttnn.matmul"(%959, %arg48) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc903)
        "ttnn.deallocate"(%959) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc903)
        "ttnn.deallocate"(%arg48) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc903)
        %961 = "ttnn.add"(%960, %99) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc904)
        "ttnn.deallocate"(%960) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc904)
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc904)
        %962 = "ttnn.to_memory_config"(%961) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1135)
        "ttnn.deallocate"(%961) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1135)
        %963 = "ttnn.reshape"(%962) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc158)
        "ttnn.deallocate"(%962) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc158)
        %964 = "ttnn.permute"(%963) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc158)
        "ttnn.deallocate"(%963) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc158)
        %965 = "ttnn.reshape"(%964) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc158)
        "ttnn.deallocate"(%964) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc158)
        %966 = "ttnn.to_memory_config"(%958) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc905)
        "ttnn.deallocate"(%958) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc905)
        %967 = "ttnn.matmul"(%966, %965) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc393)
        "ttnn.deallocate"(%966) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc393)
        "ttnn.deallocate"(%965) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc393)
        %968 = "ttnn.to_memory_config"(%967) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc906)
        "ttnn.deallocate"(%967) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc906)
        %969 = "ttnn.reshape"(%968) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc394)
        "ttnn.deallocate"(%968) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc394)
        %970 = "ttnn.concatenate_heads"(%969) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc395)
        "ttnn.deallocate"(%969) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc395)
        %971 = "ttnn.reshape"(%970) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc395)
        "ttnn.deallocate"(%970) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc395)
        %972 = "ttnn.to_memory_config"(%971) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1136)
        "ttnn.deallocate"(%971) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1136)
        %973 = "ttnn.matmul"(%972, %arg46) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc907)
        "ttnn.deallocate"(%972) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc907)
        "ttnn.deallocate"(%arg46) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc907)
        %974 = "ttnn.add"(%973, %47) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc908)
        "ttnn.deallocate"(%973) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc908)
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc908)
        %975 = "ttnn.to_memory_config"(%974) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1137)
        "ttnn.deallocate"(%974) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1137)
        %976 = "ttnn.reshape"(%975) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc69)
        "ttnn.deallocate"(%975) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc69)
        %977 = "ttnn.add"(%916, %976) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc396)
        "ttnn.deallocate"(%976) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc396)
        "ttnn.deallocate"(%916) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc396)
        %978 = "ttnn.to_memory_config"(%977) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc909)
        "ttnn.deallocate"(%977) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc909)
        %979 = "ttnn.sum"(%978) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc97)
        %980 = "ttnn.multiply"(%979, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc97)
        "ttnn.deallocate"(%979) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc97)
        %981 = "ttnn.to_memory_config"(%980) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc910)
        "ttnn.deallocate"(%980) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc910)
        %982 = "ttnn.reshape"(%981) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc97)
        "ttnn.deallocate"(%981) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc97)
        %983 = "ttnn.neg"(%982) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc911)
        "ttnn.deallocate"(%982) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc911)
        %984 = "ttnn.add"(%978, %983) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc397)
        "ttnn.deallocate"(%983) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc397)
        %985 = "ttnn.to_memory_config"(%984) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc912)
        "ttnn.deallocate"(%984) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc912)
        %986 = "ttnn.reshape"(%985) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1215)
        %987 = "ttnn.multiply"(%985, %985) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc97)
        "ttnn.deallocate"(%985) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc97)
        %988 = "ttnn.to_memory_config"(%987) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc910)
        "ttnn.deallocate"(%987) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc910)
        %989 = "ttnn.sum"(%988) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc97)
        "ttnn.deallocate"(%988) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc97)
        %990 = "ttnn.multiply"(%989, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc97)
        "ttnn.deallocate"(%989) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc97)
        %991 = "ttnn.add"(%990, %57#18) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc398)
        "ttnn.deallocate"(%990) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc398)
        "ttnn.deallocate"(%57#18) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc398)
        %992 = "ttnn.to_memory_config"(%991) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc913)
        "ttnn.deallocate"(%991) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc913)
        %993 = "ttnn.rsqrt"(%992) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc399)
        "ttnn.deallocate"(%992) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc399)
        %994 = "ttnn.reshape"(%993) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc399)
        "ttnn.deallocate"(%993) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc399)
        %995 = "ttnn.multiply"(%986, %994) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc400)
        "ttnn.deallocate"(%994) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc400)
        "ttnn.deallocate"(%986) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc400)
        %996 = "ttnn.multiply"(%995, %106) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc400)
        "ttnn.deallocate"(%995) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc400)
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc400)
        %997 = "ttnn.add"(%996, %48) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc398)
        "ttnn.deallocate"(%996) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc398)
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc398)
        %998 = "ttnn.matmul"(%997, %arg42) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc914)
        "ttnn.deallocate"(%997) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc914)
        "ttnn.deallocate"(%arg42) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc914)
        %999 = "ttnn.add"(%998, %111) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc915)
        "ttnn.deallocate"(%998) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc915)
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc915)
        %1000 = "ttnn.to_memory_config"(%999) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1139)
        "ttnn.deallocate"(%999) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1139)
        %1001 = "ttnn.multiply"(%1000, %119#8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc402)
        "ttnn.deallocate"(%119#8) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc402)
        %1002 = "ttnn.to_memory_config"(%1001) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc916)
        "ttnn.deallocate"(%1001) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc916)
        %1003 = "ttnn.sigmoid"(%1002) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc403)
        "ttnn.deallocate"(%1002) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc403)
        %1004 = "ttnn.multiply"(%1000, %1003) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc402)
        "ttnn.deallocate"(%1003) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc402)
        "ttnn.deallocate"(%1000) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc402)
        %1005 = "ttnn.matmul"(%1004, %arg40) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc917)
        "ttnn.deallocate"(%1004) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc917)
        "ttnn.deallocate"(%arg40) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc917)
        %1006 = "ttnn.add"(%1005, %110) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc918)
        "ttnn.deallocate"(%1005) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc918)
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc918)
        %1007 = "ttnn.to_memory_config"(%1006) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1140)
        "ttnn.deallocate"(%1006) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1140)
        %1008 = "ttnn.reshape"(%1007) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc174)
        "ttnn.deallocate"(%1007) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc174)
        %1009 = "ttnn.add"(%978, %1008) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc404)
        "ttnn.deallocate"(%1008) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc404)
        "ttnn.deallocate"(%978) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc404)
        %1010 = "ttnn.to_memory_config"(%1009) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc919)
        "ttnn.deallocate"(%1009) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc919)
        %1011 = "ttnn.sum"(%1010) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc98)
        %1012 = "ttnn.multiply"(%1011, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc98)
        "ttnn.deallocate"(%1011) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc98)
        %1013 = "ttnn.to_memory_config"(%1012) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc920)
        "ttnn.deallocate"(%1012) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc920)
        %1014 = "ttnn.reshape"(%1013) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc98)
        "ttnn.deallocate"(%1013) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc98)
        %1015 = "ttnn.neg"(%1014) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc921)
        "ttnn.deallocate"(%1014) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc921)
        %1016 = "ttnn.add"(%1010, %1015) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc405)
        "ttnn.deallocate"(%1015) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc405)
        %1017 = "ttnn.to_memory_config"(%1016) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc922)
        "ttnn.deallocate"(%1016) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc922)
        %1018 = "ttnn.reshape"(%1017) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1216)
        %1019 = "ttnn.multiply"(%1017, %1017) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc98)
        "ttnn.deallocate"(%1017) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc98)
        %1020 = "ttnn.to_memory_config"(%1019) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc920)
        "ttnn.deallocate"(%1019) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc920)
        %1021 = "ttnn.sum"(%1020) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc98)
        "ttnn.deallocate"(%1020) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc98)
        %1022 = "ttnn.multiply"(%1021, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc98)
        "ttnn.deallocate"(%1021) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc98)
        %1023 = "ttnn.add"(%1022, %57#19) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc406)
        "ttnn.deallocate"(%1022) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc406)
        "ttnn.deallocate"(%57#19) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc406)
        %1024 = "ttnn.to_memory_config"(%1023) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc923)
        "ttnn.deallocate"(%1023) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc923)
        %1025 = "ttnn.rsqrt"(%1024) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc407)
        "ttnn.deallocate"(%1024) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc407)
        %1026 = "ttnn.reshape"(%1025) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc407)
        "ttnn.deallocate"(%1025) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc407)
        %1027 = "ttnn.multiply"(%1018, %1026) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc408)
        "ttnn.deallocate"(%1026) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc408)
        "ttnn.deallocate"(%1018) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc408)
        %1028 = "ttnn.multiply"(%1027, %124) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc408)
        "ttnn.deallocate"(%1027) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc408)
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc408)
        %1029 = "ttnn.add"(%1028, %107) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc406)
        "ttnn.deallocate"(%1028) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc406)
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc406)
        %1030 = "ttnn.to_memory_config"(%1029) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc923)
        %1031 = "ttnn.matmul"(%1029, %arg193) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc924)
        "ttnn.deallocate"(%1029) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc924)
        "ttnn.deallocate"(%arg193) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc924)
        %1032 = "ttnn.add"(%1031, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc925)
        "ttnn.deallocate"(%1031) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc925)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc925)
        %1033 = "ttnn.to_memory_config"(%1032) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1142)
        "ttnn.deallocate"(%1032) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1142)
        %1034 = "ttnn.reshape"(%1033) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc49)
        "ttnn.deallocate"(%1033) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc49)
        %1035 = "ttnn.permute"(%1034) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc49)
        "ttnn.deallocate"(%1034) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc49)
        %1036 = "ttnn.reshape"(%1035) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc49)
        "ttnn.deallocate"(%1035) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc49)
        %1037 = "ttnn.to_memory_config"(%1030) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1143)
        %1038 = "ttnn.matmul"(%1037, %arg191) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc926)
        "ttnn.deallocate"(%1037) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc926)
        "ttnn.deallocate"(%arg191) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc926)
        %1039 = "ttnn.add"(%1038, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc927)
        "ttnn.deallocate"(%1038) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc927)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc927)
        %1040 = "ttnn.to_memory_config"(%1039) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1144)
        "ttnn.deallocate"(%1039) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1144)
        %1041 = "ttnn.reshape"(%1040) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc21)
        "ttnn.deallocate"(%1040) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc21)
        %1042 = "ttnn.permute"(%1041) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc21)
        "ttnn.deallocate"(%1041) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc21)
        %1043 = "ttnn.reshape"(%1042) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc21)
        "ttnn.deallocate"(%1042) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc21)
        %1044 = "ttnn.to_memory_config"(%1036) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc928)
        "ttnn.deallocate"(%1036) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc928)
        %1045 = "ttnn.matmul"(%1044, %1043) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc409)
        "ttnn.deallocate"(%1044) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc409)
        "ttnn.deallocate"(%1043) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc409)
        %1046 = "ttnn.to_memory_config"(%1045) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc929)
        "ttnn.deallocate"(%1045) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc929)
        %1047 = "ttnn.reshape"(%1046) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc410)
        "ttnn.deallocate"(%1046) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc410)
        %1048 = "ttnn.multiply"(%1047, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc411)
        "ttnn.deallocate"(%1047) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc411)
        %1049 = "ttnn.typecast"(%1048) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc412)
        "ttnn.deallocate"(%1048) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc412)
        %1050 = "ttnn.softmax"(%1049) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc413)
        "ttnn.deallocate"(%1049) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc413)
        %1051 = "ttnn.typecast"(%1050) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc414)
        "ttnn.deallocate"(%1050) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc414)
        %1052 = "ttnn.reshape"(%1051) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc414)
        "ttnn.deallocate"(%1051) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc414)
        %1053 = "ttnn.to_memory_config"(%1030) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1145)
        "ttnn.deallocate"(%1030) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1145)
        %1054 = "ttnn.matmul"(%1053, %arg36) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc930)
        "ttnn.deallocate"(%1053) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc930)
        "ttnn.deallocate"(%arg36) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc930)
        %1055 = "ttnn.add"(%1054, %60) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc931)
        "ttnn.deallocate"(%1054) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc931)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc931)
        %1056 = "ttnn.to_memory_config"(%1055) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1146)
        "ttnn.deallocate"(%1055) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1146)
        %1057 = "ttnn.reshape"(%1056) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc110)
        "ttnn.deallocate"(%1056) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc110)
        %1058 = "ttnn.permute"(%1057) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc110)
        "ttnn.deallocate"(%1057) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc110)
        %1059 = "ttnn.reshape"(%1058) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc110)
        "ttnn.deallocate"(%1058) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc110)
        %1060 = "ttnn.to_memory_config"(%1052) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc932)
        "ttnn.deallocate"(%1052) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc932)
        %1061 = "ttnn.matmul"(%1060, %1059) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc415)
        "ttnn.deallocate"(%1060) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc415)
        "ttnn.deallocate"(%1059) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc415)
        %1062 = "ttnn.to_memory_config"(%1061) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc933)
        "ttnn.deallocate"(%1061) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc933)
        %1063 = "ttnn.reshape"(%1062) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc416)
        "ttnn.deallocate"(%1062) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc416)
        %1064 = "ttnn.concatenate_heads"(%1063) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc417)
        "ttnn.deallocate"(%1063) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc417)
        %1065 = "ttnn.reshape"(%1064) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc417)
        "ttnn.deallocate"(%1064) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc417)
        %1066 = "ttnn.to_memory_config"(%1065) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1147)
        "ttnn.deallocate"(%1065) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1147)
        %1067 = "ttnn.matmul"(%1066, %arg34) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc934)
        "ttnn.deallocate"(%1066) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc934)
        "ttnn.deallocate"(%arg34) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc934)
        %1068 = "ttnn.add"(%1067, %102) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc935)
        "ttnn.deallocate"(%1067) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc935)
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc935)
        %1069 = "ttnn.to_memory_config"(%1068) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1148)
        "ttnn.deallocate"(%1068) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1148)
        %1070 = "ttnn.reshape"(%1069) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc165)
        "ttnn.deallocate"(%1069) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc165)
        %1071 = "ttnn.add"(%1010, %1070) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc418)
        "ttnn.deallocate"(%1070) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc418)
        "ttnn.deallocate"(%1010) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc418)
        %1072 = "ttnn.to_memory_config"(%1071) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc936)
        "ttnn.deallocate"(%1071) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc936)
        %1073 = "ttnn.sum"(%1072) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc99)
        %1074 = "ttnn.multiply"(%1073, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc99)
        "ttnn.deallocate"(%1073) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc99)
        %1075 = "ttnn.to_memory_config"(%1074) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc937)
        "ttnn.deallocate"(%1074) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc937)
        %1076 = "ttnn.reshape"(%1075) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc99)
        "ttnn.deallocate"(%1075) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc99)
        %1077 = "ttnn.neg"(%1076) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc938)
        "ttnn.deallocate"(%1076) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc938)
        %1078 = "ttnn.add"(%1072, %1077) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc419)
        "ttnn.deallocate"(%1077) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc419)
        %1079 = "ttnn.to_memory_config"(%1078) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc939)
        "ttnn.deallocate"(%1078) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc939)
        %1080 = "ttnn.reshape"(%1079) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1217)
        %1081 = "ttnn.multiply"(%1079, %1079) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc99)
        "ttnn.deallocate"(%1079) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc99)
        %1082 = "ttnn.to_memory_config"(%1081) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc937)
        "ttnn.deallocate"(%1081) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc937)
        %1083 = "ttnn.sum"(%1082) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc99)
        "ttnn.deallocate"(%1082) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc99)
        %1084 = "ttnn.multiply"(%1083, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc99)
        "ttnn.deallocate"(%1083) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc99)
        %1085 = "ttnn.add"(%1084, %57#20) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc420)
        "ttnn.deallocate"(%1084) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc420)
        "ttnn.deallocate"(%57#20) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc420)
        %1086 = "ttnn.to_memory_config"(%1085) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc940)
        "ttnn.deallocate"(%1085) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc940)
        %1087 = "ttnn.rsqrt"(%1086) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc421)
        "ttnn.deallocate"(%1086) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc421)
        %1088 = "ttnn.reshape"(%1087) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc421)
        "ttnn.deallocate"(%1087) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc421)
        %1089 = "ttnn.multiply"(%1080, %1088) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc422)
        "ttnn.deallocate"(%1088) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc422)
        "ttnn.deallocate"(%1080) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc422)
        %1090 = "ttnn.multiply"(%1089, %123) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc422)
        "ttnn.deallocate"(%1089) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc422)
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc422)
        %1091 = "ttnn.add"(%1090, %7) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc420)
        "ttnn.deallocate"(%1090) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc420)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc420)
        %1092 = "ttnn.matmul"(%1091, %arg30) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc941)
        "ttnn.deallocate"(%1091) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc941)
        "ttnn.deallocate"(%arg30) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc941)
        %1093 = "ttnn.add"(%1092, %126) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc942)
        "ttnn.deallocate"(%1092) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc942)
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc942)
        %1094 = "ttnn.to_memory_config"(%1093) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1150)
        "ttnn.deallocate"(%1093) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1150)
        %1095 = "ttnn.multiply"(%1094, %119#9) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc424)
        "ttnn.deallocate"(%119#9) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc424)
        %1096 = "ttnn.to_memory_config"(%1095) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc943)
        "ttnn.deallocate"(%1095) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc943)
        %1097 = "ttnn.sigmoid"(%1096) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc425)
        "ttnn.deallocate"(%1096) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc425)
        %1098 = "ttnn.multiply"(%1094, %1097) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc424)
        "ttnn.deallocate"(%1097) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc424)
        "ttnn.deallocate"(%1094) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc424)
        %1099 = "ttnn.matmul"(%1098, %arg28) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc944)
        "ttnn.deallocate"(%1098) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc944)
        "ttnn.deallocate"(%arg28) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc944)
        %1100 = "ttnn.add"(%1099, %22) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc945)
        "ttnn.deallocate"(%1099) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc945)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc945)
        %1101 = "ttnn.to_memory_config"(%1100) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1151)
        "ttnn.deallocate"(%1100) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1151)
        %1102 = "ttnn.reshape"(%1101) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc39)
        "ttnn.deallocate"(%1101) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc39)
        %1103 = "ttnn.add"(%1072, %1102) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc426)
        "ttnn.deallocate"(%1102) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc426)
        "ttnn.deallocate"(%1072) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc426)
        %1104 = "ttnn.to_memory_config"(%1103) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc946)
        "ttnn.deallocate"(%1103) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc946)
        %1105 = "ttnn.sum"(%1104) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc100)
        %1106 = "ttnn.multiply"(%1105, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc100)
        "ttnn.deallocate"(%1105) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc100)
        %1107 = "ttnn.to_memory_config"(%1106) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc947)
        "ttnn.deallocate"(%1106) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc947)
        %1108 = "ttnn.reshape"(%1107) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc100)
        "ttnn.deallocate"(%1107) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc100)
        %1109 = "ttnn.neg"(%1108) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc948)
        "ttnn.deallocate"(%1108) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc948)
        %1110 = "ttnn.add"(%1104, %1109) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc427)
        "ttnn.deallocate"(%1109) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc427)
        %1111 = "ttnn.to_memory_config"(%1110) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc949)
        "ttnn.deallocate"(%1110) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc949)
        %1112 = "ttnn.reshape"(%1111) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1218)
        %1113 = "ttnn.multiply"(%1111, %1111) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc100)
        "ttnn.deallocate"(%1111) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc100)
        %1114 = "ttnn.to_memory_config"(%1113) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc947)
        "ttnn.deallocate"(%1113) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc947)
        %1115 = "ttnn.sum"(%1114) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc100)
        "ttnn.deallocate"(%1114) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc100)
        %1116 = "ttnn.multiply"(%1115, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc100)
        "ttnn.deallocate"(%1115) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc100)
        %1117 = "ttnn.add"(%1116, %57#21) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc428)
        "ttnn.deallocate"(%1116) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc428)
        "ttnn.deallocate"(%57#21) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc428)
        %1118 = "ttnn.to_memory_config"(%1117) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc950)
        "ttnn.deallocate"(%1117) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc950)
        %1119 = "ttnn.rsqrt"(%1118) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc429)
        "ttnn.deallocate"(%1118) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc429)
        %1120 = "ttnn.reshape"(%1119) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc429)
        "ttnn.deallocate"(%1119) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc429)
        %1121 = "ttnn.multiply"(%1112, %1120) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc430)
        "ttnn.deallocate"(%1120) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc430)
        "ttnn.deallocate"(%1112) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc430)
        %1122 = "ttnn.multiply"(%1121, %94) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc430)
        "ttnn.deallocate"(%1121) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc430)
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc430)
        %1123 = "ttnn.add"(%1122, %73) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc428)
        "ttnn.deallocate"(%1122) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc428)
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc428)
        %1124 = "ttnn.to_memory_config"(%1123) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc950)
        %1125 = "ttnn.matmul"(%1123, %arg197) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc951)
        "ttnn.deallocate"(%1123) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc951)
        "ttnn.deallocate"(%arg197) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc951)
        %1126 = "ttnn.add"(%1125, %103) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc952)
        "ttnn.deallocate"(%1125) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc952)
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc952)
        %1127 = "ttnn.to_memory_config"(%1126) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1153)
        "ttnn.deallocate"(%1126) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1153)
        %1128 = "ttnn.reshape"(%1127) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc168)
        "ttnn.deallocate"(%1127) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc168)
        %1129 = "ttnn.permute"(%1128) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc168)
        "ttnn.deallocate"(%1128) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc168)
        %1130 = "ttnn.reshape"(%1129) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc168)
        "ttnn.deallocate"(%1129) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc168)
        %1131 = "ttnn.to_memory_config"(%1124) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1154)
        %1132 = "ttnn.matmul"(%1131, %arg195) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc953)
        "ttnn.deallocate"(%1131) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc953)
        "ttnn.deallocate"(%arg195) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc953)
        %1133 = "ttnn.add"(%1132, %18) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc954)
        "ttnn.deallocate"(%1132) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc954)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc954)
        %1134 = "ttnn.to_memory_config"(%1133) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1155)
        "ttnn.deallocate"(%1133) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1155)
        %1135 = "ttnn.reshape"(%1134) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc33)
        "ttnn.deallocate"(%1134) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc33)
        %1136 = "ttnn.permute"(%1135) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc33)
        "ttnn.deallocate"(%1135) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc33)
        %1137 = "ttnn.reshape"(%1136) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc33)
        "ttnn.deallocate"(%1136) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc33)
        %1138 = "ttnn.to_memory_config"(%1130) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc955)
        "ttnn.deallocate"(%1130) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc955)
        %1139 = "ttnn.matmul"(%1138, %1137) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc431)
        "ttnn.deallocate"(%1138) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc431)
        "ttnn.deallocate"(%1137) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc431)
        %1140 = "ttnn.to_memory_config"(%1139) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc956)
        "ttnn.deallocate"(%1139) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc956)
        %1141 = "ttnn.reshape"(%1140) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc432)
        "ttnn.deallocate"(%1140) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc432)
        %1142 = "ttnn.multiply"(%1141, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc433)
        "ttnn.deallocate"(%1141) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc433)
        %1143 = "ttnn.typecast"(%1142) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc434)
        "ttnn.deallocate"(%1142) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc434)
        %1144 = "ttnn.softmax"(%1143) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc435)
        "ttnn.deallocate"(%1143) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc435)
        %1145 = "ttnn.typecast"(%1144) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc436)
        "ttnn.deallocate"(%1144) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc436)
        %1146 = "ttnn.reshape"(%1145) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc436)
        "ttnn.deallocate"(%1145) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc436)
        %1147 = "ttnn.to_memory_config"(%1124) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1156)
        "ttnn.deallocate"(%1124) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1156)
        %1148 = "ttnn.matmul"(%1147, %arg24) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc957)
        "ttnn.deallocate"(%1147) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc957)
        "ttnn.deallocate"(%arg24) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc957)
        %1149 = "ttnn.add"(%1148, %43) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc958)
        "ttnn.deallocate"(%1148) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc958)
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc958)
        %1150 = "ttnn.to_memory_config"(%1149) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1157)
        "ttnn.deallocate"(%1149) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1157)
        %1151 = "ttnn.reshape"(%1150) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc66)
        "ttnn.deallocate"(%1150) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc66)
        %1152 = "ttnn.permute"(%1151) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc66)
        "ttnn.deallocate"(%1151) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc66)
        %1153 = "ttnn.reshape"(%1152) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc66)
        "ttnn.deallocate"(%1152) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc66)
        %1154 = "ttnn.to_memory_config"(%1146) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc959)
        "ttnn.deallocate"(%1146) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc959)
        %1155 = "ttnn.matmul"(%1154, %1153) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc437)
        "ttnn.deallocate"(%1154) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc437)
        "ttnn.deallocate"(%1153) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc437)
        %1156 = "ttnn.to_memory_config"(%1155) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc960)
        "ttnn.deallocate"(%1155) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc960)
        %1157 = "ttnn.reshape"(%1156) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc438)
        "ttnn.deallocate"(%1156) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc438)
        %1158 = "ttnn.concatenate_heads"(%1157) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc439)
        "ttnn.deallocate"(%1157) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc439)
        %1159 = "ttnn.reshape"(%1158) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc439)
        "ttnn.deallocate"(%1158) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc439)
        %1160 = "ttnn.to_memory_config"(%1159) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1158)
        "ttnn.deallocate"(%1159) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1158)
        %1161 = "ttnn.matmul"(%1160, %arg22) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc961)
        "ttnn.deallocate"(%1160) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc961)
        "ttnn.deallocate"(%arg22) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc961)
        %1162 = "ttnn.add"(%1161, %16) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc962)
        "ttnn.deallocate"(%1161) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc962)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc962)
        %1163 = "ttnn.to_memory_config"(%1162) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1159)
        "ttnn.deallocate"(%1162) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1159)
        %1164 = "ttnn.reshape"(%1163) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc27)
        "ttnn.deallocate"(%1163) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc27)
        %1165 = "ttnn.add"(%1104, %1164) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc440)
        "ttnn.deallocate"(%1164) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc440)
        "ttnn.deallocate"(%1104) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc440)
        %1166 = "ttnn.to_memory_config"(%1165) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc963)
        "ttnn.deallocate"(%1165) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc963)
        %1167 = "ttnn.sum"(%1166) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc101)
        %1168 = "ttnn.multiply"(%1167, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc101)
        "ttnn.deallocate"(%1167) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc101)
        %1169 = "ttnn.to_memory_config"(%1168) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc964)
        "ttnn.deallocate"(%1168) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc964)
        %1170 = "ttnn.reshape"(%1169) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc101)
        "ttnn.deallocate"(%1169) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc101)
        %1171 = "ttnn.neg"(%1170) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc965)
        "ttnn.deallocate"(%1170) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc965)
        %1172 = "ttnn.add"(%1166, %1171) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc441)
        "ttnn.deallocate"(%1171) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc441)
        %1173 = "ttnn.to_memory_config"(%1172) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc966)
        "ttnn.deallocate"(%1172) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc966)
        %1174 = "ttnn.reshape"(%1173) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1219)
        %1175 = "ttnn.multiply"(%1173, %1173) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc101)
        "ttnn.deallocate"(%1173) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc101)
        %1176 = "ttnn.to_memory_config"(%1175) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc964)
        "ttnn.deallocate"(%1175) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc964)
        %1177 = "ttnn.sum"(%1176) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc101)
        "ttnn.deallocate"(%1176) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc101)
        %1178 = "ttnn.multiply"(%1177, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc101)
        "ttnn.deallocate"(%1177) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc101)
        %1179 = "ttnn.add"(%1178, %57#22) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc442)
        "ttnn.deallocate"(%1178) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc442)
        "ttnn.deallocate"(%57#22) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc442)
        %1180 = "ttnn.to_memory_config"(%1179) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc967)
        "ttnn.deallocate"(%1179) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc967)
        %1181 = "ttnn.rsqrt"(%1180) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc443)
        "ttnn.deallocate"(%1180) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc443)
        %1182 = "ttnn.reshape"(%1181) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc443)
        "ttnn.deallocate"(%1181) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc443)
        %1183 = "ttnn.multiply"(%1174, %1182) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc444)
        "ttnn.deallocate"(%1182) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc444)
        "ttnn.deallocate"(%1174) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc444)
        %1184 = "ttnn.multiply"(%1183, %5) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc444)
        "ttnn.deallocate"(%1183) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc444)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc444)
        %1185 = "ttnn.add"(%1184, %50) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc442)
        "ttnn.deallocate"(%1184) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc442)
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc442)
        %1186 = "ttnn.matmul"(%1185, %arg18) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc968)
        "ttnn.deallocate"(%1185) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc968)
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc968)
        %1187 = "ttnn.add"(%1186, %54) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc969)
        "ttnn.deallocate"(%1186) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc969)
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc969)
        %1188 = "ttnn.to_memory_config"(%1187) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1161)
        "ttnn.deallocate"(%1187) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1161)
        %1189 = "ttnn.multiply"(%1188, %119#10) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc446)
        "ttnn.deallocate"(%119#10) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc446)
        %1190 = "ttnn.to_memory_config"(%1189) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc970)
        "ttnn.deallocate"(%1189) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc970)
        %1191 = "ttnn.sigmoid"(%1190) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc447)
        "ttnn.deallocate"(%1190) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc447)
        %1192 = "ttnn.multiply"(%1188, %1191) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc446)
        "ttnn.deallocate"(%1191) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc446)
        "ttnn.deallocate"(%1188) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc446)
        %1193 = "ttnn.matmul"(%1192, %arg16) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc971)
        "ttnn.deallocate"(%1192) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc971)
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc971)
        %1194 = "ttnn.add"(%1193, %19) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc972)
        "ttnn.deallocate"(%1193) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc972)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc972)
        %1195 = "ttnn.to_memory_config"(%1194) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1162)
        "ttnn.deallocate"(%1194) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1162)
        %1196 = "ttnn.reshape"(%1195) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc34)
        "ttnn.deallocate"(%1195) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc34)
        %1197 = "ttnn.add"(%1166, %1196) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc448)
        "ttnn.deallocate"(%1196) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc448)
        "ttnn.deallocate"(%1166) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc448)
        %1198 = "ttnn.to_memory_config"(%1197) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc973)
        "ttnn.deallocate"(%1197) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc973)
        %1199 = "ttnn.sum"(%1198) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc102)
        %1200 = "ttnn.multiply"(%1199, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc102)
        "ttnn.deallocate"(%1199) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc102)
        %1201 = "ttnn.to_memory_config"(%1200) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc974)
        "ttnn.deallocate"(%1200) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc974)
        %1202 = "ttnn.reshape"(%1201) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc102)
        "ttnn.deallocate"(%1201) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc102)
        %1203 = "ttnn.neg"(%1202) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc975)
        "ttnn.deallocate"(%1202) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc975)
        %1204 = "ttnn.add"(%1198, %1203) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc449)
        "ttnn.deallocate"(%1203) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc449)
        %1205 = "ttnn.to_memory_config"(%1204) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc976)
        "ttnn.deallocate"(%1204) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc976)
        %1206 = "ttnn.reshape"(%1205) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1220)
        %1207 = "ttnn.multiply"(%1205, %1205) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc102)
        "ttnn.deallocate"(%1205) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc102)
        %1208 = "ttnn.to_memory_config"(%1207) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc974)
        "ttnn.deallocate"(%1207) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc974)
        %1209 = "ttnn.sum"(%1208) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc102)
        "ttnn.deallocate"(%1208) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc102)
        %1210 = "ttnn.multiply"(%1209, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc102)
        "ttnn.deallocate"(%1209) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc102)
        %1211 = "ttnn.add"(%1210, %57#23) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc450)
        "ttnn.deallocate"(%1210) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc450)
        "ttnn.deallocate"(%57#23) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc450)
        %1212 = "ttnn.to_memory_config"(%1211) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc977)
        "ttnn.deallocate"(%1211) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc977)
        %1213 = "ttnn.rsqrt"(%1212) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc451)
        "ttnn.deallocate"(%1212) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc451)
        %1214 = "ttnn.reshape"(%1213) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc451)
        "ttnn.deallocate"(%1213) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc451)
        %1215 = "ttnn.multiply"(%1206, %1214) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc452)
        "ttnn.deallocate"(%1214) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc452)
        "ttnn.deallocate"(%1206) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc452)
        %1216 = "ttnn.multiply"(%1215, %55) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc452)
        "ttnn.deallocate"(%1215) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc452)
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc452)
        %1217 = "ttnn.add"(%1216, %40) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc450)
        "ttnn.deallocate"(%1216) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc450)
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc450)
        %1218 = "ttnn.to_memory_config"(%1217) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc977)
        %1219 = "ttnn.matmul"(%1217, %arg201) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc978)
        "ttnn.deallocate"(%1217) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc978)
        "ttnn.deallocate"(%arg201) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc978)
        %1220 = "ttnn.add"(%1219, %96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc979)
        "ttnn.deallocate"(%1219) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc979)
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc979)
        %1221 = "ttnn.to_memory_config"(%1220) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1164)
        "ttnn.deallocate"(%1220) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1164)
        %1222 = "ttnn.reshape"(%1221) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc153)
        "ttnn.deallocate"(%1221) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc153)
        %1223 = "ttnn.permute"(%1222) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc153)
        "ttnn.deallocate"(%1222) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc153)
        %1224 = "ttnn.reshape"(%1223) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc153)
        "ttnn.deallocate"(%1223) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc153)
        %1225 = "ttnn.to_memory_config"(%1218) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1165)
        %1226 = "ttnn.matmul"(%1225, %arg199) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc980)
        "ttnn.deallocate"(%1225) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc980)
        "ttnn.deallocate"(%arg199) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc980)
        %1227 = "ttnn.add"(%1226, %56) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc981)
        "ttnn.deallocate"(%1226) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc981)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc981)
        %1228 = "ttnn.to_memory_config"(%1227) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1166)
        "ttnn.deallocate"(%1227) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1166)
        %1229 = "ttnn.reshape"(%1228) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc79)
        "ttnn.deallocate"(%1228) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc79)
        %1230 = "ttnn.permute"(%1229) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x64x50xbf16, #ttnn_layout5> loc(#loc79)
        "ttnn.deallocate"(%1229) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc79)
        %1231 = "ttnn.reshape"(%1230) <{shape = [24 : i32, 64 : i32, 50 : i32]}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> tensor<24x64x50xbf16, #ttnn_layout49> loc(#loc79)
        "ttnn.deallocate"(%1230) <{force = false}> : (tensor<2x12x64x50xbf16, #ttnn_layout5>) -> () loc(#loc79)
        %1232 = "ttnn.to_memory_config"(%1224) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc982)
        "ttnn.deallocate"(%1224) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc982)
        %1233 = "ttnn.matmul"(%1232, %1231) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>, tensor<24x64x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc453)
        "ttnn.deallocate"(%1232) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc453)
        "ttnn.deallocate"(%1231) <{force = false}> : (tensor<24x64x50xbf16, #ttnn_layout49>) -> () loc(#loc453)
        %1234 = "ttnn.to_memory_config"(%1233) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> tensor<24x50x50xbf16, #ttnn_layout51> loc(#loc983)
        "ttnn.deallocate"(%1233) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc983)
        %1235 = "ttnn.reshape"(%1234) <{shape = [2 : i32, 12 : i32, 50 : i32, 50 : i32]}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc454)
        "ttnn.deallocate"(%1234) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout51>) -> () loc(#loc454)
        %1236 = "ttnn.multiply"(%1235, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>, tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc455)
        "ttnn.deallocate"(%1235) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc455)
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc455)
        %1237 = "ttnn.typecast"(%1236) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc456)
        "ttnn.deallocate"(%1236) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc456)
        %1238 = "ttnn.softmax"(%1237) <{dimension = 3 : si32, numericStable = true}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xf32, #ttnn_layout52> loc(#loc457)
        "ttnn.deallocate"(%1237) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc457)
        %1239 = "ttnn.typecast"(%1238) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> tensor<2x12x50x50xbf16, #ttnn_layout5> loc(#loc458)
        "ttnn.deallocate"(%1238) <{force = false}> : (tensor<2x12x50x50xf32, #ttnn_layout52>) -> () loc(#loc458)
        %1240 = "ttnn.reshape"(%1239) <{shape = [24 : i32, 50 : i32, 50 : i32]}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> tensor<24x50x50xbf16, #ttnn_layout49> loc(#loc458)
        "ttnn.deallocate"(%1239) <{force = false}> : (tensor<2x12x50x50xbf16, #ttnn_layout5>) -> () loc(#loc458)
        %1241 = "ttnn.to_memory_config"(%1218) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1167)
        "ttnn.deallocate"(%1218) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc1167)
        %1242 = "ttnn.matmul"(%1241, %arg12) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc984)
        "ttnn.deallocate"(%1241) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc984)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc984)
        %1243 = "ttnn.add"(%1242, %117) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc985)
        "ttnn.deallocate"(%1242) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc985)
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc985)
        %1244 = "ttnn.to_memory_config"(%1243) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1168)
        "ttnn.deallocate"(%1243) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1168)
        %1245 = "ttnn.reshape"(%1244) <{shape = [2 : i32, 50 : i32, 12 : i32, 64 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x12x64xbf16, #ttnn_layout6> loc(#loc183)
        "ttnn.deallocate"(%1244) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc183)
        %1246 = "ttnn.permute"(%1245) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc183)
        "ttnn.deallocate"(%1245) <{force = false}> : (tensor<2x50x12x64xbf16, #ttnn_layout6>) -> () loc(#loc183)
        %1247 = "ttnn.reshape"(%1246) <{shape = [24 : i32, 50 : i32, 64 : i32]}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<24x50x64xbf16, #ttnn_layout49> loc(#loc183)
        "ttnn.deallocate"(%1246) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc183)
        %1248 = "ttnn.to_memory_config"(%1240) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>]>, <32x64>, <row_major>>>}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> tensor<24x50x50xbf16, #ttnn_layout50> loc(#loc986)
        "ttnn.deallocate"(%1240) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout49>) -> () loc(#loc986)
        %1249 = "ttnn.matmul"(%1248, %1247) <{transpose_a = false, transpose_b = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>, tensor<24x50x64xbf16, #ttnn_layout49>) -> tensor<24x50x64xbf16, #ttnn_layout50> loc(#loc459)
        "ttnn.deallocate"(%1248) <{force = false}> : (tensor<24x50x50xbf16, #ttnn_layout50>) -> () loc(#loc459)
        "ttnn.deallocate"(%1247) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout49>) -> () loc(#loc459)
        %1250 = "ttnn.to_memory_config"(%1249) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> tensor<24x50x64xbf16, #ttnn_layout51> loc(#loc987)
        "ttnn.deallocate"(%1249) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout50>) -> () loc(#loc987)
        %1251 = "ttnn.reshape"(%1250) <{shape = [2 : i32, 12 : i32, 50 : i32, 64 : i32]}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> tensor<2x12x50x64xbf16, #ttnn_layout5> loc(#loc460)
        "ttnn.deallocate"(%1250) <{force = false}> : (tensor<24x50x64xbf16, #ttnn_layout51>) -> () loc(#loc460)
        %1252 = "ttnn.concatenate_heads"(%1251) : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc461)
        "ttnn.deallocate"(%1251) <{force = false}> : (tensor<2x12x50x64xbf16, #ttnn_layout5>) -> () loc(#loc461)
        %1253 = "ttnn.reshape"(%1252) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc461)
        "ttnn.deallocate"(%1252) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc461)
        %1254 = "ttnn.to_memory_config"(%1253) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,3)>]>, <32x96>, <row_major>>>}> : (tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc1169)
        "ttnn.deallocate"(%1253) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc1169)
        %1255 = "ttnn.matmul"(%1254, %arg10) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<768x768xbf16, #ttnn_layout30>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc988)
        "ttnn.deallocate"(%1254) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc988)
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<768x768xbf16, #ttnn_layout30>) -> () loc(#loc988)
        %1256 = "ttnn.add"(%1255, %71) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc989)
        "ttnn.deallocate"(%1255) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc989)
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc989)
        %1257 = "ttnn.to_memory_config"(%1256) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout47>) -> tensor<100x768xbf16, #ttnn_layout48> loc(#loc1170)
        "ttnn.deallocate"(%1256) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc1170)
        %1258 = "ttnn.reshape"(%1257) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout48>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc126)
        "ttnn.deallocate"(%1257) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout48>) -> () loc(#loc126)
        %1259 = "ttnn.add"(%1198, %1258) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc462)
        "ttnn.deallocate"(%1258) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc462)
        "ttnn.deallocate"(%1198) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc462)
        %1260 = "ttnn.to_memory_config"(%1259) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc990)
        "ttnn.deallocate"(%1259) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc990)
        %1261 = "ttnn.sum"(%1260) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc103)
        %1262 = "ttnn.multiply"(%1261, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc103)
        "ttnn.deallocate"(%1261) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc103)
        %1263 = "ttnn.to_memory_config"(%1262) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc991)
        "ttnn.deallocate"(%1262) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc991)
        %1264 = "ttnn.reshape"(%1263) <{shape = [2 : i32, 50 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc103)
        "ttnn.deallocate"(%1263) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc103)
        %1265 = "ttnn.neg"(%1264) : (tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x1xbf16, #ttnn_layout17> loc(#loc992)
        "ttnn.deallocate"(%1264) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc992)
        %1266 = "ttnn.add"(%1260, %1265) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x1xbf16, #ttnn_layout17>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc463)
        "ttnn.deallocate"(%1265) <{force = false}> : (tensor<2x50x1xbf16, #ttnn_layout17>) -> () loc(#loc463)
        %1267 = "ttnn.to_memory_config"(%1266) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc993)
        "ttnn.deallocate"(%1266) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc993)
        %1268 = "ttnn.reshape"(%1267) <{shape = [100 : i32, 768 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<100x768xbf16, #ttnn_layout2> loc(#loc1221)
        %1269 = "ttnn.multiply"(%1267, %1267) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc103)
        "ttnn.deallocate"(%1267) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc103)
        %1270 = "ttnn.to_memory_config"(%1269) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc991)
        "ttnn.deallocate"(%1269) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc991)
        %1271 = "ttnn.sum"(%1270) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc103)
        "ttnn.deallocate"(%1270) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc103)
        %1272 = "ttnn.multiply"(%1271, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout15>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc103)
        "ttnn.deallocate"(%1271) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc103)
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc103)
        %1273 = "ttnn.add"(%1272, %57#24) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50xbf16, #ttnn_layout42>, tensor<2x50xbf16, #ttnn_layout15>) -> tensor<2x50xbf16, #ttnn_layout42> loc(#loc464)
        "ttnn.deallocate"(%1272) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc464)
        "ttnn.deallocate"(%57#24) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc464)
        %1274 = "ttnn.to_memory_config"(%1273) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50xbf16, #ttnn_layout42>) -> tensor<2x50xbf16, #ttnn_layout> loc(#loc994)
        "ttnn.deallocate"(%1273) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout42>) -> () loc(#loc994)
        %1275 = "ttnn.rsqrt"(%1274) : (tensor<2x50xbf16, #ttnn_layout>) -> tensor<2x50xbf16, #ttnn_layout15> loc(#loc465)
        "ttnn.deallocate"(%1274) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout>) -> () loc(#loc465)
        %1276 = "ttnn.reshape"(%1275) <{shape = [100 : i32, 1 : i32]}> : (tensor<2x50xbf16, #ttnn_layout15>) -> tensor<100x1xbf16, #ttnn_layout46> loc(#loc465)
        "ttnn.deallocate"(%1275) <{force = false}> : (tensor<2x50xbf16, #ttnn_layout15>) -> () loc(#loc465)
        %1277 = "ttnn.multiply"(%1268, %1276) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout2>, tensor<100x1xbf16, #ttnn_layout46>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc466)
        "ttnn.deallocate"(%1276) <{force = false}> : (tensor<100x1xbf16, #ttnn_layout46>) -> () loc(#loc466)
        "ttnn.deallocate"(%1268) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc466)
        %1278 = "ttnn.multiply"(%1277, %49) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc466)
        "ttnn.deallocate"(%1277) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc466)
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc466)
        %1279 = "ttnn.add"(%1278, %109) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<100x768xbf16, #ttnn_layout47> loc(#loc464)
        "ttnn.deallocate"(%1278) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc464)
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc464)
        %1280 = "ttnn.matmul"(%1279, %arg6) <{transpose_a = false, transpose_b = true}> : (tensor<100x768xbf16, #ttnn_layout47>, tensor<3072x768xbf16, #ttnn_layout29>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc995)
        "ttnn.deallocate"(%1279) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout47>) -> () loc(#loc995)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<3072x768xbf16, #ttnn_layout29>) -> () loc(#loc995)
        %1281 = "ttnn.add"(%1280, %86) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout53>, tensor<1x3072xbf16, #ttnn_layout11>) -> tensor<100x3072xbf16, #ttnn_layout53> loc(#loc996)
        "ttnn.deallocate"(%1280) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc996)
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x3072xbf16, #ttnn_layout11>) -> () loc(#loc996)
        %1282 = "ttnn.to_memory_config"(%1281) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> tensor<100x3072xbf16, #ttnn_layout54> loc(#loc1172)
        "ttnn.deallocate"(%1281) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout53>) -> () loc(#loc1172)
        %1283 = "ttnn.multiply"(%1282, %119#11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc468)
        "ttnn.deallocate"(%119#11) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc468)
        %1284 = "ttnn.to_memory_config"(%1283) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> tensor<100x3072xbf16, #ttnn_layout56> loc(#loc997)
        "ttnn.deallocate"(%1283) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc997)
        %1285 = "ttnn.sigmoid"(%1284) : (tensor<100x3072xbf16, #ttnn_layout56>) -> tensor<100x3072xbf16, #ttnn_layout25> loc(#loc469)
        "ttnn.deallocate"(%1284) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout56>) -> () loc(#loc469)
        %1286 = "ttnn.multiply"(%1282, %1285) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x3072xbf16, #ttnn_layout54>, tensor<100x3072xbf16, #ttnn_layout25>) -> tensor<100x3072xbf16, #ttnn_layout55> loc(#loc468)
        "ttnn.deallocate"(%1285) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout25>) -> () loc(#loc468)
        "ttnn.deallocate"(%1282) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout54>) -> () loc(#loc468)
        %1287 = "ttnn.matmul"(%1286, %arg4) <{transpose_a = false, transpose_b = true}> : (tensor<100x3072xbf16, #ttnn_layout55>, tensor<768x3072xbf16, #ttnn_layout28>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc998)
        "ttnn.deallocate"(%1286) <{force = false}> : (tensor<100x3072xbf16, #ttnn_layout55>) -> () loc(#loc998)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<768x3072xbf16, #ttnn_layout28>) -> () loc(#loc998)
        %1288 = "ttnn.add"(%1287, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<100x768xbf16, #ttnn_layout57>, tensor<100x768xbf16, #ttnn_layout2>) -> tensor<100x768xbf16, #ttnn_layout57> loc(#loc999)
        "ttnn.deallocate"(%1287) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc999)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout2>) -> () loc(#loc999)
        %1289 = "ttnn.to_memory_config"(%1288) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<100x768xbf16, #ttnn_layout57>) -> tensor<100x768xbf16, #ttnn_layout46> loc(#loc1173)
        "ttnn.deallocate"(%1288) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout57>) -> () loc(#loc1173)
        %1290 = "ttnn.reshape"(%1289) <{shape = [2 : i32, 50 : i32, 768 : i32]}> : (tensor<100x768xbf16, #ttnn_layout46>) -> tensor<2x50x768xbf16, #ttnn_layout8> loc(#loc25)
        "ttnn.deallocate"(%1289) <{force = false}> : (tensor<100x768xbf16, #ttnn_layout46>) -> () loc(#loc25)
        %1291 = "ttnn.add"(%1260, %1290) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x50x768xbf16, #ttnn_layout33>, tensor<2x50x768xbf16, #ttnn_layout8>) -> tensor<2x50x768xbf16, #ttnn_layout43> loc(#loc470)
        "ttnn.deallocate"(%1290) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout8>) -> () loc(#loc470)
        "ttnn.deallocate"(%1260) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> () loc(#loc470)
        %1292 = "ttnn.to_memory_config"(%1291) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc1000)
        "ttnn.deallocate"(%1291) <{force = false}> : (tensor<2x50x768xbf16, #ttnn_layout43>) -> () loc(#loc1000)
        %1293 = "ttnn.slice_static"(%1292) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2 : i32, 1 : i32, 768 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2x50x768xbf16, #ttnn_layout33>) -> tensor<2x1x768xbf16, #ttnn_layout14> loc(#loc471)
        %1294 = "ttnn.reshape"(%1293) <{shape = [2 : i32, 768 : i32]}> : (tensor<2x1x768xbf16, #ttnn_layout14>) -> tensor<2x768xbf16, #ttnn_layout9> loc(#loc472)
        "ttnn.deallocate"(%1293) <{force = false}> : (tensor<2x1x768xbf16, #ttnn_layout14>) -> () loc(#loc472)
        %1295 = "ttnn.sum"(%1294) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<2x768xbf16, #ttnn_layout9>) -> tensor<2xbf16, #ttnn_layout16> loc(#loc473)
        %1296 = "ttnn.multiply"(%1295, %51) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2xbf16, #ttnn_layout16>, tensor<2xbf16, #ttnn_layout16>) -> tensor<2xbf16, #ttnn_layout58> loc(#loc473)
        "ttnn.deallocate"(%1295) <{force = false}> : (tensor<2xbf16, #ttnn_layout16>) -> () loc(#loc473)
        %1297 = "ttnn.to_memory_config"(%1296) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2xbf16, #ttnn_layout58>) -> tensor<2xbf16, #ttnn_layout16> loc(#loc1001)
        "ttnn.deallocate"(%1296) <{force = false}> : (tensor<2xbf16, #ttnn_layout58>) -> () loc(#loc1001)
        %1298 = "ttnn.reshape"(%1297) <{shape = [2 : i32, 1 : i32]}> : (tensor<2xbf16, #ttnn_layout16>) -> tensor<2x1xbf16, #ttnn_layout> loc(#loc473)
        "ttnn.deallocate"(%1297) <{force = false}> : (tensor<2xbf16, #ttnn_layout16>) -> () loc(#loc473)
        %1299 = "ttnn.neg"(%1298) : (tensor<2x1xbf16, #ttnn_layout>) -> tensor<2x1xbf16, #ttnn_layout> loc(#loc1002)
        "ttnn.deallocate"(%1298) <{force = false}> : (tensor<2x1xbf16, #ttnn_layout>) -> () loc(#loc1002)
        %1300 = "ttnn.add"(%1294, %1299) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x768xbf16, #ttnn_layout9>, tensor<2x1xbf16, #ttnn_layout>) -> tensor<2x768xbf16, #ttnn_layout59> loc(#loc474)
        "ttnn.deallocate"(%1299) <{force = false}> : (tensor<2x1xbf16, #ttnn_layout>) -> () loc(#loc474)
        "ttnn.deallocate"(%1294) <{force = false}> : (tensor<2x768xbf16, #ttnn_layout9>) -> () loc(#loc474)
        %1301 = "ttnn.to_memory_config"(%1300) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x768xbf16, #ttnn_layout59>) -> tensor<2x768xbf16, #ttnn_layout> loc(#loc1003)
        "ttnn.deallocate"(%1300) <{force = false}> : (tensor<2x768xbf16, #ttnn_layout59>) -> () loc(#loc1003)
        %1302 = "ttnn.multiply"(%1301, %1301) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x768xbf16, #ttnn_layout>, tensor<2x768xbf16, #ttnn_layout>) -> tensor<2x768xbf16, #ttnn_layout59> loc(#loc473)
        %1303 = "ttnn.to_memory_config"(%1302) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x768xbf16, #ttnn_layout59>) -> tensor<2x768xbf16, #ttnn_layout> loc(#loc1001)
        "ttnn.deallocate"(%1302) <{force = false}> : (tensor<2x768xbf16, #ttnn_layout59>) -> () loc(#loc1001)
        %1304 = "ttnn.sum"(%1303) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<2x768xbf16, #ttnn_layout>) -> tensor<2xbf16, #ttnn_layout16> loc(#loc473)
        "ttnn.deallocate"(%1303) <{force = false}> : (tensor<2x768xbf16, #ttnn_layout>) -> () loc(#loc473)
        %1305 = "ttnn.multiply"(%1304, %51) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2xbf16, #ttnn_layout16>, tensor<2xbf16, #ttnn_layout16>) -> tensor<2xbf16, #ttnn_layout58> loc(#loc473)
        "ttnn.deallocate"(%1304) <{force = false}> : (tensor<2xbf16, #ttnn_layout16>) -> () loc(#loc473)
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<2xbf16, #ttnn_layout16>) -> () loc(#loc473)
        %1306 = "ttnn.to_memory_config"(%1305) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2xbf16, #ttnn_layout58>) -> tensor<2xbf16, #ttnn_layout16> loc(#loc1001)
        "ttnn.deallocate"(%1305) <{force = false}> : (tensor<2xbf16, #ttnn_layout58>) -> () loc(#loc1001)
        %1307 = "ttnn.reshape"(%1306) <{shape = [2 : i32, 1 : i32]}> : (tensor<2xbf16, #ttnn_layout16>) -> tensor<2x1xbf16, #ttnn_layout> loc(#loc473)
        "ttnn.deallocate"(%1306) <{force = false}> : (tensor<2xbf16, #ttnn_layout16>) -> () loc(#loc473)
        %1308 = "ttnn.add"(%1307, %0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x1xbf16, #ttnn_layout>, tensor<2x1xbf16, #ttnn_layout>) -> tensor<2x1xbf16, #ttnn_layout60> loc(#loc75)
        "ttnn.deallocate"(%1307) <{force = false}> : (tensor<2x1xbf16, #ttnn_layout>) -> () loc(#loc75)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<2x1xbf16, #ttnn_layout>) -> () loc(#loc75)
        %1309 = "ttnn.to_memory_config"(%1308) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x1xbf16, #ttnn_layout60>) -> tensor<2x1xbf16, #ttnn_layout> loc(#loc1004)
        "ttnn.deallocate"(%1308) <{force = false}> : (tensor<2x1xbf16, #ttnn_layout60>) -> () loc(#loc1004)
        %1310 = "ttnn.rsqrt"(%1309) : (tensor<2x1xbf16, #ttnn_layout>) -> tensor<2x1xbf16, #ttnn_layout> loc(#loc475)
        "ttnn.deallocate"(%1309) <{force = false}> : (tensor<2x1xbf16, #ttnn_layout>) -> () loc(#loc475)
        %1311 = "ttnn.multiply"(%1301, %1310) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x768xbf16, #ttnn_layout>, tensor<2x1xbf16, #ttnn_layout>) -> tensor<2x768xbf16, #ttnn_layout59> loc(#loc68)
        "ttnn.deallocate"(%1310) <{force = false}> : (tensor<2x1xbf16, #ttnn_layout>) -> () loc(#loc68)
        "ttnn.deallocate"(%1301) <{force = false}> : (tensor<2x768xbf16, #ttnn_layout>) -> () loc(#loc68)
        %1312 = "ttnn.multiply"(%1311, %46) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x768xbf16, #ttnn_layout59>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<2x768xbf16, #ttnn_layout59> loc(#loc68)
        "ttnn.deallocate"(%1311) <{force = false}> : (tensor<2x768xbf16, #ttnn_layout59>) -> () loc(#loc68)
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc68)
        %1313 = "ttnn.add"(%1312, %53) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2x768xbf16, #ttnn_layout59>, tensor<1x768xbf16, #ttnn_layout9>) -> tensor<2x768xbf16, #ttnn_layout59> loc(#loc75)
        "ttnn.deallocate"(%1312) <{force = false}> : (tensor<2x768xbf16, #ttnn_layout59>) -> () loc(#loc75)
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout9>) -> () loc(#loc75)
        %1314 = "ttnn.to_memory_config"(%1313) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2x768xbf16, #ttnn_layout59>) -> tensor<2x768xbf16, #ttnn_layout> loc(#loc1004)
        "ttnn.deallocate"(%1313) <{force = false}> : (tensor<2x768xbf16, #ttnn_layout59>) -> () loc(#loc1004)
        %1315 = "ttnn.matmul"(%1314, %arg0) <{transpose_a = false, transpose_b = true}> : (tensor<2x768xbf16, #ttnn_layout>, tensor<512x768xbf16, #ttnn_layout27>) -> tensor<2x512xbf16, #ttnn_layout32> loc(#loc476)
        "ttnn.deallocate"(%1314) <{force = false}> : (tensor<2x768xbf16, #ttnn_layout>) -> () loc(#loc476)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<512x768xbf16, #ttnn_layout27>) -> () loc(#loc476)
        return %1315, %1292 : tensor<2x512xbf16, #ttnn_layout32>, tensor<2x50x768xbf16, #ttnn_layout33> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc2 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc3 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc4 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc5 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc6 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc7 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc8 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc9 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc10 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc11 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc12 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc13 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc14 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|227|aten__expand")
#loc15 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|228|aten__cat")
#loc16 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc17 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc18 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc19 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc20 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc21 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc22 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc23 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc24 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc25 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc26 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc27 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc28 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc29 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc30 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc31 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc32 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc33 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc34 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc35 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc36 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc37 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc38 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc39 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc40 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc41 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc42 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc43 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc44 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc45 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc46 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc47 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc48 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc49 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc50 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc51 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc52 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc53 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc54 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc55 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc56 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc57 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc58 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc59 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc60 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc61 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc62 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc63 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc64 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc65 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc66 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc67 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc68 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__mul")
#loc69 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc70 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc71 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc72 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc73 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc74 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc75 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__add")
#loc76 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc77 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc78 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc79 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc80 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc81 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc82 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc83 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc84 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc85 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc86 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc87 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc88 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc89 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc90 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc91 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc92 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc93 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc94 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc95 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc96 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc97 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc98 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc99 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc100 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc101 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc102 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc103 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc104 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc105 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc106 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc107 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc108 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc109 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc110 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc111 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__mul")
#loc112 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc113 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc114 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc115 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc116 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc117 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc118 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc119 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc120 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc121 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc122 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc123 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc124 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc125 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc126 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc127 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc128 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc129 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc130 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc131 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc132 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc133 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc134 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc135 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc136 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc137 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Embedding[vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__index_select")
#loc138 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__add")
#loc139 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc140 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc141 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc142 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc143 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc144 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc145 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc146 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc147 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc148 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc149 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc150 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc151 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc152 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc153 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc154 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc155 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc156 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc157 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc158 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc159 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc160 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc161 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc162 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc163 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc164 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc165 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc166 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc167 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc168 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc169 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__add")
#loc170 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc171 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc172 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc173 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc174 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc175 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc176 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc177 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc178 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc179 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc180 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Conv2d[vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|224|aten__convolution_overrideable")
#loc181 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc182 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc183 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc184 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc185 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc186 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc187 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc188 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc189 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc190 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc191 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc192 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc193 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc194 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc195 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc196 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc197 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc198 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc199 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc200 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc201 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc203 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|225|aten__view")
#loc204 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__var_mean")
#loc205 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__sub")
#loc206 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__rsqrt")
#loc207 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc208 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc209 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc210 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc211 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc212 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc213 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc214 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc215 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc216 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc217 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc218 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc219 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc220 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc221 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc222 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc223 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc224 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc225 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc226 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|QuickGELUActivation[vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc227 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|QuickGELUActivation[vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc228 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc229 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc230 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc231 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc232 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc233 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc234 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc235 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc236 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc237 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc238 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc239 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc240 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc241 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc242 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc243 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc244 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc245 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc246 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc247 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc248 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|QuickGELUActivation[vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc249 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|QuickGELUActivation[vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc250 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc251 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc252 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc253 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc254 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc255 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc256 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc257 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc258 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc259 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc260 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc261 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc262 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc263 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc264 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc265 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc266 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc267 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc268 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc269 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc270 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|QuickGELUActivation[vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc271 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|QuickGELUActivation[vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc272 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc273 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc274 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc275 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc276 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc277 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc278 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc279 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc280 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc281 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc282 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc283 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc284 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc285 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc286 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc287 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc288 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc289 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc290 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc291 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc292 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|QuickGELUActivation[vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc293 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|QuickGELUActivation[vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc294 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc295 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc296 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc297 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc298 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc299 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc300 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc301 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc302 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc303 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc304 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc305 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc306 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc307 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc308 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc309 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc310 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc311 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc312 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc313 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc314 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|QuickGELUActivation[vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc315 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|QuickGELUActivation[vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc316 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc317 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc318 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc319 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc320 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc321 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc322 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc323 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc324 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc325 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc326 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc327 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc328 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc329 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc330 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc331 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc332 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc333 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc334 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc335 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc336 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|QuickGELUActivation[vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc337 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|QuickGELUActivation[vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc338 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc339 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc340 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc341 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc342 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc343 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc344 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc345 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc346 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc347 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc348 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc349 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc350 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc351 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc352 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc353 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc354 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc355 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc356 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc357 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc358 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|QuickGELUActivation[vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc359 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|QuickGELUActivation[vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc360 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc361 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc362 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc363 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc364 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc365 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc366 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc367 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc368 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc369 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc370 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc371 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc372 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc373 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc374 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc375 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc376 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc377 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc378 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc379 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc380 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|QuickGELUActivation[vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc381 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|QuickGELUActivation[vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc382 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc383 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc384 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc385 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc386 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc387 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc388 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc389 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc390 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc391 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc392 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc393 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc394 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc395 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc396 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc397 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc398 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc399 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc400 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc401 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc402 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|QuickGELUActivation[vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc403 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|QuickGELUActivation[vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc404 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc405 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc406 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc407 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc408 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc409 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc410 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc411 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc412 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc413 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc414 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc415 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc416 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc417 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc418 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc419 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc420 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc421 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc422 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc423 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc424 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|QuickGELUActivation[vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc425 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|QuickGELUActivation[vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc426 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc427 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc428 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc429 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc430 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc431 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc432 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc433 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc434 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc435 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc436 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc437 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc438 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc439 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc440 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc441 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc442 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc443 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc444 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc445 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc446 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|QuickGELUActivation[vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc447 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|QuickGELUActivation[vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc448 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc449 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc450 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc451 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc452 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc453 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc454 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc455 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc456 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc457 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc458 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc459 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc460 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc461 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc462 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc463 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc464 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc465 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc466 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc467 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc468 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|QuickGELUActivation[vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc469 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|QuickGELUActivation[vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc470 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc471 = loc("CLIPVisionTransformer[vision_model]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|792|xla__generic_slice")
#loc472 = loc("CLIPVisionTransformer[vision_model]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|792|aten__view")
#loc473 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__var_mean")
#loc474 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__sub")
#loc475 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__rsqrt")
#loc476 = loc("Linear[visual_projection]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:1169|forward|1203|aten__mm")
#loc477 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc1))
#loc478 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc2))
#loc479 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc3))
#loc480 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc4))
#loc481 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc5))
#loc482 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc6))
#loc483 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc7))
#loc484 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc8))
#loc485 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc9))
#loc486 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc10))
#loc487 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc11))
#loc488 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc12))
#loc489 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc13))
#loc490 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc16))
#loc491 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc17))
#loc492 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc18))
#loc493 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc19))
#loc494 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc20))
#loc495 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc21))
#loc496 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc22))
#loc497 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc23))
#loc498 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc24))
#loc499 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc25))
#loc500 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc26))
#loc501 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc27))
#loc502 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc28))
#loc503 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc29))
#loc504 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc30))
#loc505 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc31))
#loc506 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc32))
#loc507 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc33))
#loc508 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc34))
#loc509 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc35))
#loc510 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc36))
#loc511 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc37))
#loc512 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc38))
#loc513 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc39))
#loc514 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc40))
#loc515 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc41))
#loc516 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc42))
#loc517 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc43))
#loc518 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc44))
#loc519 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc45))
#loc520 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc46))
#loc521 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc47))
#loc522 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc48))
#loc523 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc49))
#loc524 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc10))
#loc525 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc46))
#loc526 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc50))
#loc527 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc12))
#loc528 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc51))
#loc529 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc52))
#loc530 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc53))
#loc531 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc54))
#loc532 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc55))
#loc533 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc56))
#loc534 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc57))
#loc535 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc58))
#loc536 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc59))
#loc537 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc60))
#loc538 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc61))
#loc539 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc62))
#loc540 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc63))
#loc541 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc64))
#loc542 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc65))
#loc543 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc66))
#loc544 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc67))
#loc545 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc40))
#loc546 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc69))
#loc547 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc70))
#loc548 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc71))
#loc549 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc9))
#loc550 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc72))
#loc551 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc73))
#loc552 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc74))
#loc553 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc76))
#loc554 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc62))
#loc555 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc77))
#loc556 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc78))
#loc557 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc79))
#loc558 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc104))
#loc559 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc105))
#loc560 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc106))
#loc561 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc107))
#loc562 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc108))
#loc563 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc109))
#loc564 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc110))
#loc565 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc112))
#loc566 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc113))
#loc567 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc114))
#loc568 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc42))
#loc569 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc115))
#loc570 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc116))
#loc571 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc117))
#loc572 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc118))
#loc573 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc119))
#loc574 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc120))
#loc575 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc121))
#loc576 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc122))
#loc577 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc123))
#loc578 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc124))
#loc579 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc125))
#loc580 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc126))
#loc581 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc127))
#loc582 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc128))
#loc583 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc129))
#loc584 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc67))
#loc585 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc127))
#loc586 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc130))
#loc587 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc131))
#loc588 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc132))
#loc589 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc133))
#loc590 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc63))
#loc591 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc134))
#loc592 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc135))
#loc593 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc136))
#loc594 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc121))
#loc595 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc139))
#loc596 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc140))
#loc597 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc141))
#loc598 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc142))
#loc599 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc139))
#loc600 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc143))
#loc601 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc125))
#loc602 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc144))
#loc603 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc145))
#loc604 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc146))
#loc605 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc147))
#loc606 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc148))
#loc607 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc149))
#loc608 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc150))
#loc609 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc43))
#loc610 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc128))
#loc611 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc7))
#loc612 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc151))
#loc613 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc152))
#loc614 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc153))
#loc615 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc154))
#loc616 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc155))
#loc617 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc156))
#loc618 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc157))
#loc619 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc158))
#loc620 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc159))
#loc621 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc160))
#loc622 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc161))
#loc623 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc162))
#loc624 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc163))
#loc625 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc164))
#loc626 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc165))
#loc627 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view_tm1"(#loc166))
#loc628 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute_tm1"(#loc167))
#loc629 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_tm0"(#loc168))
#loc630 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc129))
#loc631 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc70))
#loc632 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc170))
#loc633 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc171))
#loc634 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc172))
#loc635 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc173))
#loc636 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc71))
#loc637 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc174))
#loc638 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc175))
#loc639 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc176))
#loc640 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc177))
#loc641 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc178))
#loc642 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc179))
#loc643 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm1"(#loc104))
#loc644 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Conv2d[vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|224|aten__convolution_overrideable_prepare_conv2d_weight"(#loc180))
#loc645 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm1"(#loc124))
#loc646 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc181))
#loc647 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc182))
#loc648 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc183))
#loc649 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc184))
#loc650 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc185))
#loc651 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1"(#loc186))
#loc652 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc123))
#loc653 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc187))
#loc654 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc188))
#loc655 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc189))
#loc656 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0"(#loc11))
#loc657 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0"(#loc170))
#loc658 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc190))
#loc659 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc191))
#loc660 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc192))
#loc661 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc193))
#loc662 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view_tm1"(#loc194))
#loc663 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute_tm1"(#loc195))
#loc664 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_tm0"(#loc196))
#loc665 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view_tm1"(#loc197))
#loc666 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute_tm1"(#loc198))
#loc667 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_tm0"(#loc199))
#loc668 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_tm0"(#loc200))
#loc669 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_tm0"(#loc201))
#loc670 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Conv2d[vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|224|aten__convolution_overrideable_input"(#loc180))
#loc671 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Conv2d[vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|224|aten__convolution_overrideable_reshape"(#loc180))
#loc672 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__add_spill"(#loc138))
#loc673 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__var_mean_spill"(#loc204))
#loc674 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__sub_neg"(#loc205))
#loc675 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__sub_spill"(#loc205))
#loc676 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__add_spill"(#loc169))
#loc677 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc80))
#loc678 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc207))
#loc679 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc207))
#loc680 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc208))
#loc681 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc30))
#loc682 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc30))
#loc683 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc189))
#loc684 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc189))
#loc685 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc211))
#loc686 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc211))
#loc687 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc178))
#loc688 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc178))
#loc689 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc217))
#loc690 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc217))
#loc691 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc44))
#loc692 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc44))
#loc693 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc220))
#loc694 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc81))
#loc695 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc221))
#loc696 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc221))
#loc697 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc222))
#loc698 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc225))
#loc699 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc225))
#loc700 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|QuickGELUActivation[vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc226))
#loc701 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc51))
#loc702 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc51))
#loc703 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc228))
#loc704 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc82))
#loc705 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc229))
#loc706 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc229))
#loc707 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc230))
#loc708 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc117))
#loc709 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc117))
#loc710 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc164))
#loc711 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc164))
#loc712 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc233))
#loc713 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc233))
#loc714 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc37))
#loc715 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc37))
#loc716 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc239))
#loc717 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc239))
#loc718 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc155))
#loc719 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc155))
#loc720 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc242))
#loc721 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc83))
#loc722 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc243))
#loc723 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc243))
#loc724 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc244))
#loc725 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc247))
#loc726 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc247))
#loc727 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|QuickGELUActivation[vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc248))
#loc728 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc13))
#loc729 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc13))
#loc730 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc250))
#loc731 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc84))
#loc732 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc251))
#loc733 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc251))
#loc734 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc252))
#loc735 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc161))
#loc736 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc161))
#loc737 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc173))
#loc738 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc173))
#loc739 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc255))
#loc740 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc255))
#loc741 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc136))
#loc742 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc136))
#loc743 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc261))
#loc744 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc261))
#loc745 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc50))
#loc746 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc50))
#loc747 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc264))
#loc748 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc85))
#loc749 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc265))
#loc750 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc265))
#loc751 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc266))
#loc752 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc269))
#loc753 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc269))
#loc754 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|QuickGELUActivation[vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc270))
#loc755 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc38))
#loc756 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc38))
#loc757 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc272))
#loc758 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc86))
#loc759 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc273))
#loc760 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc273))
#loc761 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc274))
#loc762 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc54))
#loc763 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc54))
#loc764 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc114))
#loc765 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc114))
#loc766 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc277))
#loc767 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc277))
#loc768 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc192))
#loc769 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc192))
#loc770 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc283))
#loc771 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc283))
#loc772 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc147))
#loc773 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc147))
#loc774 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc286))
#loc775 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc87))
#loc776 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc287))
#loc777 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc287))
#loc778 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc288))
#loc779 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc291))
#loc780 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc291))
#loc781 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|QuickGELUActivation[vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc292))
#loc782 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc149))
#loc783 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc149))
#loc784 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc294))
#loc785 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc88))
#loc786 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc295))
#loc787 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc295))
#loc788 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc296))
#loc789 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc57))
#loc790 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc57))
#loc791 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc142))
#loc792 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc142))
#loc793 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc299))
#loc794 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc299))
#loc795 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc196))
#loc796 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc196))
#loc797 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc305))
#loc798 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc305))
#loc799 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc148))
#loc800 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc148))
#loc801 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc308))
#loc802 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc89))
#loc803 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc309))
#loc804 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc309))
#loc805 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc310))
#loc806 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc313))
#loc807 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc313))
#loc808 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|QuickGELUActivation[vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc314))
#loc809 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc193))
#loc810 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc193))
#loc811 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc316))
#loc812 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc90))
#loc813 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc317))
#loc814 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc317))
#loc815 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc318))
#loc816 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc132))
#loc817 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc132))
#loc818 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc61))
#loc819 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc61))
#loc820 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc321))
#loc821 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc321))
#loc822 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc74))
#loc823 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc74))
#loc824 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc327))
#loc825 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc327))
#loc826 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc179))
#loc827 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc179))
#loc828 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc330))
#loc829 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc91))
#loc830 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc331))
#loc831 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc331))
#loc832 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc332))
#loc833 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc335))
#loc834 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc335))
#loc835 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|QuickGELUActivation[vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc336))
#loc836 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc58))
#loc837 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc58))
#loc838 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc338))
#loc839 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc92))
#loc840 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc339))
#loc841 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc339))
#loc842 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc340))
#loc843 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc146))
#loc844 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc146))
#loc845 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc3))
#loc846 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc3))
#loc847 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc343))
#loc848 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc343))
#loc849 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc120))
#loc850 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc120))
#loc851 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc349))
#loc852 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc349))
#loc853 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc200))
#loc854 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc200))
#loc855 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc352))
#loc856 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc93))
#loc857 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc353))
#loc858 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc353))
#loc859 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc354))
#loc860 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc357))
#loc861 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc357))
#loc862 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|QuickGELUActivation[vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc358))
#loc863 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc150))
#loc864 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc150))
#loc865 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc360))
#loc866 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc94))
#loc867 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc361))
#loc868 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc361))
#loc869 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc362))
#loc870 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc107))
#loc871 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc107))
#loc872 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc199))
#loc873 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc199))
#loc874 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc365))
#loc875 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc365))
#loc876 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc6))
#loc877 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc6))
#loc878 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc371))
#loc879 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc371))
#loc880 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc133))
#loc881 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc133))
#loc882 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc374))
#loc883 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc95))
#loc884 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc375))
#loc885 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc375))
#loc886 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc376))
#loc887 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc379))
#loc888 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc379))
#loc889 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|QuickGELUActivation[vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc380))
#loc890 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc201))
#loc891 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc201))
#loc892 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc382))
#loc893 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc96))
#loc894 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc383))
#loc895 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc383))
#loc896 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc384))
#loc897 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc24))
#loc898 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc24))
#loc899 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc18))
#loc900 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc18))
#loc901 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc387))
#loc902 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc387))
#loc903 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc158))
#loc904 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc158))
#loc905 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc393))
#loc906 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc393))
#loc907 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc69))
#loc908 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc69))
#loc909 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc396))
#loc910 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc97))
#loc911 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc397))
#loc912 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc397))
#loc913 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc398))
#loc914 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc401))
#loc915 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc401))
#loc916 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|QuickGELUActivation[vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc402))
#loc917 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc174))
#loc918 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc174))
#loc919 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc404))
#loc920 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc98))
#loc921 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc405))
#loc922 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc405))
#loc923 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc406))
#loc924 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc49))
#loc925 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc49))
#loc926 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc21))
#loc927 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc21))
#loc928 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc409))
#loc929 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc409))
#loc930 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc110))
#loc931 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc110))
#loc932 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc415))
#loc933 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc415))
#loc934 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc165))
#loc935 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc165))
#loc936 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc418))
#loc937 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc99))
#loc938 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc419))
#loc939 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc419))
#loc940 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc420))
#loc941 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc423))
#loc942 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc423))
#loc943 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|QuickGELUActivation[vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc424))
#loc944 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc39))
#loc945 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc39))
#loc946 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc426))
#loc947 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc100))
#loc948 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc427))
#loc949 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc427))
#loc950 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc428))
#loc951 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc168))
#loc952 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc168))
#loc953 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc33))
#loc954 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc33))
#loc955 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc431))
#loc956 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc431))
#loc957 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc66))
#loc958 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc66))
#loc959 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc437))
#loc960 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc437))
#loc961 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc27))
#loc962 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc27))
#loc963 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc440))
#loc964 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc101))
#loc965 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc441))
#loc966 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc441))
#loc967 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc442))
#loc968 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc445))
#loc969 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc445))
#loc970 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|QuickGELUActivation[vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc446))
#loc971 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc34))
#loc972 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc34))
#loc973 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc448))
#loc974 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean_spill"(#loc102))
#loc975 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_neg"(#loc449))
#loc976 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub_spill"(#loc449))
#loc977 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add_spill"(#loc450))
#loc978 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_matmul"(#loc153))
#loc979 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add"(#loc153))
#loc980 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul"(#loc79))
#loc981 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add"(#loc79))
#loc982 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_mem_reconfig"(#loc453))
#loc983 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul_spill"(#loc453))
#loc984 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul"(#loc183))
#loc985 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add"(#loc183))
#loc986 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_mem_reconfig"(#loc459))
#loc987 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul_spill"(#loc459))
#loc988 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul"(#loc126))
#loc989 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add"(#loc126))
#loc990 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add_spill"(#loc462))
#loc991 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean_spill"(#loc103))
#loc992 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_neg"(#loc463))
#loc993 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub_spill"(#loc463))
#loc994 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add_spill"(#loc464))
#loc995 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_matmul"(#loc467))
#loc996 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add"(#loc467))
#loc997 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|QuickGELUActivation[vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul_spill"(#loc468))
#loc998 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_matmul"(#loc25))
#loc999 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add"(#loc25))
#loc1000 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add_spill"(#loc470))
#loc1001 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__var_mean_spill"(#loc473))
#loc1002 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__sub_neg"(#loc474))
#loc1003 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__sub_spill"(#loc474))
#loc1004 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__add_spill"(#loc75))
#loc1005 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc484))
#loc1006 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc485))
#loc1007 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc488))
#loc1008 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc500))
#loc1009 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc515))
#loc1010 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc516))
#loc1011 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc517))
#loc1012 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc519))
#loc1013 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc524))
#loc1014 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc525))
#loc1015 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc540))
#loc1016 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc545))
#loc1017 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc548))
#loc1018 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc553))
#loc1019 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc554))
#loc1020 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc558))
#loc1021 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc575))
#loc1022 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc576))
#loc1023 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc578))
#loc1024 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc579))
#loc1025 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc581))
#loc1026 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc584))
#loc1027 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc595))
#loc1028 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc600))
#loc1029 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc610))
#loc1030 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc611))
#loc1031 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc615))
#loc1032 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc630))
#loc1033 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc631))
#loc1034 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc638))
#loc1035 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc649))
#loc1036 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc650))
#loc1037 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0"(#loc651))
#loc1038 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc652))
#loc1039 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm1"(#loc656))
#loc1040 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm1"(#loc657))
#loc1041 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Conv2d[vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|224|aten__convolution_overrideable_input_reshape"(#loc670))
#loc1042 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc611))
#loc1043 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc682))
#loc1044 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc683))
#loc1045 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc684))
#loc1046 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc687))
#loc1047 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc688))
#loc1048 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc691))
#loc1049 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc692))
#loc1050 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc578))
#loc1051 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc699))
#loc1052 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc702))
#loc1053 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc630))
#loc1054 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc709))
#loc1055 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc710))
#loc1056 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc711))
#loc1057 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc714))
#loc1058 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc715))
#loc1059 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc718))
#loc1060 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc719))
#loc1061 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc524))
#loc1062 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc726))
#loc1063 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc729))
#loc1064 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc558))
#loc1065 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc736))
#loc1066 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc737))
#loc1067 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc738))
#loc1068 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc741))
#loc1069 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc742))
#loc1070 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc745))
#loc1071 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc746))
#loc1072 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc488))
#loc1073 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc753))
#loc1074 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc756))
#loc1075 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc517))
#loc1076 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc763))
#loc1077 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc764))
#loc1078 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc765))
#loc1079 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc768))
#loc1080 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc769))
#loc1081 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc772))
#loc1082 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc773))
#loc1083 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc581))
#loc1084 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc780))
#loc1085 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc783))
#loc1086 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc545))
#loc1087 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc790))
#loc1088 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc791))
#loc1089 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc792))
#loc1090 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc795))
#loc1091 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc796))
#loc1092 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc799))
#loc1093 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc800))
#loc1094 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc575))
#loc1095 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc807))
#loc1096 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc810))
#loc1097 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc584))
#loc1098 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc817))
#loc1099 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc818))
#loc1100 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc819))
#loc1101 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc822))
#loc1102 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc823))
#loc1103 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc826))
#loc1104 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc827))
#loc1105 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc525))
#loc1106 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc834))
#loc1107 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc837))
#loc1108 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc595))
#loc1109 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc844))
#loc1110 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc845))
#loc1111 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc846))
#loc1112 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc849))
#loc1113 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc850))
#loc1114 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc853))
#loc1115 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc854))
#loc1116 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc579))
#loc1117 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc861))
#loc1118 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc864))
#loc1119 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc516))
#loc1120 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc871))
#loc1121 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc872))
#loc1122 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc873))
#loc1123 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc876))
#loc1124 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc877))
#loc1125 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc880))
#loc1126 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc881))
#loc1127 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc652))
#loc1128 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc888))
#loc1129 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc891))
#loc1130 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc540))
#loc1131 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc898))
#loc1132 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc899))
#loc1133 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc900))
#loc1134 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc903))
#loc1135 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc904))
#loc1136 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc907))
#loc1137 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc908))
#loc1138 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc631))
#loc1139 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc915))
#loc1140 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc918))
#loc1141 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc657))
#loc1142 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc925))
#loc1143 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc926))
#loc1144 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc927))
#loc1145 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc930))
#loc1146 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc931))
#loc1147 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc934))
#loc1148 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc935))
#loc1149 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc656))
#loc1150 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc942))
#loc1151 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc945))
#loc1152 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc610))
#loc1153 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc952))
#loc1154 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc953))
#loc1155 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc954))
#loc1156 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc957))
#loc1157 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc958))
#loc1158 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc961))
#loc1159 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc962))
#loc1160 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc485))
#loc1161 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc969))
#loc1162 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc972))
#loc1163 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0"(#loc554))
#loc1164 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add_decomp_add_spill"(#loc979))
#loc1165 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_matmul_mem_reconfig"(#loc980))
#loc1166 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add_decomp_add_spill"(#loc981))
#loc1167 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_matmul_mem_reconfig"(#loc984))
#loc1168 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add_decomp_add_spill"(#loc985))
#loc1169 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_matmul_mem_reconfig"(#loc988))
#loc1170 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add_decomp_add_spill"(#loc989))
#loc1171 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0"(#loc548))
#loc1172 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add_decomp_add_spill"(#loc996))
#loc1173 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add_decomp_add_spill"(#loc999))
#loc1174 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1005))
#loc1175 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1008))
#loc1176 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1009))
#loc1177 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1012))
#loc1178 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1018))
#loc1179 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1022))
#loc1180 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1028))
#loc1181 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1031))
#loc1182 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1034))
#loc1183 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1035))
#loc1184 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1005))
#loc1185 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1008))
#loc1186 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1031))
#loc1187 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1036))
#loc1188 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1035))
#loc1189 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1009))
#loc1190 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1012))
#loc1191 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1022))
#loc1192 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1034))
#loc1193 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1037))
#loc1194 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1018))
#loc1195 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm1"(#loc1028))
#loc1196 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1036))
#loc1197 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0"(#loc1037))
#loc1198 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1042))
#loc1199 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1050))
#loc1200 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1053))
#loc1201 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1061))
#loc1202 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1064))
#loc1203 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1072))
#loc1204 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1075))
#loc1205 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1083))
#loc1206 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1086))
#loc1207 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1094))
#loc1208 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1097))
#loc1209 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1105))
#loc1210 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1108))
#loc1211 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1116))
#loc1212 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1119))
#loc1213 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1127))
#loc1214 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1130))
#loc1215 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1138))
#loc1216 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1141))
#loc1217 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1149))
#loc1218 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1152))
#loc1219 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1160))
#loc1220 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view_tm0_tm0_tm0"(#loc1163))
#loc1221 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view_tm0_tm0_tm0"(#loc1171))
#loc1222 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1174))
#loc1223 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1175))
#loc1224 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1176))
#loc1225 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1177))
#loc1226 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1178))
#loc1227 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1179))
#loc1228 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1180))
#loc1229 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1181))
#loc1230 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1182))
#loc1231 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1183))
#loc1232 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1196))
#loc1233 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view_tm1_tm0_tm0_tm1"(#loc1197))
