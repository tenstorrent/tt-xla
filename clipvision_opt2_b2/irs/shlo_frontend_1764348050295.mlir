#loc1 = loc("unknown|unknown|-1|unknownxla__device_data")
module @SyncTensorsGraph.4178 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___visual_projection_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg1: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_post_layernorm_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg2: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_post_layernorm_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg3: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg4: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg5: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg6: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg7: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg8: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg9: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg10: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg11: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg12: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg13: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg14: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg15: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg16: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg17: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg18: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg19: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg20: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg21: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg22: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg23: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg24: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg25: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg26: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg27: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg28: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg29: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg30: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg31: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg32: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg33: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg34: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg35: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg36: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg37: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg38: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg39: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg40: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg41: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg42: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg43: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg44: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg45: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg46: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg47: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg48: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg49: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg50: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg51: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg52: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg53: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg54: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg55: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg56: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg57: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg58: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg59: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg60: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg61: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg62: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg63: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg64: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg65: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg66: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg67: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg68: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg69: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg70: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg71: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg72: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg73: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg74: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg75: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg76: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg77: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg78: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg79: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg80: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg81: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg82: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg83: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg84: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg85: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg86: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg87: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg88: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg89: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg90: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg91: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg92: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg93: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg94: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg95: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg96: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg97: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg98: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg99: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg100: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg101: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg102: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg103: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg104: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg105: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg106: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg107: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg108: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg109: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg110: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg111: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg112: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg113: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg114: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg115: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg116: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg117: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg118: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg119: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg120: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg121: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg122: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg123: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg124: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg125: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg126: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg127: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg128: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg129: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg130: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg131: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg132: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg133: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg134: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg135: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg136: tensor<768x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg137: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg138: tensor<3072x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg139: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm2_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg140: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm2_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg141: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_out_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg142: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_out_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg143: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_v_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg144: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_v_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg145: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm1_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg146: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm1_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg147: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_pre_layrnorm_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg148: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_pre_layrnorm_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg149: tensor<1x50xi64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "l__self___vision_model_embeddings_position_ids"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg150: tensor<50x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_embeddings_position_embedding_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg151: tensor<768x3x32x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_embeddings_patch_embedding_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg152: tensor<2x3x224x224xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg153: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_embeddings_class_embedding"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg154: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg155: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg156: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg157: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg158: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg159: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg160: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg161: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg162: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg163: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg164: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg165: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg166: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg167: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg168: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg169: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg170: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg171: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg172: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg173: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg174: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg175: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg176: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg177: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg178: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg179: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg180: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg181: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg182: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg183: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg184: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg185: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg186: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg187: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg188: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg189: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg190: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg191: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg192: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg193: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg194: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg195: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg196: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg197: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg198: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_k_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg199: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_k_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg200: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_q_proj_bias"} loc("unknown|unknown|-1|unknownxla__device_data"), %arg201: tensor<768x768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_q_proj_weight"} loc("unknown|unknown|-1|unknownxla__device_data")) -> (tensor<2x512xbf16>, tensor<2x50x768xbf16>) {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<2x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<1.304630e-03> : tensor<2xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<1.703130e+00> : tensor<2x50x3072xbf16> loc(#loc)
    %cst_2 = stablehlo.constant dense<1.250000e-01> : tensor<2x12x50x50xbf16> loc(#loc)
    %cst_3 = stablehlo.constant dense<1.001360e-05> : tensor<2x50x1xbf16> loc(#loc)
    %cst_4 = stablehlo.constant dense<1.304630e-03> : tensor<2x50xbf16> loc(#loc)
    %cst_5 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_6 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reshape %arg153 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1 = stablehlo.reshape %0 : (tensor<1x1x768xbf16>) -> tensor<1x768xbf16> loc(#loc3)
    %2 = stablehlo.broadcast_in_dim %1, dims = [1, 2] : (tensor<1x768xbf16>) -> tensor<2x1x768xbf16> loc(#loc3)
    %3 = stablehlo.convolution(%arg152, %arg151) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [32, 32]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x3x224x224xbf16>, tensor<768x3x32x32xbf16>) -> tensor<2x768x7x7xbf16> loc(#loc4)
    %4 = stablehlo.reshape %3 : (tensor<2x768x7x7xbf16>) -> tensor<2x768x49xbf16> loc(#loc5)
    %5 = stablehlo.transpose %4, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "bf16[2,49,768]{1,2,0}"} : (tensor<2x768x49xbf16>) -> tensor<2x49x768xbf16> loc(#loc6)
    %6 = stablehlo.concatenate %2, %5, dim = 1 : (tensor<2x1x768xbf16>, tensor<2x49x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc7)
    %7 = stablehlo.reshape %arg150 : (tensor<50x768xbf16>) -> tensor<1x50x768xbf16> loc(#loc2)
    %8 = stablehlo.reshape %7 : (tensor<1x50x768xbf16>) -> tensor<50x768xbf16> loc(#loc2)
    %9 = stablehlo.reshape %arg149 : (tensor<1x50xi64>) -> tensor<1x1x50xi64> loc(#loc2)
    %10 = stablehlo.reshape %9 : (tensor<1x1x50xi64>) -> tensor<50xi64> loc(#loc8)
    %11 = stablehlo.convert %10 : (tensor<50xi64>) -> tensor<50xui32> loc(#loc9)
    %12 = "stablehlo.gather"(%8, %11) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50x768xbf16>, tensor<50xui32>) -> tensor<50x768xbf16> loc(#loc9)
    %13 = stablehlo.broadcast_in_dim %12, dims = [1, 2] : (tensor<50x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc10)
    %14 = stablehlo.add %6, %13 : tensor<2x50x768xbf16> loc(#loc10)
    %15 = stablehlo.reduce(%14 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc11)
    %16 = stablehlo.multiply %15, %cst_4 : tensor<2x50xbf16> loc(#loc11)
    %17 = stablehlo.broadcast_in_dim %16, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc12)
    %18 = stablehlo.subtract %14, %17 : tensor<2x50x768xbf16> loc(#loc12)
    %19 = stablehlo.multiply %18, %18 : tensor<2x50x768xbf16> loc(#loc11)
    %20 = stablehlo.reduce(%19 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc11)
    %21 = stablehlo.multiply %20, %cst_4 : tensor<2x50xbf16> loc(#loc11)
    %22 = stablehlo.reshape %21 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc11)
    %23 = stablehlo.add %22, %cst_3 : tensor<2x50x1xbf16> loc(#loc13)
    %24 = stablehlo.rsqrt %23 : tensor<2x50x1xbf16> loc(#loc14)
    %25 = stablehlo.reshape %24 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc15)
    %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc15)
    %27 = stablehlo.multiply %18, %26 : tensor<2x50x768xbf16> loc(#loc15)
    %28 = stablehlo.reshape %arg148 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %29 = stablehlo.reshape %28 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %30 = stablehlo.broadcast_in_dim %29, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc15)
    %31 = stablehlo.multiply %27, %30 : tensor<2x50x768xbf16> loc(#loc15)
    %32 = stablehlo.reshape %arg147 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %33 = stablehlo.reshape %32 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %34 = stablehlo.broadcast_in_dim %33, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc13)
    %35 = stablehlo.add %31, %34 : tensor<2x50x768xbf16> loc(#loc13)
    %36 = stablehlo.reduce(%35 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc16)
    %37 = stablehlo.multiply %36, %cst_4 : tensor<2x50xbf16> loc(#loc16)
    %38 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc17)
    %39 = stablehlo.subtract %35, %38 : tensor<2x50x768xbf16> loc(#loc17)
    %40 = stablehlo.multiply %39, %39 : tensor<2x50x768xbf16> loc(#loc16)
    %41 = stablehlo.reduce(%40 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc16)
    %42 = stablehlo.multiply %41, %cst_4 : tensor<2x50xbf16> loc(#loc16)
    %43 = stablehlo.reshape %42 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc16)
    %44 = stablehlo.add %43, %cst_3 : tensor<2x50x1xbf16> loc(#loc18)
    %45 = stablehlo.rsqrt %44 : tensor<2x50x1xbf16> loc(#loc19)
    %46 = stablehlo.reshape %45 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc20)
    %47 = stablehlo.broadcast_in_dim %46, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc20)
    %48 = stablehlo.multiply %39, %47 : tensor<2x50x768xbf16> loc(#loc20)
    %49 = stablehlo.reshape %arg146 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %50 = stablehlo.reshape %49 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %51 = stablehlo.broadcast_in_dim %50, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc20)
    %52 = stablehlo.multiply %48, %51 : tensor<2x50x768xbf16> loc(#loc20)
    %53 = stablehlo.reshape %arg145 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %54 = stablehlo.reshape %53 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %55 = stablehlo.broadcast_in_dim %54, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc18)
    %56 = stablehlo.add %52, %55 : tensor<2x50x768xbf16> loc(#loc18)
    %57 = stablehlo.reshape %56 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc21)
    %58 = stablehlo.reshape %arg157 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %59 = stablehlo.reshape %58 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %60 = stablehlo.transpose %59, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc22)
    %61 = stablehlo.dot_general %57, %60, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc23)
    %62 = stablehlo.reshape %61 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc21)
    %63 = stablehlo.reshape %arg156 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %64 = stablehlo.reshape %63 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %65 = stablehlo.broadcast_in_dim %64, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc24)
    %66 = stablehlo.add %62, %65 : tensor<2x50x768xbf16> loc(#loc24)
    %67 = stablehlo.reshape %66 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc25)
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc26)
    %69 = stablehlo.reshape %68 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc27)
    %70 = stablehlo.reshape %arg155 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %71 = stablehlo.reshape %70 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %72 = stablehlo.transpose %71, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc28)
    %73 = stablehlo.dot_general %57, %72, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc29)
    %74 = stablehlo.reshape %73 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc30)
    %75 = stablehlo.reshape %arg154 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %76 = stablehlo.reshape %75 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %77 = stablehlo.broadcast_in_dim %76, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc31)
    %78 = stablehlo.add %74, %77 : tensor<2x50x768xbf16> loc(#loc31)
    %79 = stablehlo.reshape %78 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc32)
    %80 = stablehlo.transpose %79, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc33)
    %81 = stablehlo.reshape %80 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc27)
    %82 = stablehlo.dot_general %69, %81, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc34)
    %83 = stablehlo.reshape %82 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc27)
    %84 = stablehlo.multiply %83, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc35)
    %85 = stablehlo.convert %84 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc36)
    %86 = stablehlo.reduce(%85 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc37)
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc37)
    %88 = stablehlo.subtract %85, %87 : tensor<2x12x50x50xf32> loc(#loc37)
    %89 = stablehlo.exponential %88 : tensor<2x12x50x50xf32> loc(#loc37)
    %90 = stablehlo.reduce(%89 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc37)
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc37)
    %92 = stablehlo.divide %89, %91 : tensor<2x12x50x50xf32> loc(#loc37)
    %93 = stablehlo.convert %92 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc38)
    %94 = stablehlo.reshape %93 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc39)
    %95 = stablehlo.reshape %arg144 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %96 = stablehlo.reshape %95 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %97 = stablehlo.transpose %96, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc40)
    %98 = stablehlo.dot_general %57, %97, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc41)
    %99 = stablehlo.reshape %98 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc42)
    %100 = stablehlo.reshape %arg143 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %101 = stablehlo.reshape %100 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %102 = stablehlo.broadcast_in_dim %101, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc43)
    %103 = stablehlo.add %99, %102 : tensor<2x50x768xbf16> loc(#loc43)
    %104 = stablehlo.reshape %103 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc44)
    %105 = stablehlo.transpose %104, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc45)
    %106 = stablehlo.reshape %105 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc39)
    %107 = stablehlo.dot_general %94, %106, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc46)
    %108 = stablehlo.reshape %107 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc39)
    %109 = stablehlo.transpose %108, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc47)
    %110 = stablehlo.reshape %109 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc48)
    %111 = stablehlo.reshape %arg142 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %112 = stablehlo.reshape %111 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %113 = stablehlo.transpose %112, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc49)
    %114 = stablehlo.dot_general %110, %113, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc50)
    %115 = stablehlo.reshape %114 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc48)
    %116 = stablehlo.reshape %arg141 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %117 = stablehlo.reshape %116 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %118 = stablehlo.broadcast_in_dim %117, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc51)
    %119 = stablehlo.add %115, %118 : tensor<2x50x768xbf16> loc(#loc51)
    %120 = stablehlo.add %35, %119 : tensor<2x50x768xbf16> loc(#loc52)
    %121 = stablehlo.reduce(%120 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc53)
    %122 = stablehlo.multiply %121, %cst_4 : tensor<2x50xbf16> loc(#loc53)
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc54)
    %124 = stablehlo.subtract %120, %123 : tensor<2x50x768xbf16> loc(#loc54)
    %125 = stablehlo.multiply %124, %124 : tensor<2x50x768xbf16> loc(#loc53)
    %126 = stablehlo.reduce(%125 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc53)
    %127 = stablehlo.multiply %126, %cst_4 : tensor<2x50xbf16> loc(#loc53)
    %128 = stablehlo.reshape %127 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc53)
    %129 = stablehlo.add %128, %cst_3 : tensor<2x50x1xbf16> loc(#loc55)
    %130 = stablehlo.rsqrt %129 : tensor<2x50x1xbf16> loc(#loc56)
    %131 = stablehlo.reshape %130 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc57)
    %132 = stablehlo.broadcast_in_dim %131, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc57)
    %133 = stablehlo.multiply %124, %132 : tensor<2x50x768xbf16> loc(#loc57)
    %134 = stablehlo.reshape %arg140 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc57)
    %137 = stablehlo.multiply %133, %136 : tensor<2x50x768xbf16> loc(#loc57)
    %138 = stablehlo.reshape %arg139 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %139 = stablehlo.reshape %138 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %140 = stablehlo.broadcast_in_dim %139, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc55)
    %141 = stablehlo.add %137, %140 : tensor<2x50x768xbf16> loc(#loc55)
    %142 = stablehlo.reshape %141 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc58)
    %143 = stablehlo.reshape %arg138 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %144 = stablehlo.reshape %143 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc59)
    %146 = stablehlo.dot_general %142, %145, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc60)
    %147 = stablehlo.reshape %146 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc58)
    %148 = stablehlo.reshape %arg137 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %149 = stablehlo.reshape %148 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %150 = stablehlo.broadcast_in_dim %149, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc61)
    %151 = stablehlo.add %147, %150 : tensor<2x50x3072xbf16> loc(#loc61)
    %152 = stablehlo.multiply %151, %cst_1 : tensor<2x50x3072xbf16> loc(#loc62)
    %153 = stablehlo.logistic %152 : tensor<2x50x3072xbf16> loc(#loc63)
    %154 = stablehlo.multiply %151, %153 : tensor<2x50x3072xbf16> loc(#loc62)
    %155 = stablehlo.reshape %154 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc64)
    %156 = stablehlo.reshape %arg136 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %157 = stablehlo.reshape %156 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %158 = stablehlo.transpose %157, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc65)
    %159 = stablehlo.dot_general %155, %158, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc66)
    %160 = stablehlo.reshape %159 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc64)
    %161 = stablehlo.reshape %arg135 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %162 = stablehlo.reshape %161 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %163 = stablehlo.broadcast_in_dim %162, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc67)
    %164 = stablehlo.add %160, %163 : tensor<2x50x768xbf16> loc(#loc67)
    %165 = stablehlo.add %120, %164 : tensor<2x50x768xbf16> loc(#loc68)
    %166 = stablehlo.reduce(%165 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc69)
    %167 = stablehlo.multiply %166, %cst_4 : tensor<2x50xbf16> loc(#loc69)
    %168 = stablehlo.broadcast_in_dim %167, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc70)
    %169 = stablehlo.subtract %165, %168 : tensor<2x50x768xbf16> loc(#loc70)
    %170 = stablehlo.multiply %169, %169 : tensor<2x50x768xbf16> loc(#loc69)
    %171 = stablehlo.reduce(%170 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc69)
    %172 = stablehlo.multiply %171, %cst_4 : tensor<2x50xbf16> loc(#loc69)
    %173 = stablehlo.reshape %172 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc69)
    %174 = stablehlo.add %173, %cst_3 : tensor<2x50x1xbf16> loc(#loc71)
    %175 = stablehlo.rsqrt %174 : tensor<2x50x1xbf16> loc(#loc72)
    %176 = stablehlo.reshape %175 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc73)
    %177 = stablehlo.broadcast_in_dim %176, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc73)
    %178 = stablehlo.multiply %169, %177 : tensor<2x50x768xbf16> loc(#loc73)
    %179 = stablehlo.reshape %arg134 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %180 = stablehlo.reshape %179 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %181 = stablehlo.broadcast_in_dim %180, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc73)
    %182 = stablehlo.multiply %178, %181 : tensor<2x50x768xbf16> loc(#loc73)
    %183 = stablehlo.reshape %arg133 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %184 = stablehlo.reshape %183 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %185 = stablehlo.broadcast_in_dim %184, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc71)
    %186 = stablehlo.add %182, %185 : tensor<2x50x768xbf16> loc(#loc71)
    %187 = stablehlo.reshape %186 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc74)
    %188 = stablehlo.reshape %arg161 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %189 = stablehlo.reshape %188 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %190 = stablehlo.transpose %189, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc75)
    %191 = stablehlo.dot_general %187, %190, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc76)
    %192 = stablehlo.reshape %191 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc74)
    %193 = stablehlo.reshape %arg160 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc77)
    %196 = stablehlo.add %192, %195 : tensor<2x50x768xbf16> loc(#loc77)
    %197 = stablehlo.reshape %196 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc78)
    %198 = stablehlo.transpose %197, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc79)
    %199 = stablehlo.reshape %198 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc80)
    %200 = stablehlo.reshape %arg159 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %201 = stablehlo.reshape %200 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %202 = stablehlo.transpose %201, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc81)
    %203 = stablehlo.dot_general %187, %202, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc82)
    %204 = stablehlo.reshape %203 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc83)
    %205 = stablehlo.reshape %arg158 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %206 = stablehlo.reshape %205 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %207 = stablehlo.broadcast_in_dim %206, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc84)
    %208 = stablehlo.add %204, %207 : tensor<2x50x768xbf16> loc(#loc84)
    %209 = stablehlo.reshape %208 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc85)
    %210 = stablehlo.transpose %209, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc86)
    %211 = stablehlo.reshape %210 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc80)
    %212 = stablehlo.dot_general %199, %211, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc87)
    %213 = stablehlo.reshape %212 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc80)
    %214 = stablehlo.multiply %213, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc88)
    %215 = stablehlo.convert %214 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc89)
    %216 = stablehlo.reduce(%215 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc90)
    %217 = stablehlo.broadcast_in_dim %216, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc90)
    %218 = stablehlo.subtract %215, %217 : tensor<2x12x50x50xf32> loc(#loc90)
    %219 = stablehlo.exponential %218 : tensor<2x12x50x50xf32> loc(#loc90)
    %220 = stablehlo.reduce(%219 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc90)
    %221 = stablehlo.broadcast_in_dim %220, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc90)
    %222 = stablehlo.divide %219, %221 : tensor<2x12x50x50xf32> loc(#loc90)
    %223 = stablehlo.convert %222 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc91)
    %224 = stablehlo.reshape %223 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc92)
    %225 = stablehlo.reshape %arg132 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %226 = stablehlo.reshape %225 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %227 = stablehlo.transpose %226, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc93)
    %228 = stablehlo.dot_general %187, %227, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc94)
    %229 = stablehlo.reshape %228 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc95)
    %230 = stablehlo.reshape %arg131 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %231 = stablehlo.reshape %230 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %232 = stablehlo.broadcast_in_dim %231, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc96)
    %233 = stablehlo.add %229, %232 : tensor<2x50x768xbf16> loc(#loc96)
    %234 = stablehlo.reshape %233 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc97)
    %235 = stablehlo.transpose %234, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc98)
    %236 = stablehlo.reshape %235 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc92)
    %237 = stablehlo.dot_general %224, %236, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc99)
    %238 = stablehlo.reshape %237 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc92)
    %239 = stablehlo.transpose %238, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc100)
    %240 = stablehlo.reshape %239 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc101)
    %241 = stablehlo.reshape %arg130 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %242 = stablehlo.reshape %241 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %243 = stablehlo.transpose %242, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc102)
    %244 = stablehlo.dot_general %240, %243, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc103)
    %245 = stablehlo.reshape %244 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc101)
    %246 = stablehlo.reshape %arg129 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %247 = stablehlo.reshape %246 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %248 = stablehlo.broadcast_in_dim %247, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc104)
    %249 = stablehlo.add %245, %248 : tensor<2x50x768xbf16> loc(#loc104)
    %250 = stablehlo.add %165, %249 : tensor<2x50x768xbf16> loc(#loc105)
    %251 = stablehlo.reduce(%250 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc106)
    %252 = stablehlo.multiply %251, %cst_4 : tensor<2x50xbf16> loc(#loc106)
    %253 = stablehlo.broadcast_in_dim %252, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc107)
    %254 = stablehlo.subtract %250, %253 : tensor<2x50x768xbf16> loc(#loc107)
    %255 = stablehlo.multiply %254, %254 : tensor<2x50x768xbf16> loc(#loc106)
    %256 = stablehlo.reduce(%255 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc106)
    %257 = stablehlo.multiply %256, %cst_4 : tensor<2x50xbf16> loc(#loc106)
    %258 = stablehlo.reshape %257 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc106)
    %259 = stablehlo.add %258, %cst_3 : tensor<2x50x1xbf16> loc(#loc108)
    %260 = stablehlo.rsqrt %259 : tensor<2x50x1xbf16> loc(#loc109)
    %261 = stablehlo.reshape %260 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc110)
    %262 = stablehlo.broadcast_in_dim %261, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc110)
    %263 = stablehlo.multiply %254, %262 : tensor<2x50x768xbf16> loc(#loc110)
    %264 = stablehlo.reshape %arg128 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %265 = stablehlo.reshape %264 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %266 = stablehlo.broadcast_in_dim %265, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc110)
    %267 = stablehlo.multiply %263, %266 : tensor<2x50x768xbf16> loc(#loc110)
    %268 = stablehlo.reshape %arg127 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %269 = stablehlo.reshape %268 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %270 = stablehlo.broadcast_in_dim %269, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc108)
    %271 = stablehlo.add %267, %270 : tensor<2x50x768xbf16> loc(#loc108)
    %272 = stablehlo.reshape %271 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc111)
    %273 = stablehlo.reshape %arg126 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %274 = stablehlo.reshape %273 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %275 = stablehlo.transpose %274, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc112)
    %276 = stablehlo.dot_general %272, %275, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc113)
    %277 = stablehlo.reshape %276 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc111)
    %278 = stablehlo.reshape %arg125 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %279 = stablehlo.reshape %278 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %280 = stablehlo.broadcast_in_dim %279, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc114)
    %281 = stablehlo.add %277, %280 : tensor<2x50x3072xbf16> loc(#loc114)
    %282 = stablehlo.multiply %281, %cst_1 : tensor<2x50x3072xbf16> loc(#loc115)
    %283 = stablehlo.logistic %282 : tensor<2x50x3072xbf16> loc(#loc116)
    %284 = stablehlo.multiply %281, %283 : tensor<2x50x3072xbf16> loc(#loc115)
    %285 = stablehlo.reshape %284 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc117)
    %286 = stablehlo.reshape %arg124 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %287 = stablehlo.reshape %286 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %288 = stablehlo.transpose %287, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc118)
    %289 = stablehlo.dot_general %285, %288, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc119)
    %290 = stablehlo.reshape %289 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc117)
    %291 = stablehlo.reshape %arg123 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %292 = stablehlo.reshape %291 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %293 = stablehlo.broadcast_in_dim %292, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc120)
    %294 = stablehlo.add %290, %293 : tensor<2x50x768xbf16> loc(#loc120)
    %295 = stablehlo.add %250, %294 : tensor<2x50x768xbf16> loc(#loc121)
    %296 = stablehlo.reduce(%295 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc122)
    %297 = stablehlo.multiply %296, %cst_4 : tensor<2x50xbf16> loc(#loc122)
    %298 = stablehlo.broadcast_in_dim %297, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc123)
    %299 = stablehlo.subtract %295, %298 : tensor<2x50x768xbf16> loc(#loc123)
    %300 = stablehlo.multiply %299, %299 : tensor<2x50x768xbf16> loc(#loc122)
    %301 = stablehlo.reduce(%300 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc122)
    %302 = stablehlo.multiply %301, %cst_4 : tensor<2x50xbf16> loc(#loc122)
    %303 = stablehlo.reshape %302 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc122)
    %304 = stablehlo.add %303, %cst_3 : tensor<2x50x1xbf16> loc(#loc124)
    %305 = stablehlo.rsqrt %304 : tensor<2x50x1xbf16> loc(#loc125)
    %306 = stablehlo.reshape %305 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc126)
    %307 = stablehlo.broadcast_in_dim %306, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc126)
    %308 = stablehlo.multiply %299, %307 : tensor<2x50x768xbf16> loc(#loc126)
    %309 = stablehlo.reshape %arg122 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %310 = stablehlo.reshape %309 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %311 = stablehlo.broadcast_in_dim %310, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc126)
    %312 = stablehlo.multiply %308, %311 : tensor<2x50x768xbf16> loc(#loc126)
    %313 = stablehlo.reshape %arg121 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %314 = stablehlo.reshape %313 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %315 = stablehlo.broadcast_in_dim %314, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc124)
    %316 = stablehlo.add %312, %315 : tensor<2x50x768xbf16> loc(#loc124)
    %317 = stablehlo.reshape %316 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc127)
    %318 = stablehlo.reshape %arg165 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %319 = stablehlo.reshape %318 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %320 = stablehlo.transpose %319, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc128)
    %321 = stablehlo.dot_general %317, %320, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc129)
    %322 = stablehlo.reshape %321 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc127)
    %323 = stablehlo.reshape %arg164 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %324 = stablehlo.reshape %323 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %325 = stablehlo.broadcast_in_dim %324, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc130)
    %326 = stablehlo.add %322, %325 : tensor<2x50x768xbf16> loc(#loc130)
    %327 = stablehlo.reshape %326 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc131)
    %328 = stablehlo.transpose %327, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc132)
    %329 = stablehlo.reshape %328 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc133)
    %330 = stablehlo.reshape %arg163 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %331 = stablehlo.reshape %330 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %332 = stablehlo.transpose %331, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc134)
    %333 = stablehlo.dot_general %317, %332, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc135)
    %334 = stablehlo.reshape %333 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc136)
    %335 = stablehlo.reshape %arg162 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %336 = stablehlo.reshape %335 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %337 = stablehlo.broadcast_in_dim %336, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc137)
    %338 = stablehlo.add %334, %337 : tensor<2x50x768xbf16> loc(#loc137)
    %339 = stablehlo.reshape %338 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc138)
    %340 = stablehlo.transpose %339, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc139)
    %341 = stablehlo.reshape %340 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc133)
    %342 = stablehlo.dot_general %329, %341, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc140)
    %343 = stablehlo.reshape %342 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc133)
    %344 = stablehlo.multiply %343, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc141)
    %345 = stablehlo.convert %344 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc142)
    %346 = stablehlo.reduce(%345 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc143)
    %347 = stablehlo.broadcast_in_dim %346, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc143)
    %348 = stablehlo.subtract %345, %347 : tensor<2x12x50x50xf32> loc(#loc143)
    %349 = stablehlo.exponential %348 : tensor<2x12x50x50xf32> loc(#loc143)
    %350 = stablehlo.reduce(%349 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc143)
    %351 = stablehlo.broadcast_in_dim %350, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc143)
    %352 = stablehlo.divide %349, %351 : tensor<2x12x50x50xf32> loc(#loc143)
    %353 = stablehlo.convert %352 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc144)
    %354 = stablehlo.reshape %353 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc145)
    %355 = stablehlo.reshape %arg120 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %356 = stablehlo.reshape %355 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %357 = stablehlo.transpose %356, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc146)
    %358 = stablehlo.dot_general %317, %357, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc147)
    %359 = stablehlo.reshape %358 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc148)
    %360 = stablehlo.reshape %arg119 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %361 = stablehlo.reshape %360 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %362 = stablehlo.broadcast_in_dim %361, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc149)
    %363 = stablehlo.add %359, %362 : tensor<2x50x768xbf16> loc(#loc149)
    %364 = stablehlo.reshape %363 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc150)
    %365 = stablehlo.transpose %364, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc151)
    %366 = stablehlo.reshape %365 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc145)
    %367 = stablehlo.dot_general %354, %366, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc152)
    %368 = stablehlo.reshape %367 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc145)
    %369 = stablehlo.transpose %368, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc153)
    %370 = stablehlo.reshape %369 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc154)
    %371 = stablehlo.reshape %arg118 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %372 = stablehlo.reshape %371 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %373 = stablehlo.transpose %372, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc155)
    %374 = stablehlo.dot_general %370, %373, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc156)
    %375 = stablehlo.reshape %374 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc154)
    %376 = stablehlo.reshape %arg117 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %377 = stablehlo.reshape %376 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %378 = stablehlo.broadcast_in_dim %377, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc157)
    %379 = stablehlo.add %375, %378 : tensor<2x50x768xbf16> loc(#loc157)
    %380 = stablehlo.add %295, %379 : tensor<2x50x768xbf16> loc(#loc158)
    %381 = stablehlo.reduce(%380 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc159)
    %382 = stablehlo.multiply %381, %cst_4 : tensor<2x50xbf16> loc(#loc159)
    %383 = stablehlo.broadcast_in_dim %382, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc160)
    %384 = stablehlo.subtract %380, %383 : tensor<2x50x768xbf16> loc(#loc160)
    %385 = stablehlo.multiply %384, %384 : tensor<2x50x768xbf16> loc(#loc159)
    %386 = stablehlo.reduce(%385 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc159)
    %387 = stablehlo.multiply %386, %cst_4 : tensor<2x50xbf16> loc(#loc159)
    %388 = stablehlo.reshape %387 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc159)
    %389 = stablehlo.add %388, %cst_3 : tensor<2x50x1xbf16> loc(#loc161)
    %390 = stablehlo.rsqrt %389 : tensor<2x50x1xbf16> loc(#loc162)
    %391 = stablehlo.reshape %390 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc163)
    %392 = stablehlo.broadcast_in_dim %391, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc163)
    %393 = stablehlo.multiply %384, %392 : tensor<2x50x768xbf16> loc(#loc163)
    %394 = stablehlo.reshape %arg116 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %395 = stablehlo.reshape %394 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %396 = stablehlo.broadcast_in_dim %395, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc163)
    %397 = stablehlo.multiply %393, %396 : tensor<2x50x768xbf16> loc(#loc163)
    %398 = stablehlo.reshape %arg115 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %399 = stablehlo.reshape %398 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %400 = stablehlo.broadcast_in_dim %399, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc161)
    %401 = stablehlo.add %397, %400 : tensor<2x50x768xbf16> loc(#loc161)
    %402 = stablehlo.reshape %401 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc164)
    %403 = stablehlo.reshape %arg114 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %404 = stablehlo.reshape %403 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %405 = stablehlo.transpose %404, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc165)
    %406 = stablehlo.dot_general %402, %405, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc166)
    %407 = stablehlo.reshape %406 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc164)
    %408 = stablehlo.reshape %arg113 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %409 = stablehlo.reshape %408 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %410 = stablehlo.broadcast_in_dim %409, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc167)
    %411 = stablehlo.add %407, %410 : tensor<2x50x3072xbf16> loc(#loc167)
    %412 = stablehlo.multiply %411, %cst_1 : tensor<2x50x3072xbf16> loc(#loc168)
    %413 = stablehlo.logistic %412 : tensor<2x50x3072xbf16> loc(#loc169)
    %414 = stablehlo.multiply %411, %413 : tensor<2x50x3072xbf16> loc(#loc168)
    %415 = stablehlo.reshape %414 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc170)
    %416 = stablehlo.reshape %arg112 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %417 = stablehlo.reshape %416 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %418 = stablehlo.transpose %417, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc171)
    %419 = stablehlo.dot_general %415, %418, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc172)
    %420 = stablehlo.reshape %419 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc170)
    %421 = stablehlo.reshape %arg111 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %422 = stablehlo.reshape %421 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %423 = stablehlo.broadcast_in_dim %422, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc173)
    %424 = stablehlo.add %420, %423 : tensor<2x50x768xbf16> loc(#loc173)
    %425 = stablehlo.add %380, %424 : tensor<2x50x768xbf16> loc(#loc174)
    %426 = stablehlo.reduce(%425 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc175)
    %427 = stablehlo.multiply %426, %cst_4 : tensor<2x50xbf16> loc(#loc175)
    %428 = stablehlo.broadcast_in_dim %427, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc176)
    %429 = stablehlo.subtract %425, %428 : tensor<2x50x768xbf16> loc(#loc176)
    %430 = stablehlo.multiply %429, %429 : tensor<2x50x768xbf16> loc(#loc175)
    %431 = stablehlo.reduce(%430 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc175)
    %432 = stablehlo.multiply %431, %cst_4 : tensor<2x50xbf16> loc(#loc175)
    %433 = stablehlo.reshape %432 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc175)
    %434 = stablehlo.add %433, %cst_3 : tensor<2x50x1xbf16> loc(#loc177)
    %435 = stablehlo.rsqrt %434 : tensor<2x50x1xbf16> loc(#loc178)
    %436 = stablehlo.reshape %435 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc179)
    %437 = stablehlo.broadcast_in_dim %436, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc179)
    %438 = stablehlo.multiply %429, %437 : tensor<2x50x768xbf16> loc(#loc179)
    %439 = stablehlo.reshape %arg110 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %440 = stablehlo.reshape %439 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %441 = stablehlo.broadcast_in_dim %440, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc179)
    %442 = stablehlo.multiply %438, %441 : tensor<2x50x768xbf16> loc(#loc179)
    %443 = stablehlo.reshape %arg109 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %444 = stablehlo.reshape %443 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %445 = stablehlo.broadcast_in_dim %444, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc177)
    %446 = stablehlo.add %442, %445 : tensor<2x50x768xbf16> loc(#loc177)
    %447 = stablehlo.reshape %446 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc180)
    %448 = stablehlo.reshape %arg169 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %449 = stablehlo.reshape %448 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %450 = stablehlo.transpose %449, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc181)
    %451 = stablehlo.dot_general %447, %450, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc182)
    %452 = stablehlo.reshape %451 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc180)
    %453 = stablehlo.reshape %arg168 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %454 = stablehlo.reshape %453 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %455 = stablehlo.broadcast_in_dim %454, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc183)
    %456 = stablehlo.add %452, %455 : tensor<2x50x768xbf16> loc(#loc183)
    %457 = stablehlo.reshape %456 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc184)
    %458 = stablehlo.transpose %457, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc185)
    %459 = stablehlo.reshape %458 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc186)
    %460 = stablehlo.reshape %arg167 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %461 = stablehlo.reshape %460 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %462 = stablehlo.transpose %461, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc187)
    %463 = stablehlo.dot_general %447, %462, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc188)
    %464 = stablehlo.reshape %463 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc189)
    %465 = stablehlo.reshape %arg166 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %466 = stablehlo.reshape %465 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %467 = stablehlo.broadcast_in_dim %466, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc190)
    %468 = stablehlo.add %464, %467 : tensor<2x50x768xbf16> loc(#loc190)
    %469 = stablehlo.reshape %468 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc191)
    %470 = stablehlo.transpose %469, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc192)
    %471 = stablehlo.reshape %470 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc186)
    %472 = stablehlo.dot_general %459, %471, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc193)
    %473 = stablehlo.reshape %472 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc186)
    %474 = stablehlo.multiply %473, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc194)
    %475 = stablehlo.convert %474 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc195)
    %476 = stablehlo.reduce(%475 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc196)
    %477 = stablehlo.broadcast_in_dim %476, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc196)
    %478 = stablehlo.subtract %475, %477 : tensor<2x12x50x50xf32> loc(#loc196)
    %479 = stablehlo.exponential %478 : tensor<2x12x50x50xf32> loc(#loc196)
    %480 = stablehlo.reduce(%479 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc196)
    %481 = stablehlo.broadcast_in_dim %480, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc196)
    %482 = stablehlo.divide %479, %481 : tensor<2x12x50x50xf32> loc(#loc196)
    %483 = stablehlo.convert %482 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc197)
    %484 = stablehlo.reshape %483 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc198)
    %485 = stablehlo.reshape %arg108 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %486 = stablehlo.reshape %485 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %487 = stablehlo.transpose %486, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc199)
    %488 = stablehlo.dot_general %447, %487, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc200)
    %489 = stablehlo.reshape %488 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc201)
    %490 = stablehlo.reshape %arg107 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %491 = stablehlo.reshape %490 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %492 = stablehlo.broadcast_in_dim %491, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc202)
    %493 = stablehlo.add %489, %492 : tensor<2x50x768xbf16> loc(#loc202)
    %494 = stablehlo.reshape %493 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc203)
    %495 = stablehlo.transpose %494, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc204)
    %496 = stablehlo.reshape %495 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc198)
    %497 = stablehlo.dot_general %484, %496, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc205)
    %498 = stablehlo.reshape %497 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc198)
    %499 = stablehlo.transpose %498, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc206)
    %500 = stablehlo.reshape %499 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc207)
    %501 = stablehlo.reshape %arg106 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %502 = stablehlo.reshape %501 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %503 = stablehlo.transpose %502, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc208)
    %504 = stablehlo.dot_general %500, %503, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc209)
    %505 = stablehlo.reshape %504 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc207)
    %506 = stablehlo.reshape %arg105 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %507 = stablehlo.reshape %506 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %508 = stablehlo.broadcast_in_dim %507, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc210)
    %509 = stablehlo.add %505, %508 : tensor<2x50x768xbf16> loc(#loc210)
    %510 = stablehlo.add %425, %509 : tensor<2x50x768xbf16> loc(#loc211)
    %511 = stablehlo.reduce(%510 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc212)
    %512 = stablehlo.multiply %511, %cst_4 : tensor<2x50xbf16> loc(#loc212)
    %513 = stablehlo.broadcast_in_dim %512, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc213)
    %514 = stablehlo.subtract %510, %513 : tensor<2x50x768xbf16> loc(#loc213)
    %515 = stablehlo.multiply %514, %514 : tensor<2x50x768xbf16> loc(#loc212)
    %516 = stablehlo.reduce(%515 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc212)
    %517 = stablehlo.multiply %516, %cst_4 : tensor<2x50xbf16> loc(#loc212)
    %518 = stablehlo.reshape %517 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc212)
    %519 = stablehlo.add %518, %cst_3 : tensor<2x50x1xbf16> loc(#loc214)
    %520 = stablehlo.rsqrt %519 : tensor<2x50x1xbf16> loc(#loc215)
    %521 = stablehlo.reshape %520 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc216)
    %522 = stablehlo.broadcast_in_dim %521, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc216)
    %523 = stablehlo.multiply %514, %522 : tensor<2x50x768xbf16> loc(#loc216)
    %524 = stablehlo.reshape %arg104 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %525 = stablehlo.reshape %524 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %526 = stablehlo.broadcast_in_dim %525, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc216)
    %527 = stablehlo.multiply %523, %526 : tensor<2x50x768xbf16> loc(#loc216)
    %528 = stablehlo.reshape %arg103 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %529 = stablehlo.reshape %528 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %530 = stablehlo.broadcast_in_dim %529, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc214)
    %531 = stablehlo.add %527, %530 : tensor<2x50x768xbf16> loc(#loc214)
    %532 = stablehlo.reshape %531 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc217)
    %533 = stablehlo.reshape %arg102 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %534 = stablehlo.reshape %533 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %535 = stablehlo.transpose %534, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc218)
    %536 = stablehlo.dot_general %532, %535, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc219)
    %537 = stablehlo.reshape %536 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc217)
    %538 = stablehlo.reshape %arg101 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %539 = stablehlo.reshape %538 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %540 = stablehlo.broadcast_in_dim %539, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc220)
    %541 = stablehlo.add %537, %540 : tensor<2x50x3072xbf16> loc(#loc220)
    %542 = stablehlo.multiply %541, %cst_1 : tensor<2x50x3072xbf16> loc(#loc221)
    %543 = stablehlo.logistic %542 : tensor<2x50x3072xbf16> loc(#loc222)
    %544 = stablehlo.multiply %541, %543 : tensor<2x50x3072xbf16> loc(#loc221)
    %545 = stablehlo.reshape %544 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc223)
    %546 = stablehlo.reshape %arg100 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %547 = stablehlo.reshape %546 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %548 = stablehlo.transpose %547, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc224)
    %549 = stablehlo.dot_general %545, %548, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc225)
    %550 = stablehlo.reshape %549 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc223)
    %551 = stablehlo.reshape %arg99 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %552 = stablehlo.reshape %551 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %553 = stablehlo.broadcast_in_dim %552, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc226)
    %554 = stablehlo.add %550, %553 : tensor<2x50x768xbf16> loc(#loc226)
    %555 = stablehlo.add %510, %554 : tensor<2x50x768xbf16> loc(#loc227)
    %556 = stablehlo.reduce(%555 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc228)
    %557 = stablehlo.multiply %556, %cst_4 : tensor<2x50xbf16> loc(#loc228)
    %558 = stablehlo.broadcast_in_dim %557, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc229)
    %559 = stablehlo.subtract %555, %558 : tensor<2x50x768xbf16> loc(#loc229)
    %560 = stablehlo.multiply %559, %559 : tensor<2x50x768xbf16> loc(#loc228)
    %561 = stablehlo.reduce(%560 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc228)
    %562 = stablehlo.multiply %561, %cst_4 : tensor<2x50xbf16> loc(#loc228)
    %563 = stablehlo.reshape %562 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc228)
    %564 = stablehlo.add %563, %cst_3 : tensor<2x50x1xbf16> loc(#loc230)
    %565 = stablehlo.rsqrt %564 : tensor<2x50x1xbf16> loc(#loc231)
    %566 = stablehlo.reshape %565 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc232)
    %567 = stablehlo.broadcast_in_dim %566, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc232)
    %568 = stablehlo.multiply %559, %567 : tensor<2x50x768xbf16> loc(#loc232)
    %569 = stablehlo.reshape %arg98 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %570 = stablehlo.reshape %569 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %571 = stablehlo.broadcast_in_dim %570, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc232)
    %572 = stablehlo.multiply %568, %571 : tensor<2x50x768xbf16> loc(#loc232)
    %573 = stablehlo.reshape %arg97 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %574 = stablehlo.reshape %573 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %575 = stablehlo.broadcast_in_dim %574, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc230)
    %576 = stablehlo.add %572, %575 : tensor<2x50x768xbf16> loc(#loc230)
    %577 = stablehlo.reshape %576 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc233)
    %578 = stablehlo.reshape %arg173 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %579 = stablehlo.reshape %578 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %580 = stablehlo.transpose %579, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc234)
    %581 = stablehlo.dot_general %577, %580, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc235)
    %582 = stablehlo.reshape %581 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc233)
    %583 = stablehlo.reshape %arg172 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %584 = stablehlo.reshape %583 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %585 = stablehlo.broadcast_in_dim %584, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc236)
    %586 = stablehlo.add %582, %585 : tensor<2x50x768xbf16> loc(#loc236)
    %587 = stablehlo.reshape %586 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc237)
    %588 = stablehlo.transpose %587, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc238)
    %589 = stablehlo.reshape %588 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc239)
    %590 = stablehlo.reshape %arg171 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %591 = stablehlo.reshape %590 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %592 = stablehlo.transpose %591, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc240)
    %593 = stablehlo.dot_general %577, %592, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc241)
    %594 = stablehlo.reshape %593 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc242)
    %595 = stablehlo.reshape %arg170 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %596 = stablehlo.reshape %595 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %597 = stablehlo.broadcast_in_dim %596, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc243)
    %598 = stablehlo.add %594, %597 : tensor<2x50x768xbf16> loc(#loc243)
    %599 = stablehlo.reshape %598 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc244)
    %600 = stablehlo.transpose %599, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc245)
    %601 = stablehlo.reshape %600 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc239)
    %602 = stablehlo.dot_general %589, %601, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc246)
    %603 = stablehlo.reshape %602 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc239)
    %604 = stablehlo.multiply %603, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc247)
    %605 = stablehlo.convert %604 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc248)
    %606 = stablehlo.reduce(%605 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc249)
    %607 = stablehlo.broadcast_in_dim %606, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc249)
    %608 = stablehlo.subtract %605, %607 : tensor<2x12x50x50xf32> loc(#loc249)
    %609 = stablehlo.exponential %608 : tensor<2x12x50x50xf32> loc(#loc249)
    %610 = stablehlo.reduce(%609 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc249)
    %611 = stablehlo.broadcast_in_dim %610, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc249)
    %612 = stablehlo.divide %609, %611 : tensor<2x12x50x50xf32> loc(#loc249)
    %613 = stablehlo.convert %612 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc250)
    %614 = stablehlo.reshape %613 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc251)
    %615 = stablehlo.reshape %arg96 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %616 = stablehlo.reshape %615 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %617 = stablehlo.transpose %616, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc252)
    %618 = stablehlo.dot_general %577, %617, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc253)
    %619 = stablehlo.reshape %618 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc254)
    %620 = stablehlo.reshape %arg95 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %621 = stablehlo.reshape %620 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %622 = stablehlo.broadcast_in_dim %621, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc255)
    %623 = stablehlo.add %619, %622 : tensor<2x50x768xbf16> loc(#loc255)
    %624 = stablehlo.reshape %623 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc256)
    %625 = stablehlo.transpose %624, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc257)
    %626 = stablehlo.reshape %625 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc251)
    %627 = stablehlo.dot_general %614, %626, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc258)
    %628 = stablehlo.reshape %627 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc251)
    %629 = stablehlo.transpose %628, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc259)
    %630 = stablehlo.reshape %629 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc260)
    %631 = stablehlo.reshape %arg94 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %632 = stablehlo.reshape %631 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %633 = stablehlo.transpose %632, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc261)
    %634 = stablehlo.dot_general %630, %633, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc262)
    %635 = stablehlo.reshape %634 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc260)
    %636 = stablehlo.reshape %arg93 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %637 = stablehlo.reshape %636 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %638 = stablehlo.broadcast_in_dim %637, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc263)
    %639 = stablehlo.add %635, %638 : tensor<2x50x768xbf16> loc(#loc263)
    %640 = stablehlo.add %555, %639 : tensor<2x50x768xbf16> loc(#loc264)
    %641 = stablehlo.reduce(%640 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc265)
    %642 = stablehlo.multiply %641, %cst_4 : tensor<2x50xbf16> loc(#loc265)
    %643 = stablehlo.broadcast_in_dim %642, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc266)
    %644 = stablehlo.subtract %640, %643 : tensor<2x50x768xbf16> loc(#loc266)
    %645 = stablehlo.multiply %644, %644 : tensor<2x50x768xbf16> loc(#loc265)
    %646 = stablehlo.reduce(%645 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc265)
    %647 = stablehlo.multiply %646, %cst_4 : tensor<2x50xbf16> loc(#loc265)
    %648 = stablehlo.reshape %647 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc265)
    %649 = stablehlo.add %648, %cst_3 : tensor<2x50x1xbf16> loc(#loc267)
    %650 = stablehlo.rsqrt %649 : tensor<2x50x1xbf16> loc(#loc268)
    %651 = stablehlo.reshape %650 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc269)
    %652 = stablehlo.broadcast_in_dim %651, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc269)
    %653 = stablehlo.multiply %644, %652 : tensor<2x50x768xbf16> loc(#loc269)
    %654 = stablehlo.reshape %arg92 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %655 = stablehlo.reshape %654 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %656 = stablehlo.broadcast_in_dim %655, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc269)
    %657 = stablehlo.multiply %653, %656 : tensor<2x50x768xbf16> loc(#loc269)
    %658 = stablehlo.reshape %arg91 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %659 = stablehlo.reshape %658 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %660 = stablehlo.broadcast_in_dim %659, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc267)
    %661 = stablehlo.add %657, %660 : tensor<2x50x768xbf16> loc(#loc267)
    %662 = stablehlo.reshape %661 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc270)
    %663 = stablehlo.reshape %arg90 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %664 = stablehlo.reshape %663 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %665 = stablehlo.transpose %664, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc271)
    %666 = stablehlo.dot_general %662, %665, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc272)
    %667 = stablehlo.reshape %666 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc270)
    %668 = stablehlo.reshape %arg89 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %669 = stablehlo.reshape %668 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %670 = stablehlo.broadcast_in_dim %669, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc273)
    %671 = stablehlo.add %667, %670 : tensor<2x50x3072xbf16> loc(#loc273)
    %672 = stablehlo.multiply %671, %cst_1 : tensor<2x50x3072xbf16> loc(#loc274)
    %673 = stablehlo.logistic %672 : tensor<2x50x3072xbf16> loc(#loc275)
    %674 = stablehlo.multiply %671, %673 : tensor<2x50x3072xbf16> loc(#loc274)
    %675 = stablehlo.reshape %674 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc276)
    %676 = stablehlo.reshape %arg88 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %677 = stablehlo.reshape %676 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %678 = stablehlo.transpose %677, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc277)
    %679 = stablehlo.dot_general %675, %678, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc278)
    %680 = stablehlo.reshape %679 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc276)
    %681 = stablehlo.reshape %arg87 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %682 = stablehlo.reshape %681 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %683 = stablehlo.broadcast_in_dim %682, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc279)
    %684 = stablehlo.add %680, %683 : tensor<2x50x768xbf16> loc(#loc279)
    %685 = stablehlo.add %640, %684 : tensor<2x50x768xbf16> loc(#loc280)
    %686 = stablehlo.reduce(%685 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc281)
    %687 = stablehlo.multiply %686, %cst_4 : tensor<2x50xbf16> loc(#loc281)
    %688 = stablehlo.broadcast_in_dim %687, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc282)
    %689 = stablehlo.subtract %685, %688 : tensor<2x50x768xbf16> loc(#loc282)
    %690 = stablehlo.multiply %689, %689 : tensor<2x50x768xbf16> loc(#loc281)
    %691 = stablehlo.reduce(%690 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc281)
    %692 = stablehlo.multiply %691, %cst_4 : tensor<2x50xbf16> loc(#loc281)
    %693 = stablehlo.reshape %692 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc281)
    %694 = stablehlo.add %693, %cst_3 : tensor<2x50x1xbf16> loc(#loc283)
    %695 = stablehlo.rsqrt %694 : tensor<2x50x1xbf16> loc(#loc284)
    %696 = stablehlo.reshape %695 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc285)
    %697 = stablehlo.broadcast_in_dim %696, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc285)
    %698 = stablehlo.multiply %689, %697 : tensor<2x50x768xbf16> loc(#loc285)
    %699 = stablehlo.reshape %arg86 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %700 = stablehlo.reshape %699 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %701 = stablehlo.broadcast_in_dim %700, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc285)
    %702 = stablehlo.multiply %698, %701 : tensor<2x50x768xbf16> loc(#loc285)
    %703 = stablehlo.reshape %arg85 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %704 = stablehlo.reshape %703 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %705 = stablehlo.broadcast_in_dim %704, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc283)
    %706 = stablehlo.add %702, %705 : tensor<2x50x768xbf16> loc(#loc283)
    %707 = stablehlo.reshape %706 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc286)
    %708 = stablehlo.reshape %arg177 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %709 = stablehlo.reshape %708 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %710 = stablehlo.transpose %709, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc287)
    %711 = stablehlo.dot_general %707, %710, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc288)
    %712 = stablehlo.reshape %711 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc286)
    %713 = stablehlo.reshape %arg176 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %714 = stablehlo.reshape %713 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %715 = stablehlo.broadcast_in_dim %714, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc289)
    %716 = stablehlo.add %712, %715 : tensor<2x50x768xbf16> loc(#loc289)
    %717 = stablehlo.reshape %716 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc290)
    %718 = stablehlo.transpose %717, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc291)
    %719 = stablehlo.reshape %718 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc292)
    %720 = stablehlo.reshape %arg175 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %721 = stablehlo.reshape %720 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %722 = stablehlo.transpose %721, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc293)
    %723 = stablehlo.dot_general %707, %722, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc294)
    %724 = stablehlo.reshape %723 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc295)
    %725 = stablehlo.reshape %arg174 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %726 = stablehlo.reshape %725 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %727 = stablehlo.broadcast_in_dim %726, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc296)
    %728 = stablehlo.add %724, %727 : tensor<2x50x768xbf16> loc(#loc296)
    %729 = stablehlo.reshape %728 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc297)
    %730 = stablehlo.transpose %729, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc298)
    %731 = stablehlo.reshape %730 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc292)
    %732 = stablehlo.dot_general %719, %731, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc299)
    %733 = stablehlo.reshape %732 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc292)
    %734 = stablehlo.multiply %733, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc300)
    %735 = stablehlo.convert %734 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc301)
    %736 = stablehlo.reduce(%735 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc302)
    %737 = stablehlo.broadcast_in_dim %736, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc302)
    %738 = stablehlo.subtract %735, %737 : tensor<2x12x50x50xf32> loc(#loc302)
    %739 = stablehlo.exponential %738 : tensor<2x12x50x50xf32> loc(#loc302)
    %740 = stablehlo.reduce(%739 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc302)
    %741 = stablehlo.broadcast_in_dim %740, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc302)
    %742 = stablehlo.divide %739, %741 : tensor<2x12x50x50xf32> loc(#loc302)
    %743 = stablehlo.convert %742 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc303)
    %744 = stablehlo.reshape %743 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc304)
    %745 = stablehlo.reshape %arg84 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %746 = stablehlo.reshape %745 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %747 = stablehlo.transpose %746, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc305)
    %748 = stablehlo.dot_general %707, %747, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc306)
    %749 = stablehlo.reshape %748 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc307)
    %750 = stablehlo.reshape %arg83 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %751 = stablehlo.reshape %750 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %752 = stablehlo.broadcast_in_dim %751, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc308)
    %753 = stablehlo.add %749, %752 : tensor<2x50x768xbf16> loc(#loc308)
    %754 = stablehlo.reshape %753 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc309)
    %755 = stablehlo.transpose %754, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc310)
    %756 = stablehlo.reshape %755 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc304)
    %757 = stablehlo.dot_general %744, %756, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc311)
    %758 = stablehlo.reshape %757 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc304)
    %759 = stablehlo.transpose %758, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc312)
    %760 = stablehlo.reshape %759 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc313)
    %761 = stablehlo.reshape %arg82 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %762 = stablehlo.reshape %761 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %763 = stablehlo.transpose %762, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc314)
    %764 = stablehlo.dot_general %760, %763, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc315)
    %765 = stablehlo.reshape %764 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc313)
    %766 = stablehlo.reshape %arg81 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %767 = stablehlo.reshape %766 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %768 = stablehlo.broadcast_in_dim %767, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc316)
    %769 = stablehlo.add %765, %768 : tensor<2x50x768xbf16> loc(#loc316)
    %770 = stablehlo.add %685, %769 : tensor<2x50x768xbf16> loc(#loc317)
    %771 = stablehlo.reduce(%770 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc318)
    %772 = stablehlo.multiply %771, %cst_4 : tensor<2x50xbf16> loc(#loc318)
    %773 = stablehlo.broadcast_in_dim %772, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc319)
    %774 = stablehlo.subtract %770, %773 : tensor<2x50x768xbf16> loc(#loc319)
    %775 = stablehlo.multiply %774, %774 : tensor<2x50x768xbf16> loc(#loc318)
    %776 = stablehlo.reduce(%775 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc318)
    %777 = stablehlo.multiply %776, %cst_4 : tensor<2x50xbf16> loc(#loc318)
    %778 = stablehlo.reshape %777 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc318)
    %779 = stablehlo.add %778, %cst_3 : tensor<2x50x1xbf16> loc(#loc320)
    %780 = stablehlo.rsqrt %779 : tensor<2x50x1xbf16> loc(#loc321)
    %781 = stablehlo.reshape %780 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc322)
    %782 = stablehlo.broadcast_in_dim %781, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc322)
    %783 = stablehlo.multiply %774, %782 : tensor<2x50x768xbf16> loc(#loc322)
    %784 = stablehlo.reshape %arg80 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %785 = stablehlo.reshape %784 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %786 = stablehlo.broadcast_in_dim %785, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc322)
    %787 = stablehlo.multiply %783, %786 : tensor<2x50x768xbf16> loc(#loc322)
    %788 = stablehlo.reshape %arg79 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %789 = stablehlo.reshape %788 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %790 = stablehlo.broadcast_in_dim %789, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc320)
    %791 = stablehlo.add %787, %790 : tensor<2x50x768xbf16> loc(#loc320)
    %792 = stablehlo.reshape %791 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc323)
    %793 = stablehlo.reshape %arg78 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %794 = stablehlo.reshape %793 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %795 = stablehlo.transpose %794, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc324)
    %796 = stablehlo.dot_general %792, %795, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc325)
    %797 = stablehlo.reshape %796 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc323)
    %798 = stablehlo.reshape %arg77 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %799 = stablehlo.reshape %798 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %800 = stablehlo.broadcast_in_dim %799, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc326)
    %801 = stablehlo.add %797, %800 : tensor<2x50x3072xbf16> loc(#loc326)
    %802 = stablehlo.multiply %801, %cst_1 : tensor<2x50x3072xbf16> loc(#loc327)
    %803 = stablehlo.logistic %802 : tensor<2x50x3072xbf16> loc(#loc328)
    %804 = stablehlo.multiply %801, %803 : tensor<2x50x3072xbf16> loc(#loc327)
    %805 = stablehlo.reshape %804 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc329)
    %806 = stablehlo.reshape %arg76 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %807 = stablehlo.reshape %806 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %808 = stablehlo.transpose %807, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc330)
    %809 = stablehlo.dot_general %805, %808, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc331)
    %810 = stablehlo.reshape %809 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc329)
    %811 = stablehlo.reshape %arg75 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %812 = stablehlo.reshape %811 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %813 = stablehlo.broadcast_in_dim %812, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc332)
    %814 = stablehlo.add %810, %813 : tensor<2x50x768xbf16> loc(#loc332)
    %815 = stablehlo.add %770, %814 : tensor<2x50x768xbf16> loc(#loc333)
    %816 = stablehlo.reduce(%815 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc334)
    %817 = stablehlo.multiply %816, %cst_4 : tensor<2x50xbf16> loc(#loc334)
    %818 = stablehlo.broadcast_in_dim %817, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc335)
    %819 = stablehlo.subtract %815, %818 : tensor<2x50x768xbf16> loc(#loc335)
    %820 = stablehlo.multiply %819, %819 : tensor<2x50x768xbf16> loc(#loc334)
    %821 = stablehlo.reduce(%820 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc334)
    %822 = stablehlo.multiply %821, %cst_4 : tensor<2x50xbf16> loc(#loc334)
    %823 = stablehlo.reshape %822 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc334)
    %824 = stablehlo.add %823, %cst_3 : tensor<2x50x1xbf16> loc(#loc336)
    %825 = stablehlo.rsqrt %824 : tensor<2x50x1xbf16> loc(#loc337)
    %826 = stablehlo.reshape %825 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc338)
    %827 = stablehlo.broadcast_in_dim %826, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc338)
    %828 = stablehlo.multiply %819, %827 : tensor<2x50x768xbf16> loc(#loc338)
    %829 = stablehlo.reshape %arg74 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %830 = stablehlo.reshape %829 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %831 = stablehlo.broadcast_in_dim %830, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc338)
    %832 = stablehlo.multiply %828, %831 : tensor<2x50x768xbf16> loc(#loc338)
    %833 = stablehlo.reshape %arg73 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %834 = stablehlo.reshape %833 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %835 = stablehlo.broadcast_in_dim %834, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc336)
    %836 = stablehlo.add %832, %835 : tensor<2x50x768xbf16> loc(#loc336)
    %837 = stablehlo.reshape %836 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc339)
    %838 = stablehlo.reshape %arg181 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %839 = stablehlo.reshape %838 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %840 = stablehlo.transpose %839, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc340)
    %841 = stablehlo.dot_general %837, %840, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc341)
    %842 = stablehlo.reshape %841 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc339)
    %843 = stablehlo.reshape %arg180 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %844 = stablehlo.reshape %843 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %845 = stablehlo.broadcast_in_dim %844, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc342)
    %846 = stablehlo.add %842, %845 : tensor<2x50x768xbf16> loc(#loc342)
    %847 = stablehlo.reshape %846 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc343)
    %848 = stablehlo.transpose %847, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc344)
    %849 = stablehlo.reshape %848 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc345)
    %850 = stablehlo.reshape %arg179 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %851 = stablehlo.reshape %850 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %852 = stablehlo.transpose %851, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc346)
    %853 = stablehlo.dot_general %837, %852, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc347)
    %854 = stablehlo.reshape %853 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc348)
    %855 = stablehlo.reshape %arg178 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %856 = stablehlo.reshape %855 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %857 = stablehlo.broadcast_in_dim %856, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc349)
    %858 = stablehlo.add %854, %857 : tensor<2x50x768xbf16> loc(#loc349)
    %859 = stablehlo.reshape %858 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc350)
    %860 = stablehlo.transpose %859, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc351)
    %861 = stablehlo.reshape %860 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc345)
    %862 = stablehlo.dot_general %849, %861, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc352)
    %863 = stablehlo.reshape %862 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc345)
    %864 = stablehlo.multiply %863, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc353)
    %865 = stablehlo.convert %864 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc354)
    %866 = stablehlo.reduce(%865 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc355)
    %867 = stablehlo.broadcast_in_dim %866, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc355)
    %868 = stablehlo.subtract %865, %867 : tensor<2x12x50x50xf32> loc(#loc355)
    %869 = stablehlo.exponential %868 : tensor<2x12x50x50xf32> loc(#loc355)
    %870 = stablehlo.reduce(%869 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc355)
    %871 = stablehlo.broadcast_in_dim %870, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc355)
    %872 = stablehlo.divide %869, %871 : tensor<2x12x50x50xf32> loc(#loc355)
    %873 = stablehlo.convert %872 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc356)
    %874 = stablehlo.reshape %873 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc357)
    %875 = stablehlo.reshape %arg72 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %876 = stablehlo.reshape %875 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %877 = stablehlo.transpose %876, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc358)
    %878 = stablehlo.dot_general %837, %877, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc359)
    %879 = stablehlo.reshape %878 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc360)
    %880 = stablehlo.reshape %arg71 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %881 = stablehlo.reshape %880 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %882 = stablehlo.broadcast_in_dim %881, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc361)
    %883 = stablehlo.add %879, %882 : tensor<2x50x768xbf16> loc(#loc361)
    %884 = stablehlo.reshape %883 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc362)
    %885 = stablehlo.transpose %884, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc363)
    %886 = stablehlo.reshape %885 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc357)
    %887 = stablehlo.dot_general %874, %886, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc364)
    %888 = stablehlo.reshape %887 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc357)
    %889 = stablehlo.transpose %888, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc365)
    %890 = stablehlo.reshape %889 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc366)
    %891 = stablehlo.reshape %arg70 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %892 = stablehlo.reshape %891 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %893 = stablehlo.transpose %892, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc367)
    %894 = stablehlo.dot_general %890, %893, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc368)
    %895 = stablehlo.reshape %894 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc366)
    %896 = stablehlo.reshape %arg69 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %897 = stablehlo.reshape %896 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %898 = stablehlo.broadcast_in_dim %897, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc369)
    %899 = stablehlo.add %895, %898 : tensor<2x50x768xbf16> loc(#loc369)
    %900 = stablehlo.add %815, %899 : tensor<2x50x768xbf16> loc(#loc370)
    %901 = stablehlo.reduce(%900 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc371)
    %902 = stablehlo.multiply %901, %cst_4 : tensor<2x50xbf16> loc(#loc371)
    %903 = stablehlo.broadcast_in_dim %902, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc372)
    %904 = stablehlo.subtract %900, %903 : tensor<2x50x768xbf16> loc(#loc372)
    %905 = stablehlo.multiply %904, %904 : tensor<2x50x768xbf16> loc(#loc371)
    %906 = stablehlo.reduce(%905 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc371)
    %907 = stablehlo.multiply %906, %cst_4 : tensor<2x50xbf16> loc(#loc371)
    %908 = stablehlo.reshape %907 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc371)
    %909 = stablehlo.add %908, %cst_3 : tensor<2x50x1xbf16> loc(#loc373)
    %910 = stablehlo.rsqrt %909 : tensor<2x50x1xbf16> loc(#loc374)
    %911 = stablehlo.reshape %910 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc375)
    %912 = stablehlo.broadcast_in_dim %911, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc375)
    %913 = stablehlo.multiply %904, %912 : tensor<2x50x768xbf16> loc(#loc375)
    %914 = stablehlo.reshape %arg68 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %915 = stablehlo.reshape %914 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %916 = stablehlo.broadcast_in_dim %915, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc375)
    %917 = stablehlo.multiply %913, %916 : tensor<2x50x768xbf16> loc(#loc375)
    %918 = stablehlo.reshape %arg67 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %919 = stablehlo.reshape %918 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %920 = stablehlo.broadcast_in_dim %919, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc373)
    %921 = stablehlo.add %917, %920 : tensor<2x50x768xbf16> loc(#loc373)
    %922 = stablehlo.reshape %921 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc376)
    %923 = stablehlo.reshape %arg66 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %924 = stablehlo.reshape %923 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %925 = stablehlo.transpose %924, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc377)
    %926 = stablehlo.dot_general %922, %925, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc378)
    %927 = stablehlo.reshape %926 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc376)
    %928 = stablehlo.reshape %arg65 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %929 = stablehlo.reshape %928 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %930 = stablehlo.broadcast_in_dim %929, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc379)
    %931 = stablehlo.add %927, %930 : tensor<2x50x3072xbf16> loc(#loc379)
    %932 = stablehlo.multiply %931, %cst_1 : tensor<2x50x3072xbf16> loc(#loc380)
    %933 = stablehlo.logistic %932 : tensor<2x50x3072xbf16> loc(#loc381)
    %934 = stablehlo.multiply %931, %933 : tensor<2x50x3072xbf16> loc(#loc380)
    %935 = stablehlo.reshape %934 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc382)
    %936 = stablehlo.reshape %arg64 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %937 = stablehlo.reshape %936 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %938 = stablehlo.transpose %937, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc383)
    %939 = stablehlo.dot_general %935, %938, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc384)
    %940 = stablehlo.reshape %939 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc382)
    %941 = stablehlo.reshape %arg63 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %942 = stablehlo.reshape %941 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %943 = stablehlo.broadcast_in_dim %942, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc385)
    %944 = stablehlo.add %940, %943 : tensor<2x50x768xbf16> loc(#loc385)
    %945 = stablehlo.add %900, %944 : tensor<2x50x768xbf16> loc(#loc386)
    %946 = stablehlo.reduce(%945 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc387)
    %947 = stablehlo.multiply %946, %cst_4 : tensor<2x50xbf16> loc(#loc387)
    %948 = stablehlo.broadcast_in_dim %947, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc388)
    %949 = stablehlo.subtract %945, %948 : tensor<2x50x768xbf16> loc(#loc388)
    %950 = stablehlo.multiply %949, %949 : tensor<2x50x768xbf16> loc(#loc387)
    %951 = stablehlo.reduce(%950 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc387)
    %952 = stablehlo.multiply %951, %cst_4 : tensor<2x50xbf16> loc(#loc387)
    %953 = stablehlo.reshape %952 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc387)
    %954 = stablehlo.add %953, %cst_3 : tensor<2x50x1xbf16> loc(#loc389)
    %955 = stablehlo.rsqrt %954 : tensor<2x50x1xbf16> loc(#loc390)
    %956 = stablehlo.reshape %955 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc391)
    %957 = stablehlo.broadcast_in_dim %956, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc391)
    %958 = stablehlo.multiply %949, %957 : tensor<2x50x768xbf16> loc(#loc391)
    %959 = stablehlo.reshape %arg62 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %960 = stablehlo.reshape %959 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %961 = stablehlo.broadcast_in_dim %960, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc391)
    %962 = stablehlo.multiply %958, %961 : tensor<2x50x768xbf16> loc(#loc391)
    %963 = stablehlo.reshape %arg61 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %964 = stablehlo.reshape %963 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %965 = stablehlo.broadcast_in_dim %964, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc389)
    %966 = stablehlo.add %962, %965 : tensor<2x50x768xbf16> loc(#loc389)
    %967 = stablehlo.reshape %966 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc392)
    %968 = stablehlo.reshape %arg185 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %969 = stablehlo.reshape %968 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %970 = stablehlo.transpose %969, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc393)
    %971 = stablehlo.dot_general %967, %970, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc394)
    %972 = stablehlo.reshape %971 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc392)
    %973 = stablehlo.reshape %arg184 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %974 = stablehlo.reshape %973 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %975 = stablehlo.broadcast_in_dim %974, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc395)
    %976 = stablehlo.add %972, %975 : tensor<2x50x768xbf16> loc(#loc395)
    %977 = stablehlo.reshape %976 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc396)
    %978 = stablehlo.transpose %977, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc397)
    %979 = stablehlo.reshape %978 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc398)
    %980 = stablehlo.reshape %arg183 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %981 = stablehlo.reshape %980 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %982 = stablehlo.transpose %981, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc399)
    %983 = stablehlo.dot_general %967, %982, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc400)
    %984 = stablehlo.reshape %983 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc401)
    %985 = stablehlo.reshape %arg182 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %986 = stablehlo.reshape %985 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %987 = stablehlo.broadcast_in_dim %986, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc402)
    %988 = stablehlo.add %984, %987 : tensor<2x50x768xbf16> loc(#loc402)
    %989 = stablehlo.reshape %988 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc403)
    %990 = stablehlo.transpose %989, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc404)
    %991 = stablehlo.reshape %990 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc398)
    %992 = stablehlo.dot_general %979, %991, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc405)
    %993 = stablehlo.reshape %992 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc398)
    %994 = stablehlo.multiply %993, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc406)
    %995 = stablehlo.convert %994 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc407)
    %996 = stablehlo.reduce(%995 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc408)
    %997 = stablehlo.broadcast_in_dim %996, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc408)
    %998 = stablehlo.subtract %995, %997 : tensor<2x12x50x50xf32> loc(#loc408)
    %999 = stablehlo.exponential %998 : tensor<2x12x50x50xf32> loc(#loc408)
    %1000 = stablehlo.reduce(%999 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc408)
    %1001 = stablehlo.broadcast_in_dim %1000, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc408)
    %1002 = stablehlo.divide %999, %1001 : tensor<2x12x50x50xf32> loc(#loc408)
    %1003 = stablehlo.convert %1002 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc409)
    %1004 = stablehlo.reshape %1003 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc410)
    %1005 = stablehlo.reshape %arg60 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1006 = stablehlo.reshape %1005 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1007 = stablehlo.transpose %1006, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc411)
    %1008 = stablehlo.dot_general %967, %1007, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc412)
    %1009 = stablehlo.reshape %1008 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc413)
    %1010 = stablehlo.reshape %arg59 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1011 = stablehlo.reshape %1010 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1012 = stablehlo.broadcast_in_dim %1011, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc414)
    %1013 = stablehlo.add %1009, %1012 : tensor<2x50x768xbf16> loc(#loc414)
    %1014 = stablehlo.reshape %1013 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc415)
    %1015 = stablehlo.transpose %1014, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc416)
    %1016 = stablehlo.reshape %1015 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc410)
    %1017 = stablehlo.dot_general %1004, %1016, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc417)
    %1018 = stablehlo.reshape %1017 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc410)
    %1019 = stablehlo.transpose %1018, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc418)
    %1020 = stablehlo.reshape %1019 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc419)
    %1021 = stablehlo.reshape %arg58 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1022 = stablehlo.reshape %1021 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1023 = stablehlo.transpose %1022, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc420)
    %1024 = stablehlo.dot_general %1020, %1023, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc421)
    %1025 = stablehlo.reshape %1024 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc419)
    %1026 = stablehlo.reshape %arg57 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1027 = stablehlo.reshape %1026 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1028 = stablehlo.broadcast_in_dim %1027, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc422)
    %1029 = stablehlo.add %1025, %1028 : tensor<2x50x768xbf16> loc(#loc422)
    %1030 = stablehlo.add %945, %1029 : tensor<2x50x768xbf16> loc(#loc423)
    %1031 = stablehlo.reduce(%1030 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc424)
    %1032 = stablehlo.multiply %1031, %cst_4 : tensor<2x50xbf16> loc(#loc424)
    %1033 = stablehlo.broadcast_in_dim %1032, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc425)
    %1034 = stablehlo.subtract %1030, %1033 : tensor<2x50x768xbf16> loc(#loc425)
    %1035 = stablehlo.multiply %1034, %1034 : tensor<2x50x768xbf16> loc(#loc424)
    %1036 = stablehlo.reduce(%1035 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc424)
    %1037 = stablehlo.multiply %1036, %cst_4 : tensor<2x50xbf16> loc(#loc424)
    %1038 = stablehlo.reshape %1037 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc424)
    %1039 = stablehlo.add %1038, %cst_3 : tensor<2x50x1xbf16> loc(#loc426)
    %1040 = stablehlo.rsqrt %1039 : tensor<2x50x1xbf16> loc(#loc427)
    %1041 = stablehlo.reshape %1040 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc428)
    %1042 = stablehlo.broadcast_in_dim %1041, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc428)
    %1043 = stablehlo.multiply %1034, %1042 : tensor<2x50x768xbf16> loc(#loc428)
    %1044 = stablehlo.reshape %arg56 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1045 = stablehlo.reshape %1044 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1046 = stablehlo.broadcast_in_dim %1045, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc428)
    %1047 = stablehlo.multiply %1043, %1046 : tensor<2x50x768xbf16> loc(#loc428)
    %1048 = stablehlo.reshape %arg55 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1049 = stablehlo.reshape %1048 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1050 = stablehlo.broadcast_in_dim %1049, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc426)
    %1051 = stablehlo.add %1047, %1050 : tensor<2x50x768xbf16> loc(#loc426)
    %1052 = stablehlo.reshape %1051 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc429)
    %1053 = stablehlo.reshape %arg54 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1054 = stablehlo.reshape %1053 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1055 = stablehlo.transpose %1054, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc430)
    %1056 = stablehlo.dot_general %1052, %1055, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc431)
    %1057 = stablehlo.reshape %1056 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc429)
    %1058 = stablehlo.reshape %arg53 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1059 = stablehlo.reshape %1058 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1060 = stablehlo.broadcast_in_dim %1059, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc432)
    %1061 = stablehlo.add %1057, %1060 : tensor<2x50x3072xbf16> loc(#loc432)
    %1062 = stablehlo.multiply %1061, %cst_1 : tensor<2x50x3072xbf16> loc(#loc433)
    %1063 = stablehlo.logistic %1062 : tensor<2x50x3072xbf16> loc(#loc434)
    %1064 = stablehlo.multiply %1061, %1063 : tensor<2x50x3072xbf16> loc(#loc433)
    %1065 = stablehlo.reshape %1064 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc435)
    %1066 = stablehlo.reshape %arg52 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1067 = stablehlo.reshape %1066 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1068 = stablehlo.transpose %1067, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc436)
    %1069 = stablehlo.dot_general %1065, %1068, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc437)
    %1070 = stablehlo.reshape %1069 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc435)
    %1071 = stablehlo.reshape %arg51 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1072 = stablehlo.reshape %1071 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1073 = stablehlo.broadcast_in_dim %1072, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc438)
    %1074 = stablehlo.add %1070, %1073 : tensor<2x50x768xbf16> loc(#loc438)
    %1075 = stablehlo.add %1030, %1074 : tensor<2x50x768xbf16> loc(#loc439)
    %1076 = stablehlo.reduce(%1075 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc440)
    %1077 = stablehlo.multiply %1076, %cst_4 : tensor<2x50xbf16> loc(#loc440)
    %1078 = stablehlo.broadcast_in_dim %1077, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc441)
    %1079 = stablehlo.subtract %1075, %1078 : tensor<2x50x768xbf16> loc(#loc441)
    %1080 = stablehlo.multiply %1079, %1079 : tensor<2x50x768xbf16> loc(#loc440)
    %1081 = stablehlo.reduce(%1080 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc440)
    %1082 = stablehlo.multiply %1081, %cst_4 : tensor<2x50xbf16> loc(#loc440)
    %1083 = stablehlo.reshape %1082 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc440)
    %1084 = stablehlo.add %1083, %cst_3 : tensor<2x50x1xbf16> loc(#loc442)
    %1085 = stablehlo.rsqrt %1084 : tensor<2x50x1xbf16> loc(#loc443)
    %1086 = stablehlo.reshape %1085 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc444)
    %1087 = stablehlo.broadcast_in_dim %1086, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc444)
    %1088 = stablehlo.multiply %1079, %1087 : tensor<2x50x768xbf16> loc(#loc444)
    %1089 = stablehlo.reshape %arg50 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1090 = stablehlo.reshape %1089 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1091 = stablehlo.broadcast_in_dim %1090, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc444)
    %1092 = stablehlo.multiply %1088, %1091 : tensor<2x50x768xbf16> loc(#loc444)
    %1093 = stablehlo.reshape %arg49 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1094 = stablehlo.reshape %1093 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1095 = stablehlo.broadcast_in_dim %1094, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc442)
    %1096 = stablehlo.add %1092, %1095 : tensor<2x50x768xbf16> loc(#loc442)
    %1097 = stablehlo.reshape %1096 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc445)
    %1098 = stablehlo.reshape %arg189 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1099 = stablehlo.reshape %1098 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1100 = stablehlo.transpose %1099, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc446)
    %1101 = stablehlo.dot_general %1097, %1100, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc447)
    %1102 = stablehlo.reshape %1101 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc445)
    %1103 = stablehlo.reshape %arg188 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1104 = stablehlo.reshape %1103 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1105 = stablehlo.broadcast_in_dim %1104, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc448)
    %1106 = stablehlo.add %1102, %1105 : tensor<2x50x768xbf16> loc(#loc448)
    %1107 = stablehlo.reshape %1106 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc449)
    %1108 = stablehlo.transpose %1107, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc450)
    %1109 = stablehlo.reshape %1108 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc451)
    %1110 = stablehlo.reshape %arg187 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1111 = stablehlo.reshape %1110 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1112 = stablehlo.transpose %1111, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc452)
    %1113 = stablehlo.dot_general %1097, %1112, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc453)
    %1114 = stablehlo.reshape %1113 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc454)
    %1115 = stablehlo.reshape %arg186 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1116 = stablehlo.reshape %1115 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1117 = stablehlo.broadcast_in_dim %1116, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc455)
    %1118 = stablehlo.add %1114, %1117 : tensor<2x50x768xbf16> loc(#loc455)
    %1119 = stablehlo.reshape %1118 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc456)
    %1120 = stablehlo.transpose %1119, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc457)
    %1121 = stablehlo.reshape %1120 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc451)
    %1122 = stablehlo.dot_general %1109, %1121, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc458)
    %1123 = stablehlo.reshape %1122 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc451)
    %1124 = stablehlo.multiply %1123, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc459)
    %1125 = stablehlo.convert %1124 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc460)
    %1126 = stablehlo.reduce(%1125 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc461)
    %1127 = stablehlo.broadcast_in_dim %1126, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc461)
    %1128 = stablehlo.subtract %1125, %1127 : tensor<2x12x50x50xf32> loc(#loc461)
    %1129 = stablehlo.exponential %1128 : tensor<2x12x50x50xf32> loc(#loc461)
    %1130 = stablehlo.reduce(%1129 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc461)
    %1131 = stablehlo.broadcast_in_dim %1130, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc461)
    %1132 = stablehlo.divide %1129, %1131 : tensor<2x12x50x50xf32> loc(#loc461)
    %1133 = stablehlo.convert %1132 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc462)
    %1134 = stablehlo.reshape %1133 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc463)
    %1135 = stablehlo.reshape %arg48 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1136 = stablehlo.reshape %1135 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1137 = stablehlo.transpose %1136, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc464)
    %1138 = stablehlo.dot_general %1097, %1137, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc465)
    %1139 = stablehlo.reshape %1138 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc466)
    %1140 = stablehlo.reshape %arg47 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1141 = stablehlo.reshape %1140 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1142 = stablehlo.broadcast_in_dim %1141, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc467)
    %1143 = stablehlo.add %1139, %1142 : tensor<2x50x768xbf16> loc(#loc467)
    %1144 = stablehlo.reshape %1143 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc468)
    %1145 = stablehlo.transpose %1144, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc469)
    %1146 = stablehlo.reshape %1145 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc463)
    %1147 = stablehlo.dot_general %1134, %1146, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc470)
    %1148 = stablehlo.reshape %1147 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc463)
    %1149 = stablehlo.transpose %1148, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc471)
    %1150 = stablehlo.reshape %1149 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc472)
    %1151 = stablehlo.reshape %arg46 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1152 = stablehlo.reshape %1151 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1153 = stablehlo.transpose %1152, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc473)
    %1154 = stablehlo.dot_general %1150, %1153, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc474)
    %1155 = stablehlo.reshape %1154 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc472)
    %1156 = stablehlo.reshape %arg45 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1157 = stablehlo.reshape %1156 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1158 = stablehlo.broadcast_in_dim %1157, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc475)
    %1159 = stablehlo.add %1155, %1158 : tensor<2x50x768xbf16> loc(#loc475)
    %1160 = stablehlo.add %1075, %1159 : tensor<2x50x768xbf16> loc(#loc476)
    %1161 = stablehlo.reduce(%1160 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc477)
    %1162 = stablehlo.multiply %1161, %cst_4 : tensor<2x50xbf16> loc(#loc477)
    %1163 = stablehlo.broadcast_in_dim %1162, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc478)
    %1164 = stablehlo.subtract %1160, %1163 : tensor<2x50x768xbf16> loc(#loc478)
    %1165 = stablehlo.multiply %1164, %1164 : tensor<2x50x768xbf16> loc(#loc477)
    %1166 = stablehlo.reduce(%1165 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc477)
    %1167 = stablehlo.multiply %1166, %cst_4 : tensor<2x50xbf16> loc(#loc477)
    %1168 = stablehlo.reshape %1167 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc477)
    %1169 = stablehlo.add %1168, %cst_3 : tensor<2x50x1xbf16> loc(#loc479)
    %1170 = stablehlo.rsqrt %1169 : tensor<2x50x1xbf16> loc(#loc480)
    %1171 = stablehlo.reshape %1170 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc481)
    %1172 = stablehlo.broadcast_in_dim %1171, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc481)
    %1173 = stablehlo.multiply %1164, %1172 : tensor<2x50x768xbf16> loc(#loc481)
    %1174 = stablehlo.reshape %arg44 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1175 = stablehlo.reshape %1174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1176 = stablehlo.broadcast_in_dim %1175, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc481)
    %1177 = stablehlo.multiply %1173, %1176 : tensor<2x50x768xbf16> loc(#loc481)
    %1178 = stablehlo.reshape %arg43 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1179 = stablehlo.reshape %1178 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1180 = stablehlo.broadcast_in_dim %1179, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc479)
    %1181 = stablehlo.add %1177, %1180 : tensor<2x50x768xbf16> loc(#loc479)
    %1182 = stablehlo.reshape %1181 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc482)
    %1183 = stablehlo.reshape %arg42 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1184 = stablehlo.reshape %1183 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1185 = stablehlo.transpose %1184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc483)
    %1186 = stablehlo.dot_general %1182, %1185, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc484)
    %1187 = stablehlo.reshape %1186 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc482)
    %1188 = stablehlo.reshape %arg41 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1189 = stablehlo.reshape %1188 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1190 = stablehlo.broadcast_in_dim %1189, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc485)
    %1191 = stablehlo.add %1187, %1190 : tensor<2x50x3072xbf16> loc(#loc485)
    %1192 = stablehlo.multiply %1191, %cst_1 : tensor<2x50x3072xbf16> loc(#loc486)
    %1193 = stablehlo.logistic %1192 : tensor<2x50x3072xbf16> loc(#loc487)
    %1194 = stablehlo.multiply %1191, %1193 : tensor<2x50x3072xbf16> loc(#loc486)
    %1195 = stablehlo.reshape %1194 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc488)
    %1196 = stablehlo.reshape %arg40 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1197 = stablehlo.reshape %1196 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1198 = stablehlo.transpose %1197, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc489)
    %1199 = stablehlo.dot_general %1195, %1198, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc490)
    %1200 = stablehlo.reshape %1199 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc488)
    %1201 = stablehlo.reshape %arg39 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1202 = stablehlo.reshape %1201 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1203 = stablehlo.broadcast_in_dim %1202, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc491)
    %1204 = stablehlo.add %1200, %1203 : tensor<2x50x768xbf16> loc(#loc491)
    %1205 = stablehlo.add %1160, %1204 : tensor<2x50x768xbf16> loc(#loc492)
    %1206 = stablehlo.reduce(%1205 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc493)
    %1207 = stablehlo.multiply %1206, %cst_4 : tensor<2x50xbf16> loc(#loc493)
    %1208 = stablehlo.broadcast_in_dim %1207, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc494)
    %1209 = stablehlo.subtract %1205, %1208 : tensor<2x50x768xbf16> loc(#loc494)
    %1210 = stablehlo.multiply %1209, %1209 : tensor<2x50x768xbf16> loc(#loc493)
    %1211 = stablehlo.reduce(%1210 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc493)
    %1212 = stablehlo.multiply %1211, %cst_4 : tensor<2x50xbf16> loc(#loc493)
    %1213 = stablehlo.reshape %1212 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc493)
    %1214 = stablehlo.add %1213, %cst_3 : tensor<2x50x1xbf16> loc(#loc495)
    %1215 = stablehlo.rsqrt %1214 : tensor<2x50x1xbf16> loc(#loc496)
    %1216 = stablehlo.reshape %1215 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc497)
    %1217 = stablehlo.broadcast_in_dim %1216, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc497)
    %1218 = stablehlo.multiply %1209, %1217 : tensor<2x50x768xbf16> loc(#loc497)
    %1219 = stablehlo.reshape %arg38 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1220 = stablehlo.reshape %1219 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1221 = stablehlo.broadcast_in_dim %1220, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc497)
    %1222 = stablehlo.multiply %1218, %1221 : tensor<2x50x768xbf16> loc(#loc497)
    %1223 = stablehlo.reshape %arg37 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1224 = stablehlo.reshape %1223 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1225 = stablehlo.broadcast_in_dim %1224, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc495)
    %1226 = stablehlo.add %1222, %1225 : tensor<2x50x768xbf16> loc(#loc495)
    %1227 = stablehlo.reshape %1226 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc498)
    %1228 = stablehlo.reshape %arg193 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1229 = stablehlo.reshape %1228 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1230 = stablehlo.transpose %1229, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc499)
    %1231 = stablehlo.dot_general %1227, %1230, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc500)
    %1232 = stablehlo.reshape %1231 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc498)
    %1233 = stablehlo.reshape %arg192 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1234 = stablehlo.reshape %1233 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1235 = stablehlo.broadcast_in_dim %1234, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc501)
    %1236 = stablehlo.add %1232, %1235 : tensor<2x50x768xbf16> loc(#loc501)
    %1237 = stablehlo.reshape %1236 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc502)
    %1238 = stablehlo.transpose %1237, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc503)
    %1239 = stablehlo.reshape %1238 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc504)
    %1240 = stablehlo.reshape %arg191 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1241 = stablehlo.reshape %1240 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1242 = stablehlo.transpose %1241, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc505)
    %1243 = stablehlo.dot_general %1227, %1242, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc506)
    %1244 = stablehlo.reshape %1243 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc507)
    %1245 = stablehlo.reshape %arg190 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1246 = stablehlo.reshape %1245 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1247 = stablehlo.broadcast_in_dim %1246, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc508)
    %1248 = stablehlo.add %1244, %1247 : tensor<2x50x768xbf16> loc(#loc508)
    %1249 = stablehlo.reshape %1248 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc509)
    %1250 = stablehlo.transpose %1249, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc510)
    %1251 = stablehlo.reshape %1250 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc504)
    %1252 = stablehlo.dot_general %1239, %1251, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc511)
    %1253 = stablehlo.reshape %1252 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc504)
    %1254 = stablehlo.multiply %1253, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc512)
    %1255 = stablehlo.convert %1254 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc513)
    %1256 = stablehlo.reduce(%1255 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc514)
    %1257 = stablehlo.broadcast_in_dim %1256, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc514)
    %1258 = stablehlo.subtract %1255, %1257 : tensor<2x12x50x50xf32> loc(#loc514)
    %1259 = stablehlo.exponential %1258 : tensor<2x12x50x50xf32> loc(#loc514)
    %1260 = stablehlo.reduce(%1259 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc514)
    %1261 = stablehlo.broadcast_in_dim %1260, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc514)
    %1262 = stablehlo.divide %1259, %1261 : tensor<2x12x50x50xf32> loc(#loc514)
    %1263 = stablehlo.convert %1262 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc515)
    %1264 = stablehlo.reshape %1263 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc516)
    %1265 = stablehlo.reshape %arg36 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1266 = stablehlo.reshape %1265 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1267 = stablehlo.transpose %1266, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc517)
    %1268 = stablehlo.dot_general %1227, %1267, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc518)
    %1269 = stablehlo.reshape %1268 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc519)
    %1270 = stablehlo.reshape %arg35 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1271 = stablehlo.reshape %1270 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1272 = stablehlo.broadcast_in_dim %1271, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc520)
    %1273 = stablehlo.add %1269, %1272 : tensor<2x50x768xbf16> loc(#loc520)
    %1274 = stablehlo.reshape %1273 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc521)
    %1275 = stablehlo.transpose %1274, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc522)
    %1276 = stablehlo.reshape %1275 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc516)
    %1277 = stablehlo.dot_general %1264, %1276, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc523)
    %1278 = stablehlo.reshape %1277 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc516)
    %1279 = stablehlo.transpose %1278, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc524)
    %1280 = stablehlo.reshape %1279 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc525)
    %1281 = stablehlo.reshape %arg34 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1282 = stablehlo.reshape %1281 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1283 = stablehlo.transpose %1282, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc526)
    %1284 = stablehlo.dot_general %1280, %1283, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc527)
    %1285 = stablehlo.reshape %1284 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc525)
    %1286 = stablehlo.reshape %arg33 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1287 = stablehlo.reshape %1286 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1288 = stablehlo.broadcast_in_dim %1287, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc528)
    %1289 = stablehlo.add %1285, %1288 : tensor<2x50x768xbf16> loc(#loc528)
    %1290 = stablehlo.add %1205, %1289 : tensor<2x50x768xbf16> loc(#loc529)
    %1291 = stablehlo.reduce(%1290 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc530)
    %1292 = stablehlo.multiply %1291, %cst_4 : tensor<2x50xbf16> loc(#loc530)
    %1293 = stablehlo.broadcast_in_dim %1292, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc531)
    %1294 = stablehlo.subtract %1290, %1293 : tensor<2x50x768xbf16> loc(#loc531)
    %1295 = stablehlo.multiply %1294, %1294 : tensor<2x50x768xbf16> loc(#loc530)
    %1296 = stablehlo.reduce(%1295 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc530)
    %1297 = stablehlo.multiply %1296, %cst_4 : tensor<2x50xbf16> loc(#loc530)
    %1298 = stablehlo.reshape %1297 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc530)
    %1299 = stablehlo.add %1298, %cst_3 : tensor<2x50x1xbf16> loc(#loc532)
    %1300 = stablehlo.rsqrt %1299 : tensor<2x50x1xbf16> loc(#loc533)
    %1301 = stablehlo.reshape %1300 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc534)
    %1302 = stablehlo.broadcast_in_dim %1301, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc534)
    %1303 = stablehlo.multiply %1294, %1302 : tensor<2x50x768xbf16> loc(#loc534)
    %1304 = stablehlo.reshape %arg32 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1305 = stablehlo.reshape %1304 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1306 = stablehlo.broadcast_in_dim %1305, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc534)
    %1307 = stablehlo.multiply %1303, %1306 : tensor<2x50x768xbf16> loc(#loc534)
    %1308 = stablehlo.reshape %arg31 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1309 = stablehlo.reshape %1308 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1310 = stablehlo.broadcast_in_dim %1309, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc532)
    %1311 = stablehlo.add %1307, %1310 : tensor<2x50x768xbf16> loc(#loc532)
    %1312 = stablehlo.reshape %1311 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc535)
    %1313 = stablehlo.reshape %arg30 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1314 = stablehlo.reshape %1313 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1315 = stablehlo.transpose %1314, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc536)
    %1316 = stablehlo.dot_general %1312, %1315, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc537)
    %1317 = stablehlo.reshape %1316 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc535)
    %1318 = stablehlo.reshape %arg29 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1319 = stablehlo.reshape %1318 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1320 = stablehlo.broadcast_in_dim %1319, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc538)
    %1321 = stablehlo.add %1317, %1320 : tensor<2x50x3072xbf16> loc(#loc538)
    %1322 = stablehlo.multiply %1321, %cst_1 : tensor<2x50x3072xbf16> loc(#loc539)
    %1323 = stablehlo.logistic %1322 : tensor<2x50x3072xbf16> loc(#loc540)
    %1324 = stablehlo.multiply %1321, %1323 : tensor<2x50x3072xbf16> loc(#loc539)
    %1325 = stablehlo.reshape %1324 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc541)
    %1326 = stablehlo.reshape %arg28 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1327 = stablehlo.reshape %1326 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1328 = stablehlo.transpose %1327, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc542)
    %1329 = stablehlo.dot_general %1325, %1328, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc543)
    %1330 = stablehlo.reshape %1329 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc541)
    %1331 = stablehlo.reshape %arg27 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1332 = stablehlo.reshape %1331 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1333 = stablehlo.broadcast_in_dim %1332, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc544)
    %1334 = stablehlo.add %1330, %1333 : tensor<2x50x768xbf16> loc(#loc544)
    %1335 = stablehlo.add %1290, %1334 : tensor<2x50x768xbf16> loc(#loc545)
    %1336 = stablehlo.reduce(%1335 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc546)
    %1337 = stablehlo.multiply %1336, %cst_4 : tensor<2x50xbf16> loc(#loc546)
    %1338 = stablehlo.broadcast_in_dim %1337, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc547)
    %1339 = stablehlo.subtract %1335, %1338 : tensor<2x50x768xbf16> loc(#loc547)
    %1340 = stablehlo.multiply %1339, %1339 : tensor<2x50x768xbf16> loc(#loc546)
    %1341 = stablehlo.reduce(%1340 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc546)
    %1342 = stablehlo.multiply %1341, %cst_4 : tensor<2x50xbf16> loc(#loc546)
    %1343 = stablehlo.reshape %1342 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc546)
    %1344 = stablehlo.add %1343, %cst_3 : tensor<2x50x1xbf16> loc(#loc548)
    %1345 = stablehlo.rsqrt %1344 : tensor<2x50x1xbf16> loc(#loc549)
    %1346 = stablehlo.reshape %1345 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc550)
    %1347 = stablehlo.broadcast_in_dim %1346, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc550)
    %1348 = stablehlo.multiply %1339, %1347 : tensor<2x50x768xbf16> loc(#loc550)
    %1349 = stablehlo.reshape %arg26 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1350 = stablehlo.reshape %1349 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1351 = stablehlo.broadcast_in_dim %1350, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc550)
    %1352 = stablehlo.multiply %1348, %1351 : tensor<2x50x768xbf16> loc(#loc550)
    %1353 = stablehlo.reshape %arg25 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1354 = stablehlo.reshape %1353 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1355 = stablehlo.broadcast_in_dim %1354, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc548)
    %1356 = stablehlo.add %1352, %1355 : tensor<2x50x768xbf16> loc(#loc548)
    %1357 = stablehlo.reshape %1356 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc551)
    %1358 = stablehlo.reshape %arg197 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1359 = stablehlo.reshape %1358 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1360 = stablehlo.transpose %1359, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc552)
    %1361 = stablehlo.dot_general %1357, %1360, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc553)
    %1362 = stablehlo.reshape %1361 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc551)
    %1363 = stablehlo.reshape %arg196 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1364 = stablehlo.reshape %1363 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1365 = stablehlo.broadcast_in_dim %1364, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc554)
    %1366 = stablehlo.add %1362, %1365 : tensor<2x50x768xbf16> loc(#loc554)
    %1367 = stablehlo.reshape %1366 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc555)
    %1368 = stablehlo.transpose %1367, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc556)
    %1369 = stablehlo.reshape %1368 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc557)
    %1370 = stablehlo.reshape %arg195 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1371 = stablehlo.reshape %1370 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1372 = stablehlo.transpose %1371, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc558)
    %1373 = stablehlo.dot_general %1357, %1372, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc559)
    %1374 = stablehlo.reshape %1373 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc560)
    %1375 = stablehlo.reshape %arg194 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1376 = stablehlo.reshape %1375 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1377 = stablehlo.broadcast_in_dim %1376, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc561)
    %1378 = stablehlo.add %1374, %1377 : tensor<2x50x768xbf16> loc(#loc561)
    %1379 = stablehlo.reshape %1378 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc562)
    %1380 = stablehlo.transpose %1379, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc563)
    %1381 = stablehlo.reshape %1380 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc557)
    %1382 = stablehlo.dot_general %1369, %1381, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc564)
    %1383 = stablehlo.reshape %1382 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc557)
    %1384 = stablehlo.multiply %1383, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc565)
    %1385 = stablehlo.convert %1384 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc566)
    %1386 = stablehlo.reduce(%1385 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc567)
    %1387 = stablehlo.broadcast_in_dim %1386, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc567)
    %1388 = stablehlo.subtract %1385, %1387 : tensor<2x12x50x50xf32> loc(#loc567)
    %1389 = stablehlo.exponential %1388 : tensor<2x12x50x50xf32> loc(#loc567)
    %1390 = stablehlo.reduce(%1389 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc567)
    %1391 = stablehlo.broadcast_in_dim %1390, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc567)
    %1392 = stablehlo.divide %1389, %1391 : tensor<2x12x50x50xf32> loc(#loc567)
    %1393 = stablehlo.convert %1392 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc568)
    %1394 = stablehlo.reshape %1393 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc569)
    %1395 = stablehlo.reshape %arg24 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1396 = stablehlo.reshape %1395 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1397 = stablehlo.transpose %1396, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc570)
    %1398 = stablehlo.dot_general %1357, %1397, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc571)
    %1399 = stablehlo.reshape %1398 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc572)
    %1400 = stablehlo.reshape %arg23 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1401 = stablehlo.reshape %1400 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1402 = stablehlo.broadcast_in_dim %1401, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc573)
    %1403 = stablehlo.add %1399, %1402 : tensor<2x50x768xbf16> loc(#loc573)
    %1404 = stablehlo.reshape %1403 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc574)
    %1405 = stablehlo.transpose %1404, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc575)
    %1406 = stablehlo.reshape %1405 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc569)
    %1407 = stablehlo.dot_general %1394, %1406, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc576)
    %1408 = stablehlo.reshape %1407 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc569)
    %1409 = stablehlo.transpose %1408, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc577)
    %1410 = stablehlo.reshape %1409 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc578)
    %1411 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1412 = stablehlo.reshape %1411 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1413 = stablehlo.transpose %1412, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc579)
    %1414 = stablehlo.dot_general %1410, %1413, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc580)
    %1415 = stablehlo.reshape %1414 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc578)
    %1416 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1417 = stablehlo.reshape %1416 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1418 = stablehlo.broadcast_in_dim %1417, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc581)
    %1419 = stablehlo.add %1415, %1418 : tensor<2x50x768xbf16> loc(#loc581)
    %1420 = stablehlo.add %1335, %1419 : tensor<2x50x768xbf16> loc(#loc582)
    %1421 = stablehlo.reduce(%1420 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc583)
    %1422 = stablehlo.multiply %1421, %cst_4 : tensor<2x50xbf16> loc(#loc583)
    %1423 = stablehlo.broadcast_in_dim %1422, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc584)
    %1424 = stablehlo.subtract %1420, %1423 : tensor<2x50x768xbf16> loc(#loc584)
    %1425 = stablehlo.multiply %1424, %1424 : tensor<2x50x768xbf16> loc(#loc583)
    %1426 = stablehlo.reduce(%1425 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc583)
    %1427 = stablehlo.multiply %1426, %cst_4 : tensor<2x50xbf16> loc(#loc583)
    %1428 = stablehlo.reshape %1427 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc583)
    %1429 = stablehlo.add %1428, %cst_3 : tensor<2x50x1xbf16> loc(#loc585)
    %1430 = stablehlo.rsqrt %1429 : tensor<2x50x1xbf16> loc(#loc586)
    %1431 = stablehlo.reshape %1430 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc587)
    %1432 = stablehlo.broadcast_in_dim %1431, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc587)
    %1433 = stablehlo.multiply %1424, %1432 : tensor<2x50x768xbf16> loc(#loc587)
    %1434 = stablehlo.reshape %arg20 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1435 = stablehlo.reshape %1434 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1436 = stablehlo.broadcast_in_dim %1435, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc587)
    %1437 = stablehlo.multiply %1433, %1436 : tensor<2x50x768xbf16> loc(#loc587)
    %1438 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1439 = stablehlo.reshape %1438 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1440 = stablehlo.broadcast_in_dim %1439, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc585)
    %1441 = stablehlo.add %1437, %1440 : tensor<2x50x768xbf16> loc(#loc585)
    %1442 = stablehlo.reshape %1441 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc588)
    %1443 = stablehlo.reshape %arg18 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1444 = stablehlo.reshape %1443 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1445 = stablehlo.transpose %1444, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc589)
    %1446 = stablehlo.dot_general %1442, %1445, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc590)
    %1447 = stablehlo.reshape %1446 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc588)
    %1448 = stablehlo.reshape %arg17 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1449 = stablehlo.reshape %1448 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1450 = stablehlo.broadcast_in_dim %1449, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc591)
    %1451 = stablehlo.add %1447, %1450 : tensor<2x50x3072xbf16> loc(#loc591)
    %1452 = stablehlo.multiply %1451, %cst_1 : tensor<2x50x3072xbf16> loc(#loc592)
    %1453 = stablehlo.logistic %1452 : tensor<2x50x3072xbf16> loc(#loc593)
    %1454 = stablehlo.multiply %1451, %1453 : tensor<2x50x3072xbf16> loc(#loc592)
    %1455 = stablehlo.reshape %1454 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc594)
    %1456 = stablehlo.reshape %arg16 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1457 = stablehlo.reshape %1456 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1458 = stablehlo.transpose %1457, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc595)
    %1459 = stablehlo.dot_general %1455, %1458, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc596)
    %1460 = stablehlo.reshape %1459 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc594)
    %1461 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1462 = stablehlo.reshape %1461 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1463 = stablehlo.broadcast_in_dim %1462, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc597)
    %1464 = stablehlo.add %1460, %1463 : tensor<2x50x768xbf16> loc(#loc597)
    %1465 = stablehlo.add %1420, %1464 : tensor<2x50x768xbf16> loc(#loc598)
    %1466 = stablehlo.reduce(%1465 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc599)
    %1467 = stablehlo.multiply %1466, %cst_4 : tensor<2x50xbf16> loc(#loc599)
    %1468 = stablehlo.broadcast_in_dim %1467, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc600)
    %1469 = stablehlo.subtract %1465, %1468 : tensor<2x50x768xbf16> loc(#loc600)
    %1470 = stablehlo.multiply %1469, %1469 : tensor<2x50x768xbf16> loc(#loc599)
    %1471 = stablehlo.reduce(%1470 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc599)
    %1472 = stablehlo.multiply %1471, %cst_4 : tensor<2x50xbf16> loc(#loc599)
    %1473 = stablehlo.reshape %1472 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc599)
    %1474 = stablehlo.add %1473, %cst_3 : tensor<2x50x1xbf16> loc(#loc601)
    %1475 = stablehlo.rsqrt %1474 : tensor<2x50x1xbf16> loc(#loc602)
    %1476 = stablehlo.reshape %1475 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc603)
    %1477 = stablehlo.broadcast_in_dim %1476, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc603)
    %1478 = stablehlo.multiply %1469, %1477 : tensor<2x50x768xbf16> loc(#loc603)
    %1479 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1480 = stablehlo.reshape %1479 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1481 = stablehlo.broadcast_in_dim %1480, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc603)
    %1482 = stablehlo.multiply %1478, %1481 : tensor<2x50x768xbf16> loc(#loc603)
    %1483 = stablehlo.reshape %arg13 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1484 = stablehlo.reshape %1483 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1485 = stablehlo.broadcast_in_dim %1484, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc601)
    %1486 = stablehlo.add %1482, %1485 : tensor<2x50x768xbf16> loc(#loc601)
    %1487 = stablehlo.reshape %1486 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc604)
    %1488 = stablehlo.reshape %arg201 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1489 = stablehlo.reshape %1488 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1490 = stablehlo.transpose %1489, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc605)
    %1491 = stablehlo.dot_general %1487, %1490, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc606)
    %1492 = stablehlo.reshape %1491 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc604)
    %1493 = stablehlo.reshape %arg200 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1494 = stablehlo.reshape %1493 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1495 = stablehlo.broadcast_in_dim %1494, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc607)
    %1496 = stablehlo.add %1492, %1495 : tensor<2x50x768xbf16> loc(#loc607)
    %1497 = stablehlo.reshape %1496 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc608)
    %1498 = stablehlo.transpose %1497, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc609)
    %1499 = stablehlo.reshape %1498 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc610)
    %1500 = stablehlo.reshape %arg199 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1501 = stablehlo.reshape %1500 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1502 = stablehlo.transpose %1501, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc611)
    %1503 = stablehlo.dot_general %1487, %1502, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc612)
    %1504 = stablehlo.reshape %1503 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc613)
    %1505 = stablehlo.reshape %arg198 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1506 = stablehlo.reshape %1505 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1507 = stablehlo.broadcast_in_dim %1506, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc614)
    %1508 = stablehlo.add %1504, %1507 : tensor<2x50x768xbf16> loc(#loc614)
    %1509 = stablehlo.reshape %1508 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc615)
    %1510 = stablehlo.transpose %1509, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc616)
    %1511 = stablehlo.reshape %1510 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc610)
    %1512 = stablehlo.dot_general %1499, %1511, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc617)
    %1513 = stablehlo.reshape %1512 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc610)
    %1514 = stablehlo.multiply %1513, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc618)
    %1515 = stablehlo.convert %1514 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc619)
    %1516 = stablehlo.reduce(%1515 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc620)
    %1517 = stablehlo.broadcast_in_dim %1516, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc620)
    %1518 = stablehlo.subtract %1515, %1517 : tensor<2x12x50x50xf32> loc(#loc620)
    %1519 = stablehlo.exponential %1518 : tensor<2x12x50x50xf32> loc(#loc620)
    %1520 = stablehlo.reduce(%1519 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc620)
    %1521 = stablehlo.broadcast_in_dim %1520, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc620)
    %1522 = stablehlo.divide %1519, %1521 : tensor<2x12x50x50xf32> loc(#loc620)
    %1523 = stablehlo.convert %1522 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc621)
    %1524 = stablehlo.reshape %1523 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc622)
    %1525 = stablehlo.reshape %arg12 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1526 = stablehlo.reshape %1525 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1527 = stablehlo.transpose %1526, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc623)
    %1528 = stablehlo.dot_general %1487, %1527, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc624)
    %1529 = stablehlo.reshape %1528 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc625)
    %1530 = stablehlo.reshape %arg11 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1531 = stablehlo.reshape %1530 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1532 = stablehlo.broadcast_in_dim %1531, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc626)
    %1533 = stablehlo.add %1529, %1532 : tensor<2x50x768xbf16> loc(#loc626)
    %1534 = stablehlo.reshape %1533 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc627)
    %1535 = stablehlo.transpose %1534, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc628)
    %1536 = stablehlo.reshape %1535 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc622)
    %1537 = stablehlo.dot_general %1524, %1536, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc629)
    %1538 = stablehlo.reshape %1537 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc622)
    %1539 = stablehlo.transpose %1538, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc630)
    %1540 = stablehlo.reshape %1539 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc631)
    %1541 = stablehlo.reshape %arg10 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1542 = stablehlo.reshape %1541 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1543 = stablehlo.transpose %1542, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc632)
    %1544 = stablehlo.dot_general %1540, %1543, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc633)
    %1545 = stablehlo.reshape %1544 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc631)
    %1546 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1547 = stablehlo.reshape %1546 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1548 = stablehlo.broadcast_in_dim %1547, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc634)
    %1549 = stablehlo.add %1545, %1548 : tensor<2x50x768xbf16> loc(#loc634)
    %1550 = stablehlo.add %1465, %1549 : tensor<2x50x768xbf16> loc(#loc635)
    %1551 = stablehlo.reduce(%1550 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc636)
    %1552 = stablehlo.multiply %1551, %cst_4 : tensor<2x50xbf16> loc(#loc636)
    %1553 = stablehlo.broadcast_in_dim %1552, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc637)
    %1554 = stablehlo.subtract %1550, %1553 : tensor<2x50x768xbf16> loc(#loc637)
    %1555 = stablehlo.multiply %1554, %1554 : tensor<2x50x768xbf16> loc(#loc636)
    %1556 = stablehlo.reduce(%1555 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc636)
    %1557 = stablehlo.multiply %1556, %cst_4 : tensor<2x50xbf16> loc(#loc636)
    %1558 = stablehlo.reshape %1557 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc636)
    %1559 = stablehlo.add %1558, %cst_3 : tensor<2x50x1xbf16> loc(#loc638)
    %1560 = stablehlo.rsqrt %1559 : tensor<2x50x1xbf16> loc(#loc639)
    %1561 = stablehlo.reshape %1560 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc640)
    %1562 = stablehlo.broadcast_in_dim %1561, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc640)
    %1563 = stablehlo.multiply %1554, %1562 : tensor<2x50x768xbf16> loc(#loc640)
    %1564 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1565 = stablehlo.reshape %1564 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1566 = stablehlo.broadcast_in_dim %1565, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc640)
    %1567 = stablehlo.multiply %1563, %1566 : tensor<2x50x768xbf16> loc(#loc640)
    %1568 = stablehlo.reshape %arg7 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1569 = stablehlo.reshape %1568 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1570 = stablehlo.broadcast_in_dim %1569, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc638)
    %1571 = stablehlo.add %1567, %1570 : tensor<2x50x768xbf16> loc(#loc638)
    %1572 = stablehlo.reshape %1571 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc641)
    %1573 = stablehlo.reshape %arg6 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1574 = stablehlo.reshape %1573 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1575 = stablehlo.transpose %1574, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc642)
    %1576 = stablehlo.dot_general %1572, %1575, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc643)
    %1577 = stablehlo.reshape %1576 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc641)
    %1578 = stablehlo.reshape %arg5 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1579 = stablehlo.reshape %1578 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1580 = stablehlo.broadcast_in_dim %1579, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc644)
    %1581 = stablehlo.add %1577, %1580 : tensor<2x50x3072xbf16> loc(#loc644)
    %1582 = stablehlo.multiply %1581, %cst_1 : tensor<2x50x3072xbf16> loc(#loc645)
    %1583 = stablehlo.logistic %1582 : tensor<2x50x3072xbf16> loc(#loc646)
    %1584 = stablehlo.multiply %1581, %1583 : tensor<2x50x3072xbf16> loc(#loc645)
    %1585 = stablehlo.reshape %1584 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc647)
    %1586 = stablehlo.reshape %arg4 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1587 = stablehlo.reshape %1586 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1588 = stablehlo.transpose %1587, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc648)
    %1589 = stablehlo.dot_general %1585, %1588, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc649)
    %1590 = stablehlo.reshape %1589 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc647)
    %1591 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1592 = stablehlo.reshape %1591 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1593 = stablehlo.broadcast_in_dim %1592, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc650)
    %1594 = stablehlo.add %1590, %1593 : tensor<2x50x768xbf16> loc(#loc650)
    %1595 = stablehlo.add %1550, %1594 : tensor<2x50x768xbf16> loc(#loc651)
    %1596 = stablehlo.slice %1595 [0:2, 0:1, 0:768] : (tensor<2x50x768xbf16>) -> tensor<2x1x768xbf16> loc(#loc652)
    %1597 = stablehlo.reshape %1596 : (tensor<2x1x768xbf16>) -> tensor<2x768xbf16> loc(#loc653)
    %1598 = stablehlo.reduce(%1597 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<2x768xbf16>, tensor<bf16>) -> tensor<2xbf16> loc(#loc654)
    %1599 = stablehlo.multiply %1598, %cst_0 : tensor<2xbf16> loc(#loc654)
    %1600 = stablehlo.broadcast_in_dim %1599, dims = [0] : (tensor<2xbf16>) -> tensor<2x768xbf16> loc(#loc655)
    %1601 = stablehlo.subtract %1597, %1600 : tensor<2x768xbf16> loc(#loc655)
    %1602 = stablehlo.multiply %1601, %1601 : tensor<2x768xbf16> loc(#loc654)
    %1603 = stablehlo.reduce(%1602 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<2x768xbf16>, tensor<bf16>) -> tensor<2xbf16> loc(#loc654)
    %1604 = stablehlo.multiply %1603, %cst_0 : tensor<2xbf16> loc(#loc654)
    %1605 = stablehlo.reshape %1604 : (tensor<2xbf16>) -> tensor<2x1xbf16> loc(#loc654)
    %1606 = stablehlo.add %1605, %cst : tensor<2x1xbf16> loc(#loc656)
    %1607 = stablehlo.rsqrt %1606 : tensor<2x1xbf16> loc(#loc657)
    %1608 = stablehlo.reshape %1607 : (tensor<2x1xbf16>) -> tensor<2xbf16> loc(#loc658)
    %1609 = stablehlo.broadcast_in_dim %1608, dims = [0] : (tensor<2xbf16>) -> tensor<2x768xbf16> loc(#loc658)
    %1610 = stablehlo.multiply %1601, %1609 : tensor<2x768xbf16> loc(#loc658)
    %1611 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1612 = stablehlo.reshape %1611 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1613 = stablehlo.broadcast_in_dim %1612, dims = [1] : (tensor<768xbf16>) -> tensor<2x768xbf16> loc(#loc658)
    %1614 = stablehlo.multiply %1610, %1613 : tensor<2x768xbf16> loc(#loc658)
    %1615 = stablehlo.reshape %arg1 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1616 = stablehlo.reshape %1615 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1617 = stablehlo.broadcast_in_dim %1616, dims = [1] : (tensor<768xbf16>) -> tensor<2x768xbf16> loc(#loc656)
    %1618 = stablehlo.add %1614, %1617 : tensor<2x768xbf16> loc(#loc656)
    %1619 = stablehlo.reshape %arg0 : (tensor<512x768xbf16>) -> tensor<1x512x768xbf16> loc(#loc2)
    %1620 = stablehlo.reshape %1619 : (tensor<1x512x768xbf16>) -> tensor<512x768xbf16> loc(#loc2)
    %1621 = stablehlo.transpose %1620, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,512]{0,1}"} : (tensor<512x768xbf16>) -> tensor<768x512xbf16> loc(#loc659)
    %1622 = stablehlo.dot_general %1618, %1621, contracting_dims = [1] x [0] : (tensor<2x768xbf16>, tensor<768x512xbf16>) -> tensor<2x512xbf16> loc(#loc660)
    return %1622, %1595 : tensor<2x512xbf16>, tensor<2x50x768xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("unknown|unknown|-1|unknownaten__view")
#loc3 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|227|aten__expand")
#loc4 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Conv2d[vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|224|aten__convolution_overrideable")
#loc5 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|225|aten__view")
#loc6 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|225|aten__permute")
#loc7 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|228|aten__cat")
#loc8 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Embedding[vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__view")
#loc9 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Embedding[vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__index_select")
#loc10 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__add")
#loc11 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__var_mean")
#loc12 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__sub")
#loc13 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__add")
#loc14 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__rsqrt")
#loc15 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__mul")
#loc16 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc17 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc18 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc19 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc20 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc21 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc22 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc23 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc24 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc25 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc26 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc27 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc28 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc29 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc30 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc31 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc32 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc33 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc34 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc35 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc36 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc37 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc38 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc39 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc40 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc41 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc42 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc43 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc44 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc45 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc46 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc47 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc48 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc49 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc50 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc51 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc52 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc53 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc54 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc55 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc56 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc57 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc58 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc59 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc60 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc61 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc62 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|QuickGELUActivation[vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc63 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|QuickGELUActivation[vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc64 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc65 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc66 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc67 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc68 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc69 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc70 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc71 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc72 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc73 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc74 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc75 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc76 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc77 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc78 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc79 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc80 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc81 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc82 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc83 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc84 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc85 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc86 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc87 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc88 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc89 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc90 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc91 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc92 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc93 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc94 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc95 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc96 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc97 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc98 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc99 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc100 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc101 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc102 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc103 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc104 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc105 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc106 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc107 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc108 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc109 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc110 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc111 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc112 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc113 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc114 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc115 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|QuickGELUActivation[vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc116 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|QuickGELUActivation[vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc117 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc118 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc119 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc120 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc121 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc122 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc123 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc124 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc125 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc126 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc127 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc128 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc129 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc130 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc131 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc132 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc133 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc134 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc135 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc136 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc137 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc138 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc139 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc140 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc141 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc142 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc143 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc144 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc145 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc146 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc147 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc148 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc149 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc150 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc151 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc152 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc153 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc154 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc155 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc156 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc157 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc158 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc159 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc160 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc161 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc162 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc163 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc164 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc165 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc166 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc167 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc168 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|QuickGELUActivation[vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc169 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|QuickGELUActivation[vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc170 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc171 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc172 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc173 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc174 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc175 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc176 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc177 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc178 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc179 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc180 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc181 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc182 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc183 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc184 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc185 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc186 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc187 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc188 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc189 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc190 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc191 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc192 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc193 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc194 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc195 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc196 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc197 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc198 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc199 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc200 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc201 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc202 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc203 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc204 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc205 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc206 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc207 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc208 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc209 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc210 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc211 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc212 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc213 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc214 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc215 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc216 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc217 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc218 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc219 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc220 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc221 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|QuickGELUActivation[vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc222 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|QuickGELUActivation[vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc223 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc224 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc225 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc226 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc227 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc228 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc229 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc230 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc231 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc232 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc233 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc234 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc235 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc236 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc237 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc238 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc239 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc240 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc241 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc242 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc243 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc244 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc245 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc246 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc247 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc248 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc249 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc250 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc251 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc252 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc253 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc254 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc255 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc256 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc257 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc258 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc259 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc260 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc261 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc262 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc263 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc264 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc265 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc266 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc267 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc268 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc269 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc270 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc271 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc272 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc273 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc274 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|QuickGELUActivation[vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc275 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|QuickGELUActivation[vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc276 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc277 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc278 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc279 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc280 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc281 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc282 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc283 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc284 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc285 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc286 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc287 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc288 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc289 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc290 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc291 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc292 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc293 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc294 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc295 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc296 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc297 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc298 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc299 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc300 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc301 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc302 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc303 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc304 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc305 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc306 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc307 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc308 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc309 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc310 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc311 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc312 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc313 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc314 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc315 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc316 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc317 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc318 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc319 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc320 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc321 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc322 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc323 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc324 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc325 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc326 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc327 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|QuickGELUActivation[vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc328 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|QuickGELUActivation[vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc329 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc330 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc331 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc332 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc333 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc334 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc335 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc336 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc337 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc338 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc339 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc340 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc341 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc342 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc343 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc344 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc345 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc346 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc347 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc348 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc349 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc350 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc351 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc352 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc353 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc354 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc355 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc356 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc357 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc358 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc359 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc360 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc361 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc362 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc363 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc364 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc365 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc366 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc367 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc368 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc369 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc370 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc371 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc372 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc373 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc374 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc375 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc376 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc377 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc378 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc379 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc380 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|QuickGELUActivation[vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc381 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|QuickGELUActivation[vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc382 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc383 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc384 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc385 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc386 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc387 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc388 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc389 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc390 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc391 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc392 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc393 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc394 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc395 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc396 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc397 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc398 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc399 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc400 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc401 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc402 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc403 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc404 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc405 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc406 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc407 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc408 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc409 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc410 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc411 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc412 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc413 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc414 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc415 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc416 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc417 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc418 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc419 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc420 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc421 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc422 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc423 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc424 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc425 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc426 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc427 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc428 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc429 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc430 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc431 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc432 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc433 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|QuickGELUActivation[vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc434 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|QuickGELUActivation[vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc435 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc436 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc437 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc438 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc439 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc440 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc441 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc442 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc443 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc444 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc445 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc446 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc447 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc448 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc449 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc450 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc451 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc452 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc453 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc454 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc455 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc456 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc457 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc458 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc459 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc460 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc461 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc462 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc463 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc464 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc465 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc466 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc467 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc468 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc469 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc470 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc471 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc472 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc473 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc474 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc475 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc476 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc477 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc478 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc479 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc480 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc481 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc482 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc483 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc484 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc485 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc486 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|QuickGELUActivation[vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc487 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|QuickGELUActivation[vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc488 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc489 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc490 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc491 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc492 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc493 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc494 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc495 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc496 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc497 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc498 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc499 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc500 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc501 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc502 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc503 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc504 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc505 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc506 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc507 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc508 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc509 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc510 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc511 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc512 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc513 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc514 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc515 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc516 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc517 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc518 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc519 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc520 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc521 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc522 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc523 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc524 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc525 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc526 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc527 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc528 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc529 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc530 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc531 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc532 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc533 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc534 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc535 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc536 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc537 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc538 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc539 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|QuickGELUActivation[vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc540 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|QuickGELUActivation[vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc541 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc542 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc543 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc544 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc545 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc546 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc547 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc548 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc549 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc550 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc551 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc552 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc553 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc554 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc555 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc556 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc557 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc558 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc559 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc560 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc561 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc562 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc563 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc564 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc565 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc566 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc567 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc568 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc569 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc570 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc571 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc572 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc573 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc574 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc575 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc576 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc577 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc578 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc579 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc580 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc581 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc582 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc583 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc584 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc585 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc586 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc587 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc588 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc589 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc590 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc591 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc592 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|QuickGELUActivation[vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc593 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|QuickGELUActivation[vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc594 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc595 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc596 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc597 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc598 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc599 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc600 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc601 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc602 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc603 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc604 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc605 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc606 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc607 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc608 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc609 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc610 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc611 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc612 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc613 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc614 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc615 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc616 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc617 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc618 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc619 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc620 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc621 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc622 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc623 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc624 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc625 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc626 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc627 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc628 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc629 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc630 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc631 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc632 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc633 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc634 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc635 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc636 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc637 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc638 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc639 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc640 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc641 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc642 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc643 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc644 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc645 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|QuickGELUActivation[vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc646 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|QuickGELUActivation[vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc647 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc648 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc649 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc650 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc651 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc652 = loc("CLIPVisionTransformer[vision_model]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|792|xla__generic_slice")
#loc653 = loc("CLIPVisionTransformer[vision_model]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|792|aten__view")
#loc654 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__var_mean")
#loc655 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__sub")
#loc656 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__add")
#loc657 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__rsqrt")
#loc658 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__mul")
#loc659 = loc("Linear[visual_projection]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:1169|forward|1203|aten__permute")
#loc660 = loc("Linear[visual_projection]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:1169|forward|1203|aten__mm")
