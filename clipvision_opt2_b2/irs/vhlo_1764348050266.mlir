#loc1 = loc("unknown|unknown|-1|unknownxla__device_data")
#loc12 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__var_mean")
#loc19 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc42 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc60 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc78 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc101 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc119 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc137 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc160 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc178 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc196 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc219 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc237 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc255 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc278 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc296 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc314 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc337 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc355 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc373 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc396 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc414 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc432 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc455 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc473 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc491 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc514 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc532 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc550 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc573 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc591 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc609 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc632 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc650 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc668 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc691 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc709 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc729 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__var_mean")
module @SyncTensorsGraph.4178 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<512x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg1: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg2: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg3: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg4: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg5: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg6: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg7: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg8: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg9: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg10: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg11: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg12: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg13: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg14: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg15: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg16: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg17: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg18: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg19: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg20: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg21: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg22: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg23: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg24: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg25: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg26: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg27: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg28: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg29: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg30: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg31: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg32: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg33: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg34: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg35: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg36: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg37: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg38: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg39: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg40: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg41: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg42: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg43: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg44: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg45: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg46: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg47: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg48: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg49: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg50: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg51: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg52: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg53: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg54: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg55: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg56: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg57: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg58: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg59: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg60: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg61: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg62: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg63: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg64: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg65: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg66: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg67: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg68: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg69: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg70: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg71: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg72: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg73: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg74: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg75: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg76: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg77: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg78: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg79: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg80: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg81: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg82: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg83: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg84: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg85: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg86: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg87: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg88: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg89: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg90: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg91: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg92: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg93: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg94: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg95: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg96: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg97: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg98: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg99: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg100: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg101: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg102: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg103: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg104: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg105: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg106: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg107: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg108: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg109: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg110: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg111: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg112: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg113: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg114: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg115: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg116: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg117: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg118: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg119: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg120: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg121: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg122: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg123: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg124: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg125: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg126: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg127: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg128: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg129: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg130: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg131: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg132: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg133: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg134: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg135: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg136: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg137: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg138: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg139: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg140: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg141: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg142: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg143: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg144: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg145: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg146: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg147: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg148: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg149: !vhlo.tensor_v1<1x50x!vhlo.i64_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg150: !vhlo.tensor_v1<50x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg151: !vhlo.tensor_v1<768x3x32x32x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg152: !vhlo.tensor_v1<2x3x224x224x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg153: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg154: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg155: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg156: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg157: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg158: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg159: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg160: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg161: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg162: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg163: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg164: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg165: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg166: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg167: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg168: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg169: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg170: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg171: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg172: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg173: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg174: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg175: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg176: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg177: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg178: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg179: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg180: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg181: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg182: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg183: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg184: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg185: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg186: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg187: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg188: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg189: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg190: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg191: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg192: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg193: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg194: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg195: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg196: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg197: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg198: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg199: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg200: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data"), %arg201: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("unknown|unknown|-1|unknownxla__device_data")) -> (!vhlo.tensor_v1<2x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.001360e-05> : tensor<2x1xbf16>>}> : () -> !vhlo.tensor_v1<2x1x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.304630e-03> : tensor<2xbf16>>}> : () -> !vhlo.tensor_v1<2x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.703130e+00> : tensor<2x50x3072xbf16>>}> : () -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.250000e-01> : tensor<2x12x50x50xbf16>>}> : () -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc)
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.001360e-05> : tensor<2x50x1xbf16>>}> : () -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc)
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.304630e-03> : tensor<2x50xbf16>>}> : () -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc)
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %9 = "vhlo.reshape_v1"(%arg153) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %10 = "vhlo.custom_call_v1"(%9) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_embeddings_class_embedding">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x!vhlo.bf16_v1> loc(#loc4)
    %12 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x1x768x!vhlo.bf16_v1> loc(#loc4)
    %13 = "vhlo.custom_call_v1"(%arg152) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<2x3x224x224x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x3x224x224x!vhlo.bf16_v1> loc(#loc3)
    %14 = "vhlo.custom_call_v1"(%arg151) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_embeddings_patch_embedding_weight">}>} : (!vhlo.tensor_v1<768x3x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3x32x32x!vhlo.bf16_v1> loc(#loc3)
    %15 = "vhlo.convolution_v1"(%13, %14) <{batch_group_count = #vhlo.integer_v1<1 : i64>, feature_group_count = #vhlo.integer_v1<1 : i64>, input_batch_dimension = #vhlo.integer_v1<0 : i64>, input_feature_dimension = #vhlo.integer_v1<1 : i64>, input_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, kernel_input_feature_dimension = #vhlo.integer_v1<1 : i64>, kernel_output_feature_dimension = #vhlo.integer_v1<0 : i64>, kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, lhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, output_batch_dimension = #vhlo.integer_v1<0 : i64>, output_feature_dimension = #vhlo.integer_v1<1 : i64>, output_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, padding = #vhlo.tensor_v1<dense<0> : tensor<2x2xi64>>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, window_reversal = #vhlo.tensor_v1<dense<false> : tensor<2xi1>>, window_strides = #vhlo.tensor_v1<dense<32> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x3x224x224x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x7x7x!vhlo.bf16_v1> loc(#loc5)
    %16 = "vhlo.reshape_v1"(%15) : (!vhlo.tensor_v1<2x768x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x49x!vhlo.bf16_v1> loc(#loc6)
    %17 = "vhlo.transpose_v1"(%16) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,49,768]{1,2,0}">} : (!vhlo.tensor_v1<2x768x49x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x49x768x!vhlo.bf16_v1> loc(#loc7)
    %18 = "vhlo.concatenate_v1"(%12, %17) <{dimension = #vhlo.integer_v1<1 : i64>}> : (!vhlo.tensor_v1<2x1x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x49x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc8)
    %19 = "vhlo.reshape_v1"(%arg150) : (!vhlo.tensor_v1<50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x50x768x!vhlo.bf16_v1> loc(#loc2)
    %20 = "vhlo.custom_call_v1"(%19) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_embeddings_position_embedding_weight">}>} : (!vhlo.tensor_v1<1x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x50x768x!vhlo.bf16_v1> loc(#loc3)
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<1x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<50x768x!vhlo.bf16_v1> loc(#loc2)
    %22 = "vhlo.reshape_v1"(%arg149) : (!vhlo.tensor_v1<1x50x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x50x!vhlo.i64_v1> loc(#loc2)
    %23 = "vhlo.custom_call_v1"(%22) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"constant">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_embeddings_position_ids">}>} : (!vhlo.tensor_v1<1x1x50x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x50x!vhlo.i64_v1> loc(#loc3)
    %24 = "vhlo.reshape_v1"(%23) : (!vhlo.tensor_v1<1x1x50x!vhlo.i64_v1>) -> !vhlo.tensor_v1<50x!vhlo.i64_v1> loc(#loc9)
    %25 = "vhlo.convert_v1"(%24) : (!vhlo.tensor_v1<50x!vhlo.i64_v1>) -> !vhlo.tensor_v1<50x!vhlo.ui32_v1> loc(#loc10)
    %26 = "vhlo.gather_v2"(%21, %25) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 768]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<50x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<50x768x!vhlo.bf16_v1> loc(#loc10)
    %27 = "vhlo.broadcast_in_dim_v1"(%26) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[1, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc11)
    %28 = "vhlo.add_v1"(%18, %27) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc11)
    %29 = "vhlo.reduce_v1"(%28, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc13)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc12)
    %30 = "vhlo.multiply_v1"(%29, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc12)
    %31 = "vhlo.broadcast_in_dim_v1"(%30) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc14)
    %32 = "vhlo.subtract_v1"(%28, %31) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc14)
    %33 = "vhlo.multiply_v1"(%32, %32) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc12)
    %34 = "vhlo.reduce_v1"(%33, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc15)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc12)
    %35 = "vhlo.multiply_v1"(%34, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc12)
    %36 = "vhlo.reshape_v1"(%35) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc12)
    %37 = "vhlo.add_v1"(%36, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc16)
    %38 = "vhlo.rsqrt_v2"(%37) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc17)
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc18)
    %40 = "vhlo.broadcast_in_dim_v1"(%39) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc18)
    %41 = "vhlo.multiply_v1"(%32, %40) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc18)
    %42 = "vhlo.reshape_v1"(%arg148) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %43 = "vhlo.custom_call_v1"(%42) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_pre_layrnorm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %44 = "vhlo.reshape_v1"(%43) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %45 = "vhlo.broadcast_in_dim_v1"(%44) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc18)
    %46 = "vhlo.multiply_v1"(%41, %45) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc18)
    %47 = "vhlo.reshape_v1"(%arg147) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %48 = "vhlo.custom_call_v1"(%47) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_pre_layrnorm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %49 = "vhlo.reshape_v1"(%48) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %50 = "vhlo.broadcast_in_dim_v1"(%49) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc16)
    %51 = "vhlo.add_v1"(%46, %50) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc16)
    %52 = "vhlo.reduce_v1"(%51, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc20)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc19)
    %53 = "vhlo.multiply_v1"(%52, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc19)
    %54 = "vhlo.broadcast_in_dim_v1"(%53) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc21)
    %55 = "vhlo.subtract_v1"(%51, %54) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc21)
    %56 = "vhlo.multiply_v1"(%55, %55) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc19)
    %57 = "vhlo.reduce_v1"(%56, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc22)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc19)
    %58 = "vhlo.multiply_v1"(%57, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc19)
    %59 = "vhlo.reshape_v1"(%58) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc19)
    %60 = "vhlo.add_v1"(%59, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc23)
    %61 = "vhlo.rsqrt_v2"(%60) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc24)
    %62 = "vhlo.reshape_v1"(%61) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc25)
    %63 = "vhlo.broadcast_in_dim_v1"(%62) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc25)
    %64 = "vhlo.multiply_v1"(%55, %63) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc25)
    %65 = "vhlo.reshape_v1"(%arg146) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %66 = "vhlo.custom_call_v1"(%65) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %67 = "vhlo.reshape_v1"(%66) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %68 = "vhlo.broadcast_in_dim_v1"(%67) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc25)
    %69 = "vhlo.multiply_v1"(%64, %68) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc25)
    %70 = "vhlo.reshape_v1"(%arg145) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %71 = "vhlo.custom_call_v1"(%70) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %72 = "vhlo.reshape_v1"(%71) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %73 = "vhlo.broadcast_in_dim_v1"(%72) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc23)
    %74 = "vhlo.add_v1"(%69, %73) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc23)
    %75 = "vhlo.reshape_v1"(%74) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc26)
    %76 = "vhlo.reshape_v1"(%arg157) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %77 = "vhlo.custom_call_v1"(%76) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %78 = "vhlo.reshape_v1"(%77) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %79 = "vhlo.transpose_v1"(%78) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc27)
    %80 = "vhlo.dot_general_v2"(%75, %79) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc28)
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc26)
    %82 = "vhlo.reshape_v1"(%arg156) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %83 = "vhlo.custom_call_v1"(%82) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %84 = "vhlo.reshape_v1"(%83) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %85 = "vhlo.broadcast_in_dim_v1"(%84) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc29)
    %86 = "vhlo.add_v1"(%81, %85) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc29)
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc30)
    %88 = "vhlo.transpose_v1"(%87) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc31)
    %89 = "vhlo.reshape_v1"(%88) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc32)
    %90 = "vhlo.reshape_v1"(%arg155) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %91 = "vhlo.custom_call_v1"(%90) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %92 = "vhlo.reshape_v1"(%91) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %93 = "vhlo.transpose_v1"(%92) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc33)
    %94 = "vhlo.dot_general_v2"(%75, %93) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc34)
    %95 = "vhlo.reshape_v1"(%94) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc35)
    %96 = "vhlo.reshape_v1"(%arg154) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %97 = "vhlo.custom_call_v1"(%96) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %98 = "vhlo.reshape_v1"(%97) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %99 = "vhlo.broadcast_in_dim_v1"(%98) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc36)
    %100 = "vhlo.add_v1"(%95, %99) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc36)
    %101 = "vhlo.reshape_v1"(%100) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc37)
    %102 = "vhlo.transpose_v1"(%101) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc38)
    %103 = "vhlo.reshape_v1"(%102) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc32)
    %104 = "vhlo.dot_general_v2"(%89, %103) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc39)
    %105 = "vhlo.reshape_v1"(%104) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc32)
    %106 = "vhlo.multiply_v1"(%105, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc40)
    %107 = "vhlo.convert_v1"(%106) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc41)
    %108 = "vhlo.reduce_v1"(%107, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc43)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc42)
    %109 = "vhlo.broadcast_in_dim_v1"(%108) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc42)
    %110 = "vhlo.subtract_v1"(%107, %109) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc42)
    %111 = "vhlo.exponential_v2"(%110) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc42)
    %112 = "vhlo.reduce_v1"(%111, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc44)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc42)
    %113 = "vhlo.broadcast_in_dim_v1"(%112) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc42)
    %114 = "vhlo.divide_v1"(%111, %113) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc42)
    %115 = "vhlo.convert_v1"(%114) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc45)
    %116 = "vhlo.reshape_v1"(%115) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc46)
    %117 = "vhlo.reshape_v1"(%arg144) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %118 = "vhlo.custom_call_v1"(%117) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %119 = "vhlo.reshape_v1"(%118) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %120 = "vhlo.transpose_v1"(%119) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc47)
    %121 = "vhlo.dot_general_v2"(%75, %120) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc48)
    %122 = "vhlo.reshape_v1"(%121) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc49)
    %123 = "vhlo.reshape_v1"(%arg143) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %124 = "vhlo.custom_call_v1"(%123) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %126 = "vhlo.broadcast_in_dim_v1"(%125) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc50)
    %127 = "vhlo.add_v1"(%122, %126) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc50)
    %128 = "vhlo.reshape_v1"(%127) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc51)
    %129 = "vhlo.transpose_v1"(%128) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc52)
    %130 = "vhlo.reshape_v1"(%129) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc46)
    %131 = "vhlo.dot_general_v2"(%116, %130) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc53)
    %132 = "vhlo.reshape_v1"(%131) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc46)
    %133 = "vhlo.transpose_v1"(%132) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc54)
    %134 = "vhlo.reshape_v1"(%133) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc55)
    %135 = "vhlo.reshape_v1"(%arg142) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %136 = "vhlo.custom_call_v1"(%135) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %137 = "vhlo.reshape_v1"(%136) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %138 = "vhlo.transpose_v1"(%137) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc56)
    %139 = "vhlo.dot_general_v2"(%134, %138) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc57)
    %140 = "vhlo.reshape_v1"(%139) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc55)
    %141 = "vhlo.reshape_v1"(%arg141) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %142 = "vhlo.custom_call_v1"(%141) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %143 = "vhlo.reshape_v1"(%142) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %144 = "vhlo.broadcast_in_dim_v1"(%143) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc58)
    %145 = "vhlo.add_v1"(%140, %144) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc58)
    %146 = "vhlo.add_v1"(%51, %145) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc59)
    %147 = "vhlo.reduce_v1"(%146, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc61)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc60)
    %148 = "vhlo.multiply_v1"(%147, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc60)
    %149 = "vhlo.broadcast_in_dim_v1"(%148) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc62)
    %150 = "vhlo.subtract_v1"(%146, %149) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc62)
    %151 = "vhlo.multiply_v1"(%150, %150) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc60)
    %152 = "vhlo.reduce_v1"(%151, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc63)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc60)
    %153 = "vhlo.multiply_v1"(%152, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc60)
    %154 = "vhlo.reshape_v1"(%153) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc60)
    %155 = "vhlo.add_v1"(%154, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc64)
    %156 = "vhlo.rsqrt_v2"(%155) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc65)
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc66)
    %158 = "vhlo.broadcast_in_dim_v1"(%157) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc66)
    %159 = "vhlo.multiply_v1"(%150, %158) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc66)
    %160 = "vhlo.reshape_v1"(%arg140) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %161 = "vhlo.custom_call_v1"(%160) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %163 = "vhlo.broadcast_in_dim_v1"(%162) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc66)
    %164 = "vhlo.multiply_v1"(%159, %163) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc66)
    %165 = "vhlo.reshape_v1"(%arg139) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %166 = "vhlo.custom_call_v1"(%165) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %167 = "vhlo.reshape_v1"(%166) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %168 = "vhlo.broadcast_in_dim_v1"(%167) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc64)
    %169 = "vhlo.add_v1"(%164, %168) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc64)
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc67)
    %171 = "vhlo.reshape_v1"(%arg138) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %172 = "vhlo.custom_call_v1"(%171) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %173 = "vhlo.reshape_v1"(%172) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %174 = "vhlo.transpose_v1"(%173) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc68)
    %175 = "vhlo.dot_general_v2"(%170, %174) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc69)
    %176 = "vhlo.reshape_v1"(%175) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc67)
    %177 = "vhlo.reshape_v1"(%arg137) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %178 = "vhlo.custom_call_v1"(%177) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %179 = "vhlo.reshape_v1"(%178) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %180 = "vhlo.broadcast_in_dim_v1"(%179) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc70)
    %181 = "vhlo.add_v1"(%176, %180) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc70)
    %182 = "vhlo.multiply_v1"(%181, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc71)
    %183 = "vhlo.logistic_v2"(%182) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc72)
    %184 = "vhlo.multiply_v1"(%181, %183) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc71)
    %185 = "vhlo.reshape_v1"(%184) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc73)
    %186 = "vhlo.reshape_v1"(%arg136) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %187 = "vhlo.custom_call_v1"(%186) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %188 = "vhlo.reshape_v1"(%187) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %189 = "vhlo.transpose_v1"(%188) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc74)
    %190 = "vhlo.dot_general_v2"(%185, %189) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc75)
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc73)
    %192 = "vhlo.reshape_v1"(%arg135) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %193 = "vhlo.custom_call_v1"(%192) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_0_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %194 = "vhlo.reshape_v1"(%193) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %195 = "vhlo.broadcast_in_dim_v1"(%194) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc76)
    %196 = "vhlo.add_v1"(%191, %195) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc76)
    %197 = "vhlo.add_v1"(%146, %196) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc77)
    %198 = "vhlo.reduce_v1"(%197, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc79)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc78)
    %199 = "vhlo.multiply_v1"(%198, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc78)
    %200 = "vhlo.broadcast_in_dim_v1"(%199) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc80)
    %201 = "vhlo.subtract_v1"(%197, %200) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc80)
    %202 = "vhlo.multiply_v1"(%201, %201) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc78)
    %203 = "vhlo.reduce_v1"(%202, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc81)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc78)
    %204 = "vhlo.multiply_v1"(%203, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc78)
    %205 = "vhlo.reshape_v1"(%204) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc78)
    %206 = "vhlo.add_v1"(%205, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc82)
    %207 = "vhlo.rsqrt_v2"(%206) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc83)
    %208 = "vhlo.reshape_v1"(%207) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc84)
    %209 = "vhlo.broadcast_in_dim_v1"(%208) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc84)
    %210 = "vhlo.multiply_v1"(%201, %209) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc84)
    %211 = "vhlo.reshape_v1"(%arg134) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %212 = "vhlo.custom_call_v1"(%211) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %213 = "vhlo.reshape_v1"(%212) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %214 = "vhlo.broadcast_in_dim_v1"(%213) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc84)
    %215 = "vhlo.multiply_v1"(%210, %214) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc84)
    %216 = "vhlo.reshape_v1"(%arg133) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %217 = "vhlo.custom_call_v1"(%216) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %218 = "vhlo.reshape_v1"(%217) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %219 = "vhlo.broadcast_in_dim_v1"(%218) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc82)
    %220 = "vhlo.add_v1"(%215, %219) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc82)
    %221 = "vhlo.reshape_v1"(%220) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc85)
    %222 = "vhlo.reshape_v1"(%arg161) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %223 = "vhlo.custom_call_v1"(%222) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %225 = "vhlo.transpose_v1"(%224) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc86)
    %226 = "vhlo.dot_general_v2"(%221, %225) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc87)
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc85)
    %228 = "vhlo.reshape_v1"(%arg160) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %229 = "vhlo.custom_call_v1"(%228) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %230 = "vhlo.reshape_v1"(%229) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %231 = "vhlo.broadcast_in_dim_v1"(%230) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc88)
    %232 = "vhlo.add_v1"(%227, %231) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc88)
    %233 = "vhlo.reshape_v1"(%232) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc89)
    %234 = "vhlo.transpose_v1"(%233) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc90)
    %235 = "vhlo.reshape_v1"(%234) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc91)
    %236 = "vhlo.reshape_v1"(%arg159) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %237 = "vhlo.custom_call_v1"(%236) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %238 = "vhlo.reshape_v1"(%237) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %239 = "vhlo.transpose_v1"(%238) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc92)
    %240 = "vhlo.dot_general_v2"(%221, %239) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc93)
    %241 = "vhlo.reshape_v1"(%240) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc94)
    %242 = "vhlo.reshape_v1"(%arg158) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %243 = "vhlo.custom_call_v1"(%242) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %244 = "vhlo.reshape_v1"(%243) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %245 = "vhlo.broadcast_in_dim_v1"(%244) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc95)
    %246 = "vhlo.add_v1"(%241, %245) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc95)
    %247 = "vhlo.reshape_v1"(%246) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc96)
    %248 = "vhlo.transpose_v1"(%247) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc97)
    %249 = "vhlo.reshape_v1"(%248) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc91)
    %250 = "vhlo.dot_general_v2"(%235, %249) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc98)
    %251 = "vhlo.reshape_v1"(%250) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc91)
    %252 = "vhlo.multiply_v1"(%251, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc99)
    %253 = "vhlo.convert_v1"(%252) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc100)
    %254 = "vhlo.reduce_v1"(%253, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc102)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc101)
    %255 = "vhlo.broadcast_in_dim_v1"(%254) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc101)
    %256 = "vhlo.subtract_v1"(%253, %255) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc101)
    %257 = "vhlo.exponential_v2"(%256) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc101)
    %258 = "vhlo.reduce_v1"(%257, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc103)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc101)
    %259 = "vhlo.broadcast_in_dim_v1"(%258) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc101)
    %260 = "vhlo.divide_v1"(%257, %259) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc101)
    %261 = "vhlo.convert_v1"(%260) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc104)
    %262 = "vhlo.reshape_v1"(%261) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc105)
    %263 = "vhlo.reshape_v1"(%arg132) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %264 = "vhlo.custom_call_v1"(%263) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %265 = "vhlo.reshape_v1"(%264) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %266 = "vhlo.transpose_v1"(%265) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc106)
    %267 = "vhlo.dot_general_v2"(%221, %266) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc107)
    %268 = "vhlo.reshape_v1"(%267) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc108)
    %269 = "vhlo.reshape_v1"(%arg131) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %270 = "vhlo.custom_call_v1"(%269) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %271 = "vhlo.reshape_v1"(%270) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %272 = "vhlo.broadcast_in_dim_v1"(%271) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc109)
    %273 = "vhlo.add_v1"(%268, %272) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc109)
    %274 = "vhlo.reshape_v1"(%273) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc110)
    %275 = "vhlo.transpose_v1"(%274) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc111)
    %276 = "vhlo.reshape_v1"(%275) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc105)
    %277 = "vhlo.dot_general_v2"(%262, %276) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc112)
    %278 = "vhlo.reshape_v1"(%277) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc105)
    %279 = "vhlo.transpose_v1"(%278) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc113)
    %280 = "vhlo.reshape_v1"(%279) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc114)
    %281 = "vhlo.reshape_v1"(%arg130) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %282 = "vhlo.custom_call_v1"(%281) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %283 = "vhlo.reshape_v1"(%282) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %284 = "vhlo.transpose_v1"(%283) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc115)
    %285 = "vhlo.dot_general_v2"(%280, %284) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc116)
    %286 = "vhlo.reshape_v1"(%285) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc114)
    %287 = "vhlo.reshape_v1"(%arg129) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %288 = "vhlo.custom_call_v1"(%287) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %289 = "vhlo.reshape_v1"(%288) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %290 = "vhlo.broadcast_in_dim_v1"(%289) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc117)
    %291 = "vhlo.add_v1"(%286, %290) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc117)
    %292 = "vhlo.add_v1"(%197, %291) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc118)
    %293 = "vhlo.reduce_v1"(%292, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc120)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc119)
    %294 = "vhlo.multiply_v1"(%293, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc119)
    %295 = "vhlo.broadcast_in_dim_v1"(%294) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc121)
    %296 = "vhlo.subtract_v1"(%292, %295) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc121)
    %297 = "vhlo.multiply_v1"(%296, %296) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc119)
    %298 = "vhlo.reduce_v1"(%297, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc122)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc119)
    %299 = "vhlo.multiply_v1"(%298, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc119)
    %300 = "vhlo.reshape_v1"(%299) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc119)
    %301 = "vhlo.add_v1"(%300, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc123)
    %302 = "vhlo.rsqrt_v2"(%301) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc124)
    %303 = "vhlo.reshape_v1"(%302) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc125)
    %304 = "vhlo.broadcast_in_dim_v1"(%303) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc125)
    %305 = "vhlo.multiply_v1"(%296, %304) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc125)
    %306 = "vhlo.reshape_v1"(%arg128) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %307 = "vhlo.custom_call_v1"(%306) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %308 = "vhlo.reshape_v1"(%307) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %309 = "vhlo.broadcast_in_dim_v1"(%308) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc125)
    %310 = "vhlo.multiply_v1"(%305, %309) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc125)
    %311 = "vhlo.reshape_v1"(%arg127) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %312 = "vhlo.custom_call_v1"(%311) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %313 = "vhlo.reshape_v1"(%312) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %314 = "vhlo.broadcast_in_dim_v1"(%313) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc123)
    %315 = "vhlo.add_v1"(%310, %314) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc123)
    %316 = "vhlo.reshape_v1"(%315) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc126)
    %317 = "vhlo.reshape_v1"(%arg126) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %318 = "vhlo.custom_call_v1"(%317) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %319 = "vhlo.reshape_v1"(%318) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %320 = "vhlo.transpose_v1"(%319) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc127)
    %321 = "vhlo.dot_general_v2"(%316, %320) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc128)
    %322 = "vhlo.reshape_v1"(%321) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc126)
    %323 = "vhlo.reshape_v1"(%arg125) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %324 = "vhlo.custom_call_v1"(%323) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %325 = "vhlo.reshape_v1"(%324) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %326 = "vhlo.broadcast_in_dim_v1"(%325) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc129)
    %327 = "vhlo.add_v1"(%322, %326) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc129)
    %328 = "vhlo.multiply_v1"(%327, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc130)
    %329 = "vhlo.logistic_v2"(%328) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc131)
    %330 = "vhlo.multiply_v1"(%327, %329) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc130)
    %331 = "vhlo.reshape_v1"(%330) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc132)
    %332 = "vhlo.reshape_v1"(%arg124) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %333 = "vhlo.custom_call_v1"(%332) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %334 = "vhlo.reshape_v1"(%333) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %335 = "vhlo.transpose_v1"(%334) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc133)
    %336 = "vhlo.dot_general_v2"(%331, %335) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc134)
    %337 = "vhlo.reshape_v1"(%336) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc132)
    %338 = "vhlo.reshape_v1"(%arg123) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %339 = "vhlo.custom_call_v1"(%338) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_1_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %340 = "vhlo.reshape_v1"(%339) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %341 = "vhlo.broadcast_in_dim_v1"(%340) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc135)
    %342 = "vhlo.add_v1"(%337, %341) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc135)
    %343 = "vhlo.add_v1"(%292, %342) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc136)
    %344 = "vhlo.reduce_v1"(%343, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc138)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc137)
    %345 = "vhlo.multiply_v1"(%344, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc137)
    %346 = "vhlo.broadcast_in_dim_v1"(%345) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc139)
    %347 = "vhlo.subtract_v1"(%343, %346) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc139)
    %348 = "vhlo.multiply_v1"(%347, %347) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc137)
    %349 = "vhlo.reduce_v1"(%348, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc140)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc137)
    %350 = "vhlo.multiply_v1"(%349, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc137)
    %351 = "vhlo.reshape_v1"(%350) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc137)
    %352 = "vhlo.add_v1"(%351, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc141)
    %353 = "vhlo.rsqrt_v2"(%352) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc142)
    %354 = "vhlo.reshape_v1"(%353) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc143)
    %355 = "vhlo.broadcast_in_dim_v1"(%354) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc143)
    %356 = "vhlo.multiply_v1"(%347, %355) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc143)
    %357 = "vhlo.reshape_v1"(%arg122) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %358 = "vhlo.custom_call_v1"(%357) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %359 = "vhlo.reshape_v1"(%358) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %360 = "vhlo.broadcast_in_dim_v1"(%359) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc143)
    %361 = "vhlo.multiply_v1"(%356, %360) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc143)
    %362 = "vhlo.reshape_v1"(%arg121) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %363 = "vhlo.custom_call_v1"(%362) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %364 = "vhlo.reshape_v1"(%363) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %365 = "vhlo.broadcast_in_dim_v1"(%364) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc141)
    %366 = "vhlo.add_v1"(%361, %365) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc141)
    %367 = "vhlo.reshape_v1"(%366) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc144)
    %368 = "vhlo.reshape_v1"(%arg165) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %369 = "vhlo.custom_call_v1"(%368) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %370 = "vhlo.reshape_v1"(%369) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %371 = "vhlo.transpose_v1"(%370) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc145)
    %372 = "vhlo.dot_general_v2"(%367, %371) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc146)
    %373 = "vhlo.reshape_v1"(%372) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc144)
    %374 = "vhlo.reshape_v1"(%arg164) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %375 = "vhlo.custom_call_v1"(%374) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %376 = "vhlo.reshape_v1"(%375) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %377 = "vhlo.broadcast_in_dim_v1"(%376) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc147)
    %378 = "vhlo.add_v1"(%373, %377) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc147)
    %379 = "vhlo.reshape_v1"(%378) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc148)
    %380 = "vhlo.transpose_v1"(%379) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc149)
    %381 = "vhlo.reshape_v1"(%380) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc150)
    %382 = "vhlo.reshape_v1"(%arg163) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %383 = "vhlo.custom_call_v1"(%382) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %384 = "vhlo.reshape_v1"(%383) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %385 = "vhlo.transpose_v1"(%384) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc151)
    %386 = "vhlo.dot_general_v2"(%367, %385) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc152)
    %387 = "vhlo.reshape_v1"(%386) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc153)
    %388 = "vhlo.reshape_v1"(%arg162) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %389 = "vhlo.custom_call_v1"(%388) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %390 = "vhlo.reshape_v1"(%389) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %391 = "vhlo.broadcast_in_dim_v1"(%390) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc154)
    %392 = "vhlo.add_v1"(%387, %391) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc154)
    %393 = "vhlo.reshape_v1"(%392) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc155)
    %394 = "vhlo.transpose_v1"(%393) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc156)
    %395 = "vhlo.reshape_v1"(%394) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc150)
    %396 = "vhlo.dot_general_v2"(%381, %395) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc157)
    %397 = "vhlo.reshape_v1"(%396) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc150)
    %398 = "vhlo.multiply_v1"(%397, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc158)
    %399 = "vhlo.convert_v1"(%398) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc159)
    %400 = "vhlo.reduce_v1"(%399, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc161)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc160)
    %401 = "vhlo.broadcast_in_dim_v1"(%400) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc160)
    %402 = "vhlo.subtract_v1"(%399, %401) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc160)
    %403 = "vhlo.exponential_v2"(%402) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc160)
    %404 = "vhlo.reduce_v1"(%403, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc162)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc160)
    %405 = "vhlo.broadcast_in_dim_v1"(%404) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc160)
    %406 = "vhlo.divide_v1"(%403, %405) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc160)
    %407 = "vhlo.convert_v1"(%406) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc163)
    %408 = "vhlo.reshape_v1"(%407) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc164)
    %409 = "vhlo.reshape_v1"(%arg120) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %410 = "vhlo.custom_call_v1"(%409) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %411 = "vhlo.reshape_v1"(%410) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %412 = "vhlo.transpose_v1"(%411) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc165)
    %413 = "vhlo.dot_general_v2"(%367, %412) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc166)
    %414 = "vhlo.reshape_v1"(%413) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc167)
    %415 = "vhlo.reshape_v1"(%arg119) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %416 = "vhlo.custom_call_v1"(%415) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %417 = "vhlo.reshape_v1"(%416) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %418 = "vhlo.broadcast_in_dim_v1"(%417) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc168)
    %419 = "vhlo.add_v1"(%414, %418) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc168)
    %420 = "vhlo.reshape_v1"(%419) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc169)
    %421 = "vhlo.transpose_v1"(%420) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc170)
    %422 = "vhlo.reshape_v1"(%421) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc164)
    %423 = "vhlo.dot_general_v2"(%408, %422) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc171)
    %424 = "vhlo.reshape_v1"(%423) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc164)
    %425 = "vhlo.transpose_v1"(%424) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc172)
    %426 = "vhlo.reshape_v1"(%425) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc173)
    %427 = "vhlo.reshape_v1"(%arg118) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %428 = "vhlo.custom_call_v1"(%427) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %429 = "vhlo.reshape_v1"(%428) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %430 = "vhlo.transpose_v1"(%429) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc174)
    %431 = "vhlo.dot_general_v2"(%426, %430) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc175)
    %432 = "vhlo.reshape_v1"(%431) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc173)
    %433 = "vhlo.reshape_v1"(%arg117) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %434 = "vhlo.custom_call_v1"(%433) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %435 = "vhlo.reshape_v1"(%434) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %436 = "vhlo.broadcast_in_dim_v1"(%435) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc176)
    %437 = "vhlo.add_v1"(%432, %436) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc176)
    %438 = "vhlo.add_v1"(%343, %437) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc177)
    %439 = "vhlo.reduce_v1"(%438, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc179)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc178)
    %440 = "vhlo.multiply_v1"(%439, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc178)
    %441 = "vhlo.broadcast_in_dim_v1"(%440) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc180)
    %442 = "vhlo.subtract_v1"(%438, %441) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc180)
    %443 = "vhlo.multiply_v1"(%442, %442) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc178)
    %444 = "vhlo.reduce_v1"(%443, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc181)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc178)
    %445 = "vhlo.multiply_v1"(%444, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc178)
    %446 = "vhlo.reshape_v1"(%445) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc178)
    %447 = "vhlo.add_v1"(%446, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc182)
    %448 = "vhlo.rsqrt_v2"(%447) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc183)
    %449 = "vhlo.reshape_v1"(%448) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc184)
    %450 = "vhlo.broadcast_in_dim_v1"(%449) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc184)
    %451 = "vhlo.multiply_v1"(%442, %450) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc184)
    %452 = "vhlo.reshape_v1"(%arg116) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %453 = "vhlo.custom_call_v1"(%452) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %454 = "vhlo.reshape_v1"(%453) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %455 = "vhlo.broadcast_in_dim_v1"(%454) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc184)
    %456 = "vhlo.multiply_v1"(%451, %455) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc184)
    %457 = "vhlo.reshape_v1"(%arg115) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %458 = "vhlo.custom_call_v1"(%457) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %459 = "vhlo.reshape_v1"(%458) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %460 = "vhlo.broadcast_in_dim_v1"(%459) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc182)
    %461 = "vhlo.add_v1"(%456, %460) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc182)
    %462 = "vhlo.reshape_v1"(%461) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc185)
    %463 = "vhlo.reshape_v1"(%arg114) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %464 = "vhlo.custom_call_v1"(%463) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %465 = "vhlo.reshape_v1"(%464) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %466 = "vhlo.transpose_v1"(%465) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc186)
    %467 = "vhlo.dot_general_v2"(%462, %466) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc187)
    %468 = "vhlo.reshape_v1"(%467) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc185)
    %469 = "vhlo.reshape_v1"(%arg113) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %470 = "vhlo.custom_call_v1"(%469) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %471 = "vhlo.reshape_v1"(%470) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %472 = "vhlo.broadcast_in_dim_v1"(%471) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc188)
    %473 = "vhlo.add_v1"(%468, %472) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc188)
    %474 = "vhlo.multiply_v1"(%473, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc189)
    %475 = "vhlo.logistic_v2"(%474) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc190)
    %476 = "vhlo.multiply_v1"(%473, %475) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc189)
    %477 = "vhlo.reshape_v1"(%476) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc191)
    %478 = "vhlo.reshape_v1"(%arg112) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %479 = "vhlo.custom_call_v1"(%478) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %480 = "vhlo.reshape_v1"(%479) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %481 = "vhlo.transpose_v1"(%480) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc192)
    %482 = "vhlo.dot_general_v2"(%477, %481) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc193)
    %483 = "vhlo.reshape_v1"(%482) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc191)
    %484 = "vhlo.reshape_v1"(%arg111) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %485 = "vhlo.custom_call_v1"(%484) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_2_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %486 = "vhlo.reshape_v1"(%485) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %487 = "vhlo.broadcast_in_dim_v1"(%486) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc194)
    %488 = "vhlo.add_v1"(%483, %487) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc194)
    %489 = "vhlo.add_v1"(%438, %488) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc195)
    %490 = "vhlo.reduce_v1"(%489, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc197)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc196)
    %491 = "vhlo.multiply_v1"(%490, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc196)
    %492 = "vhlo.broadcast_in_dim_v1"(%491) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc198)
    %493 = "vhlo.subtract_v1"(%489, %492) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc198)
    %494 = "vhlo.multiply_v1"(%493, %493) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc196)
    %495 = "vhlo.reduce_v1"(%494, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc199)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc196)
    %496 = "vhlo.multiply_v1"(%495, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc196)
    %497 = "vhlo.reshape_v1"(%496) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc196)
    %498 = "vhlo.add_v1"(%497, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc200)
    %499 = "vhlo.rsqrt_v2"(%498) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc201)
    %500 = "vhlo.reshape_v1"(%499) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc202)
    %501 = "vhlo.broadcast_in_dim_v1"(%500) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc202)
    %502 = "vhlo.multiply_v1"(%493, %501) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc202)
    %503 = "vhlo.reshape_v1"(%arg110) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %504 = "vhlo.custom_call_v1"(%503) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %505 = "vhlo.reshape_v1"(%504) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %506 = "vhlo.broadcast_in_dim_v1"(%505) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc202)
    %507 = "vhlo.multiply_v1"(%502, %506) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc202)
    %508 = "vhlo.reshape_v1"(%arg109) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %509 = "vhlo.custom_call_v1"(%508) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %510 = "vhlo.reshape_v1"(%509) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %511 = "vhlo.broadcast_in_dim_v1"(%510) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc200)
    %512 = "vhlo.add_v1"(%507, %511) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc200)
    %513 = "vhlo.reshape_v1"(%512) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc203)
    %514 = "vhlo.reshape_v1"(%arg169) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %515 = "vhlo.custom_call_v1"(%514) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %516 = "vhlo.reshape_v1"(%515) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %517 = "vhlo.transpose_v1"(%516) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc204)
    %518 = "vhlo.dot_general_v2"(%513, %517) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc205)
    %519 = "vhlo.reshape_v1"(%518) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc203)
    %520 = "vhlo.reshape_v1"(%arg168) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %521 = "vhlo.custom_call_v1"(%520) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %522 = "vhlo.reshape_v1"(%521) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %523 = "vhlo.broadcast_in_dim_v1"(%522) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc206)
    %524 = "vhlo.add_v1"(%519, %523) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc206)
    %525 = "vhlo.reshape_v1"(%524) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc207)
    %526 = "vhlo.transpose_v1"(%525) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc208)
    %527 = "vhlo.reshape_v1"(%526) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc209)
    %528 = "vhlo.reshape_v1"(%arg167) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %529 = "vhlo.custom_call_v1"(%528) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %530 = "vhlo.reshape_v1"(%529) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %531 = "vhlo.transpose_v1"(%530) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc210)
    %532 = "vhlo.dot_general_v2"(%513, %531) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc211)
    %533 = "vhlo.reshape_v1"(%532) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc212)
    %534 = "vhlo.reshape_v1"(%arg166) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %535 = "vhlo.custom_call_v1"(%534) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %536 = "vhlo.reshape_v1"(%535) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %537 = "vhlo.broadcast_in_dim_v1"(%536) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc213)
    %538 = "vhlo.add_v1"(%533, %537) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc213)
    %539 = "vhlo.reshape_v1"(%538) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc214)
    %540 = "vhlo.transpose_v1"(%539) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc215)
    %541 = "vhlo.reshape_v1"(%540) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc209)
    %542 = "vhlo.dot_general_v2"(%527, %541) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc216)
    %543 = "vhlo.reshape_v1"(%542) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc209)
    %544 = "vhlo.multiply_v1"(%543, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc217)
    %545 = "vhlo.convert_v1"(%544) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc218)
    %546 = "vhlo.reduce_v1"(%545, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc220)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc219)
    %547 = "vhlo.broadcast_in_dim_v1"(%546) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc219)
    %548 = "vhlo.subtract_v1"(%545, %547) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc219)
    %549 = "vhlo.exponential_v2"(%548) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc219)
    %550 = "vhlo.reduce_v1"(%549, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc221)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc219)
    %551 = "vhlo.broadcast_in_dim_v1"(%550) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc219)
    %552 = "vhlo.divide_v1"(%549, %551) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc219)
    %553 = "vhlo.convert_v1"(%552) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc222)
    %554 = "vhlo.reshape_v1"(%553) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc223)
    %555 = "vhlo.reshape_v1"(%arg108) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %556 = "vhlo.custom_call_v1"(%555) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %557 = "vhlo.reshape_v1"(%556) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %558 = "vhlo.transpose_v1"(%557) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc224)
    %559 = "vhlo.dot_general_v2"(%513, %558) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc225)
    %560 = "vhlo.reshape_v1"(%559) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc226)
    %561 = "vhlo.reshape_v1"(%arg107) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %562 = "vhlo.custom_call_v1"(%561) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %563 = "vhlo.reshape_v1"(%562) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %564 = "vhlo.broadcast_in_dim_v1"(%563) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc227)
    %565 = "vhlo.add_v1"(%560, %564) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc227)
    %566 = "vhlo.reshape_v1"(%565) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc228)
    %567 = "vhlo.transpose_v1"(%566) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc229)
    %568 = "vhlo.reshape_v1"(%567) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc223)
    %569 = "vhlo.dot_general_v2"(%554, %568) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc230)
    %570 = "vhlo.reshape_v1"(%569) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc223)
    %571 = "vhlo.transpose_v1"(%570) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc231)
    %572 = "vhlo.reshape_v1"(%571) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc232)
    %573 = "vhlo.reshape_v1"(%arg106) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %574 = "vhlo.custom_call_v1"(%573) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %575 = "vhlo.reshape_v1"(%574) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %576 = "vhlo.transpose_v1"(%575) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc233)
    %577 = "vhlo.dot_general_v2"(%572, %576) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc234)
    %578 = "vhlo.reshape_v1"(%577) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc232)
    %579 = "vhlo.reshape_v1"(%arg105) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %580 = "vhlo.custom_call_v1"(%579) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %581 = "vhlo.reshape_v1"(%580) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %582 = "vhlo.broadcast_in_dim_v1"(%581) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc235)
    %583 = "vhlo.add_v1"(%578, %582) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc235)
    %584 = "vhlo.add_v1"(%489, %583) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc236)
    %585 = "vhlo.reduce_v1"(%584, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc238)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc237)
    %586 = "vhlo.multiply_v1"(%585, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc237)
    %587 = "vhlo.broadcast_in_dim_v1"(%586) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc239)
    %588 = "vhlo.subtract_v1"(%584, %587) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc239)
    %589 = "vhlo.multiply_v1"(%588, %588) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc237)
    %590 = "vhlo.reduce_v1"(%589, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc240)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc237)
    %591 = "vhlo.multiply_v1"(%590, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc237)
    %592 = "vhlo.reshape_v1"(%591) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc237)
    %593 = "vhlo.add_v1"(%592, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc241)
    %594 = "vhlo.rsqrt_v2"(%593) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc242)
    %595 = "vhlo.reshape_v1"(%594) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc243)
    %596 = "vhlo.broadcast_in_dim_v1"(%595) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc243)
    %597 = "vhlo.multiply_v1"(%588, %596) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc243)
    %598 = "vhlo.reshape_v1"(%arg104) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %599 = "vhlo.custom_call_v1"(%598) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %600 = "vhlo.reshape_v1"(%599) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %601 = "vhlo.broadcast_in_dim_v1"(%600) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc243)
    %602 = "vhlo.multiply_v1"(%597, %601) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc243)
    %603 = "vhlo.reshape_v1"(%arg103) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %604 = "vhlo.custom_call_v1"(%603) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %605 = "vhlo.reshape_v1"(%604) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %606 = "vhlo.broadcast_in_dim_v1"(%605) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc241)
    %607 = "vhlo.add_v1"(%602, %606) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc241)
    %608 = "vhlo.reshape_v1"(%607) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc244)
    %609 = "vhlo.reshape_v1"(%arg102) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %610 = "vhlo.custom_call_v1"(%609) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %611 = "vhlo.reshape_v1"(%610) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %612 = "vhlo.transpose_v1"(%611) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc245)
    %613 = "vhlo.dot_general_v2"(%608, %612) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc246)
    %614 = "vhlo.reshape_v1"(%613) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc244)
    %615 = "vhlo.reshape_v1"(%arg101) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %616 = "vhlo.custom_call_v1"(%615) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %617 = "vhlo.reshape_v1"(%616) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %618 = "vhlo.broadcast_in_dim_v1"(%617) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc247)
    %619 = "vhlo.add_v1"(%614, %618) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc247)
    %620 = "vhlo.multiply_v1"(%619, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc248)
    %621 = "vhlo.logistic_v2"(%620) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc249)
    %622 = "vhlo.multiply_v1"(%619, %621) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc248)
    %623 = "vhlo.reshape_v1"(%622) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc250)
    %624 = "vhlo.reshape_v1"(%arg100) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %625 = "vhlo.custom_call_v1"(%624) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %626 = "vhlo.reshape_v1"(%625) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %627 = "vhlo.transpose_v1"(%626) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc251)
    %628 = "vhlo.dot_general_v2"(%623, %627) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc252)
    %629 = "vhlo.reshape_v1"(%628) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc250)
    %630 = "vhlo.reshape_v1"(%arg99) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %631 = "vhlo.custom_call_v1"(%630) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_3_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %632 = "vhlo.reshape_v1"(%631) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %633 = "vhlo.broadcast_in_dim_v1"(%632) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc253)
    %634 = "vhlo.add_v1"(%629, %633) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc253)
    %635 = "vhlo.add_v1"(%584, %634) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc254)
    %636 = "vhlo.reduce_v1"(%635, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc256)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc255)
    %637 = "vhlo.multiply_v1"(%636, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc255)
    %638 = "vhlo.broadcast_in_dim_v1"(%637) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc257)
    %639 = "vhlo.subtract_v1"(%635, %638) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc257)
    %640 = "vhlo.multiply_v1"(%639, %639) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc255)
    %641 = "vhlo.reduce_v1"(%640, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc258)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc255)
    %642 = "vhlo.multiply_v1"(%641, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc255)
    %643 = "vhlo.reshape_v1"(%642) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc255)
    %644 = "vhlo.add_v1"(%643, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc259)
    %645 = "vhlo.rsqrt_v2"(%644) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc260)
    %646 = "vhlo.reshape_v1"(%645) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc261)
    %647 = "vhlo.broadcast_in_dim_v1"(%646) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc261)
    %648 = "vhlo.multiply_v1"(%639, %647) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc261)
    %649 = "vhlo.reshape_v1"(%arg98) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %650 = "vhlo.custom_call_v1"(%649) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %651 = "vhlo.reshape_v1"(%650) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %652 = "vhlo.broadcast_in_dim_v1"(%651) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc261)
    %653 = "vhlo.multiply_v1"(%648, %652) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc261)
    %654 = "vhlo.reshape_v1"(%arg97) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %655 = "vhlo.custom_call_v1"(%654) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %656 = "vhlo.reshape_v1"(%655) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %657 = "vhlo.broadcast_in_dim_v1"(%656) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc259)
    %658 = "vhlo.add_v1"(%653, %657) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc259)
    %659 = "vhlo.reshape_v1"(%658) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc262)
    %660 = "vhlo.reshape_v1"(%arg173) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %661 = "vhlo.custom_call_v1"(%660) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %662 = "vhlo.reshape_v1"(%661) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %663 = "vhlo.transpose_v1"(%662) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc263)
    %664 = "vhlo.dot_general_v2"(%659, %663) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc264)
    %665 = "vhlo.reshape_v1"(%664) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc262)
    %666 = "vhlo.reshape_v1"(%arg172) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %667 = "vhlo.custom_call_v1"(%666) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %668 = "vhlo.reshape_v1"(%667) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %669 = "vhlo.broadcast_in_dim_v1"(%668) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc265)
    %670 = "vhlo.add_v1"(%665, %669) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc265)
    %671 = "vhlo.reshape_v1"(%670) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc266)
    %672 = "vhlo.transpose_v1"(%671) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc267)
    %673 = "vhlo.reshape_v1"(%672) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc268)
    %674 = "vhlo.reshape_v1"(%arg171) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %675 = "vhlo.custom_call_v1"(%674) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %676 = "vhlo.reshape_v1"(%675) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %677 = "vhlo.transpose_v1"(%676) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc269)
    %678 = "vhlo.dot_general_v2"(%659, %677) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc270)
    %679 = "vhlo.reshape_v1"(%678) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc271)
    %680 = "vhlo.reshape_v1"(%arg170) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %681 = "vhlo.custom_call_v1"(%680) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %682 = "vhlo.reshape_v1"(%681) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %683 = "vhlo.broadcast_in_dim_v1"(%682) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc272)
    %684 = "vhlo.add_v1"(%679, %683) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc272)
    %685 = "vhlo.reshape_v1"(%684) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc273)
    %686 = "vhlo.transpose_v1"(%685) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc274)
    %687 = "vhlo.reshape_v1"(%686) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc268)
    %688 = "vhlo.dot_general_v2"(%673, %687) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc275)
    %689 = "vhlo.reshape_v1"(%688) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc268)
    %690 = "vhlo.multiply_v1"(%689, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc276)
    %691 = "vhlo.convert_v1"(%690) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc277)
    %692 = "vhlo.reduce_v1"(%691, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc279)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc278)
    %693 = "vhlo.broadcast_in_dim_v1"(%692) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc278)
    %694 = "vhlo.subtract_v1"(%691, %693) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc278)
    %695 = "vhlo.exponential_v2"(%694) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc278)
    %696 = "vhlo.reduce_v1"(%695, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc280)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc278)
    %697 = "vhlo.broadcast_in_dim_v1"(%696) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc278)
    %698 = "vhlo.divide_v1"(%695, %697) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc278)
    %699 = "vhlo.convert_v1"(%698) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc281)
    %700 = "vhlo.reshape_v1"(%699) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc282)
    %701 = "vhlo.reshape_v1"(%arg96) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %702 = "vhlo.custom_call_v1"(%701) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %703 = "vhlo.reshape_v1"(%702) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %704 = "vhlo.transpose_v1"(%703) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc283)
    %705 = "vhlo.dot_general_v2"(%659, %704) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc284)
    %706 = "vhlo.reshape_v1"(%705) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc285)
    %707 = "vhlo.reshape_v1"(%arg95) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %708 = "vhlo.custom_call_v1"(%707) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %709 = "vhlo.reshape_v1"(%708) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %710 = "vhlo.broadcast_in_dim_v1"(%709) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc286)
    %711 = "vhlo.add_v1"(%706, %710) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc286)
    %712 = "vhlo.reshape_v1"(%711) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc287)
    %713 = "vhlo.transpose_v1"(%712) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc288)
    %714 = "vhlo.reshape_v1"(%713) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc282)
    %715 = "vhlo.dot_general_v2"(%700, %714) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc289)
    %716 = "vhlo.reshape_v1"(%715) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc282)
    %717 = "vhlo.transpose_v1"(%716) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc290)
    %718 = "vhlo.reshape_v1"(%717) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc291)
    %719 = "vhlo.reshape_v1"(%arg94) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %720 = "vhlo.custom_call_v1"(%719) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %721 = "vhlo.reshape_v1"(%720) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %722 = "vhlo.transpose_v1"(%721) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc292)
    %723 = "vhlo.dot_general_v2"(%718, %722) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc293)
    %724 = "vhlo.reshape_v1"(%723) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc291)
    %725 = "vhlo.reshape_v1"(%arg93) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %726 = "vhlo.custom_call_v1"(%725) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %727 = "vhlo.reshape_v1"(%726) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %728 = "vhlo.broadcast_in_dim_v1"(%727) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc294)
    %729 = "vhlo.add_v1"(%724, %728) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc294)
    %730 = "vhlo.add_v1"(%635, %729) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc295)
    %731 = "vhlo.reduce_v1"(%730, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc297)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc296)
    %732 = "vhlo.multiply_v1"(%731, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc296)
    %733 = "vhlo.broadcast_in_dim_v1"(%732) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc298)
    %734 = "vhlo.subtract_v1"(%730, %733) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc298)
    %735 = "vhlo.multiply_v1"(%734, %734) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc296)
    %736 = "vhlo.reduce_v1"(%735, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc299)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc296)
    %737 = "vhlo.multiply_v1"(%736, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc296)
    %738 = "vhlo.reshape_v1"(%737) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc296)
    %739 = "vhlo.add_v1"(%738, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc300)
    %740 = "vhlo.rsqrt_v2"(%739) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc301)
    %741 = "vhlo.reshape_v1"(%740) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc302)
    %742 = "vhlo.broadcast_in_dim_v1"(%741) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc302)
    %743 = "vhlo.multiply_v1"(%734, %742) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc302)
    %744 = "vhlo.reshape_v1"(%arg92) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %745 = "vhlo.custom_call_v1"(%744) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %746 = "vhlo.reshape_v1"(%745) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %747 = "vhlo.broadcast_in_dim_v1"(%746) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc302)
    %748 = "vhlo.multiply_v1"(%743, %747) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc302)
    %749 = "vhlo.reshape_v1"(%arg91) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %750 = "vhlo.custom_call_v1"(%749) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %751 = "vhlo.reshape_v1"(%750) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %752 = "vhlo.broadcast_in_dim_v1"(%751) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc300)
    %753 = "vhlo.add_v1"(%748, %752) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc300)
    %754 = "vhlo.reshape_v1"(%753) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc303)
    %755 = "vhlo.reshape_v1"(%arg90) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %756 = "vhlo.custom_call_v1"(%755) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %757 = "vhlo.reshape_v1"(%756) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %758 = "vhlo.transpose_v1"(%757) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc304)
    %759 = "vhlo.dot_general_v2"(%754, %758) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc305)
    %760 = "vhlo.reshape_v1"(%759) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc303)
    %761 = "vhlo.reshape_v1"(%arg89) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %762 = "vhlo.custom_call_v1"(%761) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %763 = "vhlo.reshape_v1"(%762) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %764 = "vhlo.broadcast_in_dim_v1"(%763) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc306)
    %765 = "vhlo.add_v1"(%760, %764) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc306)
    %766 = "vhlo.multiply_v1"(%765, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc307)
    %767 = "vhlo.logistic_v2"(%766) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc308)
    %768 = "vhlo.multiply_v1"(%765, %767) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc307)
    %769 = "vhlo.reshape_v1"(%768) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc309)
    %770 = "vhlo.reshape_v1"(%arg88) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %771 = "vhlo.custom_call_v1"(%770) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %772 = "vhlo.reshape_v1"(%771) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %773 = "vhlo.transpose_v1"(%772) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc310)
    %774 = "vhlo.dot_general_v2"(%769, %773) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc311)
    %775 = "vhlo.reshape_v1"(%774) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc309)
    %776 = "vhlo.reshape_v1"(%arg87) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %777 = "vhlo.custom_call_v1"(%776) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_4_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %778 = "vhlo.reshape_v1"(%777) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %779 = "vhlo.broadcast_in_dim_v1"(%778) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc312)
    %780 = "vhlo.add_v1"(%775, %779) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc312)
    %781 = "vhlo.add_v1"(%730, %780) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc313)
    %782 = "vhlo.reduce_v1"(%781, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc315)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc314)
    %783 = "vhlo.multiply_v1"(%782, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc314)
    %784 = "vhlo.broadcast_in_dim_v1"(%783) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc316)
    %785 = "vhlo.subtract_v1"(%781, %784) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc316)
    %786 = "vhlo.multiply_v1"(%785, %785) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc314)
    %787 = "vhlo.reduce_v1"(%786, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc317)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc314)
    %788 = "vhlo.multiply_v1"(%787, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc314)
    %789 = "vhlo.reshape_v1"(%788) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc314)
    %790 = "vhlo.add_v1"(%789, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc318)
    %791 = "vhlo.rsqrt_v2"(%790) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc319)
    %792 = "vhlo.reshape_v1"(%791) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc320)
    %793 = "vhlo.broadcast_in_dim_v1"(%792) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc320)
    %794 = "vhlo.multiply_v1"(%785, %793) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc320)
    %795 = "vhlo.reshape_v1"(%arg86) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %796 = "vhlo.custom_call_v1"(%795) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %797 = "vhlo.reshape_v1"(%796) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %798 = "vhlo.broadcast_in_dim_v1"(%797) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc320)
    %799 = "vhlo.multiply_v1"(%794, %798) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc320)
    %800 = "vhlo.reshape_v1"(%arg85) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %801 = "vhlo.custom_call_v1"(%800) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %802 = "vhlo.reshape_v1"(%801) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %803 = "vhlo.broadcast_in_dim_v1"(%802) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc318)
    %804 = "vhlo.add_v1"(%799, %803) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc318)
    %805 = "vhlo.reshape_v1"(%804) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc321)
    %806 = "vhlo.reshape_v1"(%arg177) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %807 = "vhlo.custom_call_v1"(%806) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %808 = "vhlo.reshape_v1"(%807) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %809 = "vhlo.transpose_v1"(%808) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc322)
    %810 = "vhlo.dot_general_v2"(%805, %809) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc323)
    %811 = "vhlo.reshape_v1"(%810) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc321)
    %812 = "vhlo.reshape_v1"(%arg176) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %813 = "vhlo.custom_call_v1"(%812) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %814 = "vhlo.reshape_v1"(%813) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %815 = "vhlo.broadcast_in_dim_v1"(%814) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc324)
    %816 = "vhlo.add_v1"(%811, %815) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc324)
    %817 = "vhlo.reshape_v1"(%816) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc325)
    %818 = "vhlo.transpose_v1"(%817) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc326)
    %819 = "vhlo.reshape_v1"(%818) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc327)
    %820 = "vhlo.reshape_v1"(%arg175) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %821 = "vhlo.custom_call_v1"(%820) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %822 = "vhlo.reshape_v1"(%821) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %823 = "vhlo.transpose_v1"(%822) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc328)
    %824 = "vhlo.dot_general_v2"(%805, %823) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc329)
    %825 = "vhlo.reshape_v1"(%824) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc330)
    %826 = "vhlo.reshape_v1"(%arg174) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %827 = "vhlo.custom_call_v1"(%826) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %828 = "vhlo.reshape_v1"(%827) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %829 = "vhlo.broadcast_in_dim_v1"(%828) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc331)
    %830 = "vhlo.add_v1"(%825, %829) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc331)
    %831 = "vhlo.reshape_v1"(%830) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc332)
    %832 = "vhlo.transpose_v1"(%831) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc333)
    %833 = "vhlo.reshape_v1"(%832) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc327)
    %834 = "vhlo.dot_general_v2"(%819, %833) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc334)
    %835 = "vhlo.reshape_v1"(%834) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc327)
    %836 = "vhlo.multiply_v1"(%835, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc335)
    %837 = "vhlo.convert_v1"(%836) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc336)
    %838 = "vhlo.reduce_v1"(%837, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc338)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc337)
    %839 = "vhlo.broadcast_in_dim_v1"(%838) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc337)
    %840 = "vhlo.subtract_v1"(%837, %839) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc337)
    %841 = "vhlo.exponential_v2"(%840) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc337)
    %842 = "vhlo.reduce_v1"(%841, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc339)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc337)
    %843 = "vhlo.broadcast_in_dim_v1"(%842) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc337)
    %844 = "vhlo.divide_v1"(%841, %843) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc337)
    %845 = "vhlo.convert_v1"(%844) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc340)
    %846 = "vhlo.reshape_v1"(%845) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc341)
    %847 = "vhlo.reshape_v1"(%arg84) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %848 = "vhlo.custom_call_v1"(%847) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %849 = "vhlo.reshape_v1"(%848) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %850 = "vhlo.transpose_v1"(%849) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc342)
    %851 = "vhlo.dot_general_v2"(%805, %850) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc343)
    %852 = "vhlo.reshape_v1"(%851) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc344)
    %853 = "vhlo.reshape_v1"(%arg83) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %854 = "vhlo.custom_call_v1"(%853) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %855 = "vhlo.reshape_v1"(%854) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %856 = "vhlo.broadcast_in_dim_v1"(%855) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc345)
    %857 = "vhlo.add_v1"(%852, %856) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc345)
    %858 = "vhlo.reshape_v1"(%857) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc346)
    %859 = "vhlo.transpose_v1"(%858) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc347)
    %860 = "vhlo.reshape_v1"(%859) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc341)
    %861 = "vhlo.dot_general_v2"(%846, %860) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc348)
    %862 = "vhlo.reshape_v1"(%861) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc341)
    %863 = "vhlo.transpose_v1"(%862) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc349)
    %864 = "vhlo.reshape_v1"(%863) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc350)
    %865 = "vhlo.reshape_v1"(%arg82) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %866 = "vhlo.custom_call_v1"(%865) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %867 = "vhlo.reshape_v1"(%866) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %868 = "vhlo.transpose_v1"(%867) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc351)
    %869 = "vhlo.dot_general_v2"(%864, %868) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc352)
    %870 = "vhlo.reshape_v1"(%869) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc350)
    %871 = "vhlo.reshape_v1"(%arg81) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %872 = "vhlo.custom_call_v1"(%871) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %873 = "vhlo.reshape_v1"(%872) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %874 = "vhlo.broadcast_in_dim_v1"(%873) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc353)
    %875 = "vhlo.add_v1"(%870, %874) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc353)
    %876 = "vhlo.add_v1"(%781, %875) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc354)
    %877 = "vhlo.reduce_v1"(%876, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc356)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc355)
    %878 = "vhlo.multiply_v1"(%877, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc355)
    %879 = "vhlo.broadcast_in_dim_v1"(%878) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc357)
    %880 = "vhlo.subtract_v1"(%876, %879) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc357)
    %881 = "vhlo.multiply_v1"(%880, %880) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc355)
    %882 = "vhlo.reduce_v1"(%881, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc358)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc355)
    %883 = "vhlo.multiply_v1"(%882, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc355)
    %884 = "vhlo.reshape_v1"(%883) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc355)
    %885 = "vhlo.add_v1"(%884, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc359)
    %886 = "vhlo.rsqrt_v2"(%885) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc360)
    %887 = "vhlo.reshape_v1"(%886) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc361)
    %888 = "vhlo.broadcast_in_dim_v1"(%887) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc361)
    %889 = "vhlo.multiply_v1"(%880, %888) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc361)
    %890 = "vhlo.reshape_v1"(%arg80) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %891 = "vhlo.custom_call_v1"(%890) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %892 = "vhlo.reshape_v1"(%891) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %893 = "vhlo.broadcast_in_dim_v1"(%892) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc361)
    %894 = "vhlo.multiply_v1"(%889, %893) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc361)
    %895 = "vhlo.reshape_v1"(%arg79) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %896 = "vhlo.custom_call_v1"(%895) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %897 = "vhlo.reshape_v1"(%896) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %898 = "vhlo.broadcast_in_dim_v1"(%897) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc359)
    %899 = "vhlo.add_v1"(%894, %898) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc359)
    %900 = "vhlo.reshape_v1"(%899) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc362)
    %901 = "vhlo.reshape_v1"(%arg78) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %902 = "vhlo.custom_call_v1"(%901) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %903 = "vhlo.reshape_v1"(%902) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %904 = "vhlo.transpose_v1"(%903) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc363)
    %905 = "vhlo.dot_general_v2"(%900, %904) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc364)
    %906 = "vhlo.reshape_v1"(%905) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc362)
    %907 = "vhlo.reshape_v1"(%arg77) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %908 = "vhlo.custom_call_v1"(%907) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %909 = "vhlo.reshape_v1"(%908) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %910 = "vhlo.broadcast_in_dim_v1"(%909) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc365)
    %911 = "vhlo.add_v1"(%906, %910) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc365)
    %912 = "vhlo.multiply_v1"(%911, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc366)
    %913 = "vhlo.logistic_v2"(%912) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc367)
    %914 = "vhlo.multiply_v1"(%911, %913) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc366)
    %915 = "vhlo.reshape_v1"(%914) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc368)
    %916 = "vhlo.reshape_v1"(%arg76) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %917 = "vhlo.custom_call_v1"(%916) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %918 = "vhlo.reshape_v1"(%917) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %919 = "vhlo.transpose_v1"(%918) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc369)
    %920 = "vhlo.dot_general_v2"(%915, %919) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc370)
    %921 = "vhlo.reshape_v1"(%920) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc368)
    %922 = "vhlo.reshape_v1"(%arg75) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %923 = "vhlo.custom_call_v1"(%922) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_5_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %924 = "vhlo.reshape_v1"(%923) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %925 = "vhlo.broadcast_in_dim_v1"(%924) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc371)
    %926 = "vhlo.add_v1"(%921, %925) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc371)
    %927 = "vhlo.add_v1"(%876, %926) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc372)
    %928 = "vhlo.reduce_v1"(%927, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc374)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc373)
    %929 = "vhlo.multiply_v1"(%928, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc373)
    %930 = "vhlo.broadcast_in_dim_v1"(%929) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc375)
    %931 = "vhlo.subtract_v1"(%927, %930) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc375)
    %932 = "vhlo.multiply_v1"(%931, %931) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc373)
    %933 = "vhlo.reduce_v1"(%932, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc376)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc373)
    %934 = "vhlo.multiply_v1"(%933, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc373)
    %935 = "vhlo.reshape_v1"(%934) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc373)
    %936 = "vhlo.add_v1"(%935, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc377)
    %937 = "vhlo.rsqrt_v2"(%936) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc378)
    %938 = "vhlo.reshape_v1"(%937) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc379)
    %939 = "vhlo.broadcast_in_dim_v1"(%938) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc379)
    %940 = "vhlo.multiply_v1"(%931, %939) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc379)
    %941 = "vhlo.reshape_v1"(%arg74) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %942 = "vhlo.custom_call_v1"(%941) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %943 = "vhlo.reshape_v1"(%942) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %944 = "vhlo.broadcast_in_dim_v1"(%943) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc379)
    %945 = "vhlo.multiply_v1"(%940, %944) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc379)
    %946 = "vhlo.reshape_v1"(%arg73) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %947 = "vhlo.custom_call_v1"(%946) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %948 = "vhlo.reshape_v1"(%947) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %949 = "vhlo.broadcast_in_dim_v1"(%948) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc377)
    %950 = "vhlo.add_v1"(%945, %949) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc377)
    %951 = "vhlo.reshape_v1"(%950) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc380)
    %952 = "vhlo.reshape_v1"(%arg181) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %953 = "vhlo.custom_call_v1"(%952) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %954 = "vhlo.reshape_v1"(%953) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %955 = "vhlo.transpose_v1"(%954) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc381)
    %956 = "vhlo.dot_general_v2"(%951, %955) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc382)
    %957 = "vhlo.reshape_v1"(%956) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc380)
    %958 = "vhlo.reshape_v1"(%arg180) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %959 = "vhlo.custom_call_v1"(%958) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %960 = "vhlo.reshape_v1"(%959) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %961 = "vhlo.broadcast_in_dim_v1"(%960) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc383)
    %962 = "vhlo.add_v1"(%957, %961) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc383)
    %963 = "vhlo.reshape_v1"(%962) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc384)
    %964 = "vhlo.transpose_v1"(%963) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc385)
    %965 = "vhlo.reshape_v1"(%964) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc386)
    %966 = "vhlo.reshape_v1"(%arg179) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %967 = "vhlo.custom_call_v1"(%966) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %968 = "vhlo.reshape_v1"(%967) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %969 = "vhlo.transpose_v1"(%968) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc387)
    %970 = "vhlo.dot_general_v2"(%951, %969) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc388)
    %971 = "vhlo.reshape_v1"(%970) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc389)
    %972 = "vhlo.reshape_v1"(%arg178) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %973 = "vhlo.custom_call_v1"(%972) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %974 = "vhlo.reshape_v1"(%973) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %975 = "vhlo.broadcast_in_dim_v1"(%974) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc390)
    %976 = "vhlo.add_v1"(%971, %975) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc390)
    %977 = "vhlo.reshape_v1"(%976) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc391)
    %978 = "vhlo.transpose_v1"(%977) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc392)
    %979 = "vhlo.reshape_v1"(%978) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc386)
    %980 = "vhlo.dot_general_v2"(%965, %979) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc393)
    %981 = "vhlo.reshape_v1"(%980) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc386)
    %982 = "vhlo.multiply_v1"(%981, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc394)
    %983 = "vhlo.convert_v1"(%982) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc395)
    %984 = "vhlo.reduce_v1"(%983, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc397)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc396)
    %985 = "vhlo.broadcast_in_dim_v1"(%984) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc396)
    %986 = "vhlo.subtract_v1"(%983, %985) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc396)
    %987 = "vhlo.exponential_v2"(%986) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc396)
    %988 = "vhlo.reduce_v1"(%987, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc398)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc396)
    %989 = "vhlo.broadcast_in_dim_v1"(%988) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc396)
    %990 = "vhlo.divide_v1"(%987, %989) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc396)
    %991 = "vhlo.convert_v1"(%990) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc399)
    %992 = "vhlo.reshape_v1"(%991) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc400)
    %993 = "vhlo.reshape_v1"(%arg72) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %994 = "vhlo.custom_call_v1"(%993) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %995 = "vhlo.reshape_v1"(%994) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %996 = "vhlo.transpose_v1"(%995) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc401)
    %997 = "vhlo.dot_general_v2"(%951, %996) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc402)
    %998 = "vhlo.reshape_v1"(%997) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc403)
    %999 = "vhlo.reshape_v1"(%arg71) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1000 = "vhlo.custom_call_v1"(%999) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1001 = "vhlo.reshape_v1"(%1000) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1002 = "vhlo.broadcast_in_dim_v1"(%1001) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc404)
    %1003 = "vhlo.add_v1"(%998, %1002) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc404)
    %1004 = "vhlo.reshape_v1"(%1003) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc405)
    %1005 = "vhlo.transpose_v1"(%1004) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc406)
    %1006 = "vhlo.reshape_v1"(%1005) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc400)
    %1007 = "vhlo.dot_general_v2"(%992, %1006) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc407)
    %1008 = "vhlo.reshape_v1"(%1007) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc400)
    %1009 = "vhlo.transpose_v1"(%1008) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc408)
    %1010 = "vhlo.reshape_v1"(%1009) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc409)
    %1011 = "vhlo.reshape_v1"(%arg70) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1012 = "vhlo.custom_call_v1"(%1011) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1013 = "vhlo.reshape_v1"(%1012) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1014 = "vhlo.transpose_v1"(%1013) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc410)
    %1015 = "vhlo.dot_general_v2"(%1010, %1014) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc411)
    %1016 = "vhlo.reshape_v1"(%1015) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc409)
    %1017 = "vhlo.reshape_v1"(%arg69) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1018 = "vhlo.custom_call_v1"(%1017) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1019 = "vhlo.reshape_v1"(%1018) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1020 = "vhlo.broadcast_in_dim_v1"(%1019) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc412)
    %1021 = "vhlo.add_v1"(%1016, %1020) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc412)
    %1022 = "vhlo.add_v1"(%927, %1021) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc413)
    %1023 = "vhlo.reduce_v1"(%1022, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc415)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc414)
    %1024 = "vhlo.multiply_v1"(%1023, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc414)
    %1025 = "vhlo.broadcast_in_dim_v1"(%1024) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc416)
    %1026 = "vhlo.subtract_v1"(%1022, %1025) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc416)
    %1027 = "vhlo.multiply_v1"(%1026, %1026) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc414)
    %1028 = "vhlo.reduce_v1"(%1027, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc417)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc414)
    %1029 = "vhlo.multiply_v1"(%1028, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc414)
    %1030 = "vhlo.reshape_v1"(%1029) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc414)
    %1031 = "vhlo.add_v1"(%1030, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc418)
    %1032 = "vhlo.rsqrt_v2"(%1031) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc419)
    %1033 = "vhlo.reshape_v1"(%1032) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc420)
    %1034 = "vhlo.broadcast_in_dim_v1"(%1033) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc420)
    %1035 = "vhlo.multiply_v1"(%1026, %1034) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc420)
    %1036 = "vhlo.reshape_v1"(%arg68) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1037 = "vhlo.custom_call_v1"(%1036) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1038 = "vhlo.reshape_v1"(%1037) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1039 = "vhlo.broadcast_in_dim_v1"(%1038) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc420)
    %1040 = "vhlo.multiply_v1"(%1035, %1039) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc420)
    %1041 = "vhlo.reshape_v1"(%arg67) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1042 = "vhlo.custom_call_v1"(%1041) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1043 = "vhlo.reshape_v1"(%1042) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1044 = "vhlo.broadcast_in_dim_v1"(%1043) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc418)
    %1045 = "vhlo.add_v1"(%1040, %1044) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc418)
    %1046 = "vhlo.reshape_v1"(%1045) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc421)
    %1047 = "vhlo.reshape_v1"(%arg66) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1048 = "vhlo.custom_call_v1"(%1047) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %1049 = "vhlo.reshape_v1"(%1048) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1050 = "vhlo.transpose_v1"(%1049) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc422)
    %1051 = "vhlo.dot_general_v2"(%1046, %1050) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc423)
    %1052 = "vhlo.reshape_v1"(%1051) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc421)
    %1053 = "vhlo.reshape_v1"(%arg65) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %1054 = "vhlo.custom_call_v1"(%1053) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %1055 = "vhlo.reshape_v1"(%1054) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %1056 = "vhlo.broadcast_in_dim_v1"(%1055) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc424)
    %1057 = "vhlo.add_v1"(%1052, %1056) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc424)
    %1058 = "vhlo.multiply_v1"(%1057, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc425)
    %1059 = "vhlo.logistic_v2"(%1058) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc426)
    %1060 = "vhlo.multiply_v1"(%1057, %1059) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc425)
    %1061 = "vhlo.reshape_v1"(%1060) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc427)
    %1062 = "vhlo.reshape_v1"(%arg64) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1063 = "vhlo.custom_call_v1"(%1062) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %1064 = "vhlo.reshape_v1"(%1063) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1065 = "vhlo.transpose_v1"(%1064) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc428)
    %1066 = "vhlo.dot_general_v2"(%1061, %1065) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc429)
    %1067 = "vhlo.reshape_v1"(%1066) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc427)
    %1068 = "vhlo.reshape_v1"(%arg63) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1069 = "vhlo.custom_call_v1"(%1068) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_6_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1070 = "vhlo.reshape_v1"(%1069) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1071 = "vhlo.broadcast_in_dim_v1"(%1070) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc430)
    %1072 = "vhlo.add_v1"(%1067, %1071) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc430)
    %1073 = "vhlo.add_v1"(%1022, %1072) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc431)
    %1074 = "vhlo.reduce_v1"(%1073, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc433)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc432)
    %1075 = "vhlo.multiply_v1"(%1074, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc432)
    %1076 = "vhlo.broadcast_in_dim_v1"(%1075) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc434)
    %1077 = "vhlo.subtract_v1"(%1073, %1076) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc434)
    %1078 = "vhlo.multiply_v1"(%1077, %1077) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc432)
    %1079 = "vhlo.reduce_v1"(%1078, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc435)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc432)
    %1080 = "vhlo.multiply_v1"(%1079, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc432)
    %1081 = "vhlo.reshape_v1"(%1080) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc432)
    %1082 = "vhlo.add_v1"(%1081, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc436)
    %1083 = "vhlo.rsqrt_v2"(%1082) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc437)
    %1084 = "vhlo.reshape_v1"(%1083) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc438)
    %1085 = "vhlo.broadcast_in_dim_v1"(%1084) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc438)
    %1086 = "vhlo.multiply_v1"(%1077, %1085) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc438)
    %1087 = "vhlo.reshape_v1"(%arg62) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1088 = "vhlo.custom_call_v1"(%1087) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1089 = "vhlo.reshape_v1"(%1088) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1090 = "vhlo.broadcast_in_dim_v1"(%1089) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc438)
    %1091 = "vhlo.multiply_v1"(%1086, %1090) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc438)
    %1092 = "vhlo.reshape_v1"(%arg61) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1093 = "vhlo.custom_call_v1"(%1092) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1094 = "vhlo.reshape_v1"(%1093) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1095 = "vhlo.broadcast_in_dim_v1"(%1094) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc436)
    %1096 = "vhlo.add_v1"(%1091, %1095) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc436)
    %1097 = "vhlo.reshape_v1"(%1096) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc439)
    %1098 = "vhlo.reshape_v1"(%arg185) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1099 = "vhlo.custom_call_v1"(%1098) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1100 = "vhlo.reshape_v1"(%1099) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1101 = "vhlo.transpose_v1"(%1100) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc440)
    %1102 = "vhlo.dot_general_v2"(%1097, %1101) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc441)
    %1103 = "vhlo.reshape_v1"(%1102) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc439)
    %1104 = "vhlo.reshape_v1"(%arg184) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1105 = "vhlo.custom_call_v1"(%1104) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1106 = "vhlo.reshape_v1"(%1105) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1107 = "vhlo.broadcast_in_dim_v1"(%1106) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc442)
    %1108 = "vhlo.add_v1"(%1103, %1107) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc442)
    %1109 = "vhlo.reshape_v1"(%1108) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc443)
    %1110 = "vhlo.transpose_v1"(%1109) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc444)
    %1111 = "vhlo.reshape_v1"(%1110) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc445)
    %1112 = "vhlo.reshape_v1"(%arg183) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1113 = "vhlo.custom_call_v1"(%1112) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1114 = "vhlo.reshape_v1"(%1113) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1115 = "vhlo.transpose_v1"(%1114) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc446)
    %1116 = "vhlo.dot_general_v2"(%1097, %1115) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc447)
    %1117 = "vhlo.reshape_v1"(%1116) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc448)
    %1118 = "vhlo.reshape_v1"(%arg182) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1119 = "vhlo.custom_call_v1"(%1118) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1120 = "vhlo.reshape_v1"(%1119) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1121 = "vhlo.broadcast_in_dim_v1"(%1120) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc449)
    %1122 = "vhlo.add_v1"(%1117, %1121) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc449)
    %1123 = "vhlo.reshape_v1"(%1122) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc450)
    %1124 = "vhlo.transpose_v1"(%1123) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc451)
    %1125 = "vhlo.reshape_v1"(%1124) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc445)
    %1126 = "vhlo.dot_general_v2"(%1111, %1125) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc452)
    %1127 = "vhlo.reshape_v1"(%1126) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc445)
    %1128 = "vhlo.multiply_v1"(%1127, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc453)
    %1129 = "vhlo.convert_v1"(%1128) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc454)
    %1130 = "vhlo.reduce_v1"(%1129, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc456)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc455)
    %1131 = "vhlo.broadcast_in_dim_v1"(%1130) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc455)
    %1132 = "vhlo.subtract_v1"(%1129, %1131) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc455)
    %1133 = "vhlo.exponential_v2"(%1132) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc455)
    %1134 = "vhlo.reduce_v1"(%1133, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc457)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc455)
    %1135 = "vhlo.broadcast_in_dim_v1"(%1134) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc455)
    %1136 = "vhlo.divide_v1"(%1133, %1135) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc455)
    %1137 = "vhlo.convert_v1"(%1136) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc458)
    %1138 = "vhlo.reshape_v1"(%1137) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc459)
    %1139 = "vhlo.reshape_v1"(%arg60) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1140 = "vhlo.custom_call_v1"(%1139) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1141 = "vhlo.reshape_v1"(%1140) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1142 = "vhlo.transpose_v1"(%1141) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc460)
    %1143 = "vhlo.dot_general_v2"(%1097, %1142) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc461)
    %1144 = "vhlo.reshape_v1"(%1143) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc462)
    %1145 = "vhlo.reshape_v1"(%arg59) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1146 = "vhlo.custom_call_v1"(%1145) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1147 = "vhlo.reshape_v1"(%1146) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1148 = "vhlo.broadcast_in_dim_v1"(%1147) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc463)
    %1149 = "vhlo.add_v1"(%1144, %1148) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc463)
    %1150 = "vhlo.reshape_v1"(%1149) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc464)
    %1151 = "vhlo.transpose_v1"(%1150) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc465)
    %1152 = "vhlo.reshape_v1"(%1151) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc459)
    %1153 = "vhlo.dot_general_v2"(%1138, %1152) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc466)
    %1154 = "vhlo.reshape_v1"(%1153) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc459)
    %1155 = "vhlo.transpose_v1"(%1154) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc467)
    %1156 = "vhlo.reshape_v1"(%1155) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc468)
    %1157 = "vhlo.reshape_v1"(%arg58) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1158 = "vhlo.custom_call_v1"(%1157) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1159 = "vhlo.reshape_v1"(%1158) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1160 = "vhlo.transpose_v1"(%1159) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc469)
    %1161 = "vhlo.dot_general_v2"(%1156, %1160) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc470)
    %1162 = "vhlo.reshape_v1"(%1161) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc468)
    %1163 = "vhlo.reshape_v1"(%arg57) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1164 = "vhlo.custom_call_v1"(%1163) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1165 = "vhlo.reshape_v1"(%1164) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1166 = "vhlo.broadcast_in_dim_v1"(%1165) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc471)
    %1167 = "vhlo.add_v1"(%1162, %1166) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc471)
    %1168 = "vhlo.add_v1"(%1073, %1167) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc472)
    %1169 = "vhlo.reduce_v1"(%1168, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc474)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc473)
    %1170 = "vhlo.multiply_v1"(%1169, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc473)
    %1171 = "vhlo.broadcast_in_dim_v1"(%1170) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc475)
    %1172 = "vhlo.subtract_v1"(%1168, %1171) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc475)
    %1173 = "vhlo.multiply_v1"(%1172, %1172) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc473)
    %1174 = "vhlo.reduce_v1"(%1173, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc476)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc473)
    %1175 = "vhlo.multiply_v1"(%1174, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc473)
    %1176 = "vhlo.reshape_v1"(%1175) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc473)
    %1177 = "vhlo.add_v1"(%1176, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc477)
    %1178 = "vhlo.rsqrt_v2"(%1177) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc478)
    %1179 = "vhlo.reshape_v1"(%1178) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc479)
    %1180 = "vhlo.broadcast_in_dim_v1"(%1179) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc479)
    %1181 = "vhlo.multiply_v1"(%1172, %1180) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc479)
    %1182 = "vhlo.reshape_v1"(%arg56) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1183 = "vhlo.custom_call_v1"(%1182) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1184 = "vhlo.reshape_v1"(%1183) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1185 = "vhlo.broadcast_in_dim_v1"(%1184) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc479)
    %1186 = "vhlo.multiply_v1"(%1181, %1185) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc479)
    %1187 = "vhlo.reshape_v1"(%arg55) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1188 = "vhlo.custom_call_v1"(%1187) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1189 = "vhlo.reshape_v1"(%1188) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1190 = "vhlo.broadcast_in_dim_v1"(%1189) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc477)
    %1191 = "vhlo.add_v1"(%1186, %1190) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc477)
    %1192 = "vhlo.reshape_v1"(%1191) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc480)
    %1193 = "vhlo.reshape_v1"(%arg54) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1194 = "vhlo.custom_call_v1"(%1193) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %1195 = "vhlo.reshape_v1"(%1194) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1196 = "vhlo.transpose_v1"(%1195) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc481)
    %1197 = "vhlo.dot_general_v2"(%1192, %1196) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc482)
    %1198 = "vhlo.reshape_v1"(%1197) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc480)
    %1199 = "vhlo.reshape_v1"(%arg53) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %1200 = "vhlo.custom_call_v1"(%1199) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %1201 = "vhlo.reshape_v1"(%1200) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %1202 = "vhlo.broadcast_in_dim_v1"(%1201) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc483)
    %1203 = "vhlo.add_v1"(%1198, %1202) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc483)
    %1204 = "vhlo.multiply_v1"(%1203, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc484)
    %1205 = "vhlo.logistic_v2"(%1204) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc485)
    %1206 = "vhlo.multiply_v1"(%1203, %1205) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc484)
    %1207 = "vhlo.reshape_v1"(%1206) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc486)
    %1208 = "vhlo.reshape_v1"(%arg52) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1209 = "vhlo.custom_call_v1"(%1208) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %1210 = "vhlo.reshape_v1"(%1209) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1211 = "vhlo.transpose_v1"(%1210) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc487)
    %1212 = "vhlo.dot_general_v2"(%1207, %1211) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc488)
    %1213 = "vhlo.reshape_v1"(%1212) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc486)
    %1214 = "vhlo.reshape_v1"(%arg51) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1215 = "vhlo.custom_call_v1"(%1214) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_7_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1216 = "vhlo.reshape_v1"(%1215) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1217 = "vhlo.broadcast_in_dim_v1"(%1216) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc489)
    %1218 = "vhlo.add_v1"(%1213, %1217) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc489)
    %1219 = "vhlo.add_v1"(%1168, %1218) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc490)
    %1220 = "vhlo.reduce_v1"(%1219, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc492)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc491)
    %1221 = "vhlo.multiply_v1"(%1220, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc491)
    %1222 = "vhlo.broadcast_in_dim_v1"(%1221) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc493)
    %1223 = "vhlo.subtract_v1"(%1219, %1222) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc493)
    %1224 = "vhlo.multiply_v1"(%1223, %1223) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc491)
    %1225 = "vhlo.reduce_v1"(%1224, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc494)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc491)
    %1226 = "vhlo.multiply_v1"(%1225, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc491)
    %1227 = "vhlo.reshape_v1"(%1226) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc491)
    %1228 = "vhlo.add_v1"(%1227, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc495)
    %1229 = "vhlo.rsqrt_v2"(%1228) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc496)
    %1230 = "vhlo.reshape_v1"(%1229) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc497)
    %1231 = "vhlo.broadcast_in_dim_v1"(%1230) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc497)
    %1232 = "vhlo.multiply_v1"(%1223, %1231) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc497)
    %1233 = "vhlo.reshape_v1"(%arg50) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1234 = "vhlo.custom_call_v1"(%1233) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1235 = "vhlo.reshape_v1"(%1234) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1236 = "vhlo.broadcast_in_dim_v1"(%1235) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc497)
    %1237 = "vhlo.multiply_v1"(%1232, %1236) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc497)
    %1238 = "vhlo.reshape_v1"(%arg49) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1239 = "vhlo.custom_call_v1"(%1238) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1240 = "vhlo.reshape_v1"(%1239) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1241 = "vhlo.broadcast_in_dim_v1"(%1240) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc495)
    %1242 = "vhlo.add_v1"(%1237, %1241) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc495)
    %1243 = "vhlo.reshape_v1"(%1242) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc498)
    %1244 = "vhlo.reshape_v1"(%arg189) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1245 = "vhlo.custom_call_v1"(%1244) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1246 = "vhlo.reshape_v1"(%1245) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1247 = "vhlo.transpose_v1"(%1246) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc499)
    %1248 = "vhlo.dot_general_v2"(%1243, %1247) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc500)
    %1249 = "vhlo.reshape_v1"(%1248) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc498)
    %1250 = "vhlo.reshape_v1"(%arg188) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1251 = "vhlo.custom_call_v1"(%1250) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1252 = "vhlo.reshape_v1"(%1251) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1253 = "vhlo.broadcast_in_dim_v1"(%1252) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc501)
    %1254 = "vhlo.add_v1"(%1249, %1253) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc501)
    %1255 = "vhlo.reshape_v1"(%1254) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc502)
    %1256 = "vhlo.transpose_v1"(%1255) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc503)
    %1257 = "vhlo.reshape_v1"(%1256) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc504)
    %1258 = "vhlo.reshape_v1"(%arg187) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1259 = "vhlo.custom_call_v1"(%1258) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1260 = "vhlo.reshape_v1"(%1259) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1261 = "vhlo.transpose_v1"(%1260) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc505)
    %1262 = "vhlo.dot_general_v2"(%1243, %1261) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc506)
    %1263 = "vhlo.reshape_v1"(%1262) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc507)
    %1264 = "vhlo.reshape_v1"(%arg186) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1265 = "vhlo.custom_call_v1"(%1264) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1266 = "vhlo.reshape_v1"(%1265) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1267 = "vhlo.broadcast_in_dim_v1"(%1266) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc508)
    %1268 = "vhlo.add_v1"(%1263, %1267) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc508)
    %1269 = "vhlo.reshape_v1"(%1268) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc509)
    %1270 = "vhlo.transpose_v1"(%1269) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc510)
    %1271 = "vhlo.reshape_v1"(%1270) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc504)
    %1272 = "vhlo.dot_general_v2"(%1257, %1271) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc511)
    %1273 = "vhlo.reshape_v1"(%1272) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc504)
    %1274 = "vhlo.multiply_v1"(%1273, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc512)
    %1275 = "vhlo.convert_v1"(%1274) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc513)
    %1276 = "vhlo.reduce_v1"(%1275, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc515)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc514)
    %1277 = "vhlo.broadcast_in_dim_v1"(%1276) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc514)
    %1278 = "vhlo.subtract_v1"(%1275, %1277) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc514)
    %1279 = "vhlo.exponential_v2"(%1278) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc514)
    %1280 = "vhlo.reduce_v1"(%1279, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc516)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc514)
    %1281 = "vhlo.broadcast_in_dim_v1"(%1280) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc514)
    %1282 = "vhlo.divide_v1"(%1279, %1281) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc514)
    %1283 = "vhlo.convert_v1"(%1282) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc517)
    %1284 = "vhlo.reshape_v1"(%1283) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc518)
    %1285 = "vhlo.reshape_v1"(%arg48) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1286 = "vhlo.custom_call_v1"(%1285) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1287 = "vhlo.reshape_v1"(%1286) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1288 = "vhlo.transpose_v1"(%1287) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc519)
    %1289 = "vhlo.dot_general_v2"(%1243, %1288) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc520)
    %1290 = "vhlo.reshape_v1"(%1289) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc521)
    %1291 = "vhlo.reshape_v1"(%arg47) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1292 = "vhlo.custom_call_v1"(%1291) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1293 = "vhlo.reshape_v1"(%1292) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1294 = "vhlo.broadcast_in_dim_v1"(%1293) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc522)
    %1295 = "vhlo.add_v1"(%1290, %1294) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc522)
    %1296 = "vhlo.reshape_v1"(%1295) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc523)
    %1297 = "vhlo.transpose_v1"(%1296) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc524)
    %1298 = "vhlo.reshape_v1"(%1297) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc518)
    %1299 = "vhlo.dot_general_v2"(%1284, %1298) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc525)
    %1300 = "vhlo.reshape_v1"(%1299) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc518)
    %1301 = "vhlo.transpose_v1"(%1300) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc526)
    %1302 = "vhlo.reshape_v1"(%1301) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc527)
    %1303 = "vhlo.reshape_v1"(%arg46) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1304 = "vhlo.custom_call_v1"(%1303) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1305 = "vhlo.reshape_v1"(%1304) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1306 = "vhlo.transpose_v1"(%1305) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc528)
    %1307 = "vhlo.dot_general_v2"(%1302, %1306) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc529)
    %1308 = "vhlo.reshape_v1"(%1307) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc527)
    %1309 = "vhlo.reshape_v1"(%arg45) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1310 = "vhlo.custom_call_v1"(%1309) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1311 = "vhlo.reshape_v1"(%1310) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1312 = "vhlo.broadcast_in_dim_v1"(%1311) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc530)
    %1313 = "vhlo.add_v1"(%1308, %1312) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc530)
    %1314 = "vhlo.add_v1"(%1219, %1313) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc531)
    %1315 = "vhlo.reduce_v1"(%1314, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc533)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc532)
    %1316 = "vhlo.multiply_v1"(%1315, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc532)
    %1317 = "vhlo.broadcast_in_dim_v1"(%1316) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc534)
    %1318 = "vhlo.subtract_v1"(%1314, %1317) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc534)
    %1319 = "vhlo.multiply_v1"(%1318, %1318) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc532)
    %1320 = "vhlo.reduce_v1"(%1319, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc535)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc532)
    %1321 = "vhlo.multiply_v1"(%1320, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc532)
    %1322 = "vhlo.reshape_v1"(%1321) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc532)
    %1323 = "vhlo.add_v1"(%1322, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc536)
    %1324 = "vhlo.rsqrt_v2"(%1323) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc537)
    %1325 = "vhlo.reshape_v1"(%1324) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc538)
    %1326 = "vhlo.broadcast_in_dim_v1"(%1325) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc538)
    %1327 = "vhlo.multiply_v1"(%1318, %1326) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc538)
    %1328 = "vhlo.reshape_v1"(%arg44) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1329 = "vhlo.custom_call_v1"(%1328) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1330 = "vhlo.reshape_v1"(%1329) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1331 = "vhlo.broadcast_in_dim_v1"(%1330) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc538)
    %1332 = "vhlo.multiply_v1"(%1327, %1331) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc538)
    %1333 = "vhlo.reshape_v1"(%arg43) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1334 = "vhlo.custom_call_v1"(%1333) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1335 = "vhlo.reshape_v1"(%1334) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1336 = "vhlo.broadcast_in_dim_v1"(%1335) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc536)
    %1337 = "vhlo.add_v1"(%1332, %1336) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc536)
    %1338 = "vhlo.reshape_v1"(%1337) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc539)
    %1339 = "vhlo.reshape_v1"(%arg42) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1340 = "vhlo.custom_call_v1"(%1339) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %1341 = "vhlo.reshape_v1"(%1340) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1342 = "vhlo.transpose_v1"(%1341) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc540)
    %1343 = "vhlo.dot_general_v2"(%1338, %1342) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc541)
    %1344 = "vhlo.reshape_v1"(%1343) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc539)
    %1345 = "vhlo.reshape_v1"(%arg41) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %1346 = "vhlo.custom_call_v1"(%1345) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %1347 = "vhlo.reshape_v1"(%1346) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %1348 = "vhlo.broadcast_in_dim_v1"(%1347) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc542)
    %1349 = "vhlo.add_v1"(%1344, %1348) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc542)
    %1350 = "vhlo.multiply_v1"(%1349, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc543)
    %1351 = "vhlo.logistic_v2"(%1350) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc544)
    %1352 = "vhlo.multiply_v1"(%1349, %1351) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc543)
    %1353 = "vhlo.reshape_v1"(%1352) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc545)
    %1354 = "vhlo.reshape_v1"(%arg40) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1355 = "vhlo.custom_call_v1"(%1354) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %1356 = "vhlo.reshape_v1"(%1355) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1357 = "vhlo.transpose_v1"(%1356) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc546)
    %1358 = "vhlo.dot_general_v2"(%1353, %1357) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc547)
    %1359 = "vhlo.reshape_v1"(%1358) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc545)
    %1360 = "vhlo.reshape_v1"(%arg39) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1361 = "vhlo.custom_call_v1"(%1360) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_8_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1362 = "vhlo.reshape_v1"(%1361) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1363 = "vhlo.broadcast_in_dim_v1"(%1362) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc548)
    %1364 = "vhlo.add_v1"(%1359, %1363) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc548)
    %1365 = "vhlo.add_v1"(%1314, %1364) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc549)
    %1366 = "vhlo.reduce_v1"(%1365, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc551)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc550)
    %1367 = "vhlo.multiply_v1"(%1366, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc550)
    %1368 = "vhlo.broadcast_in_dim_v1"(%1367) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc552)
    %1369 = "vhlo.subtract_v1"(%1365, %1368) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc552)
    %1370 = "vhlo.multiply_v1"(%1369, %1369) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc550)
    %1371 = "vhlo.reduce_v1"(%1370, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc553)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc550)
    %1372 = "vhlo.multiply_v1"(%1371, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc550)
    %1373 = "vhlo.reshape_v1"(%1372) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc550)
    %1374 = "vhlo.add_v1"(%1373, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc554)
    %1375 = "vhlo.rsqrt_v2"(%1374) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc555)
    %1376 = "vhlo.reshape_v1"(%1375) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc556)
    %1377 = "vhlo.broadcast_in_dim_v1"(%1376) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc556)
    %1378 = "vhlo.multiply_v1"(%1369, %1377) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc556)
    %1379 = "vhlo.reshape_v1"(%arg38) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1380 = "vhlo.custom_call_v1"(%1379) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1381 = "vhlo.reshape_v1"(%1380) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1382 = "vhlo.broadcast_in_dim_v1"(%1381) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc556)
    %1383 = "vhlo.multiply_v1"(%1378, %1382) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc556)
    %1384 = "vhlo.reshape_v1"(%arg37) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1385 = "vhlo.custom_call_v1"(%1384) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1386 = "vhlo.reshape_v1"(%1385) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1387 = "vhlo.broadcast_in_dim_v1"(%1386) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc554)
    %1388 = "vhlo.add_v1"(%1383, %1387) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc554)
    %1389 = "vhlo.reshape_v1"(%1388) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc557)
    %1390 = "vhlo.reshape_v1"(%arg193) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1391 = "vhlo.custom_call_v1"(%1390) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1392 = "vhlo.reshape_v1"(%1391) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1393 = "vhlo.transpose_v1"(%1392) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc558)
    %1394 = "vhlo.dot_general_v2"(%1389, %1393) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc559)
    %1395 = "vhlo.reshape_v1"(%1394) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc557)
    %1396 = "vhlo.reshape_v1"(%arg192) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1397 = "vhlo.custom_call_v1"(%1396) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1398 = "vhlo.reshape_v1"(%1397) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1399 = "vhlo.broadcast_in_dim_v1"(%1398) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc560)
    %1400 = "vhlo.add_v1"(%1395, %1399) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc560)
    %1401 = "vhlo.reshape_v1"(%1400) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc561)
    %1402 = "vhlo.transpose_v1"(%1401) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc562)
    %1403 = "vhlo.reshape_v1"(%1402) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc563)
    %1404 = "vhlo.reshape_v1"(%arg191) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1405 = "vhlo.custom_call_v1"(%1404) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1406 = "vhlo.reshape_v1"(%1405) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1407 = "vhlo.transpose_v1"(%1406) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc564)
    %1408 = "vhlo.dot_general_v2"(%1389, %1407) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc565)
    %1409 = "vhlo.reshape_v1"(%1408) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc566)
    %1410 = "vhlo.reshape_v1"(%arg190) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1411 = "vhlo.custom_call_v1"(%1410) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1412 = "vhlo.reshape_v1"(%1411) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1413 = "vhlo.broadcast_in_dim_v1"(%1412) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc567)
    %1414 = "vhlo.add_v1"(%1409, %1413) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc567)
    %1415 = "vhlo.reshape_v1"(%1414) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc568)
    %1416 = "vhlo.transpose_v1"(%1415) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc569)
    %1417 = "vhlo.reshape_v1"(%1416) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc563)
    %1418 = "vhlo.dot_general_v2"(%1403, %1417) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc570)
    %1419 = "vhlo.reshape_v1"(%1418) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc563)
    %1420 = "vhlo.multiply_v1"(%1419, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc571)
    %1421 = "vhlo.convert_v1"(%1420) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc572)
    %1422 = "vhlo.reduce_v1"(%1421, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc574)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc573)
    %1423 = "vhlo.broadcast_in_dim_v1"(%1422) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc573)
    %1424 = "vhlo.subtract_v1"(%1421, %1423) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc573)
    %1425 = "vhlo.exponential_v2"(%1424) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc573)
    %1426 = "vhlo.reduce_v1"(%1425, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc575)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc573)
    %1427 = "vhlo.broadcast_in_dim_v1"(%1426) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc573)
    %1428 = "vhlo.divide_v1"(%1425, %1427) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc573)
    %1429 = "vhlo.convert_v1"(%1428) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc576)
    %1430 = "vhlo.reshape_v1"(%1429) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc577)
    %1431 = "vhlo.reshape_v1"(%arg36) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1432 = "vhlo.custom_call_v1"(%1431) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1433 = "vhlo.reshape_v1"(%1432) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1434 = "vhlo.transpose_v1"(%1433) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc578)
    %1435 = "vhlo.dot_general_v2"(%1389, %1434) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc579)
    %1436 = "vhlo.reshape_v1"(%1435) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc580)
    %1437 = "vhlo.reshape_v1"(%arg35) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1438 = "vhlo.custom_call_v1"(%1437) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1439 = "vhlo.reshape_v1"(%1438) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1440 = "vhlo.broadcast_in_dim_v1"(%1439) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc581)
    %1441 = "vhlo.add_v1"(%1436, %1440) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc581)
    %1442 = "vhlo.reshape_v1"(%1441) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc582)
    %1443 = "vhlo.transpose_v1"(%1442) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc583)
    %1444 = "vhlo.reshape_v1"(%1443) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc577)
    %1445 = "vhlo.dot_general_v2"(%1430, %1444) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc584)
    %1446 = "vhlo.reshape_v1"(%1445) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc577)
    %1447 = "vhlo.transpose_v1"(%1446) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc585)
    %1448 = "vhlo.reshape_v1"(%1447) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc586)
    %1449 = "vhlo.reshape_v1"(%arg34) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1450 = "vhlo.custom_call_v1"(%1449) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1451 = "vhlo.reshape_v1"(%1450) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1452 = "vhlo.transpose_v1"(%1451) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc587)
    %1453 = "vhlo.dot_general_v2"(%1448, %1452) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc588)
    %1454 = "vhlo.reshape_v1"(%1453) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc586)
    %1455 = "vhlo.reshape_v1"(%arg33) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1456 = "vhlo.custom_call_v1"(%1455) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1457 = "vhlo.reshape_v1"(%1456) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1458 = "vhlo.broadcast_in_dim_v1"(%1457) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc589)
    %1459 = "vhlo.add_v1"(%1454, %1458) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc589)
    %1460 = "vhlo.add_v1"(%1365, %1459) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc590)
    %1461 = "vhlo.reduce_v1"(%1460, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc592)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc591)
    %1462 = "vhlo.multiply_v1"(%1461, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc591)
    %1463 = "vhlo.broadcast_in_dim_v1"(%1462) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc593)
    %1464 = "vhlo.subtract_v1"(%1460, %1463) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc593)
    %1465 = "vhlo.multiply_v1"(%1464, %1464) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc591)
    %1466 = "vhlo.reduce_v1"(%1465, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc594)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc591)
    %1467 = "vhlo.multiply_v1"(%1466, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc591)
    %1468 = "vhlo.reshape_v1"(%1467) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc591)
    %1469 = "vhlo.add_v1"(%1468, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc595)
    %1470 = "vhlo.rsqrt_v2"(%1469) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc596)
    %1471 = "vhlo.reshape_v1"(%1470) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc597)
    %1472 = "vhlo.broadcast_in_dim_v1"(%1471) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc597)
    %1473 = "vhlo.multiply_v1"(%1464, %1472) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc597)
    %1474 = "vhlo.reshape_v1"(%arg32) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1475 = "vhlo.custom_call_v1"(%1474) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1476 = "vhlo.reshape_v1"(%1475) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1477 = "vhlo.broadcast_in_dim_v1"(%1476) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc597)
    %1478 = "vhlo.multiply_v1"(%1473, %1477) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc597)
    %1479 = "vhlo.reshape_v1"(%arg31) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1480 = "vhlo.custom_call_v1"(%1479) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1481 = "vhlo.reshape_v1"(%1480) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1482 = "vhlo.broadcast_in_dim_v1"(%1481) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc595)
    %1483 = "vhlo.add_v1"(%1478, %1482) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc595)
    %1484 = "vhlo.reshape_v1"(%1483) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc598)
    %1485 = "vhlo.reshape_v1"(%arg30) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1486 = "vhlo.custom_call_v1"(%1485) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %1487 = "vhlo.reshape_v1"(%1486) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1488 = "vhlo.transpose_v1"(%1487) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc599)
    %1489 = "vhlo.dot_general_v2"(%1484, %1488) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc600)
    %1490 = "vhlo.reshape_v1"(%1489) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc598)
    %1491 = "vhlo.reshape_v1"(%arg29) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %1492 = "vhlo.custom_call_v1"(%1491) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %1493 = "vhlo.reshape_v1"(%1492) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %1494 = "vhlo.broadcast_in_dim_v1"(%1493) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc601)
    %1495 = "vhlo.add_v1"(%1490, %1494) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc601)
    %1496 = "vhlo.multiply_v1"(%1495, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc602)
    %1497 = "vhlo.logistic_v2"(%1496) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc603)
    %1498 = "vhlo.multiply_v1"(%1495, %1497) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc602)
    %1499 = "vhlo.reshape_v1"(%1498) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc604)
    %1500 = "vhlo.reshape_v1"(%arg28) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1501 = "vhlo.custom_call_v1"(%1500) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %1502 = "vhlo.reshape_v1"(%1501) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1503 = "vhlo.transpose_v1"(%1502) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc605)
    %1504 = "vhlo.dot_general_v2"(%1499, %1503) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc606)
    %1505 = "vhlo.reshape_v1"(%1504) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc604)
    %1506 = "vhlo.reshape_v1"(%arg27) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1507 = "vhlo.custom_call_v1"(%1506) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_9_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1508 = "vhlo.reshape_v1"(%1507) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1509 = "vhlo.broadcast_in_dim_v1"(%1508) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc607)
    %1510 = "vhlo.add_v1"(%1505, %1509) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc607)
    %1511 = "vhlo.add_v1"(%1460, %1510) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc608)
    %1512 = "vhlo.reduce_v1"(%1511, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc610)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc609)
    %1513 = "vhlo.multiply_v1"(%1512, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc609)
    %1514 = "vhlo.broadcast_in_dim_v1"(%1513) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc611)
    %1515 = "vhlo.subtract_v1"(%1511, %1514) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc611)
    %1516 = "vhlo.multiply_v1"(%1515, %1515) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc609)
    %1517 = "vhlo.reduce_v1"(%1516, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc612)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc609)
    %1518 = "vhlo.multiply_v1"(%1517, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc609)
    %1519 = "vhlo.reshape_v1"(%1518) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc609)
    %1520 = "vhlo.add_v1"(%1519, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc613)
    %1521 = "vhlo.rsqrt_v2"(%1520) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc614)
    %1522 = "vhlo.reshape_v1"(%1521) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc615)
    %1523 = "vhlo.broadcast_in_dim_v1"(%1522) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc615)
    %1524 = "vhlo.multiply_v1"(%1515, %1523) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc615)
    %1525 = "vhlo.reshape_v1"(%arg26) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1526 = "vhlo.custom_call_v1"(%1525) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1527 = "vhlo.reshape_v1"(%1526) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1528 = "vhlo.broadcast_in_dim_v1"(%1527) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc615)
    %1529 = "vhlo.multiply_v1"(%1524, %1528) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc615)
    %1530 = "vhlo.reshape_v1"(%arg25) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1531 = "vhlo.custom_call_v1"(%1530) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1532 = "vhlo.reshape_v1"(%1531) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1533 = "vhlo.broadcast_in_dim_v1"(%1532) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc613)
    %1534 = "vhlo.add_v1"(%1529, %1533) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc613)
    %1535 = "vhlo.reshape_v1"(%1534) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc616)
    %1536 = "vhlo.reshape_v1"(%arg197) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1537 = "vhlo.custom_call_v1"(%1536) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1538 = "vhlo.reshape_v1"(%1537) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1539 = "vhlo.transpose_v1"(%1538) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc617)
    %1540 = "vhlo.dot_general_v2"(%1535, %1539) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc618)
    %1541 = "vhlo.reshape_v1"(%1540) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc616)
    %1542 = "vhlo.reshape_v1"(%arg196) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1543 = "vhlo.custom_call_v1"(%1542) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1544 = "vhlo.reshape_v1"(%1543) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1545 = "vhlo.broadcast_in_dim_v1"(%1544) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc619)
    %1546 = "vhlo.add_v1"(%1541, %1545) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc619)
    %1547 = "vhlo.reshape_v1"(%1546) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc620)
    %1548 = "vhlo.transpose_v1"(%1547) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc621)
    %1549 = "vhlo.reshape_v1"(%1548) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc622)
    %1550 = "vhlo.reshape_v1"(%arg195) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1551 = "vhlo.custom_call_v1"(%1550) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1552 = "vhlo.reshape_v1"(%1551) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1553 = "vhlo.transpose_v1"(%1552) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc623)
    %1554 = "vhlo.dot_general_v2"(%1535, %1553) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc624)
    %1555 = "vhlo.reshape_v1"(%1554) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc625)
    %1556 = "vhlo.reshape_v1"(%arg194) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1557 = "vhlo.custom_call_v1"(%1556) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1558 = "vhlo.reshape_v1"(%1557) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1559 = "vhlo.broadcast_in_dim_v1"(%1558) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc626)
    %1560 = "vhlo.add_v1"(%1555, %1559) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc626)
    %1561 = "vhlo.reshape_v1"(%1560) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc627)
    %1562 = "vhlo.transpose_v1"(%1561) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc628)
    %1563 = "vhlo.reshape_v1"(%1562) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc622)
    %1564 = "vhlo.dot_general_v2"(%1549, %1563) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc629)
    %1565 = "vhlo.reshape_v1"(%1564) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc622)
    %1566 = "vhlo.multiply_v1"(%1565, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc630)
    %1567 = "vhlo.convert_v1"(%1566) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc631)
    %1568 = "vhlo.reduce_v1"(%1567, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc633)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc632)
    %1569 = "vhlo.broadcast_in_dim_v1"(%1568) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc632)
    %1570 = "vhlo.subtract_v1"(%1567, %1569) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc632)
    %1571 = "vhlo.exponential_v2"(%1570) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc632)
    %1572 = "vhlo.reduce_v1"(%1571, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc634)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc632)
    %1573 = "vhlo.broadcast_in_dim_v1"(%1572) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc632)
    %1574 = "vhlo.divide_v1"(%1571, %1573) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc632)
    %1575 = "vhlo.convert_v1"(%1574) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc635)
    %1576 = "vhlo.reshape_v1"(%1575) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc636)
    %1577 = "vhlo.reshape_v1"(%arg24) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1578 = "vhlo.custom_call_v1"(%1577) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1579 = "vhlo.reshape_v1"(%1578) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1580 = "vhlo.transpose_v1"(%1579) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc637)
    %1581 = "vhlo.dot_general_v2"(%1535, %1580) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc638)
    %1582 = "vhlo.reshape_v1"(%1581) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc639)
    %1583 = "vhlo.reshape_v1"(%arg23) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1584 = "vhlo.custom_call_v1"(%1583) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1585 = "vhlo.reshape_v1"(%1584) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1586 = "vhlo.broadcast_in_dim_v1"(%1585) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc640)
    %1587 = "vhlo.add_v1"(%1582, %1586) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc640)
    %1588 = "vhlo.reshape_v1"(%1587) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc641)
    %1589 = "vhlo.transpose_v1"(%1588) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc642)
    %1590 = "vhlo.reshape_v1"(%1589) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc636)
    %1591 = "vhlo.dot_general_v2"(%1576, %1590) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc643)
    %1592 = "vhlo.reshape_v1"(%1591) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc636)
    %1593 = "vhlo.transpose_v1"(%1592) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc644)
    %1594 = "vhlo.reshape_v1"(%1593) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc645)
    %1595 = "vhlo.reshape_v1"(%arg22) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1596 = "vhlo.custom_call_v1"(%1595) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1597 = "vhlo.reshape_v1"(%1596) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1598 = "vhlo.transpose_v1"(%1597) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc646)
    %1599 = "vhlo.dot_general_v2"(%1594, %1598) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc647)
    %1600 = "vhlo.reshape_v1"(%1599) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc645)
    %1601 = "vhlo.reshape_v1"(%arg21) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1602 = "vhlo.custom_call_v1"(%1601) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1603 = "vhlo.reshape_v1"(%1602) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1604 = "vhlo.broadcast_in_dim_v1"(%1603) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc648)
    %1605 = "vhlo.add_v1"(%1600, %1604) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc648)
    %1606 = "vhlo.add_v1"(%1511, %1605) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc649)
    %1607 = "vhlo.reduce_v1"(%1606, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc651)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc650)
    %1608 = "vhlo.multiply_v1"(%1607, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc650)
    %1609 = "vhlo.broadcast_in_dim_v1"(%1608) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc652)
    %1610 = "vhlo.subtract_v1"(%1606, %1609) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc652)
    %1611 = "vhlo.multiply_v1"(%1610, %1610) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc650)
    %1612 = "vhlo.reduce_v1"(%1611, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc653)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc650)
    %1613 = "vhlo.multiply_v1"(%1612, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc650)
    %1614 = "vhlo.reshape_v1"(%1613) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc650)
    %1615 = "vhlo.add_v1"(%1614, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc654)
    %1616 = "vhlo.rsqrt_v2"(%1615) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc655)
    %1617 = "vhlo.reshape_v1"(%1616) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc656)
    %1618 = "vhlo.broadcast_in_dim_v1"(%1617) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc656)
    %1619 = "vhlo.multiply_v1"(%1610, %1618) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc656)
    %1620 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1621 = "vhlo.custom_call_v1"(%1620) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1622 = "vhlo.reshape_v1"(%1621) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1623 = "vhlo.broadcast_in_dim_v1"(%1622) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc656)
    %1624 = "vhlo.multiply_v1"(%1619, %1623) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc656)
    %1625 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1626 = "vhlo.custom_call_v1"(%1625) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1627 = "vhlo.reshape_v1"(%1626) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1628 = "vhlo.broadcast_in_dim_v1"(%1627) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc654)
    %1629 = "vhlo.add_v1"(%1624, %1628) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc654)
    %1630 = "vhlo.reshape_v1"(%1629) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc657)
    %1631 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1632 = "vhlo.custom_call_v1"(%1631) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %1633 = "vhlo.reshape_v1"(%1632) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1634 = "vhlo.transpose_v1"(%1633) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc658)
    %1635 = "vhlo.dot_general_v2"(%1630, %1634) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc659)
    %1636 = "vhlo.reshape_v1"(%1635) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc657)
    %1637 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %1638 = "vhlo.custom_call_v1"(%1637) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %1639 = "vhlo.reshape_v1"(%1638) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %1640 = "vhlo.broadcast_in_dim_v1"(%1639) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc660)
    %1641 = "vhlo.add_v1"(%1636, %1640) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc660)
    %1642 = "vhlo.multiply_v1"(%1641, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc661)
    %1643 = "vhlo.logistic_v2"(%1642) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc662)
    %1644 = "vhlo.multiply_v1"(%1641, %1643) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc661)
    %1645 = "vhlo.reshape_v1"(%1644) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc663)
    %1646 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1647 = "vhlo.custom_call_v1"(%1646) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %1648 = "vhlo.reshape_v1"(%1647) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1649 = "vhlo.transpose_v1"(%1648) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc664)
    %1650 = "vhlo.dot_general_v2"(%1645, %1649) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc665)
    %1651 = "vhlo.reshape_v1"(%1650) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc663)
    %1652 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1653 = "vhlo.custom_call_v1"(%1652) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_10_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1654 = "vhlo.reshape_v1"(%1653) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1655 = "vhlo.broadcast_in_dim_v1"(%1654) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc666)
    %1656 = "vhlo.add_v1"(%1651, %1655) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc666)
    %1657 = "vhlo.add_v1"(%1606, %1656) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc667)
    %1658 = "vhlo.reduce_v1"(%1657, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc669)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc668)
    %1659 = "vhlo.multiply_v1"(%1658, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc668)
    %1660 = "vhlo.broadcast_in_dim_v1"(%1659) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc670)
    %1661 = "vhlo.subtract_v1"(%1657, %1660) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc670)
    %1662 = "vhlo.multiply_v1"(%1661, %1661) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc668)
    %1663 = "vhlo.reduce_v1"(%1662, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc671)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc668)
    %1664 = "vhlo.multiply_v1"(%1663, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc668)
    %1665 = "vhlo.reshape_v1"(%1664) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc668)
    %1666 = "vhlo.add_v1"(%1665, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc672)
    %1667 = "vhlo.rsqrt_v2"(%1666) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc673)
    %1668 = "vhlo.reshape_v1"(%1667) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc674)
    %1669 = "vhlo.broadcast_in_dim_v1"(%1668) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc674)
    %1670 = "vhlo.multiply_v1"(%1661, %1669) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc674)
    %1671 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1672 = "vhlo.custom_call_v1"(%1671) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1673 = "vhlo.reshape_v1"(%1672) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1674 = "vhlo.broadcast_in_dim_v1"(%1673) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc674)
    %1675 = "vhlo.multiply_v1"(%1670, %1674) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc674)
    %1676 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1677 = "vhlo.custom_call_v1"(%1676) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1678 = "vhlo.reshape_v1"(%1677) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1679 = "vhlo.broadcast_in_dim_v1"(%1678) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc672)
    %1680 = "vhlo.add_v1"(%1675, %1679) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc672)
    %1681 = "vhlo.reshape_v1"(%1680) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc675)
    %1682 = "vhlo.reshape_v1"(%arg201) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1683 = "vhlo.custom_call_v1"(%1682) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1684 = "vhlo.reshape_v1"(%1683) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1685 = "vhlo.transpose_v1"(%1684) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc676)
    %1686 = "vhlo.dot_general_v2"(%1681, %1685) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc677)
    %1687 = "vhlo.reshape_v1"(%1686) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc675)
    %1688 = "vhlo.reshape_v1"(%arg200) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1689 = "vhlo.custom_call_v1"(%1688) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1690 = "vhlo.reshape_v1"(%1689) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1691 = "vhlo.broadcast_in_dim_v1"(%1690) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc678)
    %1692 = "vhlo.add_v1"(%1687, %1691) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc678)
    %1693 = "vhlo.reshape_v1"(%1692) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc679)
    %1694 = "vhlo.transpose_v1"(%1693) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc680)
    %1695 = "vhlo.reshape_v1"(%1694) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc681)
    %1696 = "vhlo.reshape_v1"(%arg199) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1697 = "vhlo.custom_call_v1"(%1696) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1698 = "vhlo.reshape_v1"(%1697) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1699 = "vhlo.transpose_v1"(%1698) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc682)
    %1700 = "vhlo.dot_general_v2"(%1681, %1699) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc683)
    %1701 = "vhlo.reshape_v1"(%1700) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc684)
    %1702 = "vhlo.reshape_v1"(%arg198) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1703 = "vhlo.custom_call_v1"(%1702) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1704 = "vhlo.reshape_v1"(%1703) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1705 = "vhlo.broadcast_in_dim_v1"(%1704) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc685)
    %1706 = "vhlo.add_v1"(%1701, %1705) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc685)
    %1707 = "vhlo.reshape_v1"(%1706) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc686)
    %1708 = "vhlo.transpose_v1"(%1707) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1> loc(#loc687)
    %1709 = "vhlo.reshape_v1"(%1708) : (!vhlo.tensor_v1<2x12x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1> loc(#loc681)
    %1710 = "vhlo.dot_general_v2"(%1695, %1709) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x64x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc688)
    %1711 = "vhlo.reshape_v1"(%1710) : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc681)
    %1712 = "vhlo.multiply_v1"(%1711, %3) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc689)
    %1713 = "vhlo.convert_v1"(%1712) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc690)
    %1714 = "vhlo.reduce_v1"(%1713, %7) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.maximum_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc692)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc691)
    %1715 = "vhlo.broadcast_in_dim_v1"(%1714) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc691)
    %1716 = "vhlo.subtract_v1"(%1713, %1715) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc691)
    %1717 = "vhlo.exponential_v2"(%1716) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc691)
    %1718 = "vhlo.reduce_v1"(%1717, %6) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax"), %arg203: !vhlo.tensor_v1<!vhlo.f32_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc693)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x!vhlo.f32_v1> loc(#loc691)
    %1719 = "vhlo.broadcast_in_dim_v1"(%1718) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x12x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc691)
    %1720 = "vhlo.divide_v1"(%1717, %1719) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>, !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1> loc(#loc691)
    %1721 = "vhlo.convert_v1"(%1720) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.f32_v1>) -> !vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1> loc(#loc694)
    %1722 = "vhlo.reshape_v1"(%1721) : (!vhlo.tensor_v1<2x12x50x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1> loc(#loc695)
    %1723 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1724 = "vhlo.custom_call_v1"(%1723) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1725 = "vhlo.reshape_v1"(%1724) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1726 = "vhlo.transpose_v1"(%1725) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc696)
    %1727 = "vhlo.dot_general_v2"(%1681, %1726) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc697)
    %1728 = "vhlo.reshape_v1"(%1727) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc698)
    %1729 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1730 = "vhlo.custom_call_v1"(%1729) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1731 = "vhlo.reshape_v1"(%1730) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1732 = "vhlo.broadcast_in_dim_v1"(%1731) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc699)
    %1733 = "vhlo.add_v1"(%1728, %1732) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc699)
    %1734 = "vhlo.reshape_v1"(%1733) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc700)
    %1735 = "vhlo.transpose_v1"(%1734) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,12,50,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc701)
    %1736 = "vhlo.reshape_v1"(%1735) : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc695)
    %1737 = "vhlo.dot_general_v2"(%1722, %1736) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x50x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1> loc(#loc702)
    %1738 = "vhlo.reshape_v1"(%1737) : (!vhlo.tensor_v1<24x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1> loc(#loc695)
    %1739 = "vhlo.transpose_v1"(%1738) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[2,50,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<2x12x50x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1> loc(#loc703)
    %1740 = "vhlo.reshape_v1"(%1739) : (!vhlo.tensor_v1<2x50x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc704)
    %1741 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc2)
    %1742 = "vhlo.custom_call_v1"(%1741) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc3)
    %1743 = "vhlo.reshape_v1"(%1742) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc2)
    %1744 = "vhlo.transpose_v1"(%1743) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc705)
    %1745 = "vhlo.dot_general_v2"(%1740, %1744) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc706)
    %1746 = "vhlo.reshape_v1"(%1745) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc704)
    %1747 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1748 = "vhlo.custom_call_v1"(%1747) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1749 = "vhlo.reshape_v1"(%1748) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1750 = "vhlo.broadcast_in_dim_v1"(%1749) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc707)
    %1751 = "vhlo.add_v1"(%1746, %1750) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc707)
    %1752 = "vhlo.add_v1"(%1657, %1751) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc708)
    %1753 = "vhlo.reduce_v1"(%1752, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc710)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc709)
    %1754 = "vhlo.multiply_v1"(%1753, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc709)
    %1755 = "vhlo.broadcast_in_dim_v1"(%1754) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc711)
    %1756 = "vhlo.subtract_v1"(%1752, %1755) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc711)
    %1757 = "vhlo.multiply_v1"(%1756, %1756) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc709)
    %1758 = "vhlo.reduce_v1"(%1757, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc712)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc709)
    %1759 = "vhlo.multiply_v1"(%1758, %5) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc709)
    %1760 = "vhlo.reshape_v1"(%1759) : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc709)
    %1761 = "vhlo.add_v1"(%1760, %4) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc713)
    %1762 = "vhlo.rsqrt_v2"(%1761) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1> loc(#loc714)
    %1763 = "vhlo.reshape_v1"(%1762) : (!vhlo.tensor_v1<2x50x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x!vhlo.bf16_v1> loc(#loc715)
    %1764 = "vhlo.broadcast_in_dim_v1"(%1763) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<2x50x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc715)
    %1765 = "vhlo.multiply_v1"(%1756, %1764) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc715)
    %1766 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1767 = "vhlo.custom_call_v1"(%1766) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1768 = "vhlo.reshape_v1"(%1767) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1769 = "vhlo.broadcast_in_dim_v1"(%1768) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc715)
    %1770 = "vhlo.multiply_v1"(%1765, %1769) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc715)
    %1771 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1772 = "vhlo.custom_call_v1"(%1771) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1773 = "vhlo.reshape_v1"(%1772) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1774 = "vhlo.broadcast_in_dim_v1"(%1773) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc713)
    %1775 = "vhlo.add_v1"(%1770, %1774) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc713)
    %1776 = "vhlo.reshape_v1"(%1775) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc716)
    %1777 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1778 = "vhlo.custom_call_v1"(%1777) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc3)
    %1779 = "vhlo.reshape_v1"(%1778) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc2)
    %1780 = "vhlo.transpose_v1"(%1779) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc717)
    %1781 = "vhlo.dot_general_v2"(%1776, %1780) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc718)
    %1782 = "vhlo.reshape_v1"(%1781) : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc716)
    %1783 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc2)
    %1784 = "vhlo.custom_call_v1"(%1783) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc3)
    %1785 = "vhlo.reshape_v1"(%1784) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc2)
    %1786 = "vhlo.broadcast_in_dim_v1"(%1785) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc719)
    %1787 = "vhlo.add_v1"(%1782, %1786) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc719)
    %1788 = "vhlo.multiply_v1"(%1787, %2) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc720)
    %1789 = "vhlo.logistic_v2"(%1788) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc721)
    %1790 = "vhlo.multiply_v1"(%1787, %1789) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1> loc(#loc720)
    %1791 = "vhlo.reshape_v1"(%1790) : (!vhlo.tensor_v1<2x50x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x3072x!vhlo.bf16_v1> loc(#loc722)
    %1792 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1793 = "vhlo.custom_call_v1"(%1792) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc3)
    %1794 = "vhlo.reshape_v1"(%1793) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc2)
    %1795 = "vhlo.transpose_v1"(%1794) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc723)
    %1796 = "vhlo.dot_general_v2"(%1791, %1795) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<100x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<100x768x!vhlo.bf16_v1> loc(#loc724)
    %1797 = "vhlo.reshape_v1"(%1796) : (!vhlo.tensor_v1<100x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc722)
    %1798 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1799 = "vhlo.custom_call_v1"(%1798) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_encoder_layers_11_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1800 = "vhlo.reshape_v1"(%1799) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1801 = "vhlo.broadcast_in_dim_v1"(%1800) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc725)
    %1802 = "vhlo.add_v1"(%1797, %1801) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc725)
    %1803 = "vhlo.add_v1"(%1752, %1802) : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1> loc(#loc726)
    %1804 = "vhlo.slice_v1"(%1803) <{limit_indices = #vhlo.tensor_v1<dense<[2, 1, 768]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<3xi64>>}> : (!vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x1x768x!vhlo.bf16_v1> loc(#loc727)
    %1805 = "vhlo.reshape_v1"(%1804) : (!vhlo.tensor_v1<2x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc728)
    %1806 = "vhlo.reduce_v1"(%1805, %8) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc730)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x!vhlo.bf16_v1> loc(#loc729)
    %1807 = "vhlo.multiply_v1"(%1806, %1) : (!vhlo.tensor_v1<2x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x!vhlo.bf16_v1> loc(#loc729)
    %1808 = "vhlo.broadcast_in_dim_v1"(%1807) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc731)
    %1809 = "vhlo.subtract_v1"(%1805, %1808) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc731)
    %1810 = "vhlo.multiply_v1"(%1809, %1809) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc729)
    %1811 = "vhlo.reduce_v1"(%1810, %8) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg202: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__var_mean"), %arg203: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__var_mean")):
      %1834 = "vhlo.add_v1"(%arg202, %arg203) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc732)
      "vhlo.return_v1"(%1834) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x!vhlo.bf16_v1> loc(#loc729)
    %1812 = "vhlo.multiply_v1"(%1811, %1) : (!vhlo.tensor_v1<2x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x!vhlo.bf16_v1> loc(#loc729)
    %1813 = "vhlo.reshape_v1"(%1812) : (!vhlo.tensor_v1<2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x1x!vhlo.bf16_v1> loc(#loc729)
    %1814 = "vhlo.add_v1"(%1813, %0) : (!vhlo.tensor_v1<2x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x1x!vhlo.bf16_v1> loc(#loc733)
    %1815 = "vhlo.rsqrt_v2"(%1814) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<2x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x1x!vhlo.bf16_v1> loc(#loc734)
    %1816 = "vhlo.reshape_v1"(%1815) : (!vhlo.tensor_v1<2x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x!vhlo.bf16_v1> loc(#loc735)
    %1817 = "vhlo.broadcast_in_dim_v1"(%1816) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc735)
    %1818 = "vhlo.multiply_v1"(%1809, %1817) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc735)
    %1819 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1820 = "vhlo.custom_call_v1"(%1819) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_post_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1821 = "vhlo.reshape_v1"(%1820) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1822 = "vhlo.broadcast_in_dim_v1"(%1821) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc735)
    %1823 = "vhlo.multiply_v1"(%1818, %1822) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc735)
    %1824 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %1825 = "vhlo.custom_call_v1"(%1824) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___vision_model_post_layernorm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc3)
    %1826 = "vhlo.reshape_v1"(%1825) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc2)
    %1827 = "vhlo.broadcast_in_dim_v1"(%1826) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc733)
    %1828 = "vhlo.add_v1"(%1823, %1827) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc733)
    %1829 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<512x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x768x!vhlo.bf16_v1> loc(#loc2)
    %1830 = "vhlo.custom_call_v1"(%1829) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___visual_projection_weight">}>} : (!vhlo.tensor_v1<1x512x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x768x!vhlo.bf16_v1> loc(#loc3)
    %1831 = "vhlo.reshape_v1"(%1830) : (!vhlo.tensor_v1<1x512x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x768x!vhlo.bf16_v1> loc(#loc2)
    %1832 = "vhlo.transpose_v1"(%1831) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,512]{0,1}">} : (!vhlo.tensor_v1<512x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x512x!vhlo.bf16_v1> loc(#loc736)
    %1833 = "vhlo.dot_general_v2"(%1828, %1832) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x512x!vhlo.bf16_v1> loc(#loc737)
    "vhlo.return_v1"(%1833, %1803) : (!vhlo.tensor_v1<2x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<2x50x768x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("unknown|unknown|-1|unknownaten__view")
#loc3 = loc("unknown|unknown|-1|unknownxla__custom_call")
#loc4 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|227|aten__expand")
#loc5 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Conv2d[vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|224|aten__convolution_overrideable")
#loc6 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|225|aten__view")
#loc7 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|225|aten__permute")
#loc8 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|228|aten__cat")
#loc9 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Embedding[vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__view")
#loc10 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Embedding[vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__index_select")
#loc11 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__add")
#loc13 = loc("add.886")
#loc14 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__sub")
#loc15 = loc("add.869")
#loc16 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__add")
#loc17 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__rsqrt")
#loc18 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__mul")
#loc20 = loc("add.961")
#loc21 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc22 = loc("add.944")
#loc23 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc24 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc25 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc26 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc27 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc28 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc29 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc30 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc31 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc32 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc33 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc34 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc35 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc36 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc37 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc38 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc39 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc40 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc41 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc43 = loc("maximum.1056")
#loc44 = loc("add.1065")
#loc45 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc46 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc47 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc48 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc49 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc50 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc51 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc52 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc53 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc54 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc55 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc56 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc57 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc58 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc59 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc61 = loc("add.1130")
#loc62 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc63 = loc("add.1113")
#loc64 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc65 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc66 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc67 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc68 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc69 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc70 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc71 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|QuickGELUActivation[vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc72 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|QuickGELUActivation[vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc73 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc74 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc75 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc76 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc77 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc79 = loc("add.1226")
#loc80 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc81 = loc("add.1209")
#loc82 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc83 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc84 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc85 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc86 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc87 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc88 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc89 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc90 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc91 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc92 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc93 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc94 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc95 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc96 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc97 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc98 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc99 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc100 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc102 = loc("maximum.1321")
#loc103 = loc("add.1330")
#loc104 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc105 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc106 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc107 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc108 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc109 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc110 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc111 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc112 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc113 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc114 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc115 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc116 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc117 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc118 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc120 = loc("add.1395")
#loc121 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc122 = loc("add.1378")
#loc123 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc124 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc125 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc126 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc127 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc128 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc129 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc130 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|QuickGELUActivation[vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc131 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|QuickGELUActivation[vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc132 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc133 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc134 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc135 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc136 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc138 = loc("add.1491")
#loc139 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc140 = loc("add.1474")
#loc141 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc142 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc143 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc144 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc145 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc146 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc147 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc148 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc149 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc150 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc151 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc152 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc153 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc154 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc155 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc156 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc157 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc158 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc159 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc161 = loc("maximum.1586")
#loc162 = loc("add.1595")
#loc163 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc164 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc165 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc166 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc167 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc168 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc169 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc170 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc171 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc172 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc173 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc174 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc175 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc176 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc177 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc179 = loc("add.1660")
#loc180 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc181 = loc("add.1643")
#loc182 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc183 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc184 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc185 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc186 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc187 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc188 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc189 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|QuickGELUActivation[vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc190 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|QuickGELUActivation[vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc191 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc192 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc193 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc194 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc195 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc197 = loc("add.1756")
#loc198 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc199 = loc("add.1739")
#loc200 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc201 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc202 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc203 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc204 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc205 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc206 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc207 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc208 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc209 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc210 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc211 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc212 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc213 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc214 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc215 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc216 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc217 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc218 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc220 = loc("maximum.1851")
#loc221 = loc("add.1860")
#loc222 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc223 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc224 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc225 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc226 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc227 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc228 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc229 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc230 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc231 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc232 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc233 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc234 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc235 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc236 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc238 = loc("add.1925")
#loc239 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc240 = loc("add.1908")
#loc241 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc242 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc243 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc244 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc245 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc246 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc247 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc248 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|QuickGELUActivation[vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc249 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|QuickGELUActivation[vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc250 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc251 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc252 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc253 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc254 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc256 = loc("add.2021")
#loc257 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc258 = loc("add.2004")
#loc259 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc260 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc261 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc262 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc263 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc264 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc265 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc266 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc267 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc268 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc269 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc270 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc271 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc272 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc273 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc274 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc275 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc276 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc277 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc279 = loc("maximum.2116")
#loc280 = loc("add.2125")
#loc281 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc282 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc283 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc284 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc285 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc286 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc287 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc288 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc289 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc290 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc291 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc292 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc293 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc294 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc295 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc297 = loc("add.2190")
#loc298 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc299 = loc("add.2173")
#loc300 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc301 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc302 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc303 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc304 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc305 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc306 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc307 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|QuickGELUActivation[vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc308 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|QuickGELUActivation[vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc309 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc310 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc311 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc312 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc313 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc315 = loc("add.2286")
#loc316 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc317 = loc("add.2269")
#loc318 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc319 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc320 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc321 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc322 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc323 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc324 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc325 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc326 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc327 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc328 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc329 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc330 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc331 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc332 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc333 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc334 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc335 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc336 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc338 = loc("maximum.2381")
#loc339 = loc("add.2390")
#loc340 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc341 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc342 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc343 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc344 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc345 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc346 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc347 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc348 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc349 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc350 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc351 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc352 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc353 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc354 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc356 = loc("add.2455")
#loc357 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc358 = loc("add.2438")
#loc359 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc360 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc361 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc362 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc363 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc364 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc365 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc366 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|QuickGELUActivation[vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc367 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|QuickGELUActivation[vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc368 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc369 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc370 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc371 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc372 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc374 = loc("add.2551")
#loc375 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc376 = loc("add.2534")
#loc377 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc378 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc379 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc380 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc381 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc382 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc383 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc384 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc385 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc386 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc387 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc388 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc389 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc390 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc391 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc392 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc393 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc394 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc395 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc397 = loc("maximum.2646")
#loc398 = loc("add.2655")
#loc399 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc400 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc401 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc402 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc403 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc404 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc405 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc406 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc407 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc408 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc409 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc410 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc411 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc412 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc413 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc415 = loc("add.2720")
#loc416 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc417 = loc("add.2703")
#loc418 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc419 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc420 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc421 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc422 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc423 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc424 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc425 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|QuickGELUActivation[vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc426 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|QuickGELUActivation[vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc427 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc428 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc429 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc430 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc431 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc433 = loc("add.2816")
#loc434 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc435 = loc("add.2799")
#loc436 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc437 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc438 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc439 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc440 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc441 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc442 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc443 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc444 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc445 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc446 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc447 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc448 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc449 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc450 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc451 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc452 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc453 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc454 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc456 = loc("maximum.2911")
#loc457 = loc("add.2920")
#loc458 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc459 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc460 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc461 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc462 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc463 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc464 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc465 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc466 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc467 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc468 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc469 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc470 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc471 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc472 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc474 = loc("add.2985")
#loc475 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc476 = loc("add.2968")
#loc477 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc478 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc479 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc480 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc481 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc482 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc483 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc484 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|QuickGELUActivation[vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc485 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|QuickGELUActivation[vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc486 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc487 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc488 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc489 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc490 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc492 = loc("add.3081")
#loc493 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc494 = loc("add.3064")
#loc495 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc496 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc497 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc498 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc499 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc500 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc501 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc502 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc503 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc504 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc505 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc506 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc507 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc508 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc509 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc510 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc511 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc512 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc513 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc515 = loc("maximum.3176")
#loc516 = loc("add.3185")
#loc517 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc518 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc519 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc520 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc521 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc522 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc523 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc524 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc525 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc526 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc527 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc528 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc529 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc530 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc531 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc533 = loc("add.3250")
#loc534 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc535 = loc("add.3233")
#loc536 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc537 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc538 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc539 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc540 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc541 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc542 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc543 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|QuickGELUActivation[vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc544 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|QuickGELUActivation[vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc545 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc546 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc547 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc548 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc549 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc551 = loc("add.3346")
#loc552 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc553 = loc("add.3329")
#loc554 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc555 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc556 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc557 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc558 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc559 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc560 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc561 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc562 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc563 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc564 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc565 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc566 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc567 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc568 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc569 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc570 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc571 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc572 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc574 = loc("maximum.3441")
#loc575 = loc("add.3450")
#loc576 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc577 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc578 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc579 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc580 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc581 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc582 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc583 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc584 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc585 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc586 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc587 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc588 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc589 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc590 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc592 = loc("add.3515")
#loc593 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc594 = loc("add.3498")
#loc595 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc596 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc597 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc598 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc599 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc600 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc601 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc602 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|QuickGELUActivation[vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc603 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|QuickGELUActivation[vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc604 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc605 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc606 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc607 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc608 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc610 = loc("add.3611")
#loc611 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc612 = loc("add.3594")
#loc613 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc614 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc615 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc616 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc617 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc618 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc619 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc620 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc621 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc622 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc623 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc624 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc625 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc626 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc627 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc628 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc629 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc630 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc631 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc633 = loc("maximum.3706")
#loc634 = loc("add.3715")
#loc635 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc636 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc637 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc638 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc639 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc640 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc641 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc642 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc643 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc644 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc645 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc646 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc647 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc648 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc649 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc651 = loc("add.3780")
#loc652 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc653 = loc("add.3763")
#loc654 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc655 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc656 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc657 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc658 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc659 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc660 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc661 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|QuickGELUActivation[vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc662 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|QuickGELUActivation[vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc663 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc664 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc665 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc666 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc667 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc669 = loc("add.3876")
#loc670 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc671 = loc("add.3859")
#loc672 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc673 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc674 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc675 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc676 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc677 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc678 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc679 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc680 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc681 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc682 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc683 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc684 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc685 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc686 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc687 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc688 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc689 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc690 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc692 = loc("maximum.3971")
#loc693 = loc("add.3980")
#loc694 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc695 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc696 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc697 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc698 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc699 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc700 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc701 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc702 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc703 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc704 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc705 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc706 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc707 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc708 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc710 = loc("add.4045")
#loc711 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc712 = loc("add.4028")
#loc713 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc714 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc715 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc716 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc717 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc718 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc719 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc720 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|QuickGELUActivation[vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc721 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|QuickGELUActivation[vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc722 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc723 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc724 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc725 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc726 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc727 = loc("CLIPVisionTransformer[vision_model]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|792|xla__generic_slice")
#loc728 = loc("CLIPVisionTransformer[vision_model]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|792|aten__view")
#loc730 = loc("add.4145")
#loc731 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__sub")
#loc732 = loc("add.4128")
#loc733 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__add")
#loc734 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__rsqrt")
#loc735 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__mul")
#loc736 = loc("Linear[visual_projection]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:1169|forward|1203|aten__permute")
#loc737 = loc("Linear[visual_projection]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:1169|forward|1203|aten__mm")
