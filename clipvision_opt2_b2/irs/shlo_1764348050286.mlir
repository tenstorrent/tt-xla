#loc1 = loc("unknown|unknown|-1|unknownxla__device_data")
module @SyncTensorsGraph.4178 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg1: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg2: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg3: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg4: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg5: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg6: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg7: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg8: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg9: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg10: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg11: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg12: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg13: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg14: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg15: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg16: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg17: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg18: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg19: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg20: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg21: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg22: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg23: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg24: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg25: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg26: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg27: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg28: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg29: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg30: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg31: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg32: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg33: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg34: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg35: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg36: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg37: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg38: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg39: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg40: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg41: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg42: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg43: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg44: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg45: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg46: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg47: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg48: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg49: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg50: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg51: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg52: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg53: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg54: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg55: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg56: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg57: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg58: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg59: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg60: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg61: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg62: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg63: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg64: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg65: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg66: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg67: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg68: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg69: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg70: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg71: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg72: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg73: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg74: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg75: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg76: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg77: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg78: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg79: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg80: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg81: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg82: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg83: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg84: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg85: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg86: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg87: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg88: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg89: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg90: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg91: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg92: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg93: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg94: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg95: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg96: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg97: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg98: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg99: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg100: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg101: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg102: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg103: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg104: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg105: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg106: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg107: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg108: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg109: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg110: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg111: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg112: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg113: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg114: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg115: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg116: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg117: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg118: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg119: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg120: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg121: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg122: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg123: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg124: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg125: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg126: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg127: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg128: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg129: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg130: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg131: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg132: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg133: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg134: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg135: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg136: tensor<768x3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg137: tensor<3072xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg138: tensor<3072x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg139: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg140: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg141: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg142: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg143: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg144: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg145: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg146: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg147: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg148: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg149: tensor<1x50xi64> loc("unknown|unknown|-1|unknownxla__device_data"), %arg150: tensor<50x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg151: tensor<768x3x32x32xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg152: tensor<2x3x224x224xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg153: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg154: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg155: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg156: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg157: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg158: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg159: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg160: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg161: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg162: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg163: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg164: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg165: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg166: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg167: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg168: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg169: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg170: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg171: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg172: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg173: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg174: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg175: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg176: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg177: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg178: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg179: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg180: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg181: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg182: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg183: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg184: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg185: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg186: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg187: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg188: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg189: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg190: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg191: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg192: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg193: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg194: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg195: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg196: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg197: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg198: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg199: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg200: tensor<768xbf16> loc("unknown|unknown|-1|unknownxla__device_data"), %arg201: tensor<768x768xbf16> loc("unknown|unknown|-1|unknownxla__device_data")) -> (tensor<2x512xbf16>, tensor<2x50x768xbf16>) {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<2x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<1.304630e-03> : tensor<2xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<1.703130e+00> : tensor<2x50x3072xbf16> loc(#loc)
    %cst_2 = stablehlo.constant dense<1.250000e-01> : tensor<2x12x50x50xbf16> loc(#loc)
    %cst_3 = stablehlo.constant dense<1.001360e-05> : tensor<2x50x1xbf16> loc(#loc)
    %cst_4 = stablehlo.constant dense<1.304630e-03> : tensor<2x50xbf16> loc(#loc)
    %cst_5 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_6 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reshape %arg153 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1 = stablehlo.custom_call @tt.mark_argument(%0) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_embeddings_class_embedding"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %2 = stablehlo.reshape %1 : (tensor<1x1x768xbf16>) -> tensor<1x768xbf16> loc(#loc4)
    %3 = stablehlo.broadcast_in_dim %2, dims = [1, 2] : (tensor<1x768xbf16>) -> tensor<2x1x768xbf16> loc(#loc4)
    %4 = stablehlo.custom_call @tt.mark_argument(%arg152) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<2x3x224x224xbf16>) -> tensor<2x3x224x224xbf16> loc(#loc3)
    %5 = stablehlo.custom_call @tt.mark_argument(%arg151) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_embeddings_patch_embedding_weight"}} : (tensor<768x3x32x32xbf16>) -> tensor<768x3x32x32xbf16> loc(#loc3)
    %6 = stablehlo.convolution(%4, %5) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [32, 32]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<2x3x224x224xbf16>, tensor<768x3x32x32xbf16>) -> tensor<2x768x7x7xbf16> loc(#loc5)
    %7 = stablehlo.reshape %6 : (tensor<2x768x7x7xbf16>) -> tensor<2x768x49xbf16> loc(#loc6)
    %8 = stablehlo.transpose %7, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "bf16[2,49,768]{1,2,0}"} : (tensor<2x768x49xbf16>) -> tensor<2x49x768xbf16> loc(#loc7)
    %9 = stablehlo.concatenate %3, %8, dim = 1 : (tensor<2x1x768xbf16>, tensor<2x49x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc8)
    %10 = stablehlo.reshape %arg150 : (tensor<50x768xbf16>) -> tensor<1x50x768xbf16> loc(#loc2)
    %11 = stablehlo.custom_call @tt.mark_argument(%10) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_embeddings_position_embedding_weight"}} : (tensor<1x50x768xbf16>) -> tensor<1x50x768xbf16> loc(#loc3)
    %12 = stablehlo.reshape %11 : (tensor<1x50x768xbf16>) -> tensor<50x768xbf16> loc(#loc2)
    %13 = stablehlo.reshape %arg149 : (tensor<1x50xi64>) -> tensor<1x1x50xi64> loc(#loc2)
    %14 = stablehlo.custom_call @tt.mark_argument(%13) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___vision_model_embeddings_position_ids"}} : (tensor<1x1x50xi64>) -> tensor<1x1x50xi64> loc(#loc3)
    %15 = stablehlo.reshape %14 : (tensor<1x1x50xi64>) -> tensor<50xi64> loc(#loc9)
    %16 = stablehlo.convert %15 : (tensor<50xi64>) -> tensor<50xui32> loc(#loc10)
    %17 = "stablehlo.gather"(%12, %16) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50x768xbf16>, tensor<50xui32>) -> tensor<50x768xbf16> loc(#loc10)
    %18 = stablehlo.broadcast_in_dim %17, dims = [1, 2] : (tensor<50x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc11)
    %19 = stablehlo.add %9, %18 : tensor<2x50x768xbf16> loc(#loc11)
    %20 = stablehlo.reduce(%19 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc12)
    %21 = stablehlo.multiply %20, %cst_4 : tensor<2x50xbf16> loc(#loc12)
    %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc13)
    %23 = stablehlo.subtract %19, %22 : tensor<2x50x768xbf16> loc(#loc13)
    %24 = stablehlo.multiply %23, %23 : tensor<2x50x768xbf16> loc(#loc12)
    %25 = stablehlo.reduce(%24 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc12)
    %26 = stablehlo.multiply %25, %cst_4 : tensor<2x50xbf16> loc(#loc12)
    %27 = stablehlo.reshape %26 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc12)
    %28 = stablehlo.add %27, %cst_3 : tensor<2x50x1xbf16> loc(#loc14)
    %29 = stablehlo.rsqrt %28 : tensor<2x50x1xbf16> loc(#loc15)
    %30 = stablehlo.reshape %29 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc16)
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc16)
    %32 = stablehlo.multiply %23, %31 : tensor<2x50x768xbf16> loc(#loc16)
    %33 = stablehlo.reshape %arg148 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %34 = stablehlo.custom_call @tt.mark_argument(%33) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_pre_layrnorm_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %35 = stablehlo.reshape %34 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %36 = stablehlo.broadcast_in_dim %35, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc16)
    %37 = stablehlo.multiply %32, %36 : tensor<2x50x768xbf16> loc(#loc16)
    %38 = stablehlo.reshape %arg147 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %39 = stablehlo.custom_call @tt.mark_argument(%38) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_pre_layrnorm_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %40 = stablehlo.reshape %39 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %41 = stablehlo.broadcast_in_dim %40, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc14)
    %42 = stablehlo.add %37, %41 : tensor<2x50x768xbf16> loc(#loc14)
    %43 = stablehlo.reduce(%42 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc17)
    %44 = stablehlo.multiply %43, %cst_4 : tensor<2x50xbf16> loc(#loc17)
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc18)
    %46 = stablehlo.subtract %42, %45 : tensor<2x50x768xbf16> loc(#loc18)
    %47 = stablehlo.multiply %46, %46 : tensor<2x50x768xbf16> loc(#loc17)
    %48 = stablehlo.reduce(%47 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc17)
    %49 = stablehlo.multiply %48, %cst_4 : tensor<2x50xbf16> loc(#loc17)
    %50 = stablehlo.reshape %49 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc17)
    %51 = stablehlo.add %50, %cst_3 : tensor<2x50x1xbf16> loc(#loc19)
    %52 = stablehlo.rsqrt %51 : tensor<2x50x1xbf16> loc(#loc20)
    %53 = stablehlo.reshape %52 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc21)
    %54 = stablehlo.broadcast_in_dim %53, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc21)
    %55 = stablehlo.multiply %46, %54 : tensor<2x50x768xbf16> loc(#loc21)
    %56 = stablehlo.reshape %arg146 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %57 = stablehlo.custom_call @tt.mark_argument(%56) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %58 = stablehlo.reshape %57 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %59 = stablehlo.broadcast_in_dim %58, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc21)
    %60 = stablehlo.multiply %55, %59 : tensor<2x50x768xbf16> loc(#loc21)
    %61 = stablehlo.reshape %arg145 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %62 = stablehlo.custom_call @tt.mark_argument(%61) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %63 = stablehlo.reshape %62 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %64 = stablehlo.broadcast_in_dim %63, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc19)
    %65 = stablehlo.add %60, %64 : tensor<2x50x768xbf16> loc(#loc19)
    %66 = stablehlo.reshape %65 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc22)
    %67 = stablehlo.reshape %arg157 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %68 = stablehlo.custom_call @tt.mark_argument(%67) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %69 = stablehlo.reshape %68 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %70 = stablehlo.transpose %69, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc23)
    %71 = stablehlo.dot_general %66, %70, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc24)
    %72 = stablehlo.reshape %71 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc22)
    %73 = stablehlo.reshape %arg156 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %74 = stablehlo.custom_call @tt.mark_argument(%73) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc25)
    %77 = stablehlo.add %72, %76 : tensor<2x50x768xbf16> loc(#loc25)
    %78 = stablehlo.reshape %77 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc26)
    %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc27)
    %80 = stablehlo.reshape %79 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc28)
    %81 = stablehlo.reshape %arg155 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %82 = stablehlo.custom_call @tt.mark_argument(%81) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %83 = stablehlo.reshape %82 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %84 = stablehlo.transpose %83, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc29)
    %85 = stablehlo.dot_general %66, %84, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc30)
    %86 = stablehlo.reshape %85 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc31)
    %87 = stablehlo.reshape %arg154 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %88 = stablehlo.custom_call @tt.mark_argument(%87) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %89 = stablehlo.reshape %88 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %90 = stablehlo.broadcast_in_dim %89, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc32)
    %91 = stablehlo.add %86, %90 : tensor<2x50x768xbf16> loc(#loc32)
    %92 = stablehlo.reshape %91 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc33)
    %93 = stablehlo.transpose %92, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc34)
    %94 = stablehlo.reshape %93 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc28)
    %95 = stablehlo.dot_general %80, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc35)
    %96 = stablehlo.reshape %95 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc28)
    %97 = stablehlo.multiply %96, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc36)
    %98 = stablehlo.convert %97 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc37)
    %99 = stablehlo.reduce(%98 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc38)
    %100 = stablehlo.broadcast_in_dim %99, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc38)
    %101 = stablehlo.subtract %98, %100 : tensor<2x12x50x50xf32> loc(#loc38)
    %102 = stablehlo.exponential %101 : tensor<2x12x50x50xf32> loc(#loc38)
    %103 = stablehlo.reduce(%102 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc38)
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc38)
    %105 = stablehlo.divide %102, %104 : tensor<2x12x50x50xf32> loc(#loc38)
    %106 = stablehlo.convert %105 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc39)
    %107 = stablehlo.reshape %106 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc40)
    %108 = stablehlo.reshape %arg144 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %109 = stablehlo.custom_call @tt.mark_argument(%108) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %110 = stablehlo.reshape %109 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %111 = stablehlo.transpose %110, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc41)
    %112 = stablehlo.dot_general %66, %111, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc42)
    %113 = stablehlo.reshape %112 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc43)
    %114 = stablehlo.reshape %arg143 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %115 = stablehlo.custom_call @tt.mark_argument(%114) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %116 = stablehlo.reshape %115 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %117 = stablehlo.broadcast_in_dim %116, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc44)
    %118 = stablehlo.add %113, %117 : tensor<2x50x768xbf16> loc(#loc44)
    %119 = stablehlo.reshape %118 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc45)
    %120 = stablehlo.transpose %119, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc46)
    %121 = stablehlo.reshape %120 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc40)
    %122 = stablehlo.dot_general %107, %121, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc47)
    %123 = stablehlo.reshape %122 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc40)
    %124 = stablehlo.transpose %123, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc48)
    %125 = stablehlo.reshape %124 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc49)
    %126 = stablehlo.reshape %arg142 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %127 = stablehlo.custom_call @tt.mark_argument(%126) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %128 = stablehlo.reshape %127 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %129 = stablehlo.transpose %128, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc50)
    %130 = stablehlo.dot_general %125, %129, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc51)
    %131 = stablehlo.reshape %130 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc49)
    %132 = stablehlo.reshape %arg141 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %133 = stablehlo.custom_call @tt.mark_argument(%132) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %134 = stablehlo.reshape %133 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %135 = stablehlo.broadcast_in_dim %134, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc52)
    %136 = stablehlo.add %131, %135 : tensor<2x50x768xbf16> loc(#loc52)
    %137 = stablehlo.add %42, %136 : tensor<2x50x768xbf16> loc(#loc53)
    %138 = stablehlo.reduce(%137 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc54)
    %139 = stablehlo.multiply %138, %cst_4 : tensor<2x50xbf16> loc(#loc54)
    %140 = stablehlo.broadcast_in_dim %139, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc55)
    %141 = stablehlo.subtract %137, %140 : tensor<2x50x768xbf16> loc(#loc55)
    %142 = stablehlo.multiply %141, %141 : tensor<2x50x768xbf16> loc(#loc54)
    %143 = stablehlo.reduce(%142 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc54)
    %144 = stablehlo.multiply %143, %cst_4 : tensor<2x50xbf16> loc(#loc54)
    %145 = stablehlo.reshape %144 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc54)
    %146 = stablehlo.add %145, %cst_3 : tensor<2x50x1xbf16> loc(#loc56)
    %147 = stablehlo.rsqrt %146 : tensor<2x50x1xbf16> loc(#loc57)
    %148 = stablehlo.reshape %147 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc58)
    %149 = stablehlo.broadcast_in_dim %148, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc58)
    %150 = stablehlo.multiply %141, %149 : tensor<2x50x768xbf16> loc(#loc58)
    %151 = stablehlo.reshape %arg140 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %152 = stablehlo.custom_call @tt.mark_argument(%151) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %153 = stablehlo.reshape %152 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %154 = stablehlo.broadcast_in_dim %153, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc58)
    %155 = stablehlo.multiply %150, %154 : tensor<2x50x768xbf16> loc(#loc58)
    %156 = stablehlo.reshape %arg139 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %157 = stablehlo.custom_call @tt.mark_argument(%156) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %159 = stablehlo.broadcast_in_dim %158, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc56)
    %160 = stablehlo.add %155, %159 : tensor<2x50x768xbf16> loc(#loc56)
    %161 = stablehlo.reshape %160 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc59)
    %162 = stablehlo.reshape %arg138 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %163 = stablehlo.custom_call @tt.mark_argument(%162) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %164 = stablehlo.reshape %163 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %165 = stablehlo.transpose %164, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc60)
    %166 = stablehlo.dot_general %161, %165, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc61)
    %167 = stablehlo.reshape %166 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc59)
    %168 = stablehlo.reshape %arg137 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %169 = stablehlo.custom_call @tt.mark_argument(%168) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %170 = stablehlo.reshape %169 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %171 = stablehlo.broadcast_in_dim %170, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc62)
    %172 = stablehlo.add %167, %171 : tensor<2x50x3072xbf16> loc(#loc62)
    %173 = stablehlo.multiply %172, %cst_1 : tensor<2x50x3072xbf16> loc(#loc63)
    %174 = stablehlo.logistic %173 : tensor<2x50x3072xbf16> loc(#loc64)
    %175 = stablehlo.multiply %172, %174 : tensor<2x50x3072xbf16> loc(#loc63)
    %176 = stablehlo.reshape %175 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc65)
    %177 = stablehlo.reshape %arg136 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %178 = stablehlo.custom_call @tt.mark_argument(%177) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %179 = stablehlo.reshape %178 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %180 = stablehlo.transpose %179, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc66)
    %181 = stablehlo.dot_general %176, %180, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc67)
    %182 = stablehlo.reshape %181 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc65)
    %183 = stablehlo.reshape %arg135 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %184 = stablehlo.custom_call @tt.mark_argument(%183) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_0_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %185 = stablehlo.reshape %184 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %186 = stablehlo.broadcast_in_dim %185, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc68)
    %187 = stablehlo.add %182, %186 : tensor<2x50x768xbf16> loc(#loc68)
    %188 = stablehlo.add %137, %187 : tensor<2x50x768xbf16> loc(#loc69)
    %189 = stablehlo.reduce(%188 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc70)
    %190 = stablehlo.multiply %189, %cst_4 : tensor<2x50xbf16> loc(#loc70)
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc71)
    %192 = stablehlo.subtract %188, %191 : tensor<2x50x768xbf16> loc(#loc71)
    %193 = stablehlo.multiply %192, %192 : tensor<2x50x768xbf16> loc(#loc70)
    %194 = stablehlo.reduce(%193 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc70)
    %195 = stablehlo.multiply %194, %cst_4 : tensor<2x50xbf16> loc(#loc70)
    %196 = stablehlo.reshape %195 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc70)
    %197 = stablehlo.add %196, %cst_3 : tensor<2x50x1xbf16> loc(#loc72)
    %198 = stablehlo.rsqrt %197 : tensor<2x50x1xbf16> loc(#loc73)
    %199 = stablehlo.reshape %198 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc74)
    %200 = stablehlo.broadcast_in_dim %199, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc74)
    %201 = stablehlo.multiply %192, %200 : tensor<2x50x768xbf16> loc(#loc74)
    %202 = stablehlo.reshape %arg134 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %203 = stablehlo.custom_call @tt.mark_argument(%202) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %204 = stablehlo.reshape %203 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %205 = stablehlo.broadcast_in_dim %204, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc74)
    %206 = stablehlo.multiply %201, %205 : tensor<2x50x768xbf16> loc(#loc74)
    %207 = stablehlo.reshape %arg133 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %208 = stablehlo.custom_call @tt.mark_argument(%207) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %209 = stablehlo.reshape %208 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %210 = stablehlo.broadcast_in_dim %209, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc72)
    %211 = stablehlo.add %206, %210 : tensor<2x50x768xbf16> loc(#loc72)
    %212 = stablehlo.reshape %211 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc75)
    %213 = stablehlo.reshape %arg161 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %214 = stablehlo.custom_call @tt.mark_argument(%213) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %215 = stablehlo.reshape %214 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %216 = stablehlo.transpose %215, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc76)
    %217 = stablehlo.dot_general %212, %216, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc77)
    %218 = stablehlo.reshape %217 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc75)
    %219 = stablehlo.reshape %arg160 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %220 = stablehlo.custom_call @tt.mark_argument(%219) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %221 = stablehlo.reshape %220 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %222 = stablehlo.broadcast_in_dim %221, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc78)
    %223 = stablehlo.add %218, %222 : tensor<2x50x768xbf16> loc(#loc78)
    %224 = stablehlo.reshape %223 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc79)
    %225 = stablehlo.transpose %224, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc80)
    %226 = stablehlo.reshape %225 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc81)
    %227 = stablehlo.reshape %arg159 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %228 = stablehlo.custom_call @tt.mark_argument(%227) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %229 = stablehlo.reshape %228 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %230 = stablehlo.transpose %229, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc82)
    %231 = stablehlo.dot_general %212, %230, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc83)
    %232 = stablehlo.reshape %231 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc84)
    %233 = stablehlo.reshape %arg158 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %234 = stablehlo.custom_call @tt.mark_argument(%233) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %235 = stablehlo.reshape %234 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %236 = stablehlo.broadcast_in_dim %235, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc85)
    %237 = stablehlo.add %232, %236 : tensor<2x50x768xbf16> loc(#loc85)
    %238 = stablehlo.reshape %237 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc86)
    %239 = stablehlo.transpose %238, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc87)
    %240 = stablehlo.reshape %239 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc81)
    %241 = stablehlo.dot_general %226, %240, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc88)
    %242 = stablehlo.reshape %241 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc81)
    %243 = stablehlo.multiply %242, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc89)
    %244 = stablehlo.convert %243 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc90)
    %245 = stablehlo.reduce(%244 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc91)
    %246 = stablehlo.broadcast_in_dim %245, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc91)
    %247 = stablehlo.subtract %244, %246 : tensor<2x12x50x50xf32> loc(#loc91)
    %248 = stablehlo.exponential %247 : tensor<2x12x50x50xf32> loc(#loc91)
    %249 = stablehlo.reduce(%248 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc91)
    %250 = stablehlo.broadcast_in_dim %249, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc91)
    %251 = stablehlo.divide %248, %250 : tensor<2x12x50x50xf32> loc(#loc91)
    %252 = stablehlo.convert %251 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc92)
    %253 = stablehlo.reshape %252 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc93)
    %254 = stablehlo.reshape %arg132 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %255 = stablehlo.custom_call @tt.mark_argument(%254) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %256 = stablehlo.reshape %255 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %257 = stablehlo.transpose %256, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc94)
    %258 = stablehlo.dot_general %212, %257, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc95)
    %259 = stablehlo.reshape %258 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc96)
    %260 = stablehlo.reshape %arg131 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %261 = stablehlo.custom_call @tt.mark_argument(%260) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %262 = stablehlo.reshape %261 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %263 = stablehlo.broadcast_in_dim %262, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc97)
    %264 = stablehlo.add %259, %263 : tensor<2x50x768xbf16> loc(#loc97)
    %265 = stablehlo.reshape %264 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc98)
    %266 = stablehlo.transpose %265, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc99)
    %267 = stablehlo.reshape %266 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc93)
    %268 = stablehlo.dot_general %253, %267, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc100)
    %269 = stablehlo.reshape %268 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc93)
    %270 = stablehlo.transpose %269, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc101)
    %271 = stablehlo.reshape %270 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc102)
    %272 = stablehlo.reshape %arg130 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %273 = stablehlo.custom_call @tt.mark_argument(%272) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %274 = stablehlo.reshape %273 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %275 = stablehlo.transpose %274, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc103)
    %276 = stablehlo.dot_general %271, %275, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc104)
    %277 = stablehlo.reshape %276 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc102)
    %278 = stablehlo.reshape %arg129 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %279 = stablehlo.custom_call @tt.mark_argument(%278) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %280 = stablehlo.reshape %279 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %281 = stablehlo.broadcast_in_dim %280, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc105)
    %282 = stablehlo.add %277, %281 : tensor<2x50x768xbf16> loc(#loc105)
    %283 = stablehlo.add %188, %282 : tensor<2x50x768xbf16> loc(#loc106)
    %284 = stablehlo.reduce(%283 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc107)
    %285 = stablehlo.multiply %284, %cst_4 : tensor<2x50xbf16> loc(#loc107)
    %286 = stablehlo.broadcast_in_dim %285, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc108)
    %287 = stablehlo.subtract %283, %286 : tensor<2x50x768xbf16> loc(#loc108)
    %288 = stablehlo.multiply %287, %287 : tensor<2x50x768xbf16> loc(#loc107)
    %289 = stablehlo.reduce(%288 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc107)
    %290 = stablehlo.multiply %289, %cst_4 : tensor<2x50xbf16> loc(#loc107)
    %291 = stablehlo.reshape %290 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc107)
    %292 = stablehlo.add %291, %cst_3 : tensor<2x50x1xbf16> loc(#loc109)
    %293 = stablehlo.rsqrt %292 : tensor<2x50x1xbf16> loc(#loc110)
    %294 = stablehlo.reshape %293 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc111)
    %295 = stablehlo.broadcast_in_dim %294, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc111)
    %296 = stablehlo.multiply %287, %295 : tensor<2x50x768xbf16> loc(#loc111)
    %297 = stablehlo.reshape %arg128 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %298 = stablehlo.custom_call @tt.mark_argument(%297) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %299 = stablehlo.reshape %298 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %300 = stablehlo.broadcast_in_dim %299, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc111)
    %301 = stablehlo.multiply %296, %300 : tensor<2x50x768xbf16> loc(#loc111)
    %302 = stablehlo.reshape %arg127 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %303 = stablehlo.custom_call @tt.mark_argument(%302) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %304 = stablehlo.reshape %303 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %305 = stablehlo.broadcast_in_dim %304, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc109)
    %306 = stablehlo.add %301, %305 : tensor<2x50x768xbf16> loc(#loc109)
    %307 = stablehlo.reshape %306 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc112)
    %308 = stablehlo.reshape %arg126 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %309 = stablehlo.custom_call @tt.mark_argument(%308) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %310 = stablehlo.reshape %309 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %311 = stablehlo.transpose %310, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc113)
    %312 = stablehlo.dot_general %307, %311, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc114)
    %313 = stablehlo.reshape %312 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc112)
    %314 = stablehlo.reshape %arg125 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %315 = stablehlo.custom_call @tt.mark_argument(%314) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %316 = stablehlo.reshape %315 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %317 = stablehlo.broadcast_in_dim %316, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc115)
    %318 = stablehlo.add %313, %317 : tensor<2x50x3072xbf16> loc(#loc115)
    %319 = stablehlo.multiply %318, %cst_1 : tensor<2x50x3072xbf16> loc(#loc116)
    %320 = stablehlo.logistic %319 : tensor<2x50x3072xbf16> loc(#loc117)
    %321 = stablehlo.multiply %318, %320 : tensor<2x50x3072xbf16> loc(#loc116)
    %322 = stablehlo.reshape %321 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc118)
    %323 = stablehlo.reshape %arg124 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %324 = stablehlo.custom_call @tt.mark_argument(%323) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %325 = stablehlo.reshape %324 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %326 = stablehlo.transpose %325, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc119)
    %327 = stablehlo.dot_general %322, %326, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc120)
    %328 = stablehlo.reshape %327 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc118)
    %329 = stablehlo.reshape %arg123 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %330 = stablehlo.custom_call @tt.mark_argument(%329) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_1_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %331 = stablehlo.reshape %330 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %332 = stablehlo.broadcast_in_dim %331, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc121)
    %333 = stablehlo.add %328, %332 : tensor<2x50x768xbf16> loc(#loc121)
    %334 = stablehlo.add %283, %333 : tensor<2x50x768xbf16> loc(#loc122)
    %335 = stablehlo.reduce(%334 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc123)
    %336 = stablehlo.multiply %335, %cst_4 : tensor<2x50xbf16> loc(#loc123)
    %337 = stablehlo.broadcast_in_dim %336, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc124)
    %338 = stablehlo.subtract %334, %337 : tensor<2x50x768xbf16> loc(#loc124)
    %339 = stablehlo.multiply %338, %338 : tensor<2x50x768xbf16> loc(#loc123)
    %340 = stablehlo.reduce(%339 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc123)
    %341 = stablehlo.multiply %340, %cst_4 : tensor<2x50xbf16> loc(#loc123)
    %342 = stablehlo.reshape %341 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc123)
    %343 = stablehlo.add %342, %cst_3 : tensor<2x50x1xbf16> loc(#loc125)
    %344 = stablehlo.rsqrt %343 : tensor<2x50x1xbf16> loc(#loc126)
    %345 = stablehlo.reshape %344 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc127)
    %346 = stablehlo.broadcast_in_dim %345, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc127)
    %347 = stablehlo.multiply %338, %346 : tensor<2x50x768xbf16> loc(#loc127)
    %348 = stablehlo.reshape %arg122 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %349 = stablehlo.custom_call @tt.mark_argument(%348) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %350 = stablehlo.reshape %349 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %351 = stablehlo.broadcast_in_dim %350, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc127)
    %352 = stablehlo.multiply %347, %351 : tensor<2x50x768xbf16> loc(#loc127)
    %353 = stablehlo.reshape %arg121 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %354 = stablehlo.custom_call @tt.mark_argument(%353) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %355 = stablehlo.reshape %354 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %356 = stablehlo.broadcast_in_dim %355, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc125)
    %357 = stablehlo.add %352, %356 : tensor<2x50x768xbf16> loc(#loc125)
    %358 = stablehlo.reshape %357 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc128)
    %359 = stablehlo.reshape %arg165 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %360 = stablehlo.custom_call @tt.mark_argument(%359) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %361 = stablehlo.reshape %360 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %362 = stablehlo.transpose %361, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc129)
    %363 = stablehlo.dot_general %358, %362, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc130)
    %364 = stablehlo.reshape %363 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc128)
    %365 = stablehlo.reshape %arg164 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %366 = stablehlo.custom_call @tt.mark_argument(%365) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %367 = stablehlo.reshape %366 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %368 = stablehlo.broadcast_in_dim %367, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc131)
    %369 = stablehlo.add %364, %368 : tensor<2x50x768xbf16> loc(#loc131)
    %370 = stablehlo.reshape %369 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc132)
    %371 = stablehlo.transpose %370, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc133)
    %372 = stablehlo.reshape %371 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc134)
    %373 = stablehlo.reshape %arg163 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %374 = stablehlo.custom_call @tt.mark_argument(%373) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %375 = stablehlo.reshape %374 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %376 = stablehlo.transpose %375, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc135)
    %377 = stablehlo.dot_general %358, %376, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc136)
    %378 = stablehlo.reshape %377 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc137)
    %379 = stablehlo.reshape %arg162 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %380 = stablehlo.custom_call @tt.mark_argument(%379) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %381 = stablehlo.reshape %380 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %382 = stablehlo.broadcast_in_dim %381, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc138)
    %383 = stablehlo.add %378, %382 : tensor<2x50x768xbf16> loc(#loc138)
    %384 = stablehlo.reshape %383 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc139)
    %385 = stablehlo.transpose %384, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc140)
    %386 = stablehlo.reshape %385 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc134)
    %387 = stablehlo.dot_general %372, %386, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc141)
    %388 = stablehlo.reshape %387 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc134)
    %389 = stablehlo.multiply %388, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc142)
    %390 = stablehlo.convert %389 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc143)
    %391 = stablehlo.reduce(%390 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc144)
    %392 = stablehlo.broadcast_in_dim %391, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc144)
    %393 = stablehlo.subtract %390, %392 : tensor<2x12x50x50xf32> loc(#loc144)
    %394 = stablehlo.exponential %393 : tensor<2x12x50x50xf32> loc(#loc144)
    %395 = stablehlo.reduce(%394 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc144)
    %396 = stablehlo.broadcast_in_dim %395, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc144)
    %397 = stablehlo.divide %394, %396 : tensor<2x12x50x50xf32> loc(#loc144)
    %398 = stablehlo.convert %397 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc145)
    %399 = stablehlo.reshape %398 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc146)
    %400 = stablehlo.reshape %arg120 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %401 = stablehlo.custom_call @tt.mark_argument(%400) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %402 = stablehlo.reshape %401 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %403 = stablehlo.transpose %402, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc147)
    %404 = stablehlo.dot_general %358, %403, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc148)
    %405 = stablehlo.reshape %404 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc149)
    %406 = stablehlo.reshape %arg119 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %407 = stablehlo.custom_call @tt.mark_argument(%406) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %408 = stablehlo.reshape %407 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %409 = stablehlo.broadcast_in_dim %408, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc150)
    %410 = stablehlo.add %405, %409 : tensor<2x50x768xbf16> loc(#loc150)
    %411 = stablehlo.reshape %410 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc151)
    %412 = stablehlo.transpose %411, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc152)
    %413 = stablehlo.reshape %412 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc146)
    %414 = stablehlo.dot_general %399, %413, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc153)
    %415 = stablehlo.reshape %414 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc146)
    %416 = stablehlo.transpose %415, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc154)
    %417 = stablehlo.reshape %416 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc155)
    %418 = stablehlo.reshape %arg118 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %419 = stablehlo.custom_call @tt.mark_argument(%418) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %420 = stablehlo.reshape %419 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %421 = stablehlo.transpose %420, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc156)
    %422 = stablehlo.dot_general %417, %421, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc157)
    %423 = stablehlo.reshape %422 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc155)
    %424 = stablehlo.reshape %arg117 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %425 = stablehlo.custom_call @tt.mark_argument(%424) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %426 = stablehlo.reshape %425 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %427 = stablehlo.broadcast_in_dim %426, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc158)
    %428 = stablehlo.add %423, %427 : tensor<2x50x768xbf16> loc(#loc158)
    %429 = stablehlo.add %334, %428 : tensor<2x50x768xbf16> loc(#loc159)
    %430 = stablehlo.reduce(%429 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc160)
    %431 = stablehlo.multiply %430, %cst_4 : tensor<2x50xbf16> loc(#loc160)
    %432 = stablehlo.broadcast_in_dim %431, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc161)
    %433 = stablehlo.subtract %429, %432 : tensor<2x50x768xbf16> loc(#loc161)
    %434 = stablehlo.multiply %433, %433 : tensor<2x50x768xbf16> loc(#loc160)
    %435 = stablehlo.reduce(%434 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc160)
    %436 = stablehlo.multiply %435, %cst_4 : tensor<2x50xbf16> loc(#loc160)
    %437 = stablehlo.reshape %436 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc160)
    %438 = stablehlo.add %437, %cst_3 : tensor<2x50x1xbf16> loc(#loc162)
    %439 = stablehlo.rsqrt %438 : tensor<2x50x1xbf16> loc(#loc163)
    %440 = stablehlo.reshape %439 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc164)
    %441 = stablehlo.broadcast_in_dim %440, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc164)
    %442 = stablehlo.multiply %433, %441 : tensor<2x50x768xbf16> loc(#loc164)
    %443 = stablehlo.reshape %arg116 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %444 = stablehlo.custom_call @tt.mark_argument(%443) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %445 = stablehlo.reshape %444 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %446 = stablehlo.broadcast_in_dim %445, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc164)
    %447 = stablehlo.multiply %442, %446 : tensor<2x50x768xbf16> loc(#loc164)
    %448 = stablehlo.reshape %arg115 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %449 = stablehlo.custom_call @tt.mark_argument(%448) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %450 = stablehlo.reshape %449 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %451 = stablehlo.broadcast_in_dim %450, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc162)
    %452 = stablehlo.add %447, %451 : tensor<2x50x768xbf16> loc(#loc162)
    %453 = stablehlo.reshape %452 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc165)
    %454 = stablehlo.reshape %arg114 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %455 = stablehlo.custom_call @tt.mark_argument(%454) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %456 = stablehlo.reshape %455 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %457 = stablehlo.transpose %456, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc166)
    %458 = stablehlo.dot_general %453, %457, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc167)
    %459 = stablehlo.reshape %458 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc165)
    %460 = stablehlo.reshape %arg113 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %461 = stablehlo.custom_call @tt.mark_argument(%460) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %462 = stablehlo.reshape %461 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %463 = stablehlo.broadcast_in_dim %462, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc168)
    %464 = stablehlo.add %459, %463 : tensor<2x50x3072xbf16> loc(#loc168)
    %465 = stablehlo.multiply %464, %cst_1 : tensor<2x50x3072xbf16> loc(#loc169)
    %466 = stablehlo.logistic %465 : tensor<2x50x3072xbf16> loc(#loc170)
    %467 = stablehlo.multiply %464, %466 : tensor<2x50x3072xbf16> loc(#loc169)
    %468 = stablehlo.reshape %467 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc171)
    %469 = stablehlo.reshape %arg112 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %470 = stablehlo.custom_call @tt.mark_argument(%469) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %471 = stablehlo.reshape %470 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %472 = stablehlo.transpose %471, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc172)
    %473 = stablehlo.dot_general %468, %472, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc173)
    %474 = stablehlo.reshape %473 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc171)
    %475 = stablehlo.reshape %arg111 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %476 = stablehlo.custom_call @tt.mark_argument(%475) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_2_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %477 = stablehlo.reshape %476 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %478 = stablehlo.broadcast_in_dim %477, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc174)
    %479 = stablehlo.add %474, %478 : tensor<2x50x768xbf16> loc(#loc174)
    %480 = stablehlo.add %429, %479 : tensor<2x50x768xbf16> loc(#loc175)
    %481 = stablehlo.reduce(%480 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc176)
    %482 = stablehlo.multiply %481, %cst_4 : tensor<2x50xbf16> loc(#loc176)
    %483 = stablehlo.broadcast_in_dim %482, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc177)
    %484 = stablehlo.subtract %480, %483 : tensor<2x50x768xbf16> loc(#loc177)
    %485 = stablehlo.multiply %484, %484 : tensor<2x50x768xbf16> loc(#loc176)
    %486 = stablehlo.reduce(%485 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc176)
    %487 = stablehlo.multiply %486, %cst_4 : tensor<2x50xbf16> loc(#loc176)
    %488 = stablehlo.reshape %487 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc176)
    %489 = stablehlo.add %488, %cst_3 : tensor<2x50x1xbf16> loc(#loc178)
    %490 = stablehlo.rsqrt %489 : tensor<2x50x1xbf16> loc(#loc179)
    %491 = stablehlo.reshape %490 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc180)
    %492 = stablehlo.broadcast_in_dim %491, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc180)
    %493 = stablehlo.multiply %484, %492 : tensor<2x50x768xbf16> loc(#loc180)
    %494 = stablehlo.reshape %arg110 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %495 = stablehlo.custom_call @tt.mark_argument(%494) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %496 = stablehlo.reshape %495 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %497 = stablehlo.broadcast_in_dim %496, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc180)
    %498 = stablehlo.multiply %493, %497 : tensor<2x50x768xbf16> loc(#loc180)
    %499 = stablehlo.reshape %arg109 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %500 = stablehlo.custom_call @tt.mark_argument(%499) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %501 = stablehlo.reshape %500 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %502 = stablehlo.broadcast_in_dim %501, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc178)
    %503 = stablehlo.add %498, %502 : tensor<2x50x768xbf16> loc(#loc178)
    %504 = stablehlo.reshape %503 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc181)
    %505 = stablehlo.reshape %arg169 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %506 = stablehlo.custom_call @tt.mark_argument(%505) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %507 = stablehlo.reshape %506 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %508 = stablehlo.transpose %507, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc182)
    %509 = stablehlo.dot_general %504, %508, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc183)
    %510 = stablehlo.reshape %509 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc181)
    %511 = stablehlo.reshape %arg168 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %512 = stablehlo.custom_call @tt.mark_argument(%511) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %513 = stablehlo.reshape %512 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %514 = stablehlo.broadcast_in_dim %513, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc184)
    %515 = stablehlo.add %510, %514 : tensor<2x50x768xbf16> loc(#loc184)
    %516 = stablehlo.reshape %515 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc185)
    %517 = stablehlo.transpose %516, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc186)
    %518 = stablehlo.reshape %517 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc187)
    %519 = stablehlo.reshape %arg167 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %520 = stablehlo.custom_call @tt.mark_argument(%519) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %521 = stablehlo.reshape %520 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %522 = stablehlo.transpose %521, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc188)
    %523 = stablehlo.dot_general %504, %522, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc189)
    %524 = stablehlo.reshape %523 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc190)
    %525 = stablehlo.reshape %arg166 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %526 = stablehlo.custom_call @tt.mark_argument(%525) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %527 = stablehlo.reshape %526 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %528 = stablehlo.broadcast_in_dim %527, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc191)
    %529 = stablehlo.add %524, %528 : tensor<2x50x768xbf16> loc(#loc191)
    %530 = stablehlo.reshape %529 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc192)
    %531 = stablehlo.transpose %530, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc193)
    %532 = stablehlo.reshape %531 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc187)
    %533 = stablehlo.dot_general %518, %532, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc194)
    %534 = stablehlo.reshape %533 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc187)
    %535 = stablehlo.multiply %534, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc195)
    %536 = stablehlo.convert %535 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc196)
    %537 = stablehlo.reduce(%536 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc197)
    %538 = stablehlo.broadcast_in_dim %537, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc197)
    %539 = stablehlo.subtract %536, %538 : tensor<2x12x50x50xf32> loc(#loc197)
    %540 = stablehlo.exponential %539 : tensor<2x12x50x50xf32> loc(#loc197)
    %541 = stablehlo.reduce(%540 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc197)
    %542 = stablehlo.broadcast_in_dim %541, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc197)
    %543 = stablehlo.divide %540, %542 : tensor<2x12x50x50xf32> loc(#loc197)
    %544 = stablehlo.convert %543 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc198)
    %545 = stablehlo.reshape %544 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc199)
    %546 = stablehlo.reshape %arg108 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %547 = stablehlo.custom_call @tt.mark_argument(%546) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %548 = stablehlo.reshape %547 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %549 = stablehlo.transpose %548, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc200)
    %550 = stablehlo.dot_general %504, %549, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc201)
    %551 = stablehlo.reshape %550 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc202)
    %552 = stablehlo.reshape %arg107 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %553 = stablehlo.custom_call @tt.mark_argument(%552) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %554 = stablehlo.reshape %553 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %555 = stablehlo.broadcast_in_dim %554, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc203)
    %556 = stablehlo.add %551, %555 : tensor<2x50x768xbf16> loc(#loc203)
    %557 = stablehlo.reshape %556 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc204)
    %558 = stablehlo.transpose %557, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc205)
    %559 = stablehlo.reshape %558 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc199)
    %560 = stablehlo.dot_general %545, %559, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc206)
    %561 = stablehlo.reshape %560 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc199)
    %562 = stablehlo.transpose %561, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc207)
    %563 = stablehlo.reshape %562 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc208)
    %564 = stablehlo.reshape %arg106 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %565 = stablehlo.custom_call @tt.mark_argument(%564) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %566 = stablehlo.reshape %565 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %567 = stablehlo.transpose %566, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc209)
    %568 = stablehlo.dot_general %563, %567, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc210)
    %569 = stablehlo.reshape %568 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc208)
    %570 = stablehlo.reshape %arg105 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %571 = stablehlo.custom_call @tt.mark_argument(%570) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %572 = stablehlo.reshape %571 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %573 = stablehlo.broadcast_in_dim %572, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc211)
    %574 = stablehlo.add %569, %573 : tensor<2x50x768xbf16> loc(#loc211)
    %575 = stablehlo.add %480, %574 : tensor<2x50x768xbf16> loc(#loc212)
    %576 = stablehlo.reduce(%575 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc213)
    %577 = stablehlo.multiply %576, %cst_4 : tensor<2x50xbf16> loc(#loc213)
    %578 = stablehlo.broadcast_in_dim %577, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc214)
    %579 = stablehlo.subtract %575, %578 : tensor<2x50x768xbf16> loc(#loc214)
    %580 = stablehlo.multiply %579, %579 : tensor<2x50x768xbf16> loc(#loc213)
    %581 = stablehlo.reduce(%580 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc213)
    %582 = stablehlo.multiply %581, %cst_4 : tensor<2x50xbf16> loc(#loc213)
    %583 = stablehlo.reshape %582 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc213)
    %584 = stablehlo.add %583, %cst_3 : tensor<2x50x1xbf16> loc(#loc215)
    %585 = stablehlo.rsqrt %584 : tensor<2x50x1xbf16> loc(#loc216)
    %586 = stablehlo.reshape %585 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc217)
    %587 = stablehlo.broadcast_in_dim %586, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc217)
    %588 = stablehlo.multiply %579, %587 : tensor<2x50x768xbf16> loc(#loc217)
    %589 = stablehlo.reshape %arg104 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %590 = stablehlo.custom_call @tt.mark_argument(%589) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %591 = stablehlo.reshape %590 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %592 = stablehlo.broadcast_in_dim %591, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc217)
    %593 = stablehlo.multiply %588, %592 : tensor<2x50x768xbf16> loc(#loc217)
    %594 = stablehlo.reshape %arg103 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %595 = stablehlo.custom_call @tt.mark_argument(%594) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %596 = stablehlo.reshape %595 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %597 = stablehlo.broadcast_in_dim %596, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc215)
    %598 = stablehlo.add %593, %597 : tensor<2x50x768xbf16> loc(#loc215)
    %599 = stablehlo.reshape %598 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc218)
    %600 = stablehlo.reshape %arg102 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %601 = stablehlo.custom_call @tt.mark_argument(%600) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %602 = stablehlo.reshape %601 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %603 = stablehlo.transpose %602, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc219)
    %604 = stablehlo.dot_general %599, %603, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc220)
    %605 = stablehlo.reshape %604 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc218)
    %606 = stablehlo.reshape %arg101 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %607 = stablehlo.custom_call @tt.mark_argument(%606) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %608 = stablehlo.reshape %607 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %609 = stablehlo.broadcast_in_dim %608, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc221)
    %610 = stablehlo.add %605, %609 : tensor<2x50x3072xbf16> loc(#loc221)
    %611 = stablehlo.multiply %610, %cst_1 : tensor<2x50x3072xbf16> loc(#loc222)
    %612 = stablehlo.logistic %611 : tensor<2x50x3072xbf16> loc(#loc223)
    %613 = stablehlo.multiply %610, %612 : tensor<2x50x3072xbf16> loc(#loc222)
    %614 = stablehlo.reshape %613 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc224)
    %615 = stablehlo.reshape %arg100 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %616 = stablehlo.custom_call @tt.mark_argument(%615) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %617 = stablehlo.reshape %616 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %618 = stablehlo.transpose %617, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc225)
    %619 = stablehlo.dot_general %614, %618, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc226)
    %620 = stablehlo.reshape %619 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc224)
    %621 = stablehlo.reshape %arg99 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %622 = stablehlo.custom_call @tt.mark_argument(%621) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_3_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %623 = stablehlo.reshape %622 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %624 = stablehlo.broadcast_in_dim %623, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc227)
    %625 = stablehlo.add %620, %624 : tensor<2x50x768xbf16> loc(#loc227)
    %626 = stablehlo.add %575, %625 : tensor<2x50x768xbf16> loc(#loc228)
    %627 = stablehlo.reduce(%626 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc229)
    %628 = stablehlo.multiply %627, %cst_4 : tensor<2x50xbf16> loc(#loc229)
    %629 = stablehlo.broadcast_in_dim %628, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc230)
    %630 = stablehlo.subtract %626, %629 : tensor<2x50x768xbf16> loc(#loc230)
    %631 = stablehlo.multiply %630, %630 : tensor<2x50x768xbf16> loc(#loc229)
    %632 = stablehlo.reduce(%631 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc229)
    %633 = stablehlo.multiply %632, %cst_4 : tensor<2x50xbf16> loc(#loc229)
    %634 = stablehlo.reshape %633 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc229)
    %635 = stablehlo.add %634, %cst_3 : tensor<2x50x1xbf16> loc(#loc231)
    %636 = stablehlo.rsqrt %635 : tensor<2x50x1xbf16> loc(#loc232)
    %637 = stablehlo.reshape %636 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc233)
    %638 = stablehlo.broadcast_in_dim %637, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc233)
    %639 = stablehlo.multiply %630, %638 : tensor<2x50x768xbf16> loc(#loc233)
    %640 = stablehlo.reshape %arg98 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %641 = stablehlo.custom_call @tt.mark_argument(%640) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %642 = stablehlo.reshape %641 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %643 = stablehlo.broadcast_in_dim %642, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc233)
    %644 = stablehlo.multiply %639, %643 : tensor<2x50x768xbf16> loc(#loc233)
    %645 = stablehlo.reshape %arg97 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %646 = stablehlo.custom_call @tt.mark_argument(%645) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %647 = stablehlo.reshape %646 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %648 = stablehlo.broadcast_in_dim %647, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc231)
    %649 = stablehlo.add %644, %648 : tensor<2x50x768xbf16> loc(#loc231)
    %650 = stablehlo.reshape %649 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc234)
    %651 = stablehlo.reshape %arg173 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %652 = stablehlo.custom_call @tt.mark_argument(%651) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %653 = stablehlo.reshape %652 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %654 = stablehlo.transpose %653, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc235)
    %655 = stablehlo.dot_general %650, %654, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc236)
    %656 = stablehlo.reshape %655 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc234)
    %657 = stablehlo.reshape %arg172 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %658 = stablehlo.custom_call @tt.mark_argument(%657) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %659 = stablehlo.reshape %658 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %660 = stablehlo.broadcast_in_dim %659, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc237)
    %661 = stablehlo.add %656, %660 : tensor<2x50x768xbf16> loc(#loc237)
    %662 = stablehlo.reshape %661 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc238)
    %663 = stablehlo.transpose %662, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc239)
    %664 = stablehlo.reshape %663 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc240)
    %665 = stablehlo.reshape %arg171 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %666 = stablehlo.custom_call @tt.mark_argument(%665) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %667 = stablehlo.reshape %666 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %668 = stablehlo.transpose %667, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc241)
    %669 = stablehlo.dot_general %650, %668, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc242)
    %670 = stablehlo.reshape %669 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc243)
    %671 = stablehlo.reshape %arg170 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %672 = stablehlo.custom_call @tt.mark_argument(%671) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %673 = stablehlo.reshape %672 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %674 = stablehlo.broadcast_in_dim %673, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc244)
    %675 = stablehlo.add %670, %674 : tensor<2x50x768xbf16> loc(#loc244)
    %676 = stablehlo.reshape %675 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc245)
    %677 = stablehlo.transpose %676, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc246)
    %678 = stablehlo.reshape %677 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc240)
    %679 = stablehlo.dot_general %664, %678, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc247)
    %680 = stablehlo.reshape %679 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc240)
    %681 = stablehlo.multiply %680, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc248)
    %682 = stablehlo.convert %681 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc249)
    %683 = stablehlo.reduce(%682 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc250)
    %684 = stablehlo.broadcast_in_dim %683, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc250)
    %685 = stablehlo.subtract %682, %684 : tensor<2x12x50x50xf32> loc(#loc250)
    %686 = stablehlo.exponential %685 : tensor<2x12x50x50xf32> loc(#loc250)
    %687 = stablehlo.reduce(%686 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc250)
    %688 = stablehlo.broadcast_in_dim %687, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc250)
    %689 = stablehlo.divide %686, %688 : tensor<2x12x50x50xf32> loc(#loc250)
    %690 = stablehlo.convert %689 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc251)
    %691 = stablehlo.reshape %690 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc252)
    %692 = stablehlo.reshape %arg96 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %693 = stablehlo.custom_call @tt.mark_argument(%692) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %694 = stablehlo.reshape %693 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %695 = stablehlo.transpose %694, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc253)
    %696 = stablehlo.dot_general %650, %695, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc254)
    %697 = stablehlo.reshape %696 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc255)
    %698 = stablehlo.reshape %arg95 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %699 = stablehlo.custom_call @tt.mark_argument(%698) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %700 = stablehlo.reshape %699 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %701 = stablehlo.broadcast_in_dim %700, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc256)
    %702 = stablehlo.add %697, %701 : tensor<2x50x768xbf16> loc(#loc256)
    %703 = stablehlo.reshape %702 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc257)
    %704 = stablehlo.transpose %703, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc258)
    %705 = stablehlo.reshape %704 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc252)
    %706 = stablehlo.dot_general %691, %705, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc259)
    %707 = stablehlo.reshape %706 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc252)
    %708 = stablehlo.transpose %707, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc260)
    %709 = stablehlo.reshape %708 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc261)
    %710 = stablehlo.reshape %arg94 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %711 = stablehlo.custom_call @tt.mark_argument(%710) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %712 = stablehlo.reshape %711 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %713 = stablehlo.transpose %712, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc262)
    %714 = stablehlo.dot_general %709, %713, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc263)
    %715 = stablehlo.reshape %714 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc261)
    %716 = stablehlo.reshape %arg93 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %717 = stablehlo.custom_call @tt.mark_argument(%716) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %718 = stablehlo.reshape %717 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %719 = stablehlo.broadcast_in_dim %718, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc264)
    %720 = stablehlo.add %715, %719 : tensor<2x50x768xbf16> loc(#loc264)
    %721 = stablehlo.add %626, %720 : tensor<2x50x768xbf16> loc(#loc265)
    %722 = stablehlo.reduce(%721 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc266)
    %723 = stablehlo.multiply %722, %cst_4 : tensor<2x50xbf16> loc(#loc266)
    %724 = stablehlo.broadcast_in_dim %723, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc267)
    %725 = stablehlo.subtract %721, %724 : tensor<2x50x768xbf16> loc(#loc267)
    %726 = stablehlo.multiply %725, %725 : tensor<2x50x768xbf16> loc(#loc266)
    %727 = stablehlo.reduce(%726 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc266)
    %728 = stablehlo.multiply %727, %cst_4 : tensor<2x50xbf16> loc(#loc266)
    %729 = stablehlo.reshape %728 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc266)
    %730 = stablehlo.add %729, %cst_3 : tensor<2x50x1xbf16> loc(#loc268)
    %731 = stablehlo.rsqrt %730 : tensor<2x50x1xbf16> loc(#loc269)
    %732 = stablehlo.reshape %731 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc270)
    %733 = stablehlo.broadcast_in_dim %732, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc270)
    %734 = stablehlo.multiply %725, %733 : tensor<2x50x768xbf16> loc(#loc270)
    %735 = stablehlo.reshape %arg92 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %736 = stablehlo.custom_call @tt.mark_argument(%735) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %737 = stablehlo.reshape %736 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %738 = stablehlo.broadcast_in_dim %737, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc270)
    %739 = stablehlo.multiply %734, %738 : tensor<2x50x768xbf16> loc(#loc270)
    %740 = stablehlo.reshape %arg91 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %741 = stablehlo.custom_call @tt.mark_argument(%740) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %742 = stablehlo.reshape %741 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %743 = stablehlo.broadcast_in_dim %742, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc268)
    %744 = stablehlo.add %739, %743 : tensor<2x50x768xbf16> loc(#loc268)
    %745 = stablehlo.reshape %744 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc271)
    %746 = stablehlo.reshape %arg90 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %747 = stablehlo.custom_call @tt.mark_argument(%746) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %748 = stablehlo.reshape %747 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %749 = stablehlo.transpose %748, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc272)
    %750 = stablehlo.dot_general %745, %749, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc273)
    %751 = stablehlo.reshape %750 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc271)
    %752 = stablehlo.reshape %arg89 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %753 = stablehlo.custom_call @tt.mark_argument(%752) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %754 = stablehlo.reshape %753 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %755 = stablehlo.broadcast_in_dim %754, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc274)
    %756 = stablehlo.add %751, %755 : tensor<2x50x3072xbf16> loc(#loc274)
    %757 = stablehlo.multiply %756, %cst_1 : tensor<2x50x3072xbf16> loc(#loc275)
    %758 = stablehlo.logistic %757 : tensor<2x50x3072xbf16> loc(#loc276)
    %759 = stablehlo.multiply %756, %758 : tensor<2x50x3072xbf16> loc(#loc275)
    %760 = stablehlo.reshape %759 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc277)
    %761 = stablehlo.reshape %arg88 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %762 = stablehlo.custom_call @tt.mark_argument(%761) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %763 = stablehlo.reshape %762 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %764 = stablehlo.transpose %763, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc278)
    %765 = stablehlo.dot_general %760, %764, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc279)
    %766 = stablehlo.reshape %765 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc277)
    %767 = stablehlo.reshape %arg87 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %768 = stablehlo.custom_call @tt.mark_argument(%767) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_4_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %769 = stablehlo.reshape %768 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %770 = stablehlo.broadcast_in_dim %769, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc280)
    %771 = stablehlo.add %766, %770 : tensor<2x50x768xbf16> loc(#loc280)
    %772 = stablehlo.add %721, %771 : tensor<2x50x768xbf16> loc(#loc281)
    %773 = stablehlo.reduce(%772 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc282)
    %774 = stablehlo.multiply %773, %cst_4 : tensor<2x50xbf16> loc(#loc282)
    %775 = stablehlo.broadcast_in_dim %774, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc283)
    %776 = stablehlo.subtract %772, %775 : tensor<2x50x768xbf16> loc(#loc283)
    %777 = stablehlo.multiply %776, %776 : tensor<2x50x768xbf16> loc(#loc282)
    %778 = stablehlo.reduce(%777 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc282)
    %779 = stablehlo.multiply %778, %cst_4 : tensor<2x50xbf16> loc(#loc282)
    %780 = stablehlo.reshape %779 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc282)
    %781 = stablehlo.add %780, %cst_3 : tensor<2x50x1xbf16> loc(#loc284)
    %782 = stablehlo.rsqrt %781 : tensor<2x50x1xbf16> loc(#loc285)
    %783 = stablehlo.reshape %782 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc286)
    %784 = stablehlo.broadcast_in_dim %783, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc286)
    %785 = stablehlo.multiply %776, %784 : tensor<2x50x768xbf16> loc(#loc286)
    %786 = stablehlo.reshape %arg86 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %787 = stablehlo.custom_call @tt.mark_argument(%786) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %788 = stablehlo.reshape %787 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %789 = stablehlo.broadcast_in_dim %788, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc286)
    %790 = stablehlo.multiply %785, %789 : tensor<2x50x768xbf16> loc(#loc286)
    %791 = stablehlo.reshape %arg85 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %792 = stablehlo.custom_call @tt.mark_argument(%791) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %793 = stablehlo.reshape %792 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %794 = stablehlo.broadcast_in_dim %793, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc284)
    %795 = stablehlo.add %790, %794 : tensor<2x50x768xbf16> loc(#loc284)
    %796 = stablehlo.reshape %795 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc287)
    %797 = stablehlo.reshape %arg177 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %798 = stablehlo.custom_call @tt.mark_argument(%797) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %799 = stablehlo.reshape %798 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %800 = stablehlo.transpose %799, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc288)
    %801 = stablehlo.dot_general %796, %800, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc289)
    %802 = stablehlo.reshape %801 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc287)
    %803 = stablehlo.reshape %arg176 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %804 = stablehlo.custom_call @tt.mark_argument(%803) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %805 = stablehlo.reshape %804 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %806 = stablehlo.broadcast_in_dim %805, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc290)
    %807 = stablehlo.add %802, %806 : tensor<2x50x768xbf16> loc(#loc290)
    %808 = stablehlo.reshape %807 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc291)
    %809 = stablehlo.transpose %808, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc292)
    %810 = stablehlo.reshape %809 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc293)
    %811 = stablehlo.reshape %arg175 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %812 = stablehlo.custom_call @tt.mark_argument(%811) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %813 = stablehlo.reshape %812 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %814 = stablehlo.transpose %813, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc294)
    %815 = stablehlo.dot_general %796, %814, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc295)
    %816 = stablehlo.reshape %815 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc296)
    %817 = stablehlo.reshape %arg174 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %818 = stablehlo.custom_call @tt.mark_argument(%817) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %819 = stablehlo.reshape %818 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %820 = stablehlo.broadcast_in_dim %819, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc297)
    %821 = stablehlo.add %816, %820 : tensor<2x50x768xbf16> loc(#loc297)
    %822 = stablehlo.reshape %821 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc298)
    %823 = stablehlo.transpose %822, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc299)
    %824 = stablehlo.reshape %823 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc293)
    %825 = stablehlo.dot_general %810, %824, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc300)
    %826 = stablehlo.reshape %825 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc293)
    %827 = stablehlo.multiply %826, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc301)
    %828 = stablehlo.convert %827 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc302)
    %829 = stablehlo.reduce(%828 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc303)
    %830 = stablehlo.broadcast_in_dim %829, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc303)
    %831 = stablehlo.subtract %828, %830 : tensor<2x12x50x50xf32> loc(#loc303)
    %832 = stablehlo.exponential %831 : tensor<2x12x50x50xf32> loc(#loc303)
    %833 = stablehlo.reduce(%832 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc303)
    %834 = stablehlo.broadcast_in_dim %833, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc303)
    %835 = stablehlo.divide %832, %834 : tensor<2x12x50x50xf32> loc(#loc303)
    %836 = stablehlo.convert %835 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc304)
    %837 = stablehlo.reshape %836 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc305)
    %838 = stablehlo.reshape %arg84 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %839 = stablehlo.custom_call @tt.mark_argument(%838) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %840 = stablehlo.reshape %839 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %841 = stablehlo.transpose %840, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc306)
    %842 = stablehlo.dot_general %796, %841, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc307)
    %843 = stablehlo.reshape %842 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc308)
    %844 = stablehlo.reshape %arg83 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %845 = stablehlo.custom_call @tt.mark_argument(%844) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %846 = stablehlo.reshape %845 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %847 = stablehlo.broadcast_in_dim %846, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc309)
    %848 = stablehlo.add %843, %847 : tensor<2x50x768xbf16> loc(#loc309)
    %849 = stablehlo.reshape %848 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc310)
    %850 = stablehlo.transpose %849, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc311)
    %851 = stablehlo.reshape %850 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc305)
    %852 = stablehlo.dot_general %837, %851, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc312)
    %853 = stablehlo.reshape %852 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc305)
    %854 = stablehlo.transpose %853, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc313)
    %855 = stablehlo.reshape %854 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc314)
    %856 = stablehlo.reshape %arg82 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %857 = stablehlo.custom_call @tt.mark_argument(%856) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %858 = stablehlo.reshape %857 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %859 = stablehlo.transpose %858, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc315)
    %860 = stablehlo.dot_general %855, %859, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc316)
    %861 = stablehlo.reshape %860 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc314)
    %862 = stablehlo.reshape %arg81 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %863 = stablehlo.custom_call @tt.mark_argument(%862) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %864 = stablehlo.reshape %863 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %865 = stablehlo.broadcast_in_dim %864, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc317)
    %866 = stablehlo.add %861, %865 : tensor<2x50x768xbf16> loc(#loc317)
    %867 = stablehlo.add %772, %866 : tensor<2x50x768xbf16> loc(#loc318)
    %868 = stablehlo.reduce(%867 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc319)
    %869 = stablehlo.multiply %868, %cst_4 : tensor<2x50xbf16> loc(#loc319)
    %870 = stablehlo.broadcast_in_dim %869, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc320)
    %871 = stablehlo.subtract %867, %870 : tensor<2x50x768xbf16> loc(#loc320)
    %872 = stablehlo.multiply %871, %871 : tensor<2x50x768xbf16> loc(#loc319)
    %873 = stablehlo.reduce(%872 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc319)
    %874 = stablehlo.multiply %873, %cst_4 : tensor<2x50xbf16> loc(#loc319)
    %875 = stablehlo.reshape %874 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc319)
    %876 = stablehlo.add %875, %cst_3 : tensor<2x50x1xbf16> loc(#loc321)
    %877 = stablehlo.rsqrt %876 : tensor<2x50x1xbf16> loc(#loc322)
    %878 = stablehlo.reshape %877 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc323)
    %879 = stablehlo.broadcast_in_dim %878, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc323)
    %880 = stablehlo.multiply %871, %879 : tensor<2x50x768xbf16> loc(#loc323)
    %881 = stablehlo.reshape %arg80 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %882 = stablehlo.custom_call @tt.mark_argument(%881) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %883 = stablehlo.reshape %882 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %884 = stablehlo.broadcast_in_dim %883, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc323)
    %885 = stablehlo.multiply %880, %884 : tensor<2x50x768xbf16> loc(#loc323)
    %886 = stablehlo.reshape %arg79 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %887 = stablehlo.custom_call @tt.mark_argument(%886) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %888 = stablehlo.reshape %887 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %889 = stablehlo.broadcast_in_dim %888, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc321)
    %890 = stablehlo.add %885, %889 : tensor<2x50x768xbf16> loc(#loc321)
    %891 = stablehlo.reshape %890 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc324)
    %892 = stablehlo.reshape %arg78 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %893 = stablehlo.custom_call @tt.mark_argument(%892) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %894 = stablehlo.reshape %893 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %895 = stablehlo.transpose %894, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc325)
    %896 = stablehlo.dot_general %891, %895, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc326)
    %897 = stablehlo.reshape %896 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc324)
    %898 = stablehlo.reshape %arg77 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %899 = stablehlo.custom_call @tt.mark_argument(%898) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %900 = stablehlo.reshape %899 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %901 = stablehlo.broadcast_in_dim %900, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc327)
    %902 = stablehlo.add %897, %901 : tensor<2x50x3072xbf16> loc(#loc327)
    %903 = stablehlo.multiply %902, %cst_1 : tensor<2x50x3072xbf16> loc(#loc328)
    %904 = stablehlo.logistic %903 : tensor<2x50x3072xbf16> loc(#loc329)
    %905 = stablehlo.multiply %902, %904 : tensor<2x50x3072xbf16> loc(#loc328)
    %906 = stablehlo.reshape %905 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc330)
    %907 = stablehlo.reshape %arg76 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %908 = stablehlo.custom_call @tt.mark_argument(%907) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %909 = stablehlo.reshape %908 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %910 = stablehlo.transpose %909, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc331)
    %911 = stablehlo.dot_general %906, %910, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc332)
    %912 = stablehlo.reshape %911 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc330)
    %913 = stablehlo.reshape %arg75 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %914 = stablehlo.custom_call @tt.mark_argument(%913) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_5_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %915 = stablehlo.reshape %914 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %916 = stablehlo.broadcast_in_dim %915, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc333)
    %917 = stablehlo.add %912, %916 : tensor<2x50x768xbf16> loc(#loc333)
    %918 = stablehlo.add %867, %917 : tensor<2x50x768xbf16> loc(#loc334)
    %919 = stablehlo.reduce(%918 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc335)
    %920 = stablehlo.multiply %919, %cst_4 : tensor<2x50xbf16> loc(#loc335)
    %921 = stablehlo.broadcast_in_dim %920, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc336)
    %922 = stablehlo.subtract %918, %921 : tensor<2x50x768xbf16> loc(#loc336)
    %923 = stablehlo.multiply %922, %922 : tensor<2x50x768xbf16> loc(#loc335)
    %924 = stablehlo.reduce(%923 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc335)
    %925 = stablehlo.multiply %924, %cst_4 : tensor<2x50xbf16> loc(#loc335)
    %926 = stablehlo.reshape %925 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc335)
    %927 = stablehlo.add %926, %cst_3 : tensor<2x50x1xbf16> loc(#loc337)
    %928 = stablehlo.rsqrt %927 : tensor<2x50x1xbf16> loc(#loc338)
    %929 = stablehlo.reshape %928 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc339)
    %930 = stablehlo.broadcast_in_dim %929, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc339)
    %931 = stablehlo.multiply %922, %930 : tensor<2x50x768xbf16> loc(#loc339)
    %932 = stablehlo.reshape %arg74 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %933 = stablehlo.custom_call @tt.mark_argument(%932) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %934 = stablehlo.reshape %933 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %935 = stablehlo.broadcast_in_dim %934, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc339)
    %936 = stablehlo.multiply %931, %935 : tensor<2x50x768xbf16> loc(#loc339)
    %937 = stablehlo.reshape %arg73 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %938 = stablehlo.custom_call @tt.mark_argument(%937) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %939 = stablehlo.reshape %938 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %940 = stablehlo.broadcast_in_dim %939, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc337)
    %941 = stablehlo.add %936, %940 : tensor<2x50x768xbf16> loc(#loc337)
    %942 = stablehlo.reshape %941 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc340)
    %943 = stablehlo.reshape %arg181 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %944 = stablehlo.custom_call @tt.mark_argument(%943) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %945 = stablehlo.reshape %944 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %946 = stablehlo.transpose %945, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc341)
    %947 = stablehlo.dot_general %942, %946, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc342)
    %948 = stablehlo.reshape %947 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc340)
    %949 = stablehlo.reshape %arg180 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %950 = stablehlo.custom_call @tt.mark_argument(%949) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %951 = stablehlo.reshape %950 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %952 = stablehlo.broadcast_in_dim %951, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc343)
    %953 = stablehlo.add %948, %952 : tensor<2x50x768xbf16> loc(#loc343)
    %954 = stablehlo.reshape %953 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc344)
    %955 = stablehlo.transpose %954, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc345)
    %956 = stablehlo.reshape %955 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc346)
    %957 = stablehlo.reshape %arg179 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %958 = stablehlo.custom_call @tt.mark_argument(%957) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %959 = stablehlo.reshape %958 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %960 = stablehlo.transpose %959, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc347)
    %961 = stablehlo.dot_general %942, %960, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc348)
    %962 = stablehlo.reshape %961 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc349)
    %963 = stablehlo.reshape %arg178 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %964 = stablehlo.custom_call @tt.mark_argument(%963) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %965 = stablehlo.reshape %964 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %966 = stablehlo.broadcast_in_dim %965, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc350)
    %967 = stablehlo.add %962, %966 : tensor<2x50x768xbf16> loc(#loc350)
    %968 = stablehlo.reshape %967 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc351)
    %969 = stablehlo.transpose %968, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc352)
    %970 = stablehlo.reshape %969 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc346)
    %971 = stablehlo.dot_general %956, %970, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc353)
    %972 = stablehlo.reshape %971 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc346)
    %973 = stablehlo.multiply %972, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc354)
    %974 = stablehlo.convert %973 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc355)
    %975 = stablehlo.reduce(%974 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc356)
    %976 = stablehlo.broadcast_in_dim %975, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc356)
    %977 = stablehlo.subtract %974, %976 : tensor<2x12x50x50xf32> loc(#loc356)
    %978 = stablehlo.exponential %977 : tensor<2x12x50x50xf32> loc(#loc356)
    %979 = stablehlo.reduce(%978 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc356)
    %980 = stablehlo.broadcast_in_dim %979, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc356)
    %981 = stablehlo.divide %978, %980 : tensor<2x12x50x50xf32> loc(#loc356)
    %982 = stablehlo.convert %981 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc357)
    %983 = stablehlo.reshape %982 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc358)
    %984 = stablehlo.reshape %arg72 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %985 = stablehlo.custom_call @tt.mark_argument(%984) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %986 = stablehlo.reshape %985 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %987 = stablehlo.transpose %986, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc359)
    %988 = stablehlo.dot_general %942, %987, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc360)
    %989 = stablehlo.reshape %988 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc361)
    %990 = stablehlo.reshape %arg71 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %991 = stablehlo.custom_call @tt.mark_argument(%990) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %992 = stablehlo.reshape %991 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %993 = stablehlo.broadcast_in_dim %992, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc362)
    %994 = stablehlo.add %989, %993 : tensor<2x50x768xbf16> loc(#loc362)
    %995 = stablehlo.reshape %994 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc363)
    %996 = stablehlo.transpose %995, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc364)
    %997 = stablehlo.reshape %996 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc358)
    %998 = stablehlo.dot_general %983, %997, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc365)
    %999 = stablehlo.reshape %998 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc358)
    %1000 = stablehlo.transpose %999, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc366)
    %1001 = stablehlo.reshape %1000 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc367)
    %1002 = stablehlo.reshape %arg70 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1003 = stablehlo.custom_call @tt.mark_argument(%1002) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1004 = stablehlo.reshape %1003 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1005 = stablehlo.transpose %1004, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc368)
    %1006 = stablehlo.dot_general %1001, %1005, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc369)
    %1007 = stablehlo.reshape %1006 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc367)
    %1008 = stablehlo.reshape %arg69 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1009 = stablehlo.custom_call @tt.mark_argument(%1008) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1010 = stablehlo.reshape %1009 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1011 = stablehlo.broadcast_in_dim %1010, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc370)
    %1012 = stablehlo.add %1007, %1011 : tensor<2x50x768xbf16> loc(#loc370)
    %1013 = stablehlo.add %918, %1012 : tensor<2x50x768xbf16> loc(#loc371)
    %1014 = stablehlo.reduce(%1013 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc372)
    %1015 = stablehlo.multiply %1014, %cst_4 : tensor<2x50xbf16> loc(#loc372)
    %1016 = stablehlo.broadcast_in_dim %1015, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc373)
    %1017 = stablehlo.subtract %1013, %1016 : tensor<2x50x768xbf16> loc(#loc373)
    %1018 = stablehlo.multiply %1017, %1017 : tensor<2x50x768xbf16> loc(#loc372)
    %1019 = stablehlo.reduce(%1018 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc372)
    %1020 = stablehlo.multiply %1019, %cst_4 : tensor<2x50xbf16> loc(#loc372)
    %1021 = stablehlo.reshape %1020 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc372)
    %1022 = stablehlo.add %1021, %cst_3 : tensor<2x50x1xbf16> loc(#loc374)
    %1023 = stablehlo.rsqrt %1022 : tensor<2x50x1xbf16> loc(#loc375)
    %1024 = stablehlo.reshape %1023 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc376)
    %1025 = stablehlo.broadcast_in_dim %1024, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc376)
    %1026 = stablehlo.multiply %1017, %1025 : tensor<2x50x768xbf16> loc(#loc376)
    %1027 = stablehlo.reshape %arg68 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1028 = stablehlo.custom_call @tt.mark_argument(%1027) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1029 = stablehlo.reshape %1028 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1030 = stablehlo.broadcast_in_dim %1029, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc376)
    %1031 = stablehlo.multiply %1026, %1030 : tensor<2x50x768xbf16> loc(#loc376)
    %1032 = stablehlo.reshape %arg67 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1033 = stablehlo.custom_call @tt.mark_argument(%1032) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1034 = stablehlo.reshape %1033 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1035 = stablehlo.broadcast_in_dim %1034, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc374)
    %1036 = stablehlo.add %1031, %1035 : tensor<2x50x768xbf16> loc(#loc374)
    %1037 = stablehlo.reshape %1036 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc377)
    %1038 = stablehlo.reshape %arg66 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1039 = stablehlo.custom_call @tt.mark_argument(%1038) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %1040 = stablehlo.reshape %1039 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1041 = stablehlo.transpose %1040, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc378)
    %1042 = stablehlo.dot_general %1037, %1041, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc379)
    %1043 = stablehlo.reshape %1042 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc377)
    %1044 = stablehlo.reshape %arg65 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1045 = stablehlo.custom_call @tt.mark_argument(%1044) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %1046 = stablehlo.reshape %1045 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1047 = stablehlo.broadcast_in_dim %1046, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc380)
    %1048 = stablehlo.add %1043, %1047 : tensor<2x50x3072xbf16> loc(#loc380)
    %1049 = stablehlo.multiply %1048, %cst_1 : tensor<2x50x3072xbf16> loc(#loc381)
    %1050 = stablehlo.logistic %1049 : tensor<2x50x3072xbf16> loc(#loc382)
    %1051 = stablehlo.multiply %1048, %1050 : tensor<2x50x3072xbf16> loc(#loc381)
    %1052 = stablehlo.reshape %1051 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc383)
    %1053 = stablehlo.reshape %arg64 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1054 = stablehlo.custom_call @tt.mark_argument(%1053) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %1055 = stablehlo.reshape %1054 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1056 = stablehlo.transpose %1055, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc384)
    %1057 = stablehlo.dot_general %1052, %1056, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc385)
    %1058 = stablehlo.reshape %1057 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc383)
    %1059 = stablehlo.reshape %arg63 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1060 = stablehlo.custom_call @tt.mark_argument(%1059) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_6_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1061 = stablehlo.reshape %1060 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1062 = stablehlo.broadcast_in_dim %1061, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc386)
    %1063 = stablehlo.add %1058, %1062 : tensor<2x50x768xbf16> loc(#loc386)
    %1064 = stablehlo.add %1013, %1063 : tensor<2x50x768xbf16> loc(#loc387)
    %1065 = stablehlo.reduce(%1064 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc388)
    %1066 = stablehlo.multiply %1065, %cst_4 : tensor<2x50xbf16> loc(#loc388)
    %1067 = stablehlo.broadcast_in_dim %1066, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc389)
    %1068 = stablehlo.subtract %1064, %1067 : tensor<2x50x768xbf16> loc(#loc389)
    %1069 = stablehlo.multiply %1068, %1068 : tensor<2x50x768xbf16> loc(#loc388)
    %1070 = stablehlo.reduce(%1069 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc388)
    %1071 = stablehlo.multiply %1070, %cst_4 : tensor<2x50xbf16> loc(#loc388)
    %1072 = stablehlo.reshape %1071 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc388)
    %1073 = stablehlo.add %1072, %cst_3 : tensor<2x50x1xbf16> loc(#loc390)
    %1074 = stablehlo.rsqrt %1073 : tensor<2x50x1xbf16> loc(#loc391)
    %1075 = stablehlo.reshape %1074 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc392)
    %1076 = stablehlo.broadcast_in_dim %1075, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc392)
    %1077 = stablehlo.multiply %1068, %1076 : tensor<2x50x768xbf16> loc(#loc392)
    %1078 = stablehlo.reshape %arg62 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1079 = stablehlo.custom_call @tt.mark_argument(%1078) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1080 = stablehlo.reshape %1079 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1081 = stablehlo.broadcast_in_dim %1080, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc392)
    %1082 = stablehlo.multiply %1077, %1081 : tensor<2x50x768xbf16> loc(#loc392)
    %1083 = stablehlo.reshape %arg61 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1084 = stablehlo.custom_call @tt.mark_argument(%1083) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1085 = stablehlo.reshape %1084 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1086 = stablehlo.broadcast_in_dim %1085, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc390)
    %1087 = stablehlo.add %1082, %1086 : tensor<2x50x768xbf16> loc(#loc390)
    %1088 = stablehlo.reshape %1087 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc393)
    %1089 = stablehlo.reshape %arg185 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1090 = stablehlo.custom_call @tt.mark_argument(%1089) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1091 = stablehlo.reshape %1090 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1092 = stablehlo.transpose %1091, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc394)
    %1093 = stablehlo.dot_general %1088, %1092, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc395)
    %1094 = stablehlo.reshape %1093 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc393)
    %1095 = stablehlo.reshape %arg184 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1096 = stablehlo.custom_call @tt.mark_argument(%1095) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1097 = stablehlo.reshape %1096 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1098 = stablehlo.broadcast_in_dim %1097, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc396)
    %1099 = stablehlo.add %1094, %1098 : tensor<2x50x768xbf16> loc(#loc396)
    %1100 = stablehlo.reshape %1099 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc397)
    %1101 = stablehlo.transpose %1100, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc398)
    %1102 = stablehlo.reshape %1101 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc399)
    %1103 = stablehlo.reshape %arg183 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1104 = stablehlo.custom_call @tt.mark_argument(%1103) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1105 = stablehlo.reshape %1104 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1106 = stablehlo.transpose %1105, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc400)
    %1107 = stablehlo.dot_general %1088, %1106, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc401)
    %1108 = stablehlo.reshape %1107 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc402)
    %1109 = stablehlo.reshape %arg182 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1110 = stablehlo.custom_call @tt.mark_argument(%1109) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1111 = stablehlo.reshape %1110 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1112 = stablehlo.broadcast_in_dim %1111, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc403)
    %1113 = stablehlo.add %1108, %1112 : tensor<2x50x768xbf16> loc(#loc403)
    %1114 = stablehlo.reshape %1113 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc404)
    %1115 = stablehlo.transpose %1114, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc405)
    %1116 = stablehlo.reshape %1115 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc399)
    %1117 = stablehlo.dot_general %1102, %1116, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc406)
    %1118 = stablehlo.reshape %1117 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc399)
    %1119 = stablehlo.multiply %1118, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc407)
    %1120 = stablehlo.convert %1119 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc408)
    %1121 = stablehlo.reduce(%1120 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc409)
    %1122 = stablehlo.broadcast_in_dim %1121, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc409)
    %1123 = stablehlo.subtract %1120, %1122 : tensor<2x12x50x50xf32> loc(#loc409)
    %1124 = stablehlo.exponential %1123 : tensor<2x12x50x50xf32> loc(#loc409)
    %1125 = stablehlo.reduce(%1124 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc409)
    %1126 = stablehlo.broadcast_in_dim %1125, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc409)
    %1127 = stablehlo.divide %1124, %1126 : tensor<2x12x50x50xf32> loc(#loc409)
    %1128 = stablehlo.convert %1127 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc410)
    %1129 = stablehlo.reshape %1128 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc411)
    %1130 = stablehlo.reshape %arg60 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1131 = stablehlo.custom_call @tt.mark_argument(%1130) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1132 = stablehlo.reshape %1131 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1133 = stablehlo.transpose %1132, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc412)
    %1134 = stablehlo.dot_general %1088, %1133, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc413)
    %1135 = stablehlo.reshape %1134 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc414)
    %1136 = stablehlo.reshape %arg59 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1137 = stablehlo.custom_call @tt.mark_argument(%1136) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1138 = stablehlo.reshape %1137 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1139 = stablehlo.broadcast_in_dim %1138, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc415)
    %1140 = stablehlo.add %1135, %1139 : tensor<2x50x768xbf16> loc(#loc415)
    %1141 = stablehlo.reshape %1140 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc416)
    %1142 = stablehlo.transpose %1141, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc417)
    %1143 = stablehlo.reshape %1142 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc411)
    %1144 = stablehlo.dot_general %1129, %1143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc418)
    %1145 = stablehlo.reshape %1144 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc411)
    %1146 = stablehlo.transpose %1145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc419)
    %1147 = stablehlo.reshape %1146 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc420)
    %1148 = stablehlo.reshape %arg58 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1149 = stablehlo.custom_call @tt.mark_argument(%1148) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1150 = stablehlo.reshape %1149 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1151 = stablehlo.transpose %1150, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc421)
    %1152 = stablehlo.dot_general %1147, %1151, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc422)
    %1153 = stablehlo.reshape %1152 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc420)
    %1154 = stablehlo.reshape %arg57 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1155 = stablehlo.custom_call @tt.mark_argument(%1154) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1156 = stablehlo.reshape %1155 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1157 = stablehlo.broadcast_in_dim %1156, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc423)
    %1158 = stablehlo.add %1153, %1157 : tensor<2x50x768xbf16> loc(#loc423)
    %1159 = stablehlo.add %1064, %1158 : tensor<2x50x768xbf16> loc(#loc424)
    %1160 = stablehlo.reduce(%1159 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc425)
    %1161 = stablehlo.multiply %1160, %cst_4 : tensor<2x50xbf16> loc(#loc425)
    %1162 = stablehlo.broadcast_in_dim %1161, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc426)
    %1163 = stablehlo.subtract %1159, %1162 : tensor<2x50x768xbf16> loc(#loc426)
    %1164 = stablehlo.multiply %1163, %1163 : tensor<2x50x768xbf16> loc(#loc425)
    %1165 = stablehlo.reduce(%1164 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc425)
    %1166 = stablehlo.multiply %1165, %cst_4 : tensor<2x50xbf16> loc(#loc425)
    %1167 = stablehlo.reshape %1166 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc425)
    %1168 = stablehlo.add %1167, %cst_3 : tensor<2x50x1xbf16> loc(#loc427)
    %1169 = stablehlo.rsqrt %1168 : tensor<2x50x1xbf16> loc(#loc428)
    %1170 = stablehlo.reshape %1169 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc429)
    %1171 = stablehlo.broadcast_in_dim %1170, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc429)
    %1172 = stablehlo.multiply %1163, %1171 : tensor<2x50x768xbf16> loc(#loc429)
    %1173 = stablehlo.reshape %arg56 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1174 = stablehlo.custom_call @tt.mark_argument(%1173) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1175 = stablehlo.reshape %1174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1176 = stablehlo.broadcast_in_dim %1175, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc429)
    %1177 = stablehlo.multiply %1172, %1176 : tensor<2x50x768xbf16> loc(#loc429)
    %1178 = stablehlo.reshape %arg55 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1179 = stablehlo.custom_call @tt.mark_argument(%1178) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1180 = stablehlo.reshape %1179 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1181 = stablehlo.broadcast_in_dim %1180, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc427)
    %1182 = stablehlo.add %1177, %1181 : tensor<2x50x768xbf16> loc(#loc427)
    %1183 = stablehlo.reshape %1182 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc430)
    %1184 = stablehlo.reshape %arg54 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1185 = stablehlo.custom_call @tt.mark_argument(%1184) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %1186 = stablehlo.reshape %1185 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1187 = stablehlo.transpose %1186, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc431)
    %1188 = stablehlo.dot_general %1183, %1187, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc432)
    %1189 = stablehlo.reshape %1188 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc430)
    %1190 = stablehlo.reshape %arg53 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1191 = stablehlo.custom_call @tt.mark_argument(%1190) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %1192 = stablehlo.reshape %1191 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1193 = stablehlo.broadcast_in_dim %1192, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc433)
    %1194 = stablehlo.add %1189, %1193 : tensor<2x50x3072xbf16> loc(#loc433)
    %1195 = stablehlo.multiply %1194, %cst_1 : tensor<2x50x3072xbf16> loc(#loc434)
    %1196 = stablehlo.logistic %1195 : tensor<2x50x3072xbf16> loc(#loc435)
    %1197 = stablehlo.multiply %1194, %1196 : tensor<2x50x3072xbf16> loc(#loc434)
    %1198 = stablehlo.reshape %1197 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc436)
    %1199 = stablehlo.reshape %arg52 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1200 = stablehlo.custom_call @tt.mark_argument(%1199) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %1201 = stablehlo.reshape %1200 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1202 = stablehlo.transpose %1201, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc437)
    %1203 = stablehlo.dot_general %1198, %1202, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc438)
    %1204 = stablehlo.reshape %1203 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc436)
    %1205 = stablehlo.reshape %arg51 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1206 = stablehlo.custom_call @tt.mark_argument(%1205) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_7_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1207 = stablehlo.reshape %1206 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1208 = stablehlo.broadcast_in_dim %1207, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc439)
    %1209 = stablehlo.add %1204, %1208 : tensor<2x50x768xbf16> loc(#loc439)
    %1210 = stablehlo.add %1159, %1209 : tensor<2x50x768xbf16> loc(#loc440)
    %1211 = stablehlo.reduce(%1210 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc441)
    %1212 = stablehlo.multiply %1211, %cst_4 : tensor<2x50xbf16> loc(#loc441)
    %1213 = stablehlo.broadcast_in_dim %1212, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc442)
    %1214 = stablehlo.subtract %1210, %1213 : tensor<2x50x768xbf16> loc(#loc442)
    %1215 = stablehlo.multiply %1214, %1214 : tensor<2x50x768xbf16> loc(#loc441)
    %1216 = stablehlo.reduce(%1215 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc441)
    %1217 = stablehlo.multiply %1216, %cst_4 : tensor<2x50xbf16> loc(#loc441)
    %1218 = stablehlo.reshape %1217 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc441)
    %1219 = stablehlo.add %1218, %cst_3 : tensor<2x50x1xbf16> loc(#loc443)
    %1220 = stablehlo.rsqrt %1219 : tensor<2x50x1xbf16> loc(#loc444)
    %1221 = stablehlo.reshape %1220 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc445)
    %1222 = stablehlo.broadcast_in_dim %1221, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc445)
    %1223 = stablehlo.multiply %1214, %1222 : tensor<2x50x768xbf16> loc(#loc445)
    %1224 = stablehlo.reshape %arg50 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1225 = stablehlo.custom_call @tt.mark_argument(%1224) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1226 = stablehlo.reshape %1225 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1227 = stablehlo.broadcast_in_dim %1226, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc445)
    %1228 = stablehlo.multiply %1223, %1227 : tensor<2x50x768xbf16> loc(#loc445)
    %1229 = stablehlo.reshape %arg49 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1230 = stablehlo.custom_call @tt.mark_argument(%1229) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1231 = stablehlo.reshape %1230 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1232 = stablehlo.broadcast_in_dim %1231, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc443)
    %1233 = stablehlo.add %1228, %1232 : tensor<2x50x768xbf16> loc(#loc443)
    %1234 = stablehlo.reshape %1233 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc446)
    %1235 = stablehlo.reshape %arg189 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1236 = stablehlo.custom_call @tt.mark_argument(%1235) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1237 = stablehlo.reshape %1236 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1238 = stablehlo.transpose %1237, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc447)
    %1239 = stablehlo.dot_general %1234, %1238, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc448)
    %1240 = stablehlo.reshape %1239 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc446)
    %1241 = stablehlo.reshape %arg188 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1242 = stablehlo.custom_call @tt.mark_argument(%1241) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1243 = stablehlo.reshape %1242 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1244 = stablehlo.broadcast_in_dim %1243, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc449)
    %1245 = stablehlo.add %1240, %1244 : tensor<2x50x768xbf16> loc(#loc449)
    %1246 = stablehlo.reshape %1245 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc450)
    %1247 = stablehlo.transpose %1246, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc451)
    %1248 = stablehlo.reshape %1247 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc452)
    %1249 = stablehlo.reshape %arg187 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1250 = stablehlo.custom_call @tt.mark_argument(%1249) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1251 = stablehlo.reshape %1250 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1252 = stablehlo.transpose %1251, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc453)
    %1253 = stablehlo.dot_general %1234, %1252, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc454)
    %1254 = stablehlo.reshape %1253 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc455)
    %1255 = stablehlo.reshape %arg186 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1256 = stablehlo.custom_call @tt.mark_argument(%1255) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1257 = stablehlo.reshape %1256 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1258 = stablehlo.broadcast_in_dim %1257, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc456)
    %1259 = stablehlo.add %1254, %1258 : tensor<2x50x768xbf16> loc(#loc456)
    %1260 = stablehlo.reshape %1259 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc457)
    %1261 = stablehlo.transpose %1260, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc458)
    %1262 = stablehlo.reshape %1261 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc452)
    %1263 = stablehlo.dot_general %1248, %1262, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc459)
    %1264 = stablehlo.reshape %1263 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc452)
    %1265 = stablehlo.multiply %1264, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc460)
    %1266 = stablehlo.convert %1265 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc461)
    %1267 = stablehlo.reduce(%1266 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc462)
    %1268 = stablehlo.broadcast_in_dim %1267, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc462)
    %1269 = stablehlo.subtract %1266, %1268 : tensor<2x12x50x50xf32> loc(#loc462)
    %1270 = stablehlo.exponential %1269 : tensor<2x12x50x50xf32> loc(#loc462)
    %1271 = stablehlo.reduce(%1270 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc462)
    %1272 = stablehlo.broadcast_in_dim %1271, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc462)
    %1273 = stablehlo.divide %1270, %1272 : tensor<2x12x50x50xf32> loc(#loc462)
    %1274 = stablehlo.convert %1273 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc463)
    %1275 = stablehlo.reshape %1274 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc464)
    %1276 = stablehlo.reshape %arg48 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1277 = stablehlo.custom_call @tt.mark_argument(%1276) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1278 = stablehlo.reshape %1277 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1279 = stablehlo.transpose %1278, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc465)
    %1280 = stablehlo.dot_general %1234, %1279, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc466)
    %1281 = stablehlo.reshape %1280 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc467)
    %1282 = stablehlo.reshape %arg47 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1283 = stablehlo.custom_call @tt.mark_argument(%1282) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1284 = stablehlo.reshape %1283 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1285 = stablehlo.broadcast_in_dim %1284, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc468)
    %1286 = stablehlo.add %1281, %1285 : tensor<2x50x768xbf16> loc(#loc468)
    %1287 = stablehlo.reshape %1286 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc469)
    %1288 = stablehlo.transpose %1287, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc470)
    %1289 = stablehlo.reshape %1288 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc464)
    %1290 = stablehlo.dot_general %1275, %1289, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc471)
    %1291 = stablehlo.reshape %1290 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc464)
    %1292 = stablehlo.transpose %1291, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc472)
    %1293 = stablehlo.reshape %1292 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc473)
    %1294 = stablehlo.reshape %arg46 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1295 = stablehlo.custom_call @tt.mark_argument(%1294) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1296 = stablehlo.reshape %1295 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1297 = stablehlo.transpose %1296, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc474)
    %1298 = stablehlo.dot_general %1293, %1297, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc475)
    %1299 = stablehlo.reshape %1298 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc473)
    %1300 = stablehlo.reshape %arg45 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1301 = stablehlo.custom_call @tt.mark_argument(%1300) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1302 = stablehlo.reshape %1301 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1303 = stablehlo.broadcast_in_dim %1302, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc476)
    %1304 = stablehlo.add %1299, %1303 : tensor<2x50x768xbf16> loc(#loc476)
    %1305 = stablehlo.add %1210, %1304 : tensor<2x50x768xbf16> loc(#loc477)
    %1306 = stablehlo.reduce(%1305 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc478)
    %1307 = stablehlo.multiply %1306, %cst_4 : tensor<2x50xbf16> loc(#loc478)
    %1308 = stablehlo.broadcast_in_dim %1307, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc479)
    %1309 = stablehlo.subtract %1305, %1308 : tensor<2x50x768xbf16> loc(#loc479)
    %1310 = stablehlo.multiply %1309, %1309 : tensor<2x50x768xbf16> loc(#loc478)
    %1311 = stablehlo.reduce(%1310 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc478)
    %1312 = stablehlo.multiply %1311, %cst_4 : tensor<2x50xbf16> loc(#loc478)
    %1313 = stablehlo.reshape %1312 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc478)
    %1314 = stablehlo.add %1313, %cst_3 : tensor<2x50x1xbf16> loc(#loc480)
    %1315 = stablehlo.rsqrt %1314 : tensor<2x50x1xbf16> loc(#loc481)
    %1316 = stablehlo.reshape %1315 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc482)
    %1317 = stablehlo.broadcast_in_dim %1316, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc482)
    %1318 = stablehlo.multiply %1309, %1317 : tensor<2x50x768xbf16> loc(#loc482)
    %1319 = stablehlo.reshape %arg44 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1320 = stablehlo.custom_call @tt.mark_argument(%1319) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1321 = stablehlo.reshape %1320 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1322 = stablehlo.broadcast_in_dim %1321, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc482)
    %1323 = stablehlo.multiply %1318, %1322 : tensor<2x50x768xbf16> loc(#loc482)
    %1324 = stablehlo.reshape %arg43 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1325 = stablehlo.custom_call @tt.mark_argument(%1324) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1326 = stablehlo.reshape %1325 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1327 = stablehlo.broadcast_in_dim %1326, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc480)
    %1328 = stablehlo.add %1323, %1327 : tensor<2x50x768xbf16> loc(#loc480)
    %1329 = stablehlo.reshape %1328 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc483)
    %1330 = stablehlo.reshape %arg42 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1331 = stablehlo.custom_call @tt.mark_argument(%1330) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %1332 = stablehlo.reshape %1331 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1333 = stablehlo.transpose %1332, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc484)
    %1334 = stablehlo.dot_general %1329, %1333, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc485)
    %1335 = stablehlo.reshape %1334 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc483)
    %1336 = stablehlo.reshape %arg41 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1337 = stablehlo.custom_call @tt.mark_argument(%1336) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %1338 = stablehlo.reshape %1337 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1339 = stablehlo.broadcast_in_dim %1338, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc486)
    %1340 = stablehlo.add %1335, %1339 : tensor<2x50x3072xbf16> loc(#loc486)
    %1341 = stablehlo.multiply %1340, %cst_1 : tensor<2x50x3072xbf16> loc(#loc487)
    %1342 = stablehlo.logistic %1341 : tensor<2x50x3072xbf16> loc(#loc488)
    %1343 = stablehlo.multiply %1340, %1342 : tensor<2x50x3072xbf16> loc(#loc487)
    %1344 = stablehlo.reshape %1343 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc489)
    %1345 = stablehlo.reshape %arg40 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1346 = stablehlo.custom_call @tt.mark_argument(%1345) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %1347 = stablehlo.reshape %1346 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1348 = stablehlo.transpose %1347, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc490)
    %1349 = stablehlo.dot_general %1344, %1348, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc491)
    %1350 = stablehlo.reshape %1349 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc489)
    %1351 = stablehlo.reshape %arg39 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1352 = stablehlo.custom_call @tt.mark_argument(%1351) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_8_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1353 = stablehlo.reshape %1352 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1354 = stablehlo.broadcast_in_dim %1353, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc492)
    %1355 = stablehlo.add %1350, %1354 : tensor<2x50x768xbf16> loc(#loc492)
    %1356 = stablehlo.add %1305, %1355 : tensor<2x50x768xbf16> loc(#loc493)
    %1357 = stablehlo.reduce(%1356 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc494)
    %1358 = stablehlo.multiply %1357, %cst_4 : tensor<2x50xbf16> loc(#loc494)
    %1359 = stablehlo.broadcast_in_dim %1358, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc495)
    %1360 = stablehlo.subtract %1356, %1359 : tensor<2x50x768xbf16> loc(#loc495)
    %1361 = stablehlo.multiply %1360, %1360 : tensor<2x50x768xbf16> loc(#loc494)
    %1362 = stablehlo.reduce(%1361 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc494)
    %1363 = stablehlo.multiply %1362, %cst_4 : tensor<2x50xbf16> loc(#loc494)
    %1364 = stablehlo.reshape %1363 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc494)
    %1365 = stablehlo.add %1364, %cst_3 : tensor<2x50x1xbf16> loc(#loc496)
    %1366 = stablehlo.rsqrt %1365 : tensor<2x50x1xbf16> loc(#loc497)
    %1367 = stablehlo.reshape %1366 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc498)
    %1368 = stablehlo.broadcast_in_dim %1367, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc498)
    %1369 = stablehlo.multiply %1360, %1368 : tensor<2x50x768xbf16> loc(#loc498)
    %1370 = stablehlo.reshape %arg38 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1371 = stablehlo.custom_call @tt.mark_argument(%1370) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1372 = stablehlo.reshape %1371 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1373 = stablehlo.broadcast_in_dim %1372, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc498)
    %1374 = stablehlo.multiply %1369, %1373 : tensor<2x50x768xbf16> loc(#loc498)
    %1375 = stablehlo.reshape %arg37 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1376 = stablehlo.custom_call @tt.mark_argument(%1375) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1377 = stablehlo.reshape %1376 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1378 = stablehlo.broadcast_in_dim %1377, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc496)
    %1379 = stablehlo.add %1374, %1378 : tensor<2x50x768xbf16> loc(#loc496)
    %1380 = stablehlo.reshape %1379 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc499)
    %1381 = stablehlo.reshape %arg193 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1382 = stablehlo.custom_call @tt.mark_argument(%1381) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1383 = stablehlo.reshape %1382 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1384 = stablehlo.transpose %1383, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc500)
    %1385 = stablehlo.dot_general %1380, %1384, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc501)
    %1386 = stablehlo.reshape %1385 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc499)
    %1387 = stablehlo.reshape %arg192 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1388 = stablehlo.custom_call @tt.mark_argument(%1387) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1389 = stablehlo.reshape %1388 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1390 = stablehlo.broadcast_in_dim %1389, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc502)
    %1391 = stablehlo.add %1386, %1390 : tensor<2x50x768xbf16> loc(#loc502)
    %1392 = stablehlo.reshape %1391 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc503)
    %1393 = stablehlo.transpose %1392, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc504)
    %1394 = stablehlo.reshape %1393 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc505)
    %1395 = stablehlo.reshape %arg191 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1396 = stablehlo.custom_call @tt.mark_argument(%1395) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1397 = stablehlo.reshape %1396 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1398 = stablehlo.transpose %1397, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc506)
    %1399 = stablehlo.dot_general %1380, %1398, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc507)
    %1400 = stablehlo.reshape %1399 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc508)
    %1401 = stablehlo.reshape %arg190 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1402 = stablehlo.custom_call @tt.mark_argument(%1401) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1403 = stablehlo.reshape %1402 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1404 = stablehlo.broadcast_in_dim %1403, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc509)
    %1405 = stablehlo.add %1400, %1404 : tensor<2x50x768xbf16> loc(#loc509)
    %1406 = stablehlo.reshape %1405 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc510)
    %1407 = stablehlo.transpose %1406, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc511)
    %1408 = stablehlo.reshape %1407 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc505)
    %1409 = stablehlo.dot_general %1394, %1408, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc512)
    %1410 = stablehlo.reshape %1409 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc505)
    %1411 = stablehlo.multiply %1410, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc513)
    %1412 = stablehlo.convert %1411 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc514)
    %1413 = stablehlo.reduce(%1412 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc515)
    %1414 = stablehlo.broadcast_in_dim %1413, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc515)
    %1415 = stablehlo.subtract %1412, %1414 : tensor<2x12x50x50xf32> loc(#loc515)
    %1416 = stablehlo.exponential %1415 : tensor<2x12x50x50xf32> loc(#loc515)
    %1417 = stablehlo.reduce(%1416 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc515)
    %1418 = stablehlo.broadcast_in_dim %1417, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc515)
    %1419 = stablehlo.divide %1416, %1418 : tensor<2x12x50x50xf32> loc(#loc515)
    %1420 = stablehlo.convert %1419 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc516)
    %1421 = stablehlo.reshape %1420 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc517)
    %1422 = stablehlo.reshape %arg36 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1423 = stablehlo.custom_call @tt.mark_argument(%1422) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1424 = stablehlo.reshape %1423 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1425 = stablehlo.transpose %1424, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc518)
    %1426 = stablehlo.dot_general %1380, %1425, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc519)
    %1427 = stablehlo.reshape %1426 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc520)
    %1428 = stablehlo.reshape %arg35 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1429 = stablehlo.custom_call @tt.mark_argument(%1428) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1430 = stablehlo.reshape %1429 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1431 = stablehlo.broadcast_in_dim %1430, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc521)
    %1432 = stablehlo.add %1427, %1431 : tensor<2x50x768xbf16> loc(#loc521)
    %1433 = stablehlo.reshape %1432 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc522)
    %1434 = stablehlo.transpose %1433, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc523)
    %1435 = stablehlo.reshape %1434 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc517)
    %1436 = stablehlo.dot_general %1421, %1435, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc524)
    %1437 = stablehlo.reshape %1436 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc517)
    %1438 = stablehlo.transpose %1437, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc525)
    %1439 = stablehlo.reshape %1438 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc526)
    %1440 = stablehlo.reshape %arg34 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1441 = stablehlo.custom_call @tt.mark_argument(%1440) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1442 = stablehlo.reshape %1441 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1443 = stablehlo.transpose %1442, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc527)
    %1444 = stablehlo.dot_general %1439, %1443, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc528)
    %1445 = stablehlo.reshape %1444 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc526)
    %1446 = stablehlo.reshape %arg33 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1447 = stablehlo.custom_call @tt.mark_argument(%1446) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1448 = stablehlo.reshape %1447 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1449 = stablehlo.broadcast_in_dim %1448, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc529)
    %1450 = stablehlo.add %1445, %1449 : tensor<2x50x768xbf16> loc(#loc529)
    %1451 = stablehlo.add %1356, %1450 : tensor<2x50x768xbf16> loc(#loc530)
    %1452 = stablehlo.reduce(%1451 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc531)
    %1453 = stablehlo.multiply %1452, %cst_4 : tensor<2x50xbf16> loc(#loc531)
    %1454 = stablehlo.broadcast_in_dim %1453, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc532)
    %1455 = stablehlo.subtract %1451, %1454 : tensor<2x50x768xbf16> loc(#loc532)
    %1456 = stablehlo.multiply %1455, %1455 : tensor<2x50x768xbf16> loc(#loc531)
    %1457 = stablehlo.reduce(%1456 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc531)
    %1458 = stablehlo.multiply %1457, %cst_4 : tensor<2x50xbf16> loc(#loc531)
    %1459 = stablehlo.reshape %1458 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc531)
    %1460 = stablehlo.add %1459, %cst_3 : tensor<2x50x1xbf16> loc(#loc533)
    %1461 = stablehlo.rsqrt %1460 : tensor<2x50x1xbf16> loc(#loc534)
    %1462 = stablehlo.reshape %1461 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc535)
    %1463 = stablehlo.broadcast_in_dim %1462, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc535)
    %1464 = stablehlo.multiply %1455, %1463 : tensor<2x50x768xbf16> loc(#loc535)
    %1465 = stablehlo.reshape %arg32 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1466 = stablehlo.custom_call @tt.mark_argument(%1465) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1467 = stablehlo.reshape %1466 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1468 = stablehlo.broadcast_in_dim %1467, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc535)
    %1469 = stablehlo.multiply %1464, %1468 : tensor<2x50x768xbf16> loc(#loc535)
    %1470 = stablehlo.reshape %arg31 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1471 = stablehlo.custom_call @tt.mark_argument(%1470) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1472 = stablehlo.reshape %1471 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1473 = stablehlo.broadcast_in_dim %1472, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc533)
    %1474 = stablehlo.add %1469, %1473 : tensor<2x50x768xbf16> loc(#loc533)
    %1475 = stablehlo.reshape %1474 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc536)
    %1476 = stablehlo.reshape %arg30 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1477 = stablehlo.custom_call @tt.mark_argument(%1476) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %1478 = stablehlo.reshape %1477 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1479 = stablehlo.transpose %1478, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc537)
    %1480 = stablehlo.dot_general %1475, %1479, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc538)
    %1481 = stablehlo.reshape %1480 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc536)
    %1482 = stablehlo.reshape %arg29 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1483 = stablehlo.custom_call @tt.mark_argument(%1482) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %1484 = stablehlo.reshape %1483 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1485 = stablehlo.broadcast_in_dim %1484, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc539)
    %1486 = stablehlo.add %1481, %1485 : tensor<2x50x3072xbf16> loc(#loc539)
    %1487 = stablehlo.multiply %1486, %cst_1 : tensor<2x50x3072xbf16> loc(#loc540)
    %1488 = stablehlo.logistic %1487 : tensor<2x50x3072xbf16> loc(#loc541)
    %1489 = stablehlo.multiply %1486, %1488 : tensor<2x50x3072xbf16> loc(#loc540)
    %1490 = stablehlo.reshape %1489 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc542)
    %1491 = stablehlo.reshape %arg28 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1492 = stablehlo.custom_call @tt.mark_argument(%1491) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %1493 = stablehlo.reshape %1492 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1494 = stablehlo.transpose %1493, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc543)
    %1495 = stablehlo.dot_general %1490, %1494, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc544)
    %1496 = stablehlo.reshape %1495 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc542)
    %1497 = stablehlo.reshape %arg27 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1498 = stablehlo.custom_call @tt.mark_argument(%1497) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_9_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1499 = stablehlo.reshape %1498 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1500 = stablehlo.broadcast_in_dim %1499, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc545)
    %1501 = stablehlo.add %1496, %1500 : tensor<2x50x768xbf16> loc(#loc545)
    %1502 = stablehlo.add %1451, %1501 : tensor<2x50x768xbf16> loc(#loc546)
    %1503 = stablehlo.reduce(%1502 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc547)
    %1504 = stablehlo.multiply %1503, %cst_4 : tensor<2x50xbf16> loc(#loc547)
    %1505 = stablehlo.broadcast_in_dim %1504, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc548)
    %1506 = stablehlo.subtract %1502, %1505 : tensor<2x50x768xbf16> loc(#loc548)
    %1507 = stablehlo.multiply %1506, %1506 : tensor<2x50x768xbf16> loc(#loc547)
    %1508 = stablehlo.reduce(%1507 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc547)
    %1509 = stablehlo.multiply %1508, %cst_4 : tensor<2x50xbf16> loc(#loc547)
    %1510 = stablehlo.reshape %1509 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc547)
    %1511 = stablehlo.add %1510, %cst_3 : tensor<2x50x1xbf16> loc(#loc549)
    %1512 = stablehlo.rsqrt %1511 : tensor<2x50x1xbf16> loc(#loc550)
    %1513 = stablehlo.reshape %1512 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc551)
    %1514 = stablehlo.broadcast_in_dim %1513, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc551)
    %1515 = stablehlo.multiply %1506, %1514 : tensor<2x50x768xbf16> loc(#loc551)
    %1516 = stablehlo.reshape %arg26 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1517 = stablehlo.custom_call @tt.mark_argument(%1516) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1518 = stablehlo.reshape %1517 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1519 = stablehlo.broadcast_in_dim %1518, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc551)
    %1520 = stablehlo.multiply %1515, %1519 : tensor<2x50x768xbf16> loc(#loc551)
    %1521 = stablehlo.reshape %arg25 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1522 = stablehlo.custom_call @tt.mark_argument(%1521) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1523 = stablehlo.reshape %1522 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1524 = stablehlo.broadcast_in_dim %1523, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc549)
    %1525 = stablehlo.add %1520, %1524 : tensor<2x50x768xbf16> loc(#loc549)
    %1526 = stablehlo.reshape %1525 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc552)
    %1527 = stablehlo.reshape %arg197 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1528 = stablehlo.custom_call @tt.mark_argument(%1527) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1529 = stablehlo.reshape %1528 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1530 = stablehlo.transpose %1529, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc553)
    %1531 = stablehlo.dot_general %1526, %1530, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc554)
    %1532 = stablehlo.reshape %1531 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc552)
    %1533 = stablehlo.reshape %arg196 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1534 = stablehlo.custom_call @tt.mark_argument(%1533) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1535 = stablehlo.reshape %1534 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1536 = stablehlo.broadcast_in_dim %1535, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc555)
    %1537 = stablehlo.add %1532, %1536 : tensor<2x50x768xbf16> loc(#loc555)
    %1538 = stablehlo.reshape %1537 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc556)
    %1539 = stablehlo.transpose %1538, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc557)
    %1540 = stablehlo.reshape %1539 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc558)
    %1541 = stablehlo.reshape %arg195 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1542 = stablehlo.custom_call @tt.mark_argument(%1541) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1543 = stablehlo.reshape %1542 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1544 = stablehlo.transpose %1543, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc559)
    %1545 = stablehlo.dot_general %1526, %1544, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc560)
    %1546 = stablehlo.reshape %1545 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc561)
    %1547 = stablehlo.reshape %arg194 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1548 = stablehlo.custom_call @tt.mark_argument(%1547) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1549 = stablehlo.reshape %1548 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1550 = stablehlo.broadcast_in_dim %1549, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc562)
    %1551 = stablehlo.add %1546, %1550 : tensor<2x50x768xbf16> loc(#loc562)
    %1552 = stablehlo.reshape %1551 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc563)
    %1553 = stablehlo.transpose %1552, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc564)
    %1554 = stablehlo.reshape %1553 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc558)
    %1555 = stablehlo.dot_general %1540, %1554, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc565)
    %1556 = stablehlo.reshape %1555 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc558)
    %1557 = stablehlo.multiply %1556, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc566)
    %1558 = stablehlo.convert %1557 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc567)
    %1559 = stablehlo.reduce(%1558 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc568)
    %1560 = stablehlo.broadcast_in_dim %1559, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc568)
    %1561 = stablehlo.subtract %1558, %1560 : tensor<2x12x50x50xf32> loc(#loc568)
    %1562 = stablehlo.exponential %1561 : tensor<2x12x50x50xf32> loc(#loc568)
    %1563 = stablehlo.reduce(%1562 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc568)
    %1564 = stablehlo.broadcast_in_dim %1563, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc568)
    %1565 = stablehlo.divide %1562, %1564 : tensor<2x12x50x50xf32> loc(#loc568)
    %1566 = stablehlo.convert %1565 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc569)
    %1567 = stablehlo.reshape %1566 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc570)
    %1568 = stablehlo.reshape %arg24 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1569 = stablehlo.custom_call @tt.mark_argument(%1568) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1570 = stablehlo.reshape %1569 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1571 = stablehlo.transpose %1570, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc571)
    %1572 = stablehlo.dot_general %1526, %1571, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc572)
    %1573 = stablehlo.reshape %1572 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc573)
    %1574 = stablehlo.reshape %arg23 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1575 = stablehlo.custom_call @tt.mark_argument(%1574) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1576 = stablehlo.reshape %1575 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1577 = stablehlo.broadcast_in_dim %1576, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc574)
    %1578 = stablehlo.add %1573, %1577 : tensor<2x50x768xbf16> loc(#loc574)
    %1579 = stablehlo.reshape %1578 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc575)
    %1580 = stablehlo.transpose %1579, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc576)
    %1581 = stablehlo.reshape %1580 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc570)
    %1582 = stablehlo.dot_general %1567, %1581, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc577)
    %1583 = stablehlo.reshape %1582 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc570)
    %1584 = stablehlo.transpose %1583, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc578)
    %1585 = stablehlo.reshape %1584 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc579)
    %1586 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1587 = stablehlo.custom_call @tt.mark_argument(%1586) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1588 = stablehlo.reshape %1587 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1589 = stablehlo.transpose %1588, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc580)
    %1590 = stablehlo.dot_general %1585, %1589, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc581)
    %1591 = stablehlo.reshape %1590 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc579)
    %1592 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1593 = stablehlo.custom_call @tt.mark_argument(%1592) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1594 = stablehlo.reshape %1593 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1595 = stablehlo.broadcast_in_dim %1594, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc582)
    %1596 = stablehlo.add %1591, %1595 : tensor<2x50x768xbf16> loc(#loc582)
    %1597 = stablehlo.add %1502, %1596 : tensor<2x50x768xbf16> loc(#loc583)
    %1598 = stablehlo.reduce(%1597 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc584)
    %1599 = stablehlo.multiply %1598, %cst_4 : tensor<2x50xbf16> loc(#loc584)
    %1600 = stablehlo.broadcast_in_dim %1599, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc585)
    %1601 = stablehlo.subtract %1597, %1600 : tensor<2x50x768xbf16> loc(#loc585)
    %1602 = stablehlo.multiply %1601, %1601 : tensor<2x50x768xbf16> loc(#loc584)
    %1603 = stablehlo.reduce(%1602 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc584)
    %1604 = stablehlo.multiply %1603, %cst_4 : tensor<2x50xbf16> loc(#loc584)
    %1605 = stablehlo.reshape %1604 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc584)
    %1606 = stablehlo.add %1605, %cst_3 : tensor<2x50x1xbf16> loc(#loc586)
    %1607 = stablehlo.rsqrt %1606 : tensor<2x50x1xbf16> loc(#loc587)
    %1608 = stablehlo.reshape %1607 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc588)
    %1609 = stablehlo.broadcast_in_dim %1608, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc588)
    %1610 = stablehlo.multiply %1601, %1609 : tensor<2x50x768xbf16> loc(#loc588)
    %1611 = stablehlo.reshape %arg20 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1612 = stablehlo.custom_call @tt.mark_argument(%1611) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1613 = stablehlo.reshape %1612 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1614 = stablehlo.broadcast_in_dim %1613, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc588)
    %1615 = stablehlo.multiply %1610, %1614 : tensor<2x50x768xbf16> loc(#loc588)
    %1616 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1617 = stablehlo.custom_call @tt.mark_argument(%1616) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1618 = stablehlo.reshape %1617 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1619 = stablehlo.broadcast_in_dim %1618, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc586)
    %1620 = stablehlo.add %1615, %1619 : tensor<2x50x768xbf16> loc(#loc586)
    %1621 = stablehlo.reshape %1620 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc589)
    %1622 = stablehlo.reshape %arg18 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1623 = stablehlo.custom_call @tt.mark_argument(%1622) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %1624 = stablehlo.reshape %1623 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1625 = stablehlo.transpose %1624, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc590)
    %1626 = stablehlo.dot_general %1621, %1625, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc591)
    %1627 = stablehlo.reshape %1626 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc589)
    %1628 = stablehlo.reshape %arg17 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1629 = stablehlo.custom_call @tt.mark_argument(%1628) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %1630 = stablehlo.reshape %1629 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1631 = stablehlo.broadcast_in_dim %1630, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc592)
    %1632 = stablehlo.add %1627, %1631 : tensor<2x50x3072xbf16> loc(#loc592)
    %1633 = stablehlo.multiply %1632, %cst_1 : tensor<2x50x3072xbf16> loc(#loc593)
    %1634 = stablehlo.logistic %1633 : tensor<2x50x3072xbf16> loc(#loc594)
    %1635 = stablehlo.multiply %1632, %1634 : tensor<2x50x3072xbf16> loc(#loc593)
    %1636 = stablehlo.reshape %1635 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc595)
    %1637 = stablehlo.reshape %arg16 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1638 = stablehlo.custom_call @tt.mark_argument(%1637) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %1639 = stablehlo.reshape %1638 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1640 = stablehlo.transpose %1639, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc596)
    %1641 = stablehlo.dot_general %1636, %1640, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc597)
    %1642 = stablehlo.reshape %1641 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc595)
    %1643 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1644 = stablehlo.custom_call @tt.mark_argument(%1643) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_10_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1645 = stablehlo.reshape %1644 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1646 = stablehlo.broadcast_in_dim %1645, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc598)
    %1647 = stablehlo.add %1642, %1646 : tensor<2x50x768xbf16> loc(#loc598)
    %1648 = stablehlo.add %1597, %1647 : tensor<2x50x768xbf16> loc(#loc599)
    %1649 = stablehlo.reduce(%1648 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc600)
    %1650 = stablehlo.multiply %1649, %cst_4 : tensor<2x50xbf16> loc(#loc600)
    %1651 = stablehlo.broadcast_in_dim %1650, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc601)
    %1652 = stablehlo.subtract %1648, %1651 : tensor<2x50x768xbf16> loc(#loc601)
    %1653 = stablehlo.multiply %1652, %1652 : tensor<2x50x768xbf16> loc(#loc600)
    %1654 = stablehlo.reduce(%1653 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc600)
    %1655 = stablehlo.multiply %1654, %cst_4 : tensor<2x50xbf16> loc(#loc600)
    %1656 = stablehlo.reshape %1655 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc600)
    %1657 = stablehlo.add %1656, %cst_3 : tensor<2x50x1xbf16> loc(#loc602)
    %1658 = stablehlo.rsqrt %1657 : tensor<2x50x1xbf16> loc(#loc603)
    %1659 = stablehlo.reshape %1658 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc604)
    %1660 = stablehlo.broadcast_in_dim %1659, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc604)
    %1661 = stablehlo.multiply %1652, %1660 : tensor<2x50x768xbf16> loc(#loc604)
    %1662 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1663 = stablehlo.custom_call @tt.mark_argument(%1662) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm1_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1664 = stablehlo.reshape %1663 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1665 = stablehlo.broadcast_in_dim %1664, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc604)
    %1666 = stablehlo.multiply %1661, %1665 : tensor<2x50x768xbf16> loc(#loc604)
    %1667 = stablehlo.reshape %arg13 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1668 = stablehlo.custom_call @tt.mark_argument(%1667) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm1_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1669 = stablehlo.reshape %1668 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1670 = stablehlo.broadcast_in_dim %1669, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc602)
    %1671 = stablehlo.add %1666, %1670 : tensor<2x50x768xbf16> loc(#loc602)
    %1672 = stablehlo.reshape %1671 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc605)
    %1673 = stablehlo.reshape %arg201 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1674 = stablehlo.custom_call @tt.mark_argument(%1673) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1675 = stablehlo.reshape %1674 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1676 = stablehlo.transpose %1675, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc606)
    %1677 = stablehlo.dot_general %1672, %1676, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc607)
    %1678 = stablehlo.reshape %1677 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc605)
    %1679 = stablehlo.reshape %arg200 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1680 = stablehlo.custom_call @tt.mark_argument(%1679) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1681 = stablehlo.reshape %1680 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1682 = stablehlo.broadcast_in_dim %1681, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc608)
    %1683 = stablehlo.add %1678, %1682 : tensor<2x50x768xbf16> loc(#loc608)
    %1684 = stablehlo.reshape %1683 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc609)
    %1685 = stablehlo.transpose %1684, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc610)
    %1686 = stablehlo.reshape %1685 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc611)
    %1687 = stablehlo.reshape %arg199 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1688 = stablehlo.custom_call @tt.mark_argument(%1687) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1689 = stablehlo.reshape %1688 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1690 = stablehlo.transpose %1689, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc612)
    %1691 = stablehlo.dot_general %1672, %1690, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc613)
    %1692 = stablehlo.reshape %1691 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc614)
    %1693 = stablehlo.reshape %arg198 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1694 = stablehlo.custom_call @tt.mark_argument(%1693) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1695 = stablehlo.reshape %1694 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1696 = stablehlo.broadcast_in_dim %1695, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc615)
    %1697 = stablehlo.add %1692, %1696 : tensor<2x50x768xbf16> loc(#loc615)
    %1698 = stablehlo.reshape %1697 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc616)
    %1699 = stablehlo.transpose %1698, dims = [0, 2, 3, 1] : (tensor<2x50x12x64xbf16>) -> tensor<2x12x64x50xbf16> loc(#loc617)
    %1700 = stablehlo.reshape %1699 : (tensor<2x12x64x50xbf16>) -> tensor<24x64x50xbf16> loc(#loc611)
    %1701 = stablehlo.dot_general %1686, %1700, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x64xbf16>, tensor<24x64x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc618)
    %1702 = stablehlo.reshape %1701 : (tensor<24x50x50xbf16>) -> tensor<2x12x50x50xbf16> loc(#loc611)
    %1703 = stablehlo.multiply %1702, %cst_2 : tensor<2x12x50x50xbf16> loc(#loc619)
    %1704 = stablehlo.convert %1703 : (tensor<2x12x50x50xbf16>) -> tensor<2x12x50x50xf32> loc(#loc620)
    %1705 = stablehlo.reduce(%1704 init: %cst_6) applies stablehlo.maximum across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc621)
    %1706 = stablehlo.broadcast_in_dim %1705, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc621)
    %1707 = stablehlo.subtract %1704, %1706 : tensor<2x12x50x50xf32> loc(#loc621)
    %1708 = stablehlo.exponential %1707 : tensor<2x12x50x50xf32> loc(#loc621)
    %1709 = stablehlo.reduce(%1708 init: %cst_5) applies stablehlo.add across dimensions = [3] : (tensor<2x12x50x50xf32>, tensor<f32>) -> tensor<2x12x50xf32> loc(#loc621)
    %1710 = stablehlo.broadcast_in_dim %1709, dims = [0, 1, 2] : (tensor<2x12x50xf32>) -> tensor<2x12x50x50xf32> loc(#loc621)
    %1711 = stablehlo.divide %1708, %1710 : tensor<2x12x50x50xf32> loc(#loc621)
    %1712 = stablehlo.convert %1711 : (tensor<2x12x50x50xf32>) -> tensor<2x12x50x50xbf16> loc(#loc622)
    %1713 = stablehlo.reshape %1712 : (tensor<2x12x50x50xbf16>) -> tensor<24x50x50xbf16> loc(#loc623)
    %1714 = stablehlo.reshape %arg12 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1715 = stablehlo.custom_call @tt.mark_argument(%1714) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1716 = stablehlo.reshape %1715 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1717 = stablehlo.transpose %1716, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc624)
    %1718 = stablehlo.dot_general %1672, %1717, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc625)
    %1719 = stablehlo.reshape %1718 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc626)
    %1720 = stablehlo.reshape %arg11 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1721 = stablehlo.custom_call @tt.mark_argument(%1720) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1722 = stablehlo.reshape %1721 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1723 = stablehlo.broadcast_in_dim %1722, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc627)
    %1724 = stablehlo.add %1719, %1723 : tensor<2x50x768xbf16> loc(#loc627)
    %1725 = stablehlo.reshape %1724 : (tensor<2x50x768xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc628)
    %1726 = stablehlo.transpose %1725, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,12,50,64]{3,1,2,0}"} : (tensor<2x50x12x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc629)
    %1727 = stablehlo.reshape %1726 : (tensor<2x12x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc623)
    %1728 = stablehlo.dot_general %1713, %1727, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x50x50xbf16>, tensor<24x50x64xbf16>) -> tensor<24x50x64xbf16> loc(#loc630)
    %1729 = stablehlo.reshape %1728 : (tensor<24x50x64xbf16>) -> tensor<2x12x50x64xbf16> loc(#loc623)
    %1730 = stablehlo.transpose %1729, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[2,50,12,64]{3,1,2,0}"} : (tensor<2x12x50x64xbf16>) -> tensor<2x50x12x64xbf16> loc(#loc631)
    %1731 = stablehlo.reshape %1730 : (tensor<2x50x12x64xbf16>) -> tensor<100x768xbf16> loc(#loc632)
    %1732 = stablehlo.reshape %arg10 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc2)
    %1733 = stablehlo.custom_call @tt.mark_argument(%1732) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc3)
    %1734 = stablehlo.reshape %1733 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc2)
    %1735 = stablehlo.transpose %1734, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc633)
    %1736 = stablehlo.dot_general %1731, %1735, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x768xbf16>) -> tensor<100x768xbf16> loc(#loc634)
    %1737 = stablehlo.reshape %1736 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc632)
    %1738 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1739 = stablehlo.custom_call @tt.mark_argument(%1738) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1740 = stablehlo.reshape %1739 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1741 = stablehlo.broadcast_in_dim %1740, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc635)
    %1742 = stablehlo.add %1737, %1741 : tensor<2x50x768xbf16> loc(#loc635)
    %1743 = stablehlo.add %1648, %1742 : tensor<2x50x768xbf16> loc(#loc636)
    %1744 = stablehlo.reduce(%1743 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc637)
    %1745 = stablehlo.multiply %1744, %cst_4 : tensor<2x50xbf16> loc(#loc637)
    %1746 = stablehlo.broadcast_in_dim %1745, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc638)
    %1747 = stablehlo.subtract %1743, %1746 : tensor<2x50x768xbf16> loc(#loc638)
    %1748 = stablehlo.multiply %1747, %1747 : tensor<2x50x768xbf16> loc(#loc637)
    %1749 = stablehlo.reduce(%1748 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<2x50x768xbf16>, tensor<bf16>) -> tensor<2x50xbf16> loc(#loc637)
    %1750 = stablehlo.multiply %1749, %cst_4 : tensor<2x50xbf16> loc(#loc637)
    %1751 = stablehlo.reshape %1750 : (tensor<2x50xbf16>) -> tensor<2x50x1xbf16> loc(#loc637)
    %1752 = stablehlo.add %1751, %cst_3 : tensor<2x50x1xbf16> loc(#loc639)
    %1753 = stablehlo.rsqrt %1752 : tensor<2x50x1xbf16> loc(#loc640)
    %1754 = stablehlo.reshape %1753 : (tensor<2x50x1xbf16>) -> tensor<2x50xbf16> loc(#loc641)
    %1755 = stablehlo.broadcast_in_dim %1754, dims = [0, 1] : (tensor<2x50xbf16>) -> tensor<2x50x768xbf16> loc(#loc641)
    %1756 = stablehlo.multiply %1747, %1755 : tensor<2x50x768xbf16> loc(#loc641)
    %1757 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1758 = stablehlo.custom_call @tt.mark_argument(%1757) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm2_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1759 = stablehlo.reshape %1758 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1760 = stablehlo.broadcast_in_dim %1759, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc641)
    %1761 = stablehlo.multiply %1756, %1760 : tensor<2x50x768xbf16> loc(#loc641)
    %1762 = stablehlo.reshape %arg7 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1763 = stablehlo.custom_call @tt.mark_argument(%1762) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_layer_norm2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1764 = stablehlo.reshape %1763 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1765 = stablehlo.broadcast_in_dim %1764, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc639)
    %1766 = stablehlo.add %1761, %1765 : tensor<2x50x768xbf16> loc(#loc639)
    %1767 = stablehlo.reshape %1766 : (tensor<2x50x768xbf16>) -> tensor<100x768xbf16> loc(#loc642)
    %1768 = stablehlo.reshape %arg6 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc2)
    %1769 = stablehlo.custom_call @tt.mark_argument(%1768) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc3)
    %1770 = stablehlo.reshape %1769 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc2)
    %1771 = stablehlo.transpose %1770, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc643)
    %1772 = stablehlo.dot_general %1767, %1771, contracting_dims = [1] x [0] : (tensor<100x768xbf16>, tensor<768x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc644)
    %1773 = stablehlo.reshape %1772 : (tensor<100x3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc642)
    %1774 = stablehlo.reshape %arg5 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc2)
    %1775 = stablehlo.custom_call @tt.mark_argument(%1774) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc3)
    %1776 = stablehlo.reshape %1775 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc2)
    %1777 = stablehlo.broadcast_in_dim %1776, dims = [2] : (tensor<3072xbf16>) -> tensor<2x50x3072xbf16> loc(#loc645)
    %1778 = stablehlo.add %1773, %1777 : tensor<2x50x3072xbf16> loc(#loc645)
    %1779 = stablehlo.multiply %1778, %cst_1 : tensor<2x50x3072xbf16> loc(#loc646)
    %1780 = stablehlo.logistic %1779 : tensor<2x50x3072xbf16> loc(#loc647)
    %1781 = stablehlo.multiply %1778, %1780 : tensor<2x50x3072xbf16> loc(#loc646)
    %1782 = stablehlo.reshape %1781 : (tensor<2x50x3072xbf16>) -> tensor<100x3072xbf16> loc(#loc648)
    %1783 = stablehlo.reshape %arg4 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc2)
    %1784 = stablehlo.custom_call @tt.mark_argument(%1783) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc3)
    %1785 = stablehlo.reshape %1784 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc2)
    %1786 = stablehlo.transpose %1785, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc649)
    %1787 = stablehlo.dot_general %1782, %1786, contracting_dims = [1] x [0] : (tensor<100x3072xbf16>, tensor<3072x768xbf16>) -> tensor<100x768xbf16> loc(#loc650)
    %1788 = stablehlo.reshape %1787 : (tensor<100x768xbf16>) -> tensor<2x50x768xbf16> loc(#loc648)
    %1789 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1790 = stablehlo.custom_call @tt.mark_argument(%1789) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_encoder_layers_11_mlp_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1791 = stablehlo.reshape %1790 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1792 = stablehlo.broadcast_in_dim %1791, dims = [2] : (tensor<768xbf16>) -> tensor<2x50x768xbf16> loc(#loc651)
    %1793 = stablehlo.add %1788, %1792 : tensor<2x50x768xbf16> loc(#loc651)
    %1794 = stablehlo.add %1743, %1793 : tensor<2x50x768xbf16> loc(#loc652)
    %1795 = stablehlo.slice %1794 [0:2, 0:1, 0:768] : (tensor<2x50x768xbf16>) -> tensor<2x1x768xbf16> loc(#loc653)
    %1796 = stablehlo.reshape %1795 : (tensor<2x1x768xbf16>) -> tensor<2x768xbf16> loc(#loc654)
    %1797 = stablehlo.reduce(%1796 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<2x768xbf16>, tensor<bf16>) -> tensor<2xbf16> loc(#loc655)
    %1798 = stablehlo.multiply %1797, %cst_0 : tensor<2xbf16> loc(#loc655)
    %1799 = stablehlo.broadcast_in_dim %1798, dims = [0] : (tensor<2xbf16>) -> tensor<2x768xbf16> loc(#loc656)
    %1800 = stablehlo.subtract %1796, %1799 : tensor<2x768xbf16> loc(#loc656)
    %1801 = stablehlo.multiply %1800, %1800 : tensor<2x768xbf16> loc(#loc655)
    %1802 = stablehlo.reduce(%1801 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<2x768xbf16>, tensor<bf16>) -> tensor<2xbf16> loc(#loc655)
    %1803 = stablehlo.multiply %1802, %cst_0 : tensor<2xbf16> loc(#loc655)
    %1804 = stablehlo.reshape %1803 : (tensor<2xbf16>) -> tensor<2x1xbf16> loc(#loc655)
    %1805 = stablehlo.add %1804, %cst : tensor<2x1xbf16> loc(#loc657)
    %1806 = stablehlo.rsqrt %1805 : tensor<2x1xbf16> loc(#loc658)
    %1807 = stablehlo.reshape %1806 : (tensor<2x1xbf16>) -> tensor<2xbf16> loc(#loc659)
    %1808 = stablehlo.broadcast_in_dim %1807, dims = [0] : (tensor<2xbf16>) -> tensor<2x768xbf16> loc(#loc659)
    %1809 = stablehlo.multiply %1800, %1808 : tensor<2x768xbf16> loc(#loc659)
    %1810 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1811 = stablehlo.custom_call @tt.mark_argument(%1810) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_post_layernorm_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1812 = stablehlo.reshape %1811 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1813 = stablehlo.broadcast_in_dim %1812, dims = [1] : (tensor<768xbf16>) -> tensor<2x768xbf16> loc(#loc659)
    %1814 = stablehlo.multiply %1809, %1813 : tensor<2x768xbf16> loc(#loc659)
    %1815 = stablehlo.reshape %arg1 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %1816 = stablehlo.custom_call @tt.mark_argument(%1815) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___vision_model_post_layernorm_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %1817 = stablehlo.reshape %1816 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc2)
    %1818 = stablehlo.broadcast_in_dim %1817, dims = [1] : (tensor<768xbf16>) -> tensor<2x768xbf16> loc(#loc657)
    %1819 = stablehlo.add %1814, %1818 : tensor<2x768xbf16> loc(#loc657)
    %1820 = stablehlo.reshape %arg0 : (tensor<512x768xbf16>) -> tensor<1x512x768xbf16> loc(#loc2)
    %1821 = stablehlo.custom_call @tt.mark_argument(%1820) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___visual_projection_weight"}} : (tensor<1x512x768xbf16>) -> tensor<1x512x768xbf16> loc(#loc3)
    %1822 = stablehlo.reshape %1821 : (tensor<1x512x768xbf16>) -> tensor<512x768xbf16> loc(#loc2)
    %1823 = stablehlo.transpose %1822, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,512]{0,1}"} : (tensor<512x768xbf16>) -> tensor<768x512xbf16> loc(#loc660)
    %1824 = stablehlo.dot_general %1819, %1823, contracting_dims = [1] x [0] : (tensor<2x768xbf16>, tensor<768x512xbf16>) -> tensor<2x512xbf16> loc(#loc661)
    return %1824, %1794 : tensor<2x512xbf16>, tensor<2x50x768xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("unknown|unknown|-1|unknownaten__view")
#loc3 = loc("unknown|unknown|-1|unknownxla__custom_call")
#loc4 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|227|aten__expand")
#loc5 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Conv2d[vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|224|aten__convolution_overrideable")
#loc6 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|225|aten__view")
#loc7 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|225|aten__permute")
#loc8 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|228|aten__cat")
#loc9 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Embedding[vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__view")
#loc10 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|Embedding[vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__index_select")
#loc11 = loc("CLIPVisionTransformer[vision_model]|CLIPVisionEmbeddings[vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:217|forward|232|aten__add")
#loc12 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__var_mean")
#loc13 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__sub")
#loc14 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__add")
#loc15 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__rsqrt")
#loc16 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|783|aten__mul")
#loc17 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc18 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc19 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc20 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc21 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc22 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc23 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc24 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc25 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc26 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc27 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc28 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc29 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc30 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc31 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc32 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc33 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc34 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc35 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc36 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc37 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc38 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc39 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc40 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc41 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc42 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc43 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc44 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc45 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc46 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc47 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc48 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc49 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc50 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc51 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc52 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPAttention[vision_model.encoder.layers[0].self_attn]|Linear[vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc53 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc54 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc55 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc56 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc57 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc58 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|LayerNorm[vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc59 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc60 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc61 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc62 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc63 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|QuickGELUActivation[vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc64 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|QuickGELUActivation[vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc65 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc66 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc67 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc68 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|CLIPMLP[vision_model.encoder.layers[0].mlp]|Linear[vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc69 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc70 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc71 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc72 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc73 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc74 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc75 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc76 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc77 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc78 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc79 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc80 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc81 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc82 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc83 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc84 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc85 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc86 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc87 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc88 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc89 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc90 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc91 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc92 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc93 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc94 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc95 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc96 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc97 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc98 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc99 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc100 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc101 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc102 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc103 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc104 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc105 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPAttention[vision_model.encoder.layers[1].self_attn]|Linear[vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc106 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc107 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc108 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc109 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc110 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc111 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|LayerNorm[vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc112 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc113 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc114 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc115 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc116 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|QuickGELUActivation[vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc117 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|QuickGELUActivation[vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc118 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc119 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc120 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc121 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|CLIPMLP[vision_model.encoder.layers[1].mlp]|Linear[vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc122 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc123 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc124 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc125 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc126 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc127 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc128 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc129 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc130 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc131 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc132 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc133 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc134 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc135 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc136 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc137 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc138 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc139 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc140 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc141 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc142 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc143 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc144 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc145 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc146 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc147 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc148 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc149 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc150 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc151 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc152 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc153 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc154 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc155 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc156 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc157 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc158 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPAttention[vision_model.encoder.layers[2].self_attn]|Linear[vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc159 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc160 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc161 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc162 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc163 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc164 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|LayerNorm[vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc165 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc166 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc167 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc168 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc169 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|QuickGELUActivation[vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc170 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|QuickGELUActivation[vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc171 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc172 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc173 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc174 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|CLIPMLP[vision_model.encoder.layers[2].mlp]|Linear[vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc175 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc176 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc177 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc178 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc179 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc180 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc181 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc182 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc183 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc184 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc185 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc186 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc187 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc188 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc189 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc190 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc191 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc192 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc193 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc194 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc195 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc196 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc197 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc198 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc199 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc200 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc201 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc202 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc203 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc204 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc205 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc206 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc207 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc208 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc209 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc210 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc211 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPAttention[vision_model.encoder.layers[3].self_attn]|Linear[vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc212 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc213 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc214 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc215 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc216 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc217 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|LayerNorm[vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc218 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc219 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc220 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc221 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc222 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|QuickGELUActivation[vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc223 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|QuickGELUActivation[vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc224 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc225 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc226 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc227 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|CLIPMLP[vision_model.encoder.layers[3].mlp]|Linear[vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc228 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc229 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc230 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc231 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc232 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc233 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc234 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc235 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc236 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc237 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc238 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc239 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc240 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc241 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc242 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc243 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc244 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc245 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc246 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc247 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc248 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc249 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc250 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc251 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc252 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc253 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc254 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc255 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc256 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc257 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc258 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc259 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc260 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc261 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc262 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc263 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc264 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPAttention[vision_model.encoder.layers[4].self_attn]|Linear[vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc265 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc266 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc267 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc268 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc269 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc270 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|LayerNorm[vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc271 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc272 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc273 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc274 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc275 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|QuickGELUActivation[vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc276 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|QuickGELUActivation[vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc277 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc278 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc279 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc280 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|CLIPMLP[vision_model.encoder.layers[4].mlp]|Linear[vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc281 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc282 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc283 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc284 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc285 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc286 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc287 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc288 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc289 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc290 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc291 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc292 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc293 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc294 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc295 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc296 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc297 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc298 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc299 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc300 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc301 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc302 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc303 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc304 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc305 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc306 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc307 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc308 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc309 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc310 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc311 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc312 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc313 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc314 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc315 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc316 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc317 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPAttention[vision_model.encoder.layers[5].self_attn]|Linear[vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc318 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc319 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc320 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc321 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc322 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc323 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|LayerNorm[vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc324 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc325 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc326 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc327 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc328 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|QuickGELUActivation[vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc329 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|QuickGELUActivation[vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc330 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc331 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc332 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc333 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|CLIPMLP[vision_model.encoder.layers[5].mlp]|Linear[vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc334 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc335 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc336 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc337 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc338 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc339 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc340 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc341 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc342 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc343 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc344 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc345 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc346 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc347 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc348 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc349 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc350 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc351 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc352 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc353 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc354 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc355 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc356 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc357 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc358 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc359 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc360 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc361 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc362 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc363 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc364 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc365 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc366 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc367 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc368 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc369 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc370 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPAttention[vision_model.encoder.layers[6].self_attn]|Linear[vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc371 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc372 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc373 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc374 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc375 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc376 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|LayerNorm[vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc377 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc378 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc379 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc380 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc381 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|QuickGELUActivation[vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc382 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|QuickGELUActivation[vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc383 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc384 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc385 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc386 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|CLIPMLP[vision_model.encoder.layers[6].mlp]|Linear[vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc387 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc388 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc389 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc390 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc391 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc392 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc393 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc394 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc395 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc396 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc397 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc398 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc399 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc400 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc401 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc402 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc403 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc404 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc405 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc406 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc407 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc408 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc409 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc410 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc411 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc412 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc413 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc414 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc415 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc416 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc417 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc418 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc419 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc420 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc421 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc422 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc423 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPAttention[vision_model.encoder.layers[7].self_attn]|Linear[vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc424 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc425 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc426 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc427 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc428 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc429 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|LayerNorm[vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc430 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc431 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc432 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc433 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc434 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|QuickGELUActivation[vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc435 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|QuickGELUActivation[vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc436 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc437 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc438 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc439 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|CLIPMLP[vision_model.encoder.layers[7].mlp]|Linear[vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc440 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc441 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc442 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc443 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc444 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc445 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc446 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc447 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc448 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc449 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc450 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc451 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc452 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc453 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc454 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc455 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc456 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc457 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc458 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc459 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc460 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc461 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc462 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc463 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc464 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc465 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc466 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc467 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc468 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc469 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc470 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc471 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc472 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc473 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc474 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc475 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc476 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPAttention[vision_model.encoder.layers[8].self_attn]|Linear[vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc477 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc478 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc479 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc480 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc481 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc482 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|LayerNorm[vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc483 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc484 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc485 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc486 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc487 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|QuickGELUActivation[vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc488 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|QuickGELUActivation[vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc489 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc490 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc491 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc492 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|CLIPMLP[vision_model.encoder.layers[8].mlp]|Linear[vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc493 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc494 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc495 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc496 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc497 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc498 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc499 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc500 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc501 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc502 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc503 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc504 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc505 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc506 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc507 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc508 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc509 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc510 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc511 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc512 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc513 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc514 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc515 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc516 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc517 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc518 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc519 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc520 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc521 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc522 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc523 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc524 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc525 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc526 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc527 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc528 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc529 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPAttention[vision_model.encoder.layers[9].self_attn]|Linear[vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc530 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc531 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc532 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc533 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc534 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc535 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|LayerNorm[vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc536 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc537 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc538 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc539 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc540 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|QuickGELUActivation[vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc541 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|QuickGELUActivation[vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc542 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc543 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc544 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc545 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|CLIPMLP[vision_model.encoder.layers[9].mlp]|Linear[vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc546 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc547 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc548 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc549 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc550 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc551 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc552 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc553 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc554 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc555 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc556 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc557 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc558 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc559 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc560 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc561 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc562 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc563 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc564 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc565 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc566 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc567 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc568 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc569 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc570 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc571 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc572 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc573 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc574 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc575 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc576 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc577 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc578 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc579 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc580 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc581 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc582 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPAttention[vision_model.encoder.layers[10].self_attn]|Linear[vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc583 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc584 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc585 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc586 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc587 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc588 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|LayerNorm[vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc589 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc590 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc591 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc592 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc593 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|QuickGELUActivation[vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc594 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|QuickGELUActivation[vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc595 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc596 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc597 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc598 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|CLIPMLP[vision_model.encoder.layers[10].mlp]|Linear[vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc599 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc600 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__var_mean")
#loc601 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__sub")
#loc602 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__add")
#loc603 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__rsqrt")
#loc604 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|424|aten__mul")
#loc605 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__view")
#loc606 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__permute")
#loc607 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__mm")
#loc608 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|334|aten__add")
#loc609 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__view")
#loc610 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|338|aten__permute")
#loc611 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__view")
#loc612 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__permute")
#loc613 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__mm")
#loc614 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__view")
#loc615 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|335|aten__add")
#loc616 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|339|aten__view")
#loc617 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__permute")
#loc618 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__matmul")
#loc619 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|287|aten__mul")
#loc620 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|xla__cast")
#loc621 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|290|aten__softmax")
#loc622 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|291|xla__cast")
#loc623 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__view")
#loc624 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__permute")
#loc625 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__mm")
#loc626 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__view")
#loc627 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|336|aten__add")
#loc628 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__view")
#loc629 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|340|aten__permute")
#loc630 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|293|aten__matmul")
#loc631 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:276|eager_attention_forward|294|aten__permute")
#loc632 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__view")
#loc633 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__permute")
#loc634 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__mm")
#loc635 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPAttention[vision_model.encoder.layers[11].self_attn]|Linear[vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:323|forward|374|aten__add")
#loc636 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|431|aten__add")
#loc637 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__var_mean")
#loc638 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__sub")
#loc639 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__add")
#loc640 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__rsqrt")
#loc641 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|LayerNorm[vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|434|aten__mul")
#loc642 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__view")
#loc643 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__permute")
#loc644 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__mm")
#loc645 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|390|aten__add")
#loc646 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|QuickGELUActivation[vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__mul")
#loc647 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|QuickGELUActivation[vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/activations.py:86|forward|87|aten__sigmoid")
#loc648 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__view")
#loc649 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__permute")
#loc650 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__mm")
#loc651 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|CLIPMLP[vision_model.encoder.layers[11].mlp]|Linear[vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:389|forward|392|aten__add")
#loc652 = loc("CLIPVisionTransformer[vision_model]|CLIPEncoder[vision_model.encoder]|CLIPEncoderLayer[vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:405|forward|436|aten__add")
#loc653 = loc("CLIPVisionTransformer[vision_model]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|792|xla__generic_slice")
#loc654 = loc("CLIPVisionTransformer[vision_model]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|792|aten__view")
#loc655 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__var_mean")
#loc656 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__sub")
#loc657 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__add")
#loc658 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__rsqrt")
#loc659 = loc("CLIPVisionTransformer[vision_model]|LayerNorm[vision_model.post_layernorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:767|forward|793|aten__mul")
#loc660 = loc("Linear[visual_projection]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:1169|forward|1203|aten__permute")
#loc661 = loc("Linear[visual_projection]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:1169|forward|1203|aten__mm")
