WARNING:root:Defaulting to PJRT_DEVICE=CPU
Using TT-Metal from the source tree: /localdev/hshah/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /localdev/hshah/tt-xla/venv/bin/python
cachedir: .pytest_cache
rootdir: /localdev/hshah/tt-xla
configfile: pytest.ini
plugins: anyio-4.11.0, jaxtyping-0.3.3, forked-1.6.0, split-0.10.0
collecting ... collected 1 item

tests/torch/single_chip/graphs/test_simple_matmul.py::test_matmul_correct_sharding 2025-10-23 04:33:09.107 (   0.000s) [        69EA1480]      dylib_platform.cc:47       1| DylibPlatform::SubclassInitialize
2025-10-23 04:33:09.114 (   0.007s) [        69EA1480]     client_instance.cc:44       1| ClientInstance::ClientInstance
2025-10-23 04:33:09.114 (   0.007s) [        69EA1480]              client.cc:18       1| TTClientInstance::TTClientInstance
2025-10-23 04:33:09.114 (   0.007s) [        69EA1480]     client_instance.cc:73       1| ClientInstance::Initialize
2025-10-23 04:33:22.289 (  13.182s) [        69EA1480]              stubs.inc:106   WARN| STUB: PJRT_Client_TopologyDescription
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     client_instance.cc:510      1| ClientInstance::PJRT_Client_PlatformVersion
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     client_instance.cc:490      1| ClientInstance::PJRT_Client_PlatformName
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     client_instance.cc:522      1| ClientInstance::PJRT_Client_Devices
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:33:22.289 (  13.183s) [        69EA1480]     client_instance.cc:535      1| ClientInstance::PJRT_Client_AddressableDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     client_instance.cc:585      1| ClientInstance::PJRT_Client_AddressableMemories
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]        api_bindings.cc:76       1| PJRT_Plugin_Attributes
2025-10-23 04:33:22.290144: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.290 (  13.183s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.604 (  13.497s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.604 (  13.497s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.604 (  13.497s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.604 (  13.497s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.604 (  13.497s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.604 (  13.497s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.604 (  13.497s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.604 (  13.497s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.615 (  13.508s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.615 (  13.508s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.615 (  13.508s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.615 (  13.508s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.615 (  13.508s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.615 (  13.508s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.615 (  13.508s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.615 (  13.508s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.617 (  13.510s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.617 (  13.510s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.617 (  13.510s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.617 (  13.510s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.617 (  13.510s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.617 (  13.510s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.617 (  13.510s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.617 (  13.510s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.617 (  13.511s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.618 (  13.511s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.618 (  13.512s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.618 (  13.512s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.618 (  13.512s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.619 (  13.512s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.619 (  13.513s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.619 (  13.513s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.619 (  13.513s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.619 (  13.513s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.622 (  13.515s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.622 (  13.515s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.622 (  13.515s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.622 (  13.516s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.622 (  13.516s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.622 (  13.516s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.622 (  13.516s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.622 (  13.516s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.623 (  13.516s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.623 (  13.517s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.623 (  13.517s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.623 (  13.517s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.623 (  13.517s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.623 (  13.517s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.623 (  13.517s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.623 (  13.517s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.627 (  13.520s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.627 (  13.521s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.628 (  13.521s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.628 (  13.522s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.522s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.523s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.629 (  13.523s) [        69EA1480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:33:22.635 (  13.528s) [        69EA1480]     client_instance.cc:598      1| ClientInstance::PJRT_Client_Compile
2025-10-23 04:33:22.635 (  13.528s) [        69EA1480]      module_builder.cc:221      1| ModuleBuilder::buildModule
2025-10-23 04:33:22.636 (  13.530s) [        69EA1480]      module_builder.cc:963      1| MLIR Module vhlo:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc("p0.1"), %arg1: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc("p1.2"), %arg2: !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc("p2.4"), %arg3: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc("p3.5"), %arg4: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc("p4.7")) -> (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.transpose_v1"(%arg3) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc6)
    %3 = "vhlo.dot_general_v2"(%arg4, %2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc7)
    %4 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc8)
    %5 = "vhlo.add_v1"(%3, %4) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc9)
    %6 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc10)
    %7 = "vhlo.dot_general_v2"(%5, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc11)
    %8 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc12)
    %9 = "vhlo.add_v1"(%7, %8) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc13)
    %10 = "vhlo.maximum_v1"(%9, %1) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc14)
    "vhlo.return_v1"(%10) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("transpose.3")
#loc11 = loc("dot.14")
#loc12 = loc("broadcast.18")
#loc13 = loc("add.19")
#loc14 = loc("maximum.22")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<2048x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<2048x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %2 = "vhlo.transpose_v1"(%arg3) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %3 = "vhlo.dot_general_v2"(%arg4, %2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %5 = "vhlo.add_v1"(%3, %4) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %6 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %7 = "vhlo.dot_general_v2"(%5, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %8 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %9 = "vhlo.add_v1"(%7, %8) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %10 = "vhlo.maximum_v1"(%9, %1) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    "vhlo.return_v1"(%10) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<2048x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<2048x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %2 = "vhlo.transpose_v1"(%arg3) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %3 = "vhlo.dot_general_v2"(%arg4, %2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %5 = "vhlo.add_v1"(%3, %4) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %6 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %7 = "vhlo.dot_general_v2"(%5, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %8 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %9 = "vhlo.add_v1"(%7, %8) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %10 = "vhlo.maximum_v1"(%9, %1) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    "vhlo.return_v1"(%10) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


2025-10-23 04:33:22.641 (  13.535s) [        69EA1480]      module_builder.cc:963      1| MLIR Module shlo:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p0.1"), %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} loc("p1.2"), %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"} loc("p2.4"), %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"} loc("p3.5"), %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p4.7")) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16> loc(#loc)
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc6)
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc7)
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc8)
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16> loc(#loc9)
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc10)
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc11)
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16> loc(#loc13)
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16> loc(#loc14)
    return %9 : tensor<2048x2048xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("transpose.3")
#loc11 = loc("dot.14")
#loc12 = loc("broadcast.18")
#loc13 = loc("add.19")
#loc14 = loc("maximum.22")
------------------ END OF MLIR MODULE ------------------
2025-10-23 04:33:22.643 (  13.536s) [        69EA1480]      module_builder.cc:963      1| MLIR Module shlo_frontend:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p0.1"), %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p1.2"), %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p2.4"), %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p3.5"), %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p4.7")) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16> loc(#loc)
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc6)
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc7)
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc8)
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16> loc(#loc9)
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc10)
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc11)
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16> loc(#loc13)
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16> loc(#loc14)
    return %9 : tensor<2048x2048xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("transpose.3")
#loc11 = loc("dot.14")
#loc12 = loc("broadcast.18")
#loc13 = loc("add.19")
#loc14 = loc("maximum.22")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.add %6, %7 : tensor<2048x2048xbf16>
    %9 = stablehlo.maximum %8, %0 : tensor<2048x2048xbf16>
    return %9 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = sdy.all_reduce {"_axis_0"} %6 out_sharding=<@mesh, [{}, {}]> : tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = sdy.all_reduce {"_axis_0"} %6 out_sharding=<@mesh, [{}, {}]> : tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {?}]>] manual_axes={} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x2048xbf16>, %arg7: tensor<2048xbf16>, %arg8: tensor<2048x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %5 = stablehlo.add %3, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = sdy.all_reduce {"_axis_0"} %7 out_sharding=<@mesh, [{}, {}]> : tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {?}]>] manual_axes={} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x2048xbf16>, %arg7: tensor<2048xbf16>, %arg8: tensor<2048x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %5 = stablehlo.add %3, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = sdy.all_reduce {"_axis_0"} %7 out_sharding=<@mesh, [{}, {}]> : tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {?}]>] manual_axes={} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x2048xbf16>, %arg7: tensor<2048xbf16>, %arg8: tensor<2048x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %5 = stablehlo.add %3, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = sdy.all_reduce {"_axis_0"} %7 out_sharding=<@mesh, [{}, {}]> : tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


2025-10-23 04:33:22.657 (  13.551s) [        69EA1480]      module_builder.cc:963      1| MLIR Module shlo_compiler:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
#loc11 = loc("dot.14")
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]> loc(#loc)
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p0.1"), %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p1.2"), %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p2.4"), %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p3.5"), %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p4.7")) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16> loc("p0.1"), %arg6: tensor<2048x256xbf16> loc("p1.2"), %arg7: tensor<256xbf16> loc("p2.4"), %arg8: tensor<256x2048xbf16> loc("p3.5"), %arg9: tensor<2048x2048xbf16> loc("p4.7")) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16> loc(#loc)
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16> loc(#loc6)
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc7)
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16> loc(#loc8)
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16> loc(#loc9)
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16> loc(#loc10)
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc11)
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16> loc("dot.14"), %arg11: tensor<bf16> loc("dot.14")):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16> loc(#loc11)
        stablehlo.return %12 : tensor<bf16> loc(#loc11)
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc11)
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16> loc(#loc13)
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16> loc(#loc14)
      sdy.return %11 : tensor<2048x2048xbf16> loc(#loc)
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc)
    return %0 : tensor<2048x2048xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("transpose.3")
#loc12 = loc("broadcast.18")
#loc13 = loc("add.19")
#loc14 = loc("maximum.22")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before LegalizeStableHLOCompositeToTTIR (legalize-stablehlo-composite-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = "stablehlo.all_reduce"(%7) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %12 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %12 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.add %8, %9 : tensor<2048x2048xbf16>
      %11 = stablehlo.maximum %10, %1 : tensor<2048x2048xbf16>
      sdy.return %11 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<2048xbf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %6 = ttir.empty() : tensor<1x1xbf16>
    %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %8 = ttir.empty() : tensor<2048x2048xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %10 = ttir.empty() : tensor<2048x256xbf16>
    %11 = "ttir.permute"(%3, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %12 = "ttir.dot_general"(%4, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %13 = ttir.empty() : tensor<1x256xbf16>
    %14 = "ttir.reshape"(%2, %13) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<2048x256xbf16>
    %18 = "ttir.add"(%12, %16, %17) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %19 = ttir.empty() : tensor<256x2048xbf16>
    %20 = "ttir.permute"(%1, %19) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %21 = "ttir.dot_general"(%18, %20) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %22 = ttir.empty() : tensor<2048x2048xbf16>
    %23 = "ttir.all_reduce"(%21, %22) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %24 = ttir.empty() : tensor<1x2048xbf16>
    %25 = "ttir.reshape"(%0, %24) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
    %26 = ttir.empty() : tensor<2048x2048xbf16>
    %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %28 = ttir.empty() : tensor<2048x2048xbf16>
    %29 = "ttir.add"(%23, %27, %28) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %30 = ttir.empty() : tensor<2048x2048xbf16>
    %31 = "ttir.maximum"(%29, %9, %30) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %32 = "ttir.mesh_shard"(%31) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %32 : tensor<2048x2048xbf16>
  }
}


2025-10-23 04:33:22.664 (  13.557s) [        69EA1480]      module_builder.cc:963      1| MLIR Module ttir:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p0.1"), %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p1.2"), %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p2.4"), %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p3.5"), %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p4.7")) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc)
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16> loc(#loc)
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16> loc(#loc)
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16> loc(#loc)
    %4 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc)
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
    %6 = ttir.empty() : tensor<1x1xbf16> loc(#loc)
    %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16> loc(#loc)
    %8 = ttir.empty() : tensor<2048x2048xbf16> loc(#loc)
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc)
    %10 = ttir.empty() : tensor<2048x256xbf16> loc(#loc6)
    %11 = "ttir.permute"(%3, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc6)
    %12 = "ttir.dot_general"(%4, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc7)
    %13 = ttir.empty() : tensor<1x256xbf16> loc(#loc8)
    %14 = "ttir.reshape"(%2, %13) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16> loc(#loc8)
    %15 = ttir.empty() : tensor<2048x256xbf16> loc(#loc8)
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc8)
    %17 = ttir.empty() : tensor<2048x256xbf16> loc(#loc9)
    %18 = "ttir.add"(%12, %16, %17) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc9)
    %19 = ttir.empty() : tensor<256x2048xbf16> loc(#loc10)
    %20 = "ttir.permute"(%1, %19) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16> loc(#loc10)
    %21 = "ttir.dot_general"(%18, %20) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc11)
    %22 = ttir.empty() : tensor<2048x2048xbf16> loc(#loc11)
    %23 = "ttir.all_reduce"(%21, %22) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc11)
    %24 = ttir.empty() : tensor<1x2048xbf16> loc(#loc12)
    %25 = "ttir.reshape"(%0, %24) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16> loc(#loc12)
    %26 = ttir.empty() : tensor<2048x2048xbf16> loc(#loc12)
    %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
    %28 = ttir.empty() : tensor<2048x2048xbf16> loc(#loc13)
    %29 = "ttir.add"(%23, %27, %28) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc13)
    %30 = ttir.empty() : tensor<2048x2048xbf16> loc(#loc14)
    %31 = "ttir.maximum"(%29, %9, %30) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc14)
    %32 = "ttir.mesh_shard"(%31) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc)
    return %32 : tensor<2048x2048xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("transpose.3")
#loc11 = loc("dot.14")
#loc12 = loc("broadcast.18")
#loc13 = loc("add.19")
#loc14 = loc("maximum.22")
------------------ END OF MLIR MODULE ------------------
2025-10-23 04:33:22.665 (  13.559s) [        69EA1480]      module_builder.cc:772   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-10-23 04:33:22.665 (  13.559s) [        69EA1480]      module_builder.cc:786   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-10-23 04:33:22.665 (  13.559s) [        69EA1480]      module_builder.cc:798   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-10-23 04:33:22.666 (  13.559s) [        69EA1480]     client_instance.cc:373      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 8]
2025-10-23 04:33:22.666 (  13.559s) [        69EA1480]     client_instance.cc:471      1| ClientInstance::getOrCreateOptimizerSubmesh - creating optimizer submesh
// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<2048xbf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %6 = ttir.empty() : tensor<1x1xbf16>
    %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %8 = ttir.empty() : tensor<2048x2048xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %10 = ttir.empty() : tensor<2048x256xbf16>
    %11 = "ttir.permute"(%3, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %12 = "ttir.dot_general"(%4, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %13 = ttir.empty() : tensor<1x256xbf16>
    %14 = "ttir.reshape"(%2, %13) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<2048x256xbf16>
    %18 = "ttir.add"(%12, %16, %17) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %19 = ttir.empty() : tensor<256x2048xbf16>
    %20 = "ttir.permute"(%1, %19) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %21 = "ttir.dot_general"(%18, %20) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %22 = ttir.empty() : tensor<2048x2048xbf16>
    %23 = "ttir.all_reduce"(%21, %22) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %24 = ttir.empty() : tensor<1x2048xbf16>
    %25 = "ttir.reshape"(%0, %24) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
    %26 = ttir.empty() : tensor<2048x2048xbf16>
    %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %28 = ttir.empty() : tensor<2048x2048xbf16>
    %29 = "ttir.add"(%23, %27, %28) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %30 = ttir.empty() : tensor<2048x2048xbf16>
    %31 = "ttir.maximum"(%29, %9, %30) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %32 = "ttir.mesh_shard"(%31) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %32 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x2048xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = "ttir.dot_general"(%arg4, %9) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %11 = ttir.empty() : tensor<1x256xbf16>
    %12 = "ttir.reshape"(%2, %11) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %13 = ttir.empty() : tensor<2048x256xbf16>
    %14 = "ttir.broadcast"(%12, %13) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.add"(%10, %14, %15) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<256x2048xbf16>
    %18 = "ttir.permute"(%1, %17) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %20 = ttir.empty() : tensor<2048x2048xbf16>
    %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %22 = ttir.empty() : tensor<1x2048xbf16>
    %23 = "ttir.reshape"(%arg0, %22) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
    %24 = ttir.empty() : tensor<2048x2048xbf16>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %26 = ttir.empty() : tensor<2048x2048xbf16>
    %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %28 = ttir.empty() : tensor<2048x2048xbf16>
    %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %30 = "ttir.mesh_shard"(%29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %30 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x2048xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = "ttir.dot_general"(%arg4, %9) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %11 = ttir.empty() : tensor<1x256xbf16>
    %12 = "ttir.reshape"(%2, %11) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %13 = ttir.empty() : tensor<2048x256xbf16>
    %14 = "ttir.broadcast"(%12, %13) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.add"(%10, %14, %15) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<256x2048xbf16>
    %18 = "ttir.permute"(%1, %17) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %20 = ttir.empty() : tensor<2048x2048xbf16>
    %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %22 = ttir.empty() : tensor<1x2048xbf16>
    %23 = "ttir.reshape"(%arg0, %22) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
    %24 = ttir.empty() : tensor<2048x2048xbf16>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %26 = ttir.empty() : tensor<2048x2048xbf16>
    %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %28 = ttir.empty() : tensor<2048x2048xbf16>
    %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %30 = "ttir.mesh_shard"(%29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %30 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x2048xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = "ttir.dot_general"(%arg4, %9) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %11 = ttir.empty() : tensor<1x256xbf16>
    %12 = "ttir.reshape"(%2, %11) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %13 = ttir.empty() : tensor<2048x256xbf16>
    %14 = "ttir.broadcast"(%12, %13) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.add"(%10, %14, %15) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<256x2048xbf16>
    %18 = "ttir.permute"(%1, %17) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %19 = "ttir.dot_general"(%16, %18) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %20 = ttir.empty() : tensor<2048x2048xbf16>
    %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %22 = ttir.empty() : tensor<1x2048xbf16>
    %23 = "ttir.reshape"(%arg0, %22) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
    %24 = ttir.empty() : tensor<2048x2048xbf16>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %26 = ttir.empty() : tensor<2048x2048xbf16>
    %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %28 = ttir.empty() : tensor<2048x2048xbf16>
    %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %30 = "ttir.mesh_shard"(%29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %30 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x2048xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = ttir.empty() : tensor<2048x2048xbf16>
    %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %12 = ttir.empty() : tensor<2048x256xbf16>
    %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %14 = ttir.empty() : tensor<2048x2048xbf16>
    %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %16 = ttir.empty() : tensor<2048x256xbf16>
    %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %18 = ttir.empty() : tensor<2048x256xbf16>
    %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %20 = ttir.empty() : tensor<2048x256xbf16>
    %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %22 = ttir.empty() : tensor<1x256xbf16>
    %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %24 = ttir.empty() : tensor<2048x256xbf16>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %26 = ttir.empty() : tensor<2048x256xbf16>
    %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %28 = ttir.empty() : tensor<256x2048xbf16>
    %29 = "ttir.permute"(%1, %28) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %30 = ttir.empty() : tensor<2048x256xbf16>
    %31 = "ttir.permute"(%27, %30) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %32 = ttir.empty() : tensor<256x2048xbf16>
    %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %34 = ttir.empty() : tensor<2048x256xbf16>
    %35 = "ttir.reshape"(%31, %34) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %36 = ttir.empty() : tensor<256x2048xbf16>
    %37 = "ttir.reshape"(%33, %36) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %38 = ttir.empty() : tensor<2048x2048xbf16>
    %39 = "ttir.matmul"(%35, %37, %38) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %40 = ttir.empty() : tensor<2048x2048xbf16>
    %41 = "ttir.reshape"(%39, %40) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %42 = ttir.empty() : tensor<2048x2048xbf16>
    %43 = "ttir.all_reduce"(%41, %42) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %44 = ttir.empty() : tensor<1x2048xbf16>
    %45 = "ttir.reshape"(%arg0, %44) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
    %46 = ttir.empty() : tensor<2048x2048xbf16>
    %47 = "ttir.broadcast"(%45, %46) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %48 = ttir.empty() : tensor<2048x2048xbf16>
    %49 = "ttir.add"(%43, %47, %48) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %50 = ttir.empty() : tensor<2048x2048xbf16>
    %51 = "ttir.maximum"(%49, %7, %50) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %52 = "ttir.mesh_shard"(%51) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %52 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x2048xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = ttir.empty() : tensor<2048x2048xbf16>
    %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %12 = ttir.empty() : tensor<2048x256xbf16>
    %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %14 = ttir.empty() : tensor<2048x2048xbf16>
    %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %16 = ttir.empty() : tensor<2048x256xbf16>
    %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %18 = ttir.empty() : tensor<2048x256xbf16>
    %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %20 = ttir.empty() : tensor<2048x256xbf16>
    %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %22 = ttir.empty() : tensor<1x256xbf16>
    %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %24 = ttir.empty() : tensor<2048x256xbf16>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %26 = ttir.empty() : tensor<2048x256xbf16>
    %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %28 = ttir.empty() : tensor<256x2048xbf16>
    %29 = "ttir.permute"(%1, %28) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %30 = ttir.empty() : tensor<2048x256xbf16>
    %31 = "ttir.permute"(%27, %30) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %32 = ttir.empty() : tensor<256x2048xbf16>
    %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %34 = ttir.empty() : tensor<2048x256xbf16>
    %35 = "ttir.reshape"(%31, %34) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %36 = ttir.empty() : tensor<256x2048xbf16>
    %37 = "ttir.reshape"(%33, %36) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %38 = ttir.empty() : tensor<2048x2048xbf16>
    %39 = "ttir.matmul"(%35, %37, %38) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %40 = ttir.empty() : tensor<2048x2048xbf16>
    %41 = "ttir.reshape"(%39, %40) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %42 = ttir.empty() : tensor<2048x2048xbf16>
    %43 = "ttir.all_reduce"(%41, %42) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %44 = ttir.empty() : tensor<1x2048xbf16>
    %45 = "ttir.reshape"(%arg0, %44) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
    %46 = ttir.empty() : tensor<2048x2048xbf16>
    %47 = "ttir.broadcast"(%45, %46) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %48 = ttir.empty() : tensor<2048x2048xbf16>
    %49 = "ttir.add"(%43, %47, %48) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %50 = ttir.empty() : tensor<2048x2048xbf16>
    %51 = "ttir.maximum"(%49, %7, %50) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %52 = "ttir.mesh_shard"(%51) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %52 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<256x2048xbf16>
        %29 = "ttir.permute"(%1, %28) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %30 = ttir.empty() : tensor<2048x256xbf16>
        %31 = "ttir.permute"(%27, %30) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %32 = ttir.empty() : tensor<256x2048xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.reshape"(%31, %34) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<256x2048xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %38 = ttir.empty() : tensor<2048x2048xbf16>
        %39 = "ttir.matmul"(%35, %37, %38) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.reshape"(%39, %40) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.all_reduce"(%41, %42) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<1x2048xbf16>
        %45 = "ttir.reshape"(%arg0, %44) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %46 = ttir.empty() : tensor<2048x2048xbf16>
        %47 = "ttir.broadcast"(%45, %46) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %48 = ttir.empty() : tensor<2048x2048xbf16>
        %49 = "ttir.add"(%43, %47, %48) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %50 = ttir.empty() : tensor<2048x2048xbf16>
        %51 = "ttir.maximum"(%49, %7, %50) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %52 = "ttir.mesh_shard"(%51) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %52 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRHoistTransform (ttir-cpu-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<256x2048xbf16>
        %29 = "ttir.permute"(%1, %28) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %30 = ttir.empty() : tensor<2048x256xbf16>
        %31 = "ttir.permute"(%27, %30) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %32 = ttir.empty() : tensor<256x2048xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.reshape"(%31, %34) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<256x2048xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %38 = ttir.empty() : tensor<2048x2048xbf16>
        %39 = "ttir.matmul"(%35, %37, %38) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.reshape"(%39, %40) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.all_reduce"(%41, %42) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<1x2048xbf16>
        %45 = "ttir.reshape"(%arg0, %44) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %46 = ttir.empty() : tensor<2048x2048xbf16>
        %47 = "ttir.broadcast"(%45, %46) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %48 = ttir.empty() : tensor<2048x2048xbf16>
        %49 = "ttir.add"(%43, %47, %48) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %50 = ttir.empty() : tensor<2048x2048xbf16>
        %51 = "ttir.maximum"(%49, %7, %50) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %52 = "ttir.mesh_shard"(%51) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %52 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<256x2048xbf16>
        %29 = "ttir.permute"(%1, %28) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %30 = ttir.empty() : tensor<2048x256xbf16>
        %31 = "ttir.permute"(%27, %30) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %32 = ttir.empty() : tensor<256x2048xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.reshape"(%31, %34) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<256x2048xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %38 = ttir.empty() : tensor<2048x2048xbf16>
        %39 = "ttir.matmul"(%35, %37, %38) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.reshape"(%39, %40) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.all_reduce"(%41, %42) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<1x2048xbf16>
        %45 = "ttir.reshape"(%arg0, %44) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %46 = ttir.empty() : tensor<2048x2048xbf16>
        %47 = "ttir.broadcast"(%45, %46) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %48 = ttir.empty() : tensor<2048x2048xbf16>
        %49 = "ttir.add"(%43, %47, %48) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %50 = ttir.empty() : tensor<2048x2048xbf16>
        %51 = "ttir.maximum"(%49, %7, %50) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %52 = "ttir.mesh_shard"(%51) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %52 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<256x2048xbf16>
        %29 = "ttir.permute"(%1, %28) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %30 = ttir.empty() : tensor<2048x256xbf16>
        %31 = "ttir.permute"(%27, %30) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %32 = ttir.empty() : tensor<256x2048xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.reshape"(%31, %34) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<256x2048xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %38 = ttir.empty() : tensor<2048x2048xbf16>
        %39 = "ttir.matmul"(%35, %37, %38) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.reshape"(%39, %40) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.all_reduce"(%41, %42) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<1x2048xbf16>
        %45 = "ttir.reshape"(%arg0, %44) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %46 = ttir.empty() : tensor<2048x2048xbf16>
        %47 = "ttir.broadcast"(%45, %46) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %48 = ttir.empty() : tensor<2048x2048xbf16>
        %49 = "ttir.add"(%43, %47, %48) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %50 = ttir.empty() : tensor<2048x2048xbf16>
        %51 = "ttir.maximum"(%49, %7, %50) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %52 = "ttir.mesh_shard"(%51) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %52 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<256x2048xbf16>
        %29 = "ttir.permute"(%1, %28) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %30 = ttir.empty() : tensor<2048x256xbf16>
        %31 = "ttir.permute"(%27, %30) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %32 = ttir.empty() : tensor<256x2048xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.reshape"(%31, %34) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<256x2048xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %38 = ttir.empty() : tensor<2048x2048xbf16>
        %39 = "ttir.matmul"(%35, %37, %38) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.reshape"(%39, %40) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.all_reduce"(%41, %42) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<1x2048xbf16>
        %45 = "ttir.reshape"(%arg0, %44) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %46 = ttir.empty() : tensor<2048x2048xbf16>
        %47 = "ttir.broadcast"(%45, %46) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %48 = ttir.empty() : tensor<2048x2048xbf16>
        %49 = "ttir.add"(%43, %47, %48) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %50 = ttir.empty() : tensor<2048x2048xbf16>
        %51 = "ttir.maximum"(%49, %7, %50) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %52 = "ttir.mesh_shard"(%51) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %52 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<256x2048xbf16>
        %29 = "ttir.permute"(%1, %28) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %30 = ttir.empty() : tensor<2048x256xbf16>
        %31 = "ttir.permute"(%27, %30) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %32 = ttir.empty() : tensor<256x2048xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.reshape"(%31, %34) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<256x2048xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %38 = ttir.empty() : tensor<2048x2048xbf16>
        %39 = "ttir.matmul"(%35, %37, %38) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.reshape"(%39, %40) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.all_reduce"(%41, %42) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<1x2048xbf16>
        %45 = "ttir.reshape"(%arg0, %44) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %46 = ttir.empty() : tensor<2048x2048xbf16>
        %47 = "ttir.broadcast"(%45, %46) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %48 = ttir.empty() : tensor<2048x2048xbf16>
        %49 = "ttir.add"(%43, %47, %48) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %50 = ttir.empty() : tensor<2048x2048xbf16>
        %51 = "ttir.maximum"(%49, %7, %50) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %52 = "ttir.mesh_shard"(%51) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %52 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.matmul"(%arg4, %3, %8) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<1x256xbf16>
        %11 = "ttir.reshape"(%2, %10) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.add"(%9, %13, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<1x2048xbf16>
        %21 = "ttir.reshape"(%arg0, %20) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.add"(%19, %23, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = ttir.empty() : tensor<2048x2048xbf16>
        %27 = "ttir.maximum"(%25, %7, %26) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %28 = "ttir.mesh_shard"(%27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %28 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.matmul"(%arg4, %3, %8) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<1x256xbf16>
        %11 = "ttir.reshape"(%2, %10) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.add"(%9, %13, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<1x2048xbf16>
        %21 = "ttir.reshape"(%arg0, %20) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.add"(%19, %23, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = ttir.empty() : tensor<2048x2048xbf16>
        %27 = "ttir.maximum"(%25, %7, %26) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %28 = "ttir.mesh_shard"(%27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %28 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDequantConversion (ttir-quant-dequant-conversion) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRExplicateTMs (ttir-explicate-tms) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x2048xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 2048>}> : (tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<1x2048xbf16>
        %19 = "ttir.reshape"(%arg0, %18) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x2048xbf16>
        %23 = "ttir.add"(%17, %21, %22) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %24 = ttir.empty() : tensor<2048x2048xbf16>
        %25 = "ttir.maximum"(%23, %7, %24) : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %26 = "ttir.mesh_shard"(%25) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %26 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<1x256xbf16>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x2048xbf16>
        %13 = "ttir.matmul"(%11, %1, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.all_reduce"(%13, %14) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<1x2048xbf16>
        %17 = "ttir.reshape"(%arg0, %16) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.add"(%15, %17, %18) : (tensor<2048x2048xbf16>, tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.maximum"(%19, %5, %20) : (tensor<2048x2048xbf16>, tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = "ttir.mesh_shard"(%21) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %22 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDataTypeConversionPass (ttir-quant-data-type-conversion) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<1x256xbf16>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x2048xbf16>
        %13 = "ttir.matmul"(%11, %1, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.all_reduce"(%13, %14) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<1x2048xbf16>
        %17 = "ttir.reshape"(%arg0, %16) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.add"(%15, %17, %18) : (tensor<2048x2048xbf16>, tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.maximum"(%19, %5, %20) : (tensor<2048x2048xbf16>, tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = "ttir.mesh_shard"(%21) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %22 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<1x256xbf16>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x2048xbf16>
        %13 = "ttir.matmul"(%11, %1, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.all_reduce"(%13, %14) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<1x2048xbf16>
        %17 = "ttir.reshape"(%arg0, %16) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.add"(%15, %17, %18) : (tensor<2048x2048xbf16>, tensor<1x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.maximum"(%19, %5, %20) : (tensor<2048x2048xbf16>, tensor<1x1xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = "ttir.mesh_shard"(%21) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        return %22 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout3>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<256xbf16, #ttnn_layout5>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<256x2048xbf16, #ttnn_layout6>
        %4 = ttir.empty() : tensor<1x1xbf16, #ttnn_layout7>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %8 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %12 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %13 = "ttir.matmul"(%11, %1, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %14 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %15 = "ttir.all_reduce"(%13, %14) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = ttir.empty() : tensor<1x2048xbf16, #ttnn_layout9>
        %17 = "ttir.reshape"(%arg0, %16) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout>, tensor<1x2048xbf16, #ttnn_layout9>) -> tensor<1x2048xbf16, #ttnn_layout9>
        %18 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %19 = "ttir.add"(%15, %17, %18) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x2048xbf16, #ttnn_layout9>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %20 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %21 = "ttir.maximum"(%19, %5, %20) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x1xbf16, #ttnn_layout7>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %22 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout2>
        %23 = ttir.to_layout %21, %22 : tensor<2048x2048xbf16, #ttnn_layout1> into tensor<2048x2048xbf16, #ttnn_layout2> -> tensor<2048x2048xbf16, #ttnn_layout2>
        %24 = "ttir.mesh_shard"(%23) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout2>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %24 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout3>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<256xbf16, #ttnn_layout5>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<256x2048xbf16, #ttnn_layout6>
        %4 = ttir.empty() : tensor<1x1xbf16, #ttnn_layout7>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %8 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %12 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %13 = "ttir.matmul"(%11, %1, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %14 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %15 = "ttir.all_reduce"(%13, %14) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = ttir.empty() : tensor<1x2048xbf16, #ttnn_layout9>
        %17 = "ttir.reshape"(%arg0, %16) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout>, tensor<1x2048xbf16, #ttnn_layout9>) -> tensor<1x2048xbf16, #ttnn_layout9>
        %18 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %19 = "ttir.add"(%15, %17, %18) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x2048xbf16, #ttnn_layout9>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %20 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %21 = "ttir.maximum"(%19, %5, %20) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x1xbf16, #ttnn_layout7>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %22 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout2>
        %23 = ttir.to_layout %21, %22 : tensor<2048x2048xbf16, #ttnn_layout1> into tensor<2048x2048xbf16, #ttnn_layout2> -> tensor<2048x2048xbf16, #ttnn_layout2>
        %24 = "ttir.mesh_shard"(%23) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout2>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %24 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %7 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout8>
        %8 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %9 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = "ttnn.repeat"(%8) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %11 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %12 = "ttnn.linear"(%arg4, %4, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %13 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %14 = "ttnn.matmul"(%12, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %15 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = "ttnn.all_reduce"(%14, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %17 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x2048>}> : (!ttnn.device) -> tensor<1x2048xbf16, #ttnn_layout9>
        %18 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<1x2048xbf16, #ttnn_layout9>
        %19 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %20 = "ttnn.add"(%16, %18) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %21 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %22 = "ttnn.maximum"(%20, %6) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %23 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<2048x2048>}> : () -> tensor<2048x2048xbf16, #ttnn_layout2>
        %24 = "ttnn.to_layout"(%22) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        %25 = "ttnn.mesh_shard"(%24, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %25 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before TTNNFusing (ttnn-fusing) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %7 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout8>
        %8 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %9 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = "ttnn.repeat"(%8) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %11 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %12 = "ttnn.linear"(%arg4, %4, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %13 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %14 = "ttnn.matmul"(%12, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %15 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = "ttnn.all_reduce"(%14, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %17 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x2048>}> : (!ttnn.device) -> tensor<1x2048xbf16, #ttnn_layout9>
        %18 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<1x2048xbf16, #ttnn_layout9>
        %19 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %20 = "ttnn.add"(%16, %18) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %21 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %22 = "ttnn.maximum"(%20, %6) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %23 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<2048x2048>}> : () -> tensor<2048x2048xbf16, #ttnn_layout2>
        %24 = "ttnn.to_layout"(%22) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        %25 = "ttnn.mesh_shard"(%24, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %25 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After TTNNFusing (ttnn-fusing) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %10 = "ttnn.all_reduce"(%9, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %11 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<1x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %13 = "ttnn.maximum"(%12, %5) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %14 = "ttnn.to_layout"(%13) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        %15 = "ttnn.mesh_shard"(%14, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %15 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %10 = "ttnn.all_reduce"(%9, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %11 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<1x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.add"(%10, %11) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %13 = "ttnn.maximum"(%12, %5) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %14 = "ttnn.to_layout"(%13) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        %15 = "ttnn.mesh_shard"(%14, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %15 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %0) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %14 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %15 = "ttnn.add"(%13, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = "ttnn.maximum"(%15, %5) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %17 = "ttnn.to_layout"(%16) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        %18 = "ttnn.mesh_shard"(%17, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %18 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %0) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %14 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %15 = "ttnn.add"(%13, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = "ttnn.maximum"(%15, %5) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %17 = "ttnn.to_layout"(%16) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        %18 = "ttnn.mesh_shard"(%17, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %18 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %0) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %14 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %15 = "ttnn.add"(%13, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = "ttnn.maximum"(%15, %5) : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %17 = "ttnn.to_layout"(%16) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        %18 = "ttnn.mesh_shard"(%17, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %18 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.matmul"(%7, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %9 = "ttnn.reshape"(%8) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %10 = "ttnn.reduce_scatter"(%9, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %11 = "ttnn.all_gather"(%10, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %13 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %14 = "ttnn.add"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %15 = "ttnn.maximum"(%14, %0) : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %16 = "ttnn.to_layout"(%15) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<2048x2048xbf16, #ttnn_layout4>
        %17 = "ttnn.mesh_shard"(%16, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %17 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump Before ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.matmul"(%7, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %9 = "ttnn.reshape"(%8) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %10 = "ttnn.reduce_scatter"(%9, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %11 = "ttnn.all_gather"(%10, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %13 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %14 = "ttnn.add"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %15 = "ttnn.maximum"(%14, %0) : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %16 = "ttnn.to_layout"(%15) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<2048x2048xbf16, #ttnn_layout4>
        %17 = "ttnn.mesh_shard"(%16, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %17 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump After ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.matmul"(%7, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %9 = "ttnn.reshape"(%8) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %10 = "ttnn.reduce_scatter"(%9, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %11 = "ttnn.all_gather"(%10, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %13 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %14 = "ttnn.add"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %15 = "ttnn.maximum"(%14, %0) : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %16 = "ttnn.to_layout"(%15) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<2048x2048xbf16, #ttnn_layout4>
        %17 = "ttnn.mesh_shard"(%16, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %17 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump Before TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.matmul"(%7, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %9 = "ttnn.reshape"(%8) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %10 = "ttnn.reduce_scatter"(%9, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %11 = "ttnn.all_gather"(%10, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %13 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %14 = "ttnn.add"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %15 = "ttnn.maximum"(%14, %0) : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %16 = "ttnn.to_layout"(%15) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<2048x2048xbf16, #ttnn_layout4>
        %17 = "ttnn.mesh_shard"(%16, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %17 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump After TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.matmul"(%7, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %9 = "ttnn.reshape"(%8) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %10 = "ttnn.reduce_scatter"(%9, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %11 = "ttnn.all_gather"(%10, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %13 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %14 = "ttnn.add"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %15 = "ttnn.maximum"(%14, %0) : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<2048x2048xbf16, #ttnn_layout12>
        %17 = "ttnn.from_device"(%16) : (tensor<2048x2048xbf16, #ttnn_layout12>) -> tensor<2048x2048xbf16, #ttnn_layout4>
        %18 = "ttnn.mesh_shard"(%17, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %18 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump Before TTCoreOptimizationBarrierFold (ttcore-optimization-barrier-fold) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.matmul"(%7, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %9 = "ttnn.reshape"(%8) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %10 = "ttnn.reduce_scatter"(%9, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %11 = "ttnn.all_gather"(%10, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %13 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %14 = "ttnn.add"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %15 = "ttnn.maximum"(%14, %0) : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<2048x2048xbf16, #ttnn_layout12>
        %17 = "ttnn.from_device"(%16) : (tensor<2048x2048xbf16, #ttnn_layout12>) -> tensor<2048x2048xbf16, #ttnn_layout4>
        %18 = "ttnn.mesh_shard"(%17, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %18 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump Before TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.matmul"(%7, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %9 = "ttnn.reshape"(%8) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %10 = "ttnn.reduce_scatter"(%9, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %11 = "ttnn.all_gather"(%10, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %13 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<1x2048xbf16, #ttnn_layout11>
        %14 = "ttnn.add"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %15 = "ttnn.maximum"(%14, %0) : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<2048x2048xbf16, #ttnn_layout12>
        %17 = "ttnn.from_device"(%16) : (tensor<2048x2048xbf16, #ttnn_layout12>) -> tensor<2048x2048xbf16, #ttnn_layout4>
        %18 = "ttnn.mesh_shard"(%17, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %18 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump After TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation: @SyncTensorsGraph.24) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn_layout1>) -> ()
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<2048xbf16, #ttnn_layout2>) -> ()
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> ()
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<256x2048xbf16, #ttnn_layout7>) -> ()
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %8 = "ttnn.matmul"(%7, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        %9 = "ttnn.reshape"(%8) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %10 = "ttnn.reduce_scatter"(%9, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> ()
        %11 = "ttnn.all_gather"(%10, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>) -> ()
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> ()
        %13 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<1x2048xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<2048xbf16, #ttnn_layout2>) -> ()
        %14 = "ttnn.add"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x2048xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %15 = "ttnn.maximum"(%14, %0) : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1xbf16, #ttnn_layout>) -> ()
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<2048x2048xbf16, #ttnn_layout12>
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %17 = "ttnn.from_device"(%16) : (tensor<2048x2048xbf16, #ttnn_layout12>) -> tensor<2048x2048xbf16, #ttnn_layout4>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout12>) -> ()
        %18 = "ttnn.mesh_shard"(%17, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout4>) -> ()
        return %18 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


2025-10-23 04:33:22.759 (  13.652s) [        69EA1480]      module_builder.cc:963      1| MLIR Module ttnn:
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #dram>, <interleaved>>
module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.24 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]> loc(#loc)
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1> loc(#loc)
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn_layout1>) -> () loc(#loc)
        return %2 : tensor<1x1xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p0.1"), %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p1.2"), %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p2.4"), %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p3.5"), %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p4.7")) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout> loc(#loc)
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device loc(#loc)
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<2048xbf16, #ttnn_layout2>) -> () loc(#loc)
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc6)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc6)
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc6)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc6)
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc7)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc7)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<256x2048xbf16, #ttnn_layout7>) -> () loc(#loc7)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc7)
        %8 = "ttnn.matmul"(%7, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3> loc(#loc8)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc8)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc8)
        %9 = "ttnn.reshape"(%8) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9> loc(#loc14)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc14)
        %10 = "ttnn.reduce_scatter"(%9, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10> loc(#loc15)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> () loc(#loc15)
        %11 = "ttnn.all_gather"(%10, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9> loc(#loc13)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>) -> () loc(#loc13)
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x2048xbf16, #ttnn_layout3> loc(#loc8)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> () loc(#loc8)
        %13 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<1x2048xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<2048xbf16, #ttnn_layout2>) -> () loc(#loc9)
        %14 = "ttnn.add"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x2048xbf16, #ttnn_layout11>) -> tensor<2048x2048xbf16, #ttnn_layout3> loc(#loc10)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x2048xbf16, #ttnn_layout11>) -> () loc(#loc10)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc10)
        %15 = "ttnn.maximum"(%14, %0) : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x2048xbf16, #ttnn_layout3> loc(#loc11)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc11)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1xbf16, #ttnn_layout>) -> () loc(#loc11)
        %16 = "ttnn.to_layout"(%15) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<2048x2048xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc)
        %17 = "ttnn.from_device"(%16) : (tensor<2048x2048xbf16, #ttnn_layout12>) -> tensor<2048x2048xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout12>) -> () loc(#loc)
        %18 = "ttnn.mesh_shard"(%17, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<2048x2048xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout4>) -> () loc(#loc)
        return %18 : tensor<2048x2048xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("broadcast.12")
#loc7 = loc("add.13")
#loc8 = loc("dot.14")
#loc9 = loc("broadcast.18")
#loc10 = loc("add.19")
#loc11 = loc("maximum.22")
#loc12 = loc("dot.14_reduceScatter"(#loc8))
#loc13 = loc("dot.14_all_gather_4d"(#loc8))
#loc14 = loc("dot.14_reduceScatter_reshape_to_4d"(#loc12))
#loc15 = loc("dot.14_reduceScatter_reduce_scatter_4d"(#loc12))
------------------ END OF MLIR MODULE ------------------
2025-10-23 04:33:22.776 (  13.670s) [        69EA1480]loaded_executable_insta:69       1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-10-23 04:33:22.776 (  13.670s) [        69EA1480]loaded_executable_insta:88       1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-10-23 04:33:22.777 (  13.670s) [        69EA1480]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-10-23 04:33:22.777 (  13.670s) [        69EA1480]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-10-23 04:33:22.777 (  13.670s) [        69EA1480]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-10-23 04:33:22.777 (  13.670s) [        69EA1480]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-10-23 04:33:22.777 (  13.670s) [        69EA1480] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-23 04:33:22.777 (  13.670s) [        69EA1480] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-23 04:33:22.780 (  13.673s) [        69EA1480] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-23 04:33:22.780 (  13.673s) [        69EA1480] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.677s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.784 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:33:22.785 (  13.678s) [        127FC640] executable_instance.cc:139      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]loaded_executable_insta:125      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]flatbuffer_loaded_execu:412      1| FlatbufferLoadedExecutableInstance::Execute
2025-10-23 04:33:22.785 (  13.678s) [        127FC640]     client_instance.cc:373      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 8]
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.431 (  32.324s) [        127FC640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:33:41.431 (  32.324s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.431 (  32.324s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.431 (  32.325s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.431 (  32.325s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.432 (  32.325s) [        69EA1480]     buffer_instance.cc:428      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-10-23 04:33:41.432 (  32.325s) [        69EA1480]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:33:41.432 (  32.325s) [        69EA1480]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:33:41.432 (  32.325s) [        69EA1480]     buffer_instance.cc:450      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-10-23 04:33:41.432 (  32.325s) [        69EA1480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:33:41.433 (  32.326s) [        7A7FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:33:41.472 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.366s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.473 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.367s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.368s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.474 (  32.368s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.475 (  32.368s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.475 (  32.368s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.475 (  32.368s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.475 (  32.368s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.475 (  32.368s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.475 (  32.368s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:33:41.475 (  32.368s) [        69EA1480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
PASSED

============================== 1 passed in 32.62s ==============================
