#dram = #ttnn.buffer_type<dram>
#l1 = #ttnn.buffer_type<l1>
#loc = loc(unknown)
#loc108 = loc("p0.3")
#loc109 = loc("p1.10")
#loc110 = loc("p2.18")
#loc111 = loc("p3.22")
#loc112 = loc("p4.26")
#loc113 = loc("p5.30")
#loc114 = loc("p6.34")
#loc115 = loc("p7.42")
#loc116 = loc("p8.46")
#loc117 = loc("p9.50")
#loc118 = loc("p10.54")
#loc119 = loc("p11.58")
#loc120 = loc("p12.64")
#loc121 = loc("p13.68")
#loc122 = loc("p14.72")
#loc123 = loc("p15.76")
#loc124 = loc("p16.80")
#loc125 = loc("p17.85")
#loc126 = loc("p18.89")
#loc127 = loc("p19.93")
#loc128 = loc("p20.97")
#loc129 = loc("p21.101")
#loc130 = loc("p22.103")
#loc131 = loc("p23.107")
#loc132 = loc("p24.111")
#loc133 = loc("p25.115")
#loc134 = loc("p26.119")
#loc135 = loc("p27.121")
#loc136 = loc("p28.310")
#loc137 = loc("p29.314")
#loc138 = loc("p30.318")
#loc139 = loc("p31.322")
#loc140 = loc("p32.326")
#loc141 = loc("p33.328")
#loc142 = loc("p34.332")
#loc143 = loc("p35.336")
#loc144 = loc("p36.340")
#loc145 = loc("p37.344")
#loc146 = loc("p38.346")
#loc147 = loc("p39.350")
#loc148 = loc("p40.354")
#loc149 = loc("p41.358")
#loc150 = loc("p42.362")
#loc151 = loc("p43.394")
#loc152 = loc("p44.398")
#loc153 = loc("p45.402")
#loc154 = loc("p46.406")
#loc155 = loc("p47.410")
#loc156 = loc("p48.412")
#loc157 = loc("p49.416")
#loc158 = loc("p50.420")
#loc159 = loc("p51.424")
#loc160 = loc("p52.428")
#loc161 = loc("p53.430")
#loc162 = loc("p54.434")
#loc163 = loc("p55.438")
#loc164 = loc("p56.442")
#loc165 = loc("p57.446")
#loc166 = loc("p58.478")
#loc167 = loc("p59.482")
#loc168 = loc("p60.486")
#loc169 = loc("p61.490")
#loc170 = loc("p62.494")
#loc171 = loc("p63.496")
#loc172 = loc("p64.500")
#loc173 = loc("p65.504")
#loc174 = loc("p66.508")
#loc175 = loc("p67.512")
#loc176 = loc("p68.514")
#loc177 = loc("p69.518")
#loc178 = loc("p70.522")
#loc179 = loc("p71.526")
#loc180 = loc("p72.530")
#loc181 = loc("p73.568")
#loc182 = loc("p74.572")
#loc183 = loc("p75.576")
#loc184 = loc("p76.580")
#loc185 = loc("p77.584")
#loc186 = loc("p78.586")
#loc187 = loc("p79.590")
#loc188 = loc("p80.594")
#loc189 = loc("p81.598")
#loc190 = loc("p82.602")
#loc191 = loc("p83.604")
#loc192 = loc("p84.608")
#loc193 = loc("p85.612")
#loc194 = loc("p86.616")
#loc195 = loc("p87.620")
#loc196 = loc("p88.652")
#loc197 = loc("p89.656")
#loc198 = loc("p90.660")
#loc199 = loc("p91.664")
#loc200 = loc("p92.668")
#loc201 = loc("p93.670")
#loc202 = loc("p94.674")
#loc203 = loc("p95.678")
#loc204 = loc("p96.682")
#loc205 = loc("p97.686")
#loc206 = loc("p98.688")
#loc207 = loc("p99.692")
#loc208 = loc("p100.696")
#loc209 = loc("p101.700")
#loc210 = loc("p102.704")
#loc211 = loc("p103.736")
#loc212 = loc("p104.740")
#loc213 = loc("p105.744")
#loc214 = loc("p106.748")
#loc215 = loc("p107.752")
#loc216 = loc("p108.754")
#loc217 = loc("p109.758")
#loc218 = loc("p110.762")
#loc219 = loc("p111.766")
#loc220 = loc("p112.770")
#loc221 = loc("p113.772")
#loc222 = loc("p114.776")
#loc223 = loc("p115.780")
#loc224 = loc("p116.784")
#loc225 = loc("p117.788")
#loc226 = loc("p118.820")
#loc227 = loc("p119.824")
#loc228 = loc("p120.828")
#loc229 = loc("p121.832")
#loc230 = loc("p122.836")
#loc231 = loc("p123.838")
#loc232 = loc("p124.842")
#loc233 = loc("p125.846")
#loc234 = loc("p126.850")
#loc235 = loc("p127.854")
#loc236 = loc("p128.856")
#loc237 = loc("p129.860")
#loc238 = loc("p130.864")
#loc239 = loc("p131.868")
#loc240 = loc("p132.872")
#loc241 = loc("p133.910")
#loc242 = loc("p134.914")
#loc243 = loc("p135.918")
#loc244 = loc("p136.922")
#loc245 = loc("p137.926")
#loc246 = loc("p138.928")
#loc247 = loc("p139.932")
#loc248 = loc("p140.936")
#loc249 = loc("p141.940")
#loc250 = loc("p142.944")
#loc251 = loc("p143.946")
#loc252 = loc("p144.950")
#loc253 = loc("p145.954")
#loc254 = loc("p146.958")
#loc255 = loc("p147.962")
#loc256 = loc("p148.994")
#loc257 = loc("p149.998")
#loc258 = loc("p150.1002")
#loc259 = loc("p151.1006")
#loc260 = loc("p152.1010")
#loc261 = loc("p153.1012")
#loc262 = loc("p154.1016")
#loc263 = loc("p155.1020")
#loc264 = loc("p156.1024")
#loc265 = loc("p157.1028")
#loc266 = loc("p158.1030")
#loc267 = loc("p159.1034")
#loc268 = loc("p160.1038")
#loc269 = loc("p161.1042")
#loc270 = loc("p162.1046")
#loc271 = loc("p163.1078")
#loc272 = loc("p164.1082")
#loc273 = loc("p165.1086")
#loc274 = loc("p166.1090")
#loc275 = loc("p167.1094")
#loc276 = loc("p168.1096")
#loc277 = loc("p169.1100")
#loc278 = loc("p170.1104")
#loc279 = loc("p171.1108")
#loc280 = loc("p172.1112")
#loc281 = loc("p173.1114")
#loc282 = loc("p174.1118")
#loc283 = loc("p175.1122")
#loc284 = loc("p176.1126")
#loc285 = loc("p177.1130")
#loc286 = loc("p178.1162")
#loc287 = loc("p179.1166")
#loc288 = loc("p180.1170")
#loc289 = loc("p181.1174")
#loc290 = loc("p182.1178")
#loc291 = loc("p183.1180")
#loc292 = loc("p184.1184")
#loc293 = loc("p185.1188")
#loc294 = loc("p186.1192")
#loc295 = loc("p187.1196")
#loc296 = loc("p188.1198")
#loc297 = loc("p189.1202")
#loc298 = loc("p190.1206")
#loc299 = loc("p191.1210")
#loc300 = loc("p192.1214")
#loc301 = loc("p193.1246")
#loc302 = loc("p194.1250")
#loc303 = loc("p195.1254")
#loc304 = loc("p196.1258")
#loc305 = loc("p197.1262")
#loc306 = loc("p198.1264")
#loc307 = loc("p199.1268")
#loc308 = loc("p200.1272")
#loc309 = loc("p201.1276")
#loc310 = loc("p202.1280")
#loc311 = loc("p203.1282")
#loc312 = loc("p204.1286")
#loc313 = loc("p205.1290")
#loc314 = loc("p206.1294")
#loc315 = loc("p207.1298")
#loc316 = loc("p208.1330")
#loc317 = loc("p209.1334")
#loc318 = loc("p210.1338")
#loc319 = loc("p211.1342")
#loc320 = loc("p212.1346")
#loc321 = loc("p213.1348")
#loc322 = loc("p214.1352")
#loc323 = loc("p215.1356")
#loc324 = loc("p216.1360")
#loc325 = loc("p217.1364")
#loc326 = loc("p218.1366")
#loc327 = loc("p219.1370")
#loc328 = loc("p220.1374")
#loc329 = loc("p221.1378")
#loc330 = loc("p222.1382")
#loc331 = loc("p223.1420")
#loc332 = loc("p224.1424")
#loc333 = loc("p225.1428")
#loc334 = loc("p226.1432")
#loc335 = loc("p227.1436")
#loc336 = loc("p228.1438")
#loc337 = loc("p229.1442")
#loc338 = loc("p230.1446")
#loc339 = loc("p231.1450")
#loc340 = loc("p232.1454")
#loc341 = loc("p233.1456")
#loc342 = loc("p234.1460")
#loc343 = loc("p235.1464")
#loc344 = loc("p236.1468")
#loc345 = loc("p237.1472")
#loc346 = loc("p238.1504")
#loc347 = loc("p239.1508")
#loc348 = loc("p240.1512")
#loc349 = loc("p241.1516")
#loc350 = loc("p242.1520")
#loc351 = loc("p243.1522")
#loc352 = loc("p244.1526")
#loc353 = loc("p245.1530")
#loc354 = loc("p246.1534")
#loc355 = loc("p247.1538")
#loc356 = loc("p248.1540")
#loc357 = loc("p249.1544")
#loc358 = loc("p250.1548")
#loc359 = loc("p251.1552")
#loc360 = loc("p252.1556")
#loc361 = loc("p253.1588")
#loc362 = loc("p254.1592")
#loc363 = loc("p255.1596")
#loc364 = loc("p256.1600")
#loc365 = loc("p257.1604")
#loc366 = loc("p258.1606")
#loc367 = loc("p259.1610")
#loc368 = loc("p260.1614")
#loc369 = loc("p261.1618")
#loc370 = loc("p262.1622")
#loc371 = loc("p263.1624")
#loc372 = loc("p264.1628")
#loc373 = loc("p265.1632")
#loc374 = loc("p266.1636")
#loc375 = loc("p267.1640")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101664, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073131840, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<524288x1xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 1024 + d2, d3), <1x1>, memref<32x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<524288x1xbf16, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32768 + d1 * 32 + d2, d3), <1x1>, memref<524288x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x512xbf16, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 192 + d1 * 3 + d2, d3), <1x1>, memref<12288x3xbf16, #system_memory>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 192 + d1 * 3 + d2, d3), <1x1>, memref<12288x3xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<4096x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x64xbf16, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x64xbf16, #system_memory>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 + d2, d3), <1x1>, memref<65536x1xbf16, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 + d2, d3), <1x1>, memref<65536x1xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 32 + d2, d3), <1x1>, memref<65536x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<262144x1xbf16, #system_memory>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32768 + d1 * 32 + d2, d3), <1x1>, memref<1024x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1024x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<262144x1xbf16, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 32 + d2, d3), <1x1>, memref<262144x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x1024xbf16, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x1024xbf16, #system_memory>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<16384x1xbf16, #system_memory>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<16384x1xbf16, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 32 + d2, d3), <1x1>, memref<16384x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 + d2, d3), <1x1>, memref<1048576x1xbf16, #system_memory>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 + d2, d3), <1x1>, memref<1048576x1xbf16, #dram>, <interleaved>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 65536 + d1 * 32 + d2, d3), <1x1>, memref<1048576x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 3 + d2, d3), <1x1>, memref<196608x3xbf16, #system_memory>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2304 + d1 * 2304 + d2, d3), <1x1>, memref<72x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 32 + d2, d3), <1x1>, memref<256x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<256x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 3 + d2, d3), <1x1>, memref<196608x3xbf16, #dram>, <interleaved>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 32 + d2, d3), <1x1>, memref<65536x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x256xbf16, #dram>, <interleaved>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 3 + d2, d3), <1x1>, memref<49152x3xbf16, #system_memory>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1152 + d1 * 1152 + d2, d3), <1x1>, memref<36x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout58 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout59 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout60 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout61 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 3 + d2, d3), <1x1>, memref<49152x3xbf16, #dram>, <interleaved>>
#ttnn_layout62 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 32 + d2, d3), <1x1>, memref<16384x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout63 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x128xbf16, #dram>, <interleaved>>
#ttnn_layout64 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x128xbf16, #system_memory>>
#ttnn_layout65 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 + d2, d3), <1x1>, memref<16384x1xbf16, #system_memory>>
#ttnn_layout66 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 64 + d2, d3), <1x1>, memref<2x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout67 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 + d2, d3), <1x1>, memref<16384x1xbf16, #dram>, <interleaved>>
#ttnn_layout68 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<16384x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout69 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<65536x1xbf16, #system_memory>>
#ttnn_layout70 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout71 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<65536x1xbf16, #dram>, <interleaved>>
#ttnn_layout72 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<65536x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout73 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<32768x1xbf16, #system_memory>>
#ttnn_layout74 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout75 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<32768x1xbf16, #dram>, <interleaved>>
#ttnn_layout76 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 32 + d2, d3), <1x1>, memref<32768x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout77 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 + d2, d3), <1x1>, memref<4096x1xbf16, #system_memory>>
#ttnn_layout78 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 64 + d2, d3), <1x1>, memref<2x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout79 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 + d2, d3), <1x1>, memref<4096x1xbf16, #dram>, <interleaved>>
#ttnn_layout80 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<262144x1xbf16, #system_memory>>
#ttnn_layout81 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 1024 + d2, d3), <1x1>, memref<32x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout82 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<262144x1xbf16, #dram>, <interleaved>>
#ttnn_layout83 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32768 + d1 * 32 + d2, d3), <1x1>, memref<262144x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout84 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout85 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<2097152x1xbf16, #system_memory>>
#ttnn_layout86 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 1024 + d2, d3), <1x1>, memref<32x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout87 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout88 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 65536 + d1 * 32 + d2, d3), <1x1>, memref<2048x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout89 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<2048x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout90 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout91 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<2097152x1xbf16, #dram>, <interleaved>>
#ttnn_layout92 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32768 + d1 * 32 + d2, d3), <1x1>, memref<2097152x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout93 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout94 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x2048xbf16, #dram>, <interleaved>>
#ttnn_layout95 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), <1x1>, memref<1x2048xbf16, #system_memory>>
#ttnn_layout96 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<1048576x1xbf16, #system_memory>>
#ttnn_layout97 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout98 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<1048576x1xbf16, #dram>, <interleaved>>
#ttnn_layout99 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<1048576x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout100 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 21 + d1 * 7 + d2, d3), <1x1>, memref<1344x7xbf16, #system_memory>>
#ttnn_layout101 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout102 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 21 + d1 * 7 + d2, d3), <1x1>, memref<1344x7xbf16, #dram>, <interleaved>>
#ttnn_layout103 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 32 + d2, d3), <1x1>, memref<192x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout104 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<131072x1xbf16, #system_memory>>
#ttnn_layout105 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout106 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<131072x1xbf16, #dram>, <interleaved>>
#ttnn_layout107 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<131072x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout108 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<131072x1xbf16, #system_memory>>
#ttnn_layout109 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout110 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<131072x1xbf16, #dram>, <interleaved>>
#ttnn_layout111 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 32 + d2, d3), <1x1>, memref<131072x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout112 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout113 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 3 + d2, d3), <1x1>, memref<786432x3xbf16, #system_memory>>
#ttnn_layout114 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4608 + d1 * 4608 + d2, d3), <1x1>, memref<144x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout115 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 3 + d2, d3), <1x1>, memref<786432x3xbf16, #dram>, <interleaved>>
#ttnn_layout116 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<262144x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout117 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<524288x1xbf16, #system_memory>>
#ttnn_layout118 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout119 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<524288x1xbf16, #dram>, <interleaved>>
#ttnn_layout120 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<524288x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout121 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout122 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 672 + d1 * 224 + d2, d3), <1x1>, memref<168x7x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout123 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 224 + d2, d3), <1x1>, memref<12544x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout124 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 401408 + d1 * 401408 + d2, d3), <1x1>, memref<12544x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout125 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 100352 + d1 * 100352 + d2, d3), <1x1>, memref<3136x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout126 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 25088 + d1 * 25088 + d2, d3), <1x1>, memref<25088x64xbf16, #dram>, <interleaved>>
#ttnn_layout127 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 25088 + d1 * 25088 + d2, d3), <1x1>, memref<784x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout128 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 25088 + d1 * 25088 + d2, d3), <61x1, (d0, d1) -> (0, d0 floordiv 8, d0 mod 8)>, memref<13x2x!ttcore.tile<32x32, bf16>, #l1>, <height_sharded>>
#ttnn_layout129 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 25088 + d1 * 25088 + d2, d3), <61x1, (d0, d1) -> (0, d0 floordiv 8, d0 mod 8)>, memref<13x8x!ttcore.tile<32x32, bf16>, #l1>, <height_sharded>>
#ttnn_layout130 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 25088 + d1 * 25088 + d2, d3), <1x1>, memref<13x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout131 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 25088 + d1 * 25088 + d2, d3), <8x8, (d0, d1) -> (0, d0, d1)>, memref<98x1x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout132 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6272 + d1 * 6272 + d2, d3), <8x8, (d0, d1) -> (0, d0, d1)>, memref<25x2x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout133 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6272 + d1 * 6272 + d2, d3), <1x1>, memref<25x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout134 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 25088 + d1 * 25088 + d2, d3), <61x1, (d0, d1) -> (0, d0 floordiv 8, d0 mod 8)>, memref<13x4x!ttcore.tile<32x32, bf16>, #l1>, <height_sharded>>
#ttnn_layout135 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 25088 + d1 * 25088 + d2, d3), <49x1, (d0, d1) -> (0, d0 floordiv 8, d0 mod 8)>, memref<16x4x!ttcore.tile<32x32, bf16>, #l1>, <height_sharded>>
#ttnn_layout136 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6272 + d1 * 6272 + d2, d3), <49x1, (d0, d1) -> (0, d0 floordiv 8, d0 mod 8)>, memref<4x4x!ttcore.tile<32x32, bf16>, #l1>, <height_sharded>>
#ttnn_layout137 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6272 + d1 * 6272 + d2, d3), <49x1, (d0, d1) -> (0, d0 floordiv 8, d0 mod 8)>, memref<4x16x!ttcore.tile<32x32, bf16>, #l1>, <height_sharded>>
#ttnn_layout138 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6272 + d1 * 6272 + d2, d3), <1x1>, memref<4x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout139 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6272 + d1 * 6272 + d2, d3), <7x8, (d0, d1) -> (0, d0, d1)>, memref<28x2x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout140 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1568 + d1 * 1568 + d2, d3), <7x8, (d0, d1) -> (0, d0, d1)>, memref<7x4x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout141 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1568 + d1 * 1568 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout142 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6272 + d1 * 6272 + d2, d3), <8x8, (d0, d1) -> (0, d0, d1)>, memref<25x1x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout143 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6272 + d1 * 6272 + d2, d3), <7x8, (d0, d1) -> (0, d0, d1)>, memref<28x1x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout144 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1568 + d1 * 1568 + d2, d3), <7x8, (d0, d1) -> (0, d0, d1)>, memref<7x1x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout145 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 416 + d2, d3), <7x8, (d0, d1) -> (0, d0, d1)>, memref<2x8x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout146 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 416 + d2, d3), <1x1>, memref<2x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout147 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1568 + d1 * 1568 + d2, d3), <7x8, (d0, d1) -> (0, d0, d1)>, memref<7x2x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout148 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 416 + d2, d3), <7x8, (d0, d1) -> (0, d0, d1)>, memref<2x2x!ttcore.tile<32x32, bf16>, #l1>, <block_sharded>>
#ttnn_layout149 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<56x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout150 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 65536 + d1 * 32 + d2, d3), <1x1>, memref<16384x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout151 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x64, (d0, d1) -> (0, d1 floordiv 8, d1 mod 8)>, memref<1x1x!ttcore.tile<32x32, bf16>, #l1>, <width_sharded>>
#ttnn_layout152 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.1698 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.1698 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main_const_eval_0() -> tensor<8x2048xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.0203857422 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<8x2048>}> : (!ttnn.device) -> tensor<8x2048xbf16, #ttnn_layout> loc(#loc)
        return %1 : tensor<8x2048xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_1(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x1024x1x1xbf16, #ttnn_layout2> loc(unknown)) -> (tensor<1x1x1024x512xbf16, #ttnn_layout3>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc1)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc1)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc1)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc412)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc412)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc1)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc1)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc1)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc1)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc412)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x1024x1x1xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<512x1024x1x1xbf16, #ttnn_layout9> loc(#loc413)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x1024x1x1xbf16, #ttnn_layout9>) -> tensor<512x1024x1x1xbf16, #ttnn_layout10> loc(#loc413)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout9>) -> () loc(#loc413)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x1024x1x1xbf16, #ttnn_layout10>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x1024x1x1xbf16, #ttnn_layout10> loc(#loc414)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout10>) -> () loc(#loc414)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc414)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc920)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc921)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc921)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc1)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc1)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc811)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc1)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc1)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x1024x1x1xbf16, #ttnn_layout10>) -> tensor<512x1024x1x1xbf16, #ttnn_layout9> loc(#loc416)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout10>) -> () loc(#loc416)
        %18 = "ttnn.from_device"(%17) : (tensor<512x1024x1x1xbf16, #ttnn_layout9>) -> tensor<512x1024x1x1xbf16, #ttnn_layout2> loc(#loc416)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout9>) -> () loc(#loc416)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc415)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc415)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc415)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc415)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<512x1024x1x1xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x1024x512xbf16, #ttnn_layout3> loc(#loc417)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout2>) -> () loc(#loc417)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc418)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc418)
        return %21, %22 : tensor<1x1x1024x512xbf16, #ttnn_layout3>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_2(%arg0: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg1: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg2: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg3: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg4: tensor<64x64x3x3xbf16, #ttnn_layout14> loc(unknown)) -> (tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc3)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc3)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc3)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc419)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc419)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc3)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc3)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc3)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc3)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc3)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc3)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc3)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc3)
        %8 = "ttnn.reshape"(%7) <{shape = [64 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<64x1x1x1xbf16, #ttnn_layout18> loc(#loc419)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<64x64x3x3xbf16, #ttnn_layout19> loc(#loc420)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> tensor<64x64x3x3xbf16, #ttnn_layout20> loc(#loc420)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> () loc(#loc420)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>, tensor<64x1x1x1xbf16, #ttnn_layout18>) -> tensor<64x64x3x3xbf16, #ttnn_layout20> loc(#loc421)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>) -> () loc(#loc421)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<64x1x1x1xbf16, #ttnn_layout18>) -> () loc(#loc421)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc922)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc923)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc923)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc3)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc3)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc3)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc813)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc3)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc3)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc3)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>) -> tensor<64x64x3x3xbf16, #ttnn_layout19> loc(#loc423)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>) -> () loc(#loc423)
        %18 = "ttnn.from_device"(%17) : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> tensor<64x64x3x3xbf16, #ttnn_layout14> loc(#loc423)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> () loc(#loc423)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout21> loc(#loc422)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc422)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout22> loc(#loc422)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> () loc(#loc422)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout15> loc(#loc424)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>) -> () loc(#loc424)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc425)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>) -> () loc(#loc425)
        return %21, %22 : tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_3(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x128x1x1xbf16, #ttnn_layout23> loc(unknown)) -> (tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc5)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc5)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc5)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc426)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc426)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc5)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc5)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc5)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc5)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc5)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc5)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc5)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc5)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc426)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<512x128x1x1xbf16, #ttnn_layout25> loc(#loc427)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> tensor<512x128x1x1xbf16, #ttnn_layout26> loc(#loc427)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> () loc(#loc427)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x128x1x1xbf16, #ttnn_layout26> loc(#loc428)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> () loc(#loc428)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc428)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc924)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc925)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc925)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc5)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc5)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc5)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc815)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc5)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc5)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc5)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> tensor<512x128x1x1xbf16, #ttnn_layout25> loc(#loc430)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> () loc(#loc430)
        %18 = "ttnn.from_device"(%17) : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> tensor<512x128x1x1xbf16, #ttnn_layout23> loc(#loc430)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> () loc(#loc430)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc429)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc429)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc429)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc429)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x128x512xbf16, #ttnn_layout24> loc(#loc431)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>) -> () loc(#loc431)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc432)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc432)
        return %21, %22 : tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_4(%arg0: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg1: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg2: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg3: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg4: tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(unknown)) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc7)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc7)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc7)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc433)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc433)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc7)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc7)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc7)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc7)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc7)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc7)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc7)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc7)
        %8 = "ttnn.reshape"(%7) <{shape = [1024 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1024x1x1x1xbf16, #ttnn_layout32> loc(#loc433)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc434)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc434)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc434)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>, tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc435)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc435)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> () loc(#loc435)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc926)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc927)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc927)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc7)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc7)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc7)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc817)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc7)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc7)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc7)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc437)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc437)
        %18 = "ttnn.from_device"(%17) : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(#loc437)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc437)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout35> loc(#loc436)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc436)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> tensor<1x1x1x1024xbf16, #ttnn_layout36> loc(#loc436)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> () loc(#loc436)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1x1x256x1024xbf16, #ttnn_layout29> loc(#loc438)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc438)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>, !ttnn.device) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc439)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>) -> () loc(#loc439)
        return %21, %22 : tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_5(%arg0: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg1: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg2: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg3: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg4: tensor<64x256x1x1xbf16, #ttnn_layout37> loc(unknown)) -> (tensor<1x1x256x64xbf16, #ttnn_layout38>, tensor<1x1x1x64xbf16, #ttnn_layout16>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc9)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc9)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc9)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc440)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc440)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc9)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc9)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc9)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc9)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc9)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc9)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc9)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc9)
        %8 = "ttnn.reshape"(%7) <{shape = [64 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<64x1x1x1xbf16, #ttnn_layout18> loc(#loc440)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64x256x1x1xbf16, #ttnn_layout37>, !ttnn.device) -> tensor<64x256x1x1xbf16, #ttnn_layout39> loc(#loc441)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<64x256x1x1xbf16, #ttnn_layout39>) -> tensor<64x256x1x1xbf16, #ttnn_layout40> loc(#loc441)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout39>) -> () loc(#loc441)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<64x256x1x1xbf16, #ttnn_layout40>, tensor<64x1x1x1xbf16, #ttnn_layout18>) -> tensor<64x256x1x1xbf16, #ttnn_layout40> loc(#loc442)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout40>) -> () loc(#loc442)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<64x1x1x1xbf16, #ttnn_layout18>) -> () loc(#loc442)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc928)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc929)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc929)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc9)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc9)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc9)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc819)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc9)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc9)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc9)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<64x256x1x1xbf16, #ttnn_layout40>) -> tensor<64x256x1x1xbf16, #ttnn_layout39> loc(#loc444)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout40>) -> () loc(#loc444)
        %18 = "ttnn.from_device"(%17) : (tensor<64x256x1x1xbf16, #ttnn_layout39>) -> tensor<64x256x1x1xbf16, #ttnn_layout37> loc(#loc444)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout39>) -> () loc(#loc444)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout21> loc(#loc443)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc443)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout22> loc(#loc443)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> () loc(#loc443)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<64x256x1x1xbf16, #ttnn_layout37>, !ttnn.device) -> tensor<1x1x256x64xbf16, #ttnn_layout38> loc(#loc445)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout37>) -> () loc(#loc445)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc446)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>) -> () loc(#loc446)
        return %21, %22 : tensor<1x1x256x64xbf16, #ttnn_layout38>, tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_6(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x2048x1x1xbf16, #ttnn_layout41> loc(unknown)) -> (tensor<1x1x2048x512xbf16, #ttnn_layout42>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc11)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc11)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc11)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc447)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc447)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc11)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc11)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc11)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc11)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc11)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc11)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc11)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc11)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc447)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x2048x1x1xbf16, #ttnn_layout41>, !ttnn.device) -> tensor<512x2048x1x1xbf16, #ttnn_layout43> loc(#loc448)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x2048x1x1xbf16, #ttnn_layout43>) -> tensor<512x2048x1x1xbf16, #ttnn_layout44> loc(#loc448)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout43>) -> () loc(#loc448)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x2048x1x1xbf16, #ttnn_layout44>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x2048x1x1xbf16, #ttnn_layout44> loc(#loc449)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout44>) -> () loc(#loc449)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc449)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc930)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc931)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc931)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc11)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc11)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc11)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc821)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc11)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc11)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc11)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x2048x1x1xbf16, #ttnn_layout44>) -> tensor<512x2048x1x1xbf16, #ttnn_layout43> loc(#loc451)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout44>) -> () loc(#loc451)
        %18 = "ttnn.from_device"(%17) : (tensor<512x2048x1x1xbf16, #ttnn_layout43>) -> tensor<512x2048x1x1xbf16, #ttnn_layout41> loc(#loc451)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout43>) -> () loc(#loc451)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc450)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc450)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc450)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc450)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 2048 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<512x2048x1x1xbf16, #ttnn_layout41>, !ttnn.device) -> tensor<1x1x2048x512xbf16, #ttnn_layout42> loc(#loc452)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout41>) -> () loc(#loc452)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 2048 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc453)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc453)
        return %21, %22 : tensor<1x1x2048x512xbf16, #ttnn_layout42>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_7(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x256x3x3xbf16, #ttnn_layout46> loc(unknown)) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc13)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc13)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc13)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc454)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc454)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc13)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc13)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc13)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc13)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc13)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc13)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc13)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc13)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc454)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc455)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc455)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc455)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc456)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc456)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc456)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc932)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc933)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc933)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc13)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc13)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc13)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc823)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc13)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc13)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc13)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc458)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc458)
        %18 = "ttnn.from_device"(%17) : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout46> loc(#loc458)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc458)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc457)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc457)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc457)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc457)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <896x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>, weights_format = "OIHW"}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x2304x256xbf16, #ttnn_layout47> loc(#loc459)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc459)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <896x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc460)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc460)
        return %21, %22 : tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_8(%arg0: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg1: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg2: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg3: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg4: tensor<128x128x3x3xbf16, #ttnn_layout56> loc(unknown)) -> (tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc15)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc15)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc15)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc461)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc461)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc15)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc15)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc15)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc15)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc15)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc15)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc15)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc15)
        %8 = "ttnn.reshape"(%7) <{shape = [128 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<128x1x1x1xbf16, #ttnn_layout60> loc(#loc461)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<128x128x3x3xbf16, #ttnn_layout61> loc(#loc462)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> tensor<128x128x3x3xbf16, #ttnn_layout62> loc(#loc462)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> () loc(#loc462)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>, tensor<128x1x1x1xbf16, #ttnn_layout60>) -> tensor<128x128x3x3xbf16, #ttnn_layout62> loc(#loc463)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> () loc(#loc463)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128x1x1x1xbf16, #ttnn_layout60>) -> () loc(#loc463)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc934)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc935)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc935)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc15)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc15)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc15)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc825)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc15)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc15)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc15)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> tensor<128x128x3x3xbf16, #ttnn_layout61> loc(#loc465)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> () loc(#loc465)
        %18 = "ttnn.from_device"(%17) : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> tensor<128x128x3x3xbf16, #ttnn_layout56> loc(#loc465)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> () loc(#loc465)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout63> loc(#loc464)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc464)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> tensor<1x1x1x128xbf16, #ttnn_layout64> loc(#loc464)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> () loc(#loc464)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1152x128xbf16, #ttnn_layout57> loc(#loc466)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc466)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>, !ttnn.device) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc467)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>) -> () loc(#loc467)
        return %21, %22 : tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_9(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x64x1x1xbf16, #ttnn_layout65> loc(unknown)) -> (tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc17)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc17)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc17)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc468)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc468)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc17)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc17)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc17)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc17)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc17)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc17)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc17)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc17)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc468)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>, !ttnn.device) -> tensor<256x64x1x1xbf16, #ttnn_layout67> loc(#loc469)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> tensor<256x64x1x1xbf16, #ttnn_layout68> loc(#loc469)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> () loc(#loc469)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x64x1x1xbf16, #ttnn_layout68> loc(#loc470)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> () loc(#loc470)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc470)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc936)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc937)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc937)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc17)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc17)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc17)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc827)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc17)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc17)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc17)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> tensor<256x64x1x1xbf16, #ttnn_layout67> loc(#loc472)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> () loc(#loc472)
        %18 = "ttnn.from_device"(%17) : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> tensor<256x64x1x1xbf16, #ttnn_layout65> loc(#loc472)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> () loc(#loc472)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc471)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc471)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc471)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc471)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>, !ttnn.device) -> tensor<1x1x64x256xbf16, #ttnn_layout66> loc(#loc473)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>) -> () loc(#loc473)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc474)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc474)
        return %21, %22 : tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_10(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x256x3x3xbf16, #ttnn_layout46> loc(unknown)) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc19)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc19)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc19)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc475)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc475)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc19)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc19)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc19)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc19)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc19)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc19)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc19)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc19)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc475)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc476)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc476)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc476)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc477)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc477)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc477)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc938)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc939)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc939)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc19)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc19)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc19)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc829)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc19)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc19)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc19)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc479)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc479)
        %18 = "ttnn.from_device"(%17) : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout46> loc(#loc479)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc479)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc478)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc478)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc478)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc478)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x2304x256xbf16, #ttnn_layout47> loc(#loc480)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc480)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc481)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc481)
        return %21, %22 : tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_11(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x256x3x3xbf16, #ttnn_layout46> loc(unknown)) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc21)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc21)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc21)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc482)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc482)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc21)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc21)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc21)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc21)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc21)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc21)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc21)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc21)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc482)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc483)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc483)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc483)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc484)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc484)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc484)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc940)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc941)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc941)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc21)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc21)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc21)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc831)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc21)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc21)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc21)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc486)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc486)
        %18 = "ttnn.from_device"(%17) : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout46> loc(#loc486)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc486)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc485)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc485)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc485)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc485)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x2304x256xbf16, #ttnn_layout47> loc(#loc487)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc487)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc488)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc488)
        return %21, %22 : tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_12(%arg0: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg1: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg2: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg3: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg4: tensor<128x512x1x1xbf16, #ttnn_layout69> loc(unknown)) -> (tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc23)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc23)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc23)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc489)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc489)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc23)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc23)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc23)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc23)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc23)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc23)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc23)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc23)
        %8 = "ttnn.reshape"(%7) <{shape = [128 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<128x1x1x1xbf16, #ttnn_layout60> loc(#loc489)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>, !ttnn.device) -> tensor<128x512x1x1xbf16, #ttnn_layout71> loc(#loc490)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> tensor<128x512x1x1xbf16, #ttnn_layout72> loc(#loc490)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> () loc(#loc490)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>, tensor<128x1x1x1xbf16, #ttnn_layout60>) -> tensor<128x512x1x1xbf16, #ttnn_layout72> loc(#loc491)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>) -> () loc(#loc491)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128x1x1x1xbf16, #ttnn_layout60>) -> () loc(#loc491)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc942)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc943)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc943)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc23)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc23)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc23)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc833)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc23)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc23)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc23)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>) -> tensor<128x512x1x1xbf16, #ttnn_layout71> loc(#loc493)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>) -> () loc(#loc493)
        %18 = "ttnn.from_device"(%17) : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> tensor<128x512x1x1xbf16, #ttnn_layout69> loc(#loc493)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> () loc(#loc493)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout63> loc(#loc492)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc492)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> tensor<1x1x1x128xbf16, #ttnn_layout64> loc(#loc492)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> () loc(#loc492)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x512>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>, !ttnn.device) -> tensor<1x1x512x128xbf16, #ttnn_layout70> loc(#loc494)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>) -> () loc(#loc494)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x512>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>, !ttnn.device) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc495)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>) -> () loc(#loc495)
        return %21, %22 : tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_13(%arg0: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg1: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg2: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg3: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg4: tensor<128x256x1x1xbf16, #ttnn_layout73> loc(unknown)) -> (tensor<1x1x256x128xbf16, #ttnn_layout74>, tensor<1x1x1x128xbf16, #ttnn_layout58>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc25)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc25)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc25)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc496)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc496)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc25)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc25)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc25)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc25)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc25)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc25)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc25)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc25)
        %8 = "ttnn.reshape"(%7) <{shape = [128 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<128x1x1x1xbf16, #ttnn_layout60> loc(#loc496)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128x256x1x1xbf16, #ttnn_layout73>, !ttnn.device) -> tensor<128x256x1x1xbf16, #ttnn_layout75> loc(#loc497)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<128x256x1x1xbf16, #ttnn_layout75>) -> tensor<128x256x1x1xbf16, #ttnn_layout76> loc(#loc497)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<128x256x1x1xbf16, #ttnn_layout75>) -> () loc(#loc497)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<128x256x1x1xbf16, #ttnn_layout76>, tensor<128x1x1x1xbf16, #ttnn_layout60>) -> tensor<128x256x1x1xbf16, #ttnn_layout76> loc(#loc498)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<128x256x1x1xbf16, #ttnn_layout76>) -> () loc(#loc498)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128x1x1x1xbf16, #ttnn_layout60>) -> () loc(#loc498)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc944)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc945)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc945)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc25)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc25)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc25)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc835)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc25)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc25)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc25)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<128x256x1x1xbf16, #ttnn_layout76>) -> tensor<128x256x1x1xbf16, #ttnn_layout75> loc(#loc500)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<128x256x1x1xbf16, #ttnn_layout76>) -> () loc(#loc500)
        %18 = "ttnn.from_device"(%17) : (tensor<128x256x1x1xbf16, #ttnn_layout75>) -> tensor<128x256x1x1xbf16, #ttnn_layout73> loc(#loc500)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128x256x1x1xbf16, #ttnn_layout75>) -> () loc(#loc500)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout63> loc(#loc499)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc499)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> tensor<1x1x1x128xbf16, #ttnn_layout64> loc(#loc499)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> () loc(#loc499)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<128x256x1x1xbf16, #ttnn_layout73>, !ttnn.device) -> tensor<1x1x256x128xbf16, #ttnn_layout74> loc(#loc501)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<128x256x1x1xbf16, #ttnn_layout73>) -> () loc(#loc501)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>, !ttnn.device) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc502)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>) -> () loc(#loc502)
        return %21, %22 : tensor<1x1x256x128xbf16, #ttnn_layout74>, tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_14(%arg0: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg1: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg2: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg3: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg4: tensor<64x64x3x3xbf16, #ttnn_layout14> loc(unknown)) -> (tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc27)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc27)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc27)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc503)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc503)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc27)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc27)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc27)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc27)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc27)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc27)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc27)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc27)
        %8 = "ttnn.reshape"(%7) <{shape = [64 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<64x1x1x1xbf16, #ttnn_layout18> loc(#loc503)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<64x64x3x3xbf16, #ttnn_layout19> loc(#loc504)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> tensor<64x64x3x3xbf16, #ttnn_layout20> loc(#loc504)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> () loc(#loc504)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>, tensor<64x1x1x1xbf16, #ttnn_layout18>) -> tensor<64x64x3x3xbf16, #ttnn_layout20> loc(#loc505)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>) -> () loc(#loc505)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<64x1x1x1xbf16, #ttnn_layout18>) -> () loc(#loc505)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc946)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc947)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc947)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc27)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc27)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc27)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc837)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc27)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc27)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc27)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>) -> tensor<64x64x3x3xbf16, #ttnn_layout19> loc(#loc507)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>) -> () loc(#loc507)
        %18 = "ttnn.from_device"(%17) : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> tensor<64x64x3x3xbf16, #ttnn_layout14> loc(#loc507)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> () loc(#loc507)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout21> loc(#loc506)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc506)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout22> loc(#loc506)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> () loc(#loc506)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout15> loc(#loc508)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>) -> () loc(#loc508)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc509)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>) -> () loc(#loc509)
        return %21, %22 : tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_15(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x128x1x1xbf16, #ttnn_layout23> loc(unknown)) -> (tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc29)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc29)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc29)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc510)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc510)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc29)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc29)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc29)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc29)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc29)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc29)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc29)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc29)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc510)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<512x128x1x1xbf16, #ttnn_layout25> loc(#loc511)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> tensor<512x128x1x1xbf16, #ttnn_layout26> loc(#loc511)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> () loc(#loc511)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x128x1x1xbf16, #ttnn_layout26> loc(#loc512)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> () loc(#loc512)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc512)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc948)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc949)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc949)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc29)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc29)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc29)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc839)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc29)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc29)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc29)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> tensor<512x128x1x1xbf16, #ttnn_layout25> loc(#loc514)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> () loc(#loc514)
        %18 = "ttnn.from_device"(%17) : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> tensor<512x128x1x1xbf16, #ttnn_layout23> loc(#loc514)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> () loc(#loc514)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc513)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc513)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc513)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc513)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x128x512xbf16, #ttnn_layout24> loc(#loc515)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>) -> () loc(#loc515)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc516)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc516)
        return %21, %22 : tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_16(%arg0: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg1: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg2: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg3: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg4: tensor<64x64x1x1xbf16, #ttnn_layout77> loc(unknown)) -> (tensor<1x1x64x64xbf16, #ttnn_layout78>, tensor<1x1x1x64xbf16, #ttnn_layout16>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc31)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc31)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc31)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc517)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc517)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc31)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc31)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc31)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc31)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc31)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc31)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc31)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc31)
        %8 = "ttnn.reshape"(%7) <{shape = [64 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<64x1x1x1xbf16, #ttnn_layout18> loc(#loc517)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64x64x1x1xbf16, #ttnn_layout77>, !ttnn.device) -> tensor<64x64x1x1xbf16, #ttnn_layout79> loc(#loc518)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<64x64x1x1xbf16, #ttnn_layout79>) -> tensor<64x64x1x1xbf16, #ttnn_layout20> loc(#loc518)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<64x64x1x1xbf16, #ttnn_layout79>) -> () loc(#loc518)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<64x64x1x1xbf16, #ttnn_layout20>, tensor<64x1x1x1xbf16, #ttnn_layout18>) -> tensor<64x64x1x1xbf16, #ttnn_layout20> loc(#loc519)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<64x64x1x1xbf16, #ttnn_layout20>) -> () loc(#loc519)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<64x1x1x1xbf16, #ttnn_layout18>) -> () loc(#loc519)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc950)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc951)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc951)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc31)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc31)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc31)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc841)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc31)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc31)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc31)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<64x64x1x1xbf16, #ttnn_layout20>) -> tensor<64x64x1x1xbf16, #ttnn_layout79> loc(#loc521)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<64x64x1x1xbf16, #ttnn_layout20>) -> () loc(#loc521)
        %18 = "ttnn.from_device"(%17) : (tensor<64x64x1x1xbf16, #ttnn_layout79>) -> tensor<64x64x1x1xbf16, #ttnn_layout77> loc(#loc521)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<64x64x1x1xbf16, #ttnn_layout79>) -> () loc(#loc521)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout21> loc(#loc520)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc520)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout22> loc(#loc520)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> () loc(#loc520)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<64x64x1x1xbf16, #ttnn_layout77>, !ttnn.device) -> tensor<1x1x64x64xbf16, #ttnn_layout78> loc(#loc522)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<64x64x1x1xbf16, #ttnn_layout77>) -> () loc(#loc522)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc523)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>) -> () loc(#loc523)
        return %21, %22 : tensor<1x1x64x64xbf16, #ttnn_layout78>, tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_17(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x64x1x1xbf16, #ttnn_layout65> loc(unknown)) -> (tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc33)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc33)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc33)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc524)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc524)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc33)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc33)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc33)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc33)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc33)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc33)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc33)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc33)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc524)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>, !ttnn.device) -> tensor<256x64x1x1xbf16, #ttnn_layout67> loc(#loc525)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> tensor<256x64x1x1xbf16, #ttnn_layout68> loc(#loc525)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> () loc(#loc525)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x64x1x1xbf16, #ttnn_layout68> loc(#loc526)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> () loc(#loc526)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc526)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc952)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc953)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc953)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc33)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc33)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc33)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc843)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc33)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc33)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc33)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> tensor<256x64x1x1xbf16, #ttnn_layout67> loc(#loc528)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> () loc(#loc528)
        %18 = "ttnn.from_device"(%17) : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> tensor<256x64x1x1xbf16, #ttnn_layout65> loc(#loc528)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> () loc(#loc528)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc527)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc527)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc527)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc527)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>, !ttnn.device) -> tensor<1x1x64x256xbf16, #ttnn_layout66> loc(#loc529)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>) -> () loc(#loc529)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc530)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc530)
        return %21, %22 : tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_18(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x2048x1x1xbf16, #ttnn_layout41> loc(unknown)) -> (tensor<1x1x2048x512xbf16, #ttnn_layout42>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc35)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc35)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc35)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc531)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc531)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc35)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc35)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc35)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc35)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc35)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc35)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc35)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc35)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc531)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x2048x1x1xbf16, #ttnn_layout41>, !ttnn.device) -> tensor<512x2048x1x1xbf16, #ttnn_layout43> loc(#loc532)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x2048x1x1xbf16, #ttnn_layout43>) -> tensor<512x2048x1x1xbf16, #ttnn_layout44> loc(#loc532)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout43>) -> () loc(#loc532)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x2048x1x1xbf16, #ttnn_layout44>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x2048x1x1xbf16, #ttnn_layout44> loc(#loc533)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout44>) -> () loc(#loc533)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc533)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc954)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc955)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc955)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc35)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc35)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc35)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc845)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc35)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc35)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc35)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x2048x1x1xbf16, #ttnn_layout44>) -> tensor<512x2048x1x1xbf16, #ttnn_layout43> loc(#loc535)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout44>) -> () loc(#loc535)
        %18 = "ttnn.from_device"(%17) : (tensor<512x2048x1x1xbf16, #ttnn_layout43>) -> tensor<512x2048x1x1xbf16, #ttnn_layout41> loc(#loc535)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout43>) -> () loc(#loc535)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc534)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc534)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc534)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc534)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 2048 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<512x2048x1x1xbf16, #ttnn_layout41>, !ttnn.device) -> tensor<1x1x2048x512xbf16, #ttnn_layout42> loc(#loc536)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout41>) -> () loc(#loc536)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 2048 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc537)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc537)
        return %21, %22 : tensor<1x1x2048x512xbf16, #ttnn_layout42>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_19(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(unknown)) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc37)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc37)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc37)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc538)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc538)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc37)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc37)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc37)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc37)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc37)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc37)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc37)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc37)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc538)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc539)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc539)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc539)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc540)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc540)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc540)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc956)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc957)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc957)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc37)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc37)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc37)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc847)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc37)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc37)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc37)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc542)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc542)
        %18 = "ttnn.from_device"(%17) : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(#loc542)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc542)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc541)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc541)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc541)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc541)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<1x1x1024x256xbf16, #ttnn_layout81> loc(#loc543)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc543)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc544)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc544)
        return %21, %22 : tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_20(%arg0: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg1: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg2: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg3: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg4: tensor<128x512x1x1xbf16, #ttnn_layout69> loc(unknown)) -> (tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc39)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc39)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc39)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc545)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc545)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc39)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc39)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc39)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc39)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc39)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc39)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc39)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc39)
        %8 = "ttnn.reshape"(%7) <{shape = [128 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<128x1x1x1xbf16, #ttnn_layout60> loc(#loc545)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>, !ttnn.device) -> tensor<128x512x1x1xbf16, #ttnn_layout71> loc(#loc546)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> tensor<128x512x1x1xbf16, #ttnn_layout72> loc(#loc546)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> () loc(#loc546)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>, tensor<128x1x1x1xbf16, #ttnn_layout60>) -> tensor<128x512x1x1xbf16, #ttnn_layout72> loc(#loc547)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>) -> () loc(#loc547)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128x1x1x1xbf16, #ttnn_layout60>) -> () loc(#loc547)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc958)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc959)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc959)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc39)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc39)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc39)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc849)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc39)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc39)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc39)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>) -> tensor<128x512x1x1xbf16, #ttnn_layout71> loc(#loc549)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>) -> () loc(#loc549)
        %18 = "ttnn.from_device"(%17) : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> tensor<128x512x1x1xbf16, #ttnn_layout69> loc(#loc549)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> () loc(#loc549)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout63> loc(#loc548)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc548)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> tensor<1x1x1x128xbf16, #ttnn_layout64> loc(#loc548)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> () loc(#loc548)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x512>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>, !ttnn.device) -> tensor<1x1x512x128xbf16, #ttnn_layout70> loc(#loc550)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>) -> () loc(#loc550)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x512>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>, !ttnn.device) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc551)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>) -> () loc(#loc551)
        return %21, %22 : tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_21(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x128x1x1xbf16, #ttnn_layout23> loc(unknown)) -> (tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc41)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc41)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc41)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc552)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc552)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc41)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc41)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc41)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc41)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc41)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc41)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc41)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc41)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc552)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<512x128x1x1xbf16, #ttnn_layout25> loc(#loc553)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> tensor<512x128x1x1xbf16, #ttnn_layout26> loc(#loc553)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> () loc(#loc553)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x128x1x1xbf16, #ttnn_layout26> loc(#loc554)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> () loc(#loc554)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc554)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc960)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc961)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc961)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc41)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc41)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc41)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc851)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc41)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc41)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc41)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> tensor<512x128x1x1xbf16, #ttnn_layout25> loc(#loc556)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> () loc(#loc556)
        %18 = "ttnn.from_device"(%17) : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> tensor<512x128x1x1xbf16, #ttnn_layout23> loc(#loc556)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> () loc(#loc556)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc555)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc555)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc555)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc555)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x128x512xbf16, #ttnn_layout24> loc(#loc557)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>) -> () loc(#loc557)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc558)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc558)
        return %21, %22 : tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_22(%arg0: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg1: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg2: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg3: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg4: tensor<64x256x1x1xbf16, #ttnn_layout37> loc(unknown)) -> (tensor<1x1x256x64xbf16, #ttnn_layout38>, tensor<1x1x1x64xbf16, #ttnn_layout16>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc43)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc43)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc43)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc559)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc559)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc43)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc43)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc43)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc43)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc43)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc43)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc43)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc43)
        %8 = "ttnn.reshape"(%7) <{shape = [64 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<64x1x1x1xbf16, #ttnn_layout18> loc(#loc559)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64x256x1x1xbf16, #ttnn_layout37>, !ttnn.device) -> tensor<64x256x1x1xbf16, #ttnn_layout39> loc(#loc560)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<64x256x1x1xbf16, #ttnn_layout39>) -> tensor<64x256x1x1xbf16, #ttnn_layout40> loc(#loc560)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout39>) -> () loc(#loc560)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<64x256x1x1xbf16, #ttnn_layout40>, tensor<64x1x1x1xbf16, #ttnn_layout18>) -> tensor<64x256x1x1xbf16, #ttnn_layout40> loc(#loc561)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout40>) -> () loc(#loc561)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<64x1x1x1xbf16, #ttnn_layout18>) -> () loc(#loc561)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc962)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc963)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc963)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc43)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc43)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc43)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc853)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc43)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc43)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc43)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<64x256x1x1xbf16, #ttnn_layout40>) -> tensor<64x256x1x1xbf16, #ttnn_layout39> loc(#loc563)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout40>) -> () loc(#loc563)
        %18 = "ttnn.from_device"(%17) : (tensor<64x256x1x1xbf16, #ttnn_layout39>) -> tensor<64x256x1x1xbf16, #ttnn_layout37> loc(#loc563)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout39>) -> () loc(#loc563)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout21> loc(#loc562)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc562)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout22> loc(#loc562)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> () loc(#loc562)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<64x256x1x1xbf16, #ttnn_layout37>, !ttnn.device) -> tensor<1x1x256x64xbf16, #ttnn_layout38> loc(#loc564)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout37>) -> () loc(#loc564)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x256>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc565)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>) -> () loc(#loc565)
        return %21, %22 : tensor<1x1x256x64xbf16, #ttnn_layout38>, tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_23(%arg0: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg1: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg2: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg3: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg4: tensor<2048x1024x1x1xbf16, #ttnn_layout85> loc(unknown)) -> (tensor<1x1x1024x2048xbf16, #ttnn_layout86>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc45)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc45)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc45)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc566)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc566)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc45)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc45)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc45)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc45)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc45)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>, tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc45)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc45)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc45)
        %8 = "ttnn.reshape"(%7) <{shape = [2048 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<2048x1x1x1xbf16, #ttnn_layout89> loc(#loc566)
        %9 = "ttnn.permute"(%8) <{permutation = array<i64: 2, 3, 0, 1>}> : (tensor<2048x1x1x1xbf16, #ttnn_layout89>) -> tensor<1x1x2048x1xbf16, #ttnn_layout90> loc(#loc854)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<2048x1x1x1xbf16, #ttnn_layout89>) -> () loc(#loc854)
        %10 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout85>, !ttnn.device) -> tensor<2048x1024x1x1xbf16, #ttnn_layout91> loc(#loc568)
        %11 = "ttnn.to_layout"(%10) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout91>) -> tensor<2048x1024x1x1xbf16, #ttnn_layout92> loc(#loc568)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout91>) -> () loc(#loc568)
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 2, 3, 0, 1>}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout92>) -> tensor<1x1x2048x1024xbf16, #ttnn_layout93> loc(#loc855)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout92>) -> () loc(#loc855)
        %13 = "ttnn.multiply"(%12, %9) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x2048x1024xbf16, #ttnn_layout93>, tensor<1x1x2048x1xbf16, #ttnn_layout90>) -> tensor<1x1x2048x1024xbf16, #ttnn_layout93> loc(#loc567)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x2048x1024xbf16, #ttnn_layout93>) -> () loc(#loc567)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x2048x1xbf16, #ttnn_layout90>) -> () loc(#loc567)
        %14 = "ttnn.permute"(%13) <{permutation = array<i64: 2, 3, 0, 1>}> : (tensor<1x1x2048x1024xbf16, #ttnn_layout93>) -> tensor<2048x1024x1x1xbf16, #ttnn_layout92> loc(#loc856)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x2048x1024xbf16, #ttnn_layout93>) -> () loc(#loc856)
        %15 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc964)
        %16 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc965)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc965)
        %17 = "ttnn.multiply"(%15, %16) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc45)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc45)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc45)
        %18 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc858)
        %19 = "ttnn.subtract"(%18, %17) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc45)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc45)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc45)
        %20 = "ttnn.to_layout"(%14) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout92>) -> tensor<2048x1024x1x1xbf16, #ttnn_layout91> loc(#loc570)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout92>) -> () loc(#loc570)
        %21 = "ttnn.from_device"(%20) : (tensor<2048x1024x1x1xbf16, #ttnn_layout91>) -> tensor<2048x1024x1x1xbf16, #ttnn_layout85> loc(#loc570)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout91>) -> () loc(#loc570)
        %22 = "ttnn.to_layout"(%19) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout94> loc(#loc569)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc569)
        %23 = "ttnn.from_device"(%22) : (tensor<1x1x1x2048xbf16, #ttnn_layout94>) -> tensor<1x1x1x2048xbf16, #ttnn_layout95> loc(#loc569)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout94>) -> () loc(#loc569)
        %24 = "ttnn.prepare_conv2d_weights"(%21, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>, weights_format = "OIHW"}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout85>, !ttnn.device) -> tensor<1x1x1024x2048xbf16, #ttnn_layout86> loc(#loc571)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout85>) -> () loc(#loc571)
        %25 = "ttnn.prepare_conv2d_bias"(%23, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout95>, !ttnn.device) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc572)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout95>) -> () loc(#loc572)
        return %24, %25 : tensor<1x1x1024x2048xbf16, #ttnn_layout86>, tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_24(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(unknown)) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc47)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc47)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc47)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc573)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc573)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc47)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc47)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc47)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc47)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc47)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc47)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc47)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc47)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc573)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc574)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc574)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc574)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc575)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc575)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc575)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc966)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc967)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc967)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc47)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc47)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc47)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc860)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc47)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc47)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc47)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc577)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc577)
        %18 = "ttnn.from_device"(%17) : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(#loc577)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc577)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc576)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc576)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc576)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc576)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<1x1x1024x256xbf16, #ttnn_layout81> loc(#loc578)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc578)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc579)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc579)
        return %21, %22 : tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_25(%arg0: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg1: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg2: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg3: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg4: tensor<128x128x3x3xbf16, #ttnn_layout56> loc(unknown)) -> (tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc49)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc49)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc49)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc580)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc580)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc49)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc49)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc49)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc49)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc49)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc49)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc49)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc49)
        %8 = "ttnn.reshape"(%7) <{shape = [128 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<128x1x1x1xbf16, #ttnn_layout60> loc(#loc580)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<128x128x3x3xbf16, #ttnn_layout61> loc(#loc581)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> tensor<128x128x3x3xbf16, #ttnn_layout62> loc(#loc581)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> () loc(#loc581)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>, tensor<128x1x1x1xbf16, #ttnn_layout60>) -> tensor<128x128x3x3xbf16, #ttnn_layout62> loc(#loc582)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> () loc(#loc582)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128x1x1x1xbf16, #ttnn_layout60>) -> () loc(#loc582)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc968)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc969)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc969)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc49)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc49)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc49)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc862)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc49)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc49)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc49)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> tensor<128x128x3x3xbf16, #ttnn_layout61> loc(#loc584)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> () loc(#loc584)
        %18 = "ttnn.from_device"(%17) : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> tensor<128x128x3x3xbf16, #ttnn_layout56> loc(#loc584)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> () loc(#loc584)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout63> loc(#loc583)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc583)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> tensor<1x1x1x128xbf16, #ttnn_layout64> loc(#loc583)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> () loc(#loc583)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <512x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>, weights_format = "OIHW"}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1152x128xbf16, #ttnn_layout57> loc(#loc585)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc585)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <512x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>, !ttnn.device) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc586)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>) -> () loc(#loc586)
        return %21, %22 : tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_26(%arg0: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg1: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg2: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg3: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg4: tensor<2048x512x1x1xbf16, #ttnn_layout96> loc(unknown)) -> (tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc51)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc51)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc51)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc587)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc587)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc51)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc51)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc51)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc51)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc51)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>, tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc51)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc51)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc51)
        %8 = "ttnn.reshape"(%7) <{shape = [2048 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<2048x1x1x1xbf16, #ttnn_layout89> loc(#loc587)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>, !ttnn.device) -> tensor<2048x512x1x1xbf16, #ttnn_layout98> loc(#loc588)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> tensor<2048x512x1x1xbf16, #ttnn_layout99> loc(#loc588)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> () loc(#loc588)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>, tensor<2048x1x1x1xbf16, #ttnn_layout89>) -> tensor<2048x512x1x1xbf16, #ttnn_layout99> loc(#loc589)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>) -> () loc(#loc589)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<2048x1x1x1xbf16, #ttnn_layout89>) -> () loc(#loc589)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc970)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc971)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc971)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc51)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc51)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc51)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc864)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc51)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc51)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc51)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>) -> tensor<2048x512x1x1xbf16, #ttnn_layout98> loc(#loc591)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>) -> () loc(#loc591)
        %18 = "ttnn.from_device"(%17) : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> tensor<2048x512x1x1xbf16, #ttnn_layout96> loc(#loc591)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> () loc(#loc591)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout94> loc(#loc590)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc590)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x2048xbf16, #ttnn_layout94>) -> tensor<1x1x1x2048xbf16, #ttnn_layout95> loc(#loc590)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout94>) -> () loc(#loc590)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>, !ttnn.device) -> tensor<1x1x512x2048xbf16, #ttnn_layout97> loc(#loc592)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>) -> () loc(#loc592)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout95>, !ttnn.device) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc593)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout95>) -> () loc(#loc593)
        return %21, %22 : tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_27(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x256x3x3xbf16, #ttnn_layout46> loc(unknown)) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc53)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc53)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc53)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc594)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc594)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc53)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc53)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc53)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc53)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc53)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc53)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc53)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc53)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc594)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc595)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc595)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc595)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc596)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc596)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc596)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc972)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc973)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc973)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc53)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc53)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc53)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc866)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc53)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc53)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc53)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc598)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc598)
        %18 = "ttnn.from_device"(%17) : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout46> loc(#loc598)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc598)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc597)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc597)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc597)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc597)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x2304x256xbf16, #ttnn_layout47> loc(#loc599)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc599)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc600)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc600)
        return %21, %22 : tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_28(%arg0: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg1: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg2: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg3: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg4: tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(unknown)) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc55)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc55)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc55)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc601)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc601)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc55)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc55)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc55)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc55)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc55)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc55)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc55)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc55)
        %8 = "ttnn.reshape"(%7) <{shape = [1024 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1024x1x1x1xbf16, #ttnn_layout32> loc(#loc601)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc602)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc602)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc602)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>, tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc603)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc603)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> () loc(#loc603)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc974)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc975)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc975)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc55)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc55)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc55)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc868)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc55)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc55)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc55)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc605)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc605)
        %18 = "ttnn.from_device"(%17) : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(#loc605)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc605)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout35> loc(#loc604)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc604)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> tensor<1x1x1x1024xbf16, #ttnn_layout36> loc(#loc604)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> () loc(#loc604)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1x1x256x1024xbf16, #ttnn_layout29> loc(#loc606)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc606)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>, !ttnn.device) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc607)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>) -> () loc(#loc607)
        return %21, %22 : tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_29(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x64x1x1xbf16, #ttnn_layout65> loc(unknown)) -> (tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc57)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc57)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc57)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc608)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc608)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc57)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc57)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc57)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc57)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc57)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc57)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc57)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc57)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc608)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>, !ttnn.device) -> tensor<256x64x1x1xbf16, #ttnn_layout67> loc(#loc609)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> tensor<256x64x1x1xbf16, #ttnn_layout68> loc(#loc609)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> () loc(#loc609)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x64x1x1xbf16, #ttnn_layout68> loc(#loc610)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> () loc(#loc610)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc610)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc976)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc977)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc977)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc57)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc57)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc57)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc870)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc57)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc57)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc57)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> tensor<256x64x1x1xbf16, #ttnn_layout67> loc(#loc612)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> () loc(#loc612)
        %18 = "ttnn.from_device"(%17) : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> tensor<256x64x1x1xbf16, #ttnn_layout65> loc(#loc612)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> () loc(#loc612)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc611)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc611)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc611)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc611)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>, !ttnn.device) -> tensor<1x1x64x256xbf16, #ttnn_layout66> loc(#loc613)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>) -> () loc(#loc613)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc614)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc614)
        return %21, %22 : tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_30(%arg0: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg1: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg2: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg3: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg4: tensor<64x3x7x7xbf16, #ttnn_layout100> loc(unknown)) -> (tensor<1x1x147x64xbf16, #ttnn_layout101>, tensor<1x1x1x64xbf16, #ttnn_layout16>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc59)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc59)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc59)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc615)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc615)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc59)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc59)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc59)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc59)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc59)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc59)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc59)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc59)
        %8 = "ttnn.reshape"(%7) <{shape = [64 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<64x1x1x1xbf16, #ttnn_layout18> loc(#loc615)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64x3x7x7xbf16, #ttnn_layout100>, !ttnn.device) -> tensor<64x3x7x7xbf16, #ttnn_layout102> loc(#loc616)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<64x3x7x7xbf16, #ttnn_layout102>) -> tensor<64x3x7x7xbf16, #ttnn_layout103> loc(#loc616)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<64x3x7x7xbf16, #ttnn_layout102>) -> () loc(#loc616)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<64x3x7x7xbf16, #ttnn_layout103>, tensor<64x1x1x1xbf16, #ttnn_layout18>) -> tensor<64x3x7x7xbf16, #ttnn_layout103> loc(#loc617)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<64x3x7x7xbf16, #ttnn_layout103>) -> () loc(#loc617)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<64x1x1x1xbf16, #ttnn_layout18>) -> () loc(#loc617)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc978)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc979)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc979)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc59)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc59)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc59)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc872)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc59)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc59)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc59)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<64x3x7x7xbf16, #ttnn_layout103>) -> tensor<64x3x7x7xbf16, #ttnn_layout102> loc(#loc619)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<64x3x7x7xbf16, #ttnn_layout103>) -> () loc(#loc619)
        %18 = "ttnn.from_device"(%17) : (tensor<64x3x7x7xbf16, #ttnn_layout102>) -> tensor<64x3x7x7xbf16, #ttnn_layout100> loc(#loc619)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<64x3x7x7xbf16, #ttnn_layout102>) -> () loc(#loc619)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout21> loc(#loc618)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc618)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout22> loc(#loc618)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> () loc(#loc618)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 3 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 224 : i32, input_memory_config = #ttnn.memory_config<#dram, <interleaved>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 224 : i32, kernel_size = array<i32: 7, 7>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 3, 3, 3, 3>, stride = array<i32: 2, 2>, weights_format = "OIHW"}> : (tensor<64x3x7x7xbf16, #ttnn_layout100>, !ttnn.device) -> tensor<1x1x147x64xbf16, #ttnn_layout101> loc(#loc620)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<64x3x7x7xbf16, #ttnn_layout100>) -> () loc(#loc620)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 3 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 224 : i32, input_memory_config = #ttnn.memory_config<#dram, <interleaved>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 224 : i32, kernel_size = array<i32: 7, 7>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 3, 3, 3, 3>, stride = array<i32: 2, 2>}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc621)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>) -> () loc(#loc621)
        return %21, %22 : tensor<1x1x147x64xbf16, #ttnn_layout101>, tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_31(%arg0: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg1: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg2: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg3: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg4: tensor<2048x512x1x1xbf16, #ttnn_layout96> loc(unknown)) -> (tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc61)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc61)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc61)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc622)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc622)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc61)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc61)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc61)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc61)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc61)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>, tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc61)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc61)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc61)
        %8 = "ttnn.reshape"(%7) <{shape = [2048 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<2048x1x1x1xbf16, #ttnn_layout89> loc(#loc622)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>, !ttnn.device) -> tensor<2048x512x1x1xbf16, #ttnn_layout98> loc(#loc623)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> tensor<2048x512x1x1xbf16, #ttnn_layout99> loc(#loc623)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> () loc(#loc623)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>, tensor<2048x1x1x1xbf16, #ttnn_layout89>) -> tensor<2048x512x1x1xbf16, #ttnn_layout99> loc(#loc624)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>) -> () loc(#loc624)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<2048x1x1x1xbf16, #ttnn_layout89>) -> () loc(#loc624)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc980)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc981)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc981)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc61)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc61)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc61)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc874)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc61)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc61)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc61)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>) -> tensor<2048x512x1x1xbf16, #ttnn_layout98> loc(#loc626)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>) -> () loc(#loc626)
        %18 = "ttnn.from_device"(%17) : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> tensor<2048x512x1x1xbf16, #ttnn_layout96> loc(#loc626)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> () loc(#loc626)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout94> loc(#loc625)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc625)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x2048xbf16, #ttnn_layout94>) -> tensor<1x1x1x2048xbf16, #ttnn_layout95> loc(#loc625)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout94>) -> () loc(#loc625)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>, !ttnn.device) -> tensor<1x1x512x2048xbf16, #ttnn_layout97> loc(#loc627)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>) -> () loc(#loc627)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout95>, !ttnn.device) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc628)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout95>) -> () loc(#loc628)
        return %21, %22 : tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_32(%arg0: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg1: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg2: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg3: tensor<64xbf16, #ttnn_layout13> loc(unknown), %arg4: tensor<64x64x3x3xbf16, #ttnn_layout14> loc(unknown)) -> (tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc63)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc63)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc63)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc629)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc629)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc63)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc63)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc63)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc63)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc63)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>, tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x64x1x1xbf16, #ttnn_layout17> loc(#loc63)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc63)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc63)
        %8 = "ttnn.reshape"(%7) <{shape = [64 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<64x1x1x1xbf16, #ttnn_layout18> loc(#loc629)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<64x64x3x3xbf16, #ttnn_layout19> loc(#loc630)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> tensor<64x64x3x3xbf16, #ttnn_layout20> loc(#loc630)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> () loc(#loc630)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>, tensor<64x1x1x1xbf16, #ttnn_layout18>) -> tensor<64x64x3x3xbf16, #ttnn_layout20> loc(#loc631)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>) -> () loc(#loc631)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<64x1x1x1xbf16, #ttnn_layout18>) -> () loc(#loc631)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc982)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc983)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc983)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc63)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc63)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc63)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc876)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>, tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc63)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc63)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc63)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>) -> tensor<64x64x3x3xbf16, #ttnn_layout19> loc(#loc633)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout20>) -> () loc(#loc633)
        %18 = "ttnn.from_device"(%17) : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> tensor<64x64x3x3xbf16, #ttnn_layout14> loc(#loc633)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout19>) -> () loc(#loc633)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> tensor<1x1x1x64xbf16, #ttnn_layout21> loc(#loc632)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc632)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout22> loc(#loc632)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout21>) -> () loc(#loc632)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout15> loc(#loc634)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>) -> () loc(#loc634)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc635)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout22>) -> () loc(#loc635)
        return %21, %22 : tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_33(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x256x3x3xbf16, #ttnn_layout46> loc(unknown)) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc65)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc65)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc65)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc636)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc636)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc65)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc65)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc65)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc65)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc65)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc65)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc65)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc65)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc636)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc637)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc637)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc637)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc638)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc638)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc638)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc984)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc985)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc985)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc65)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc65)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc65)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc878)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc65)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc65)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc65)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc640)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc640)
        %18 = "ttnn.from_device"(%17) : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout46> loc(#loc640)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc640)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc639)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc639)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc639)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc639)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x2304x256xbf16, #ttnn_layout47> loc(#loc641)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc641)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc642)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc642)
        return %21, %22 : tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_34(%arg0: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg1: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg2: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg3: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg4: tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(unknown)) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc67)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc67)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc67)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc643)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc643)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc67)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc67)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc67)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc67)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc67)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc67)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc67)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc67)
        %8 = "ttnn.reshape"(%7) <{shape = [1024 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1024x1x1x1xbf16, #ttnn_layout32> loc(#loc643)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc644)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc644)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc644)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>, tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc645)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc645)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> () loc(#loc645)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc986)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc987)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc987)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc67)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc67)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc67)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc880)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc67)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc67)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc67)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc647)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc647)
        %18 = "ttnn.from_device"(%17) : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(#loc647)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc647)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout35> loc(#loc646)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc646)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> tensor<1x1x1x1024xbf16, #ttnn_layout36> loc(#loc646)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> () loc(#loc646)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1x1x256x1024xbf16, #ttnn_layout29> loc(#loc648)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc648)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>, !ttnn.device) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc649)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>) -> () loc(#loc649)
        return %21, %22 : tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_35(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(unknown)) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc69)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc69)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc69)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc650)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc650)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc69)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc69)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc69)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc69)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc69)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc69)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc69)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc69)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc650)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc651)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc651)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc651)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc652)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc652)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc652)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc988)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc989)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc989)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc69)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc69)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc69)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc882)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc69)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc69)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc69)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc654)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc654)
        %18 = "ttnn.from_device"(%17) : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(#loc654)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc654)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc653)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc653)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc653)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc653)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<1x1x1024x256xbf16, #ttnn_layout81> loc(#loc655)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc655)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc656)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc656)
        return %21, %22 : tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_36(%arg0: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg1: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg2: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg3: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg4: tensor<128x128x3x3xbf16, #ttnn_layout56> loc(unknown)) -> (tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc71)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc71)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc71)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc657)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc657)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc71)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc71)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc71)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc71)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc71)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc71)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc71)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc71)
        %8 = "ttnn.reshape"(%7) <{shape = [128 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<128x1x1x1xbf16, #ttnn_layout60> loc(#loc657)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<128x128x3x3xbf16, #ttnn_layout61> loc(#loc658)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> tensor<128x128x3x3xbf16, #ttnn_layout62> loc(#loc658)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> () loc(#loc658)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>, tensor<128x1x1x1xbf16, #ttnn_layout60>) -> tensor<128x128x3x3xbf16, #ttnn_layout62> loc(#loc659)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> () loc(#loc659)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128x1x1x1xbf16, #ttnn_layout60>) -> () loc(#loc659)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc990)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc991)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc991)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc71)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc71)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc71)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc884)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc71)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc71)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc71)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> tensor<128x128x3x3xbf16, #ttnn_layout61> loc(#loc661)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> () loc(#loc661)
        %18 = "ttnn.from_device"(%17) : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> tensor<128x128x3x3xbf16, #ttnn_layout56> loc(#loc661)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> () loc(#loc661)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout63> loc(#loc660)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc660)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> tensor<1x1x1x128xbf16, #ttnn_layout64> loc(#loc660)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> () loc(#loc660)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1152x128xbf16, #ttnn_layout57> loc(#loc662)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc662)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>, !ttnn.device) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc663)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>) -> () loc(#loc663)
        return %21, %22 : tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_37(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(unknown)) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc73)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc73)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc73)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc664)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc664)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc73)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc73)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc73)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc73)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc73)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc73)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc73)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc73)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc664)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc665)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc665)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc665)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc666)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc666)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc666)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc992)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc993)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc993)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc73)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc73)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc73)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc886)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc73)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc73)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc73)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc668)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc668)
        %18 = "ttnn.from_device"(%17) : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(#loc668)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc668)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc667)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc667)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc667)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc667)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<1x1x1024x256xbf16, #ttnn_layout81> loc(#loc669)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc669)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc670)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc670)
        return %21, %22 : tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_38(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x512x1x1xbf16, #ttnn_layout104> loc(unknown)) -> (tensor<1x1x512x256xbf16, #ttnn_layout105>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc75)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc75)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc75)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc671)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc671)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc75)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc75)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc75)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc75)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc75)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc75)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc75)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc75)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc671)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x512x1x1xbf16, #ttnn_layout104>, !ttnn.device) -> tensor<256x512x1x1xbf16, #ttnn_layout106> loc(#loc672)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x512x1x1xbf16, #ttnn_layout106>) -> tensor<256x512x1x1xbf16, #ttnn_layout107> loc(#loc672)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout106>) -> () loc(#loc672)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x512x1x1xbf16, #ttnn_layout107>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x512x1x1xbf16, #ttnn_layout107> loc(#loc673)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout107>) -> () loc(#loc673)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc673)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc994)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc995)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc995)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc75)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc75)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc75)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc888)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc75)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc75)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc75)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x512x1x1xbf16, #ttnn_layout107>) -> tensor<256x512x1x1xbf16, #ttnn_layout106> loc(#loc675)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout107>) -> () loc(#loc675)
        %18 = "ttnn.from_device"(%17) : (tensor<256x512x1x1xbf16, #ttnn_layout106>) -> tensor<256x512x1x1xbf16, #ttnn_layout104> loc(#loc675)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout106>) -> () loc(#loc675)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc674)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc674)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc674)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc674)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,7)>]>, <800x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x512x1x1xbf16, #ttnn_layout104>, !ttnn.device) -> tensor<1x1x512x256xbf16, #ttnn_layout105> loc(#loc676)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout104>) -> () loc(#loc676)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,7)>]>, <800x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc677)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc677)
        return %21, %22 : tensor<1x1x512x256xbf16, #ttnn_layout105>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_39(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x256x1x1xbf16, #ttnn_layout108> loc(unknown)) -> (tensor<1x1x256x512xbf16, #ttnn_layout109>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc77)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc77)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc77)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc678)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc678)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc77)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc77)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc77)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc77)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc77)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc77)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc77)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc77)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc678)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x256x1x1xbf16, #ttnn_layout108>, !ttnn.device) -> tensor<512x256x1x1xbf16, #ttnn_layout110> loc(#loc679)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x256x1x1xbf16, #ttnn_layout110>) -> tensor<512x256x1x1xbf16, #ttnn_layout111> loc(#loc679)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout110>) -> () loc(#loc679)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x256x1x1xbf16, #ttnn_layout111>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x256x1x1xbf16, #ttnn_layout111> loc(#loc680)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout111>) -> () loc(#loc680)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc680)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc996)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc997)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc997)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc77)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc77)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc77)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc890)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc77)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc77)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc77)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x256x1x1xbf16, #ttnn_layout111>) -> tensor<512x256x1x1xbf16, #ttnn_layout110> loc(#loc682)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout111>) -> () loc(#loc682)
        %18 = "ttnn.from_device"(%17) : (tensor<512x256x1x1xbf16, #ttnn_layout110>) -> tensor<512x256x1x1xbf16, #ttnn_layout108> loc(#loc682)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout110>) -> () loc(#loc682)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc681)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc681)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc681)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc681)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,7)>]>, <3136x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>, weights_format = "OIHW"}> : (tensor<512x256x1x1xbf16, #ttnn_layout108>, !ttnn.device) -> tensor<1x1x256x512xbf16, #ttnn_layout109> loc(#loc683)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout108>) -> () loc(#loc683)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,7)>]>, <3136x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc684)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc684)
        return %21, %22 : tensor<1x1x256x512xbf16, #ttnn_layout109>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_40(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x64x1x1xbf16, #ttnn_layout65> loc(unknown)) -> (tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc79)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc79)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc79)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc685)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc685)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc79)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc79)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc79)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc79)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc79)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc79)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc79)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc79)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc685)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>, !ttnn.device) -> tensor<256x64x1x1xbf16, #ttnn_layout67> loc(#loc686)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> tensor<256x64x1x1xbf16, #ttnn_layout68> loc(#loc686)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> () loc(#loc686)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x64x1x1xbf16, #ttnn_layout68> loc(#loc687)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> () loc(#loc687)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc687)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc998)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc999)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc999)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc79)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc79)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc79)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc892)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc79)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc79)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc79)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> tensor<256x64x1x1xbf16, #ttnn_layout67> loc(#loc689)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout68>) -> () loc(#loc689)
        %18 = "ttnn.from_device"(%17) : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> tensor<256x64x1x1xbf16, #ttnn_layout65> loc(#loc689)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout67>) -> () loc(#loc689)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc688)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc688)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc688)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc688)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>, !ttnn.device) -> tensor<1x1x64x256xbf16, #ttnn_layout66> loc(#loc690)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>) -> () loc(#loc690)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 64 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 56 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc691)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc691)
        return %21, %22 : tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_41(%arg0: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg1: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg2: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg3: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg4: tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(unknown)) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc81)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc81)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc81)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc692)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc692)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc81)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc81)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc81)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc81)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc81)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc81)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc81)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc81)
        %8 = "ttnn.reshape"(%7) <{shape = [1024 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1024x1x1x1xbf16, #ttnn_layout32> loc(#loc692)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc693)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc693)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc693)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>, tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc694)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc694)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> () loc(#loc694)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc1000)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc1001)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc1001)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc81)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc81)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc81)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc894)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc81)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc81)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc81)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc696)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc696)
        %18 = "ttnn.from_device"(%17) : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(#loc696)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc696)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout35> loc(#loc695)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc695)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> tensor<1x1x1x1024xbf16, #ttnn_layout36> loc(#loc695)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> () loc(#loc695)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1x1x256x1024xbf16, #ttnn_layout29> loc(#loc697)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc697)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>, !ttnn.device) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc698)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>) -> () loc(#loc698)
        return %21, %22 : tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_42(%arg0: tensor<1000xbf16, #ttnn_layout27> loc(unknown)) -> tensor<8x1000xbf16, #ttnn_layout112> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1000 : i32]}> : (tensor<1000xbf16, #ttnn_layout27>) -> tensor<1x1000xbf16, #ttnn_layout112> loc(#loc83)
        %1 = "ttnn.repeat"(%0) <{repeat_dims = #ttnn.shape<8x1>}> : (tensor<1x1000xbf16, #ttnn_layout112>) -> tensor<8x1000xbf16, #ttnn_layout112> loc(#loc83)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1000xbf16, #ttnn_layout112>) -> () loc(#loc83)
        return %1 : tensor<8x1000xbf16, #ttnn_layout112> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_43(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x256x3x3xbf16, #ttnn_layout46> loc(unknown)) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc84)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc84)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc84)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc699)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc699)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc84)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc84)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc84)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc84)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc84)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc84)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc84)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc84)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc699)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc700)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc700)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc700)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x256x3x3xbf16, #ttnn_layout52> loc(#loc701)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc701)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc701)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc1002)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc1003)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc1003)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc84)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc84)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc84)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc896)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc84)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc84)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc84)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> tensor<256x256x3x3xbf16, #ttnn_layout51> loc(#loc703)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout52>) -> () loc(#loc703)
        %18 = "ttnn.from_device"(%17) : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> tensor<256x256x3x3xbf16, #ttnn_layout46> loc(#loc703)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout51>) -> () loc(#loc703)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc702)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc702)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc702)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc702)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x2304x256xbf16, #ttnn_layout47> loc(#loc704)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc704)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc705)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc705)
        return %21, %22 : tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_44(%arg0: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg1: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg2: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg3: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg4: tensor<128x128x3x3xbf16, #ttnn_layout56> loc(unknown)) -> (tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc86)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc86)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc86)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc706)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc706)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc86)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc86)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc86)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc86)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc86)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc86)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc86)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc86)
        %8 = "ttnn.reshape"(%7) <{shape = [128 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<128x1x1x1xbf16, #ttnn_layout60> loc(#loc706)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<128x128x3x3xbf16, #ttnn_layout61> loc(#loc707)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> tensor<128x128x3x3xbf16, #ttnn_layout62> loc(#loc707)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> () loc(#loc707)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>, tensor<128x1x1x1xbf16, #ttnn_layout60>) -> tensor<128x128x3x3xbf16, #ttnn_layout62> loc(#loc708)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> () loc(#loc708)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128x1x1x1xbf16, #ttnn_layout60>) -> () loc(#loc708)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc1004)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc1005)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc1005)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc86)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc86)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc86)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc898)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc86)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc86)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc86)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> tensor<128x128x3x3xbf16, #ttnn_layout61> loc(#loc710)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout62>) -> () loc(#loc710)
        %18 = "ttnn.from_device"(%17) : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> tensor<128x128x3x3xbf16, #ttnn_layout56> loc(#loc710)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout61>) -> () loc(#loc710)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout63> loc(#loc709)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc709)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> tensor<1x1x1x128xbf16, #ttnn_layout64> loc(#loc709)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> () loc(#loc709)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1152x128xbf16, #ttnn_layout57> loc(#loc711)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc711)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>, !ttnn.device) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc712)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>) -> () loc(#loc712)
        return %21, %22 : tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_45(%arg0: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg1: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg2: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg3: tensor<256xbf16, #ttnn_layout45> loc(unknown), %arg4: tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(unknown)) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc88)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc88)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc88)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc713)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc713)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc88)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc88)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc88)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc88)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc88)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>, tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x256x1x1xbf16, #ttnn_layout49> loc(#loc88)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc88)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc88)
        %8 = "ttnn.reshape"(%7) <{shape = [256 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<256x1x1x1xbf16, #ttnn_layout50> loc(#loc713)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc714)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc714)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc714)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>, tensor<256x1x1x1xbf16, #ttnn_layout50>) -> tensor<256x1024x1x1xbf16, #ttnn_layout83> loc(#loc715)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc715)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<256x1x1x1xbf16, #ttnn_layout50>) -> () loc(#loc715)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc1006)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc1007)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout49>) -> () loc(#loc1007)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc88)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc88)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc88)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout45>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc900)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>, tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc88)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc88)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc88)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> tensor<256x1024x1x1xbf16, #ttnn_layout82> loc(#loc717)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout83>) -> () loc(#loc717)
        %18 = "ttnn.from_device"(%17) : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> tensor<256x1024x1x1xbf16, #ttnn_layout80> loc(#loc717)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout82>) -> () loc(#loc717)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> tensor<1x1x1x256xbf16, #ttnn_layout53> loc(#loc716)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc716)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> tensor<1x1x1x256xbf16, #ttnn_layout54> loc(#loc716)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout53>) -> () loc(#loc716)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>, !ttnn.device) -> tensor<1x1x1024x256xbf16, #ttnn_layout81> loc(#loc718)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc718)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1024 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc719)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout54>) -> () loc(#loc719)
        return %21, %22 : tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_46(%arg0: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg1: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg2: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg3: tensor<128xbf16, #ttnn_layout55> loc(unknown), %arg4: tensor<128x512x1x1xbf16, #ttnn_layout69> loc(unknown)) -> (tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc90)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc90)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc90)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc720)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc720)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc90)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc90)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc90)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc90)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc90)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>, tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x128x1x1xbf16, #ttnn_layout59> loc(#loc90)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc90)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc90)
        %8 = "ttnn.reshape"(%7) <{shape = [128 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<128x1x1x1xbf16, #ttnn_layout60> loc(#loc720)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>, !ttnn.device) -> tensor<128x512x1x1xbf16, #ttnn_layout71> loc(#loc721)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> tensor<128x512x1x1xbf16, #ttnn_layout72> loc(#loc721)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> () loc(#loc721)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>, tensor<128x1x1x1xbf16, #ttnn_layout60>) -> tensor<128x512x1x1xbf16, #ttnn_layout72> loc(#loc722)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>) -> () loc(#loc722)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<128x1x1x1xbf16, #ttnn_layout60>) -> () loc(#loc722)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc1008)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc1009)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout59>) -> () loc(#loc1009)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc90)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc90)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc90)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout55>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc902)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>, tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc90)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc90)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc90)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>) -> tensor<128x512x1x1xbf16, #ttnn_layout71> loc(#loc724)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout72>) -> () loc(#loc724)
        %18 = "ttnn.from_device"(%17) : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> tensor<128x512x1x1xbf16, #ttnn_layout69> loc(#loc724)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout71>) -> () loc(#loc724)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> tensor<1x1x1x128xbf16, #ttnn_layout63> loc(#loc723)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc723)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> tensor<1x1x1x128xbf16, #ttnn_layout64> loc(#loc723)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout63>) -> () loc(#loc723)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x512>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>, !ttnn.device) -> tensor<1x1x512x128xbf16, #ttnn_layout70> loc(#loc725)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>) -> () loc(#loc725)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x512>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>, !ttnn.device) -> tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc726)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout64>) -> () loc(#loc726)
        return %21, %22 : tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_47(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x512x3x3xbf16, #ttnn_layout113> loc(unknown)) -> (tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc92)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc92)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc92)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc727)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc727)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc92)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc92)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc92)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc92)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc92)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc92)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc92)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc92)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc727)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>, !ttnn.device) -> tensor<512x512x3x3xbf16, #ttnn_layout115> loc(#loc728)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> tensor<512x512x3x3xbf16, #ttnn_layout116> loc(#loc728)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> () loc(#loc728)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x512x3x3xbf16, #ttnn_layout116> loc(#loc729)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>) -> () loc(#loc729)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc729)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1010)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1011)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1011)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc92)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc92)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc92)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc904)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc92)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc92)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc92)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>) -> tensor<512x512x3x3xbf16, #ttnn_layout115> loc(#loc731)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>) -> () loc(#loc731)
        %18 = "ttnn.from_device"(%17) : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> tensor<512x512x3x3xbf16, #ttnn_layout113> loc(#loc731)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> () loc(#loc731)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc730)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc730)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc730)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc730)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>, weights_format = "OIHW"}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>, !ttnn.device) -> tensor<1x1x4608x512xbf16, #ttnn_layout114> loc(#loc732)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>) -> () loc(#loc732)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc733)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc733)
        return %21, %22 : tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_48(%arg0: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg1: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg2: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg3: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg4: tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(unknown)) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc94)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc94)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc94)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc734)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc734)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc94)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc94)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc94)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc94)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc94)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc94)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc94)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc94)
        %8 = "ttnn.reshape"(%7) <{shape = [1024 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1024x1x1x1xbf16, #ttnn_layout32> loc(#loc734)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc735)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc735)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc735)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>, tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc736)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc736)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> () loc(#loc736)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc1012)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc1013)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc1013)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc94)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc94)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc94)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc906)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc94)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc94)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc94)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc738)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc738)
        %18 = "ttnn.from_device"(%17) : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(#loc738)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc738)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout35> loc(#loc737)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc737)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> tensor<1x1x1x1024xbf16, #ttnn_layout36> loc(#loc737)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> () loc(#loc737)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1x1x256x1024xbf16, #ttnn_layout29> loc(#loc739)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc739)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>, !ttnn.device) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc740)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>) -> () loc(#loc740)
        return %21, %22 : tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_49(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x512x3x3xbf16, #ttnn_layout113> loc(unknown)) -> (tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc96)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc96)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc96)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc741)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc741)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc96)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc96)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc96)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc96)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc96)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc96)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc96)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc96)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc741)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>, !ttnn.device) -> tensor<512x512x3x3xbf16, #ttnn_layout115> loc(#loc742)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> tensor<512x512x3x3xbf16, #ttnn_layout116> loc(#loc742)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> () loc(#loc742)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x512x3x3xbf16, #ttnn_layout116> loc(#loc743)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>) -> () loc(#loc743)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc743)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1014)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1015)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1015)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc96)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc96)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc96)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc908)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc96)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc96)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc96)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>) -> tensor<512x512x3x3xbf16, #ttnn_layout115> loc(#loc745)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>) -> () loc(#loc745)
        %18 = "ttnn.from_device"(%17) : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> tensor<512x512x3x3xbf16, #ttnn_layout113> loc(#loc745)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> () loc(#loc745)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc744)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc744)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc744)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc744)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>, !ttnn.device) -> tensor<1x1x4608x512xbf16, #ttnn_layout114> loc(#loc746)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>) -> () loc(#loc746)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc747)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc747)
        return %21, %22 : tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_50(%arg0: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg1: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg2: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg3: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg4: tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(unknown)) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc98)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc98)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc98)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc748)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc748)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc98)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc98)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc98)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc98)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc98)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc98)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc98)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc98)
        %8 = "ttnn.reshape"(%7) <{shape = [1024 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1024x1x1x1xbf16, #ttnn_layout32> loc(#loc748)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc749)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc749)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc749)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>, tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> tensor<1024x256x1x1xbf16, #ttnn_layout34> loc(#loc750)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc750)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> () loc(#loc750)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc1016)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc1017)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc1017)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc98)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc98)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc98)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc910)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc98)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc98)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc98)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> tensor<1024x256x1x1xbf16, #ttnn_layout33> loc(#loc752)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout34>) -> () loc(#loc752)
        %18 = "ttnn.from_device"(%17) : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> tensor<1024x256x1x1xbf16, #ttnn_layout28> loc(#loc752)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout33>) -> () loc(#loc752)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout35> loc(#loc751)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc751)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> tensor<1x1x1x1024xbf16, #ttnn_layout36> loc(#loc751)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> () loc(#loc751)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1x1x256x1024xbf16, #ttnn_layout29> loc(#loc753)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc753)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 256 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 14 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <224x32>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>, !ttnn.device) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc754)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>) -> () loc(#loc754)
        return %21, %22 : tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_51(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x128x1x1xbf16, #ttnn_layout23> loc(unknown)) -> (tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc100)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc100)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc100)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc755)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc755)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc100)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc100)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc100)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc100)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc100)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc100)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc100)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc100)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc755)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<512x128x1x1xbf16, #ttnn_layout25> loc(#loc756)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> tensor<512x128x1x1xbf16, #ttnn_layout26> loc(#loc756)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> () loc(#loc756)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x128x1x1xbf16, #ttnn_layout26> loc(#loc757)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> () loc(#loc757)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc757)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1018)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1019)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1019)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc100)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc100)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc100)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc912)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc100)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc100)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc100)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> tensor<512x128x1x1xbf16, #ttnn_layout25> loc(#loc759)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout26>) -> () loc(#loc759)
        %18 = "ttnn.from_device"(%17) : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> tensor<512x128x1x1xbf16, #ttnn_layout23> loc(#loc759)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout25>) -> () loc(#loc759)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc758)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc758)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc758)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc758)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x128x512xbf16, #ttnn_layout24> loc(#loc760)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>) -> () loc(#loc760)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 128 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <128x128>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc761)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc761)
        return %21, %22 : tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_52(%arg0: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg1: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg2: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg3: tensor<2048xbf16, #ttnn_layout84> loc(unknown), %arg4: tensor<2048x512x1x1xbf16, #ttnn_layout96> loc(unknown)) -> (tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc102)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc102)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc102)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc762)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc762)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc102)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc102)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc102)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc102)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc102)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>, tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x2048x1x1xbf16, #ttnn_layout88> loc(#loc102)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc102)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc102)
        %8 = "ttnn.reshape"(%7) <{shape = [2048 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<2048x1x1x1xbf16, #ttnn_layout89> loc(#loc762)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>, !ttnn.device) -> tensor<2048x512x1x1xbf16, #ttnn_layout98> loc(#loc763)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> tensor<2048x512x1x1xbf16, #ttnn_layout99> loc(#loc763)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> () loc(#loc763)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>, tensor<2048x1x1x1xbf16, #ttnn_layout89>) -> tensor<2048x512x1x1xbf16, #ttnn_layout99> loc(#loc764)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>) -> () loc(#loc764)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<2048x1x1x1xbf16, #ttnn_layout89>) -> () loc(#loc764)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc1020)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc1021)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout88>) -> () loc(#loc1021)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc102)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc102)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc102)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout84>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc914)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc102)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc102)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc102)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>) -> tensor<2048x512x1x1xbf16, #ttnn_layout98> loc(#loc766)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout99>) -> () loc(#loc766)
        %18 = "ttnn.from_device"(%17) : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> tensor<2048x512x1x1xbf16, #ttnn_layout96> loc(#loc766)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout98>) -> () loc(#loc766)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> tensor<1x1x1x2048xbf16, #ttnn_layout94> loc(#loc765)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc765)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x2048xbf16, #ttnn_layout94>) -> tensor<1x1x1x2048xbf16, #ttnn_layout95> loc(#loc765)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout94>) -> () loc(#loc765)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>, !ttnn.device) -> tensor<1x1x512x2048xbf16, #ttnn_layout97> loc(#loc767)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>) -> () loc(#loc767)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x2048xbf16, #ttnn_layout95>, !ttnn.device) -> tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc768)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout95>) -> () loc(#loc768)
        return %21, %22 : tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_53(%arg0: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg1: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg2: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg3: tensor<1024xbf16, #ttnn_layout27> loc(unknown), %arg4: tensor<1024x512x1x1xbf16, #ttnn_layout117> loc(unknown)) -> (tensor<1x1x512x1024xbf16, #ttnn_layout118>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc104)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc104)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc104)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc769)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc769)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc104)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc104)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc104)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc104)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc104)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>, tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1024x1x1xbf16, #ttnn_layout31> loc(#loc104)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc104)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc104)
        %8 = "ttnn.reshape"(%7) <{shape = [1024 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1024x1x1x1xbf16, #ttnn_layout32> loc(#loc769)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1024x512x1x1xbf16, #ttnn_layout117>, !ttnn.device) -> tensor<1024x512x1x1xbf16, #ttnn_layout119> loc(#loc770)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<1024x512x1x1xbf16, #ttnn_layout119>) -> tensor<1024x512x1x1xbf16, #ttnn_layout120> loc(#loc770)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout119>) -> () loc(#loc770)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1024x512x1x1xbf16, #ttnn_layout120>, tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> tensor<1024x512x1x1xbf16, #ttnn_layout120> loc(#loc771)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout120>) -> () loc(#loc771)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1024x1x1x1xbf16, #ttnn_layout32>) -> () loc(#loc771)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc1022)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc1023)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc1023)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc104)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc104)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc104)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16, #ttnn_layout27>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc916)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc104)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc104)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc104)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<1024x512x1x1xbf16, #ttnn_layout120>) -> tensor<1024x512x1x1xbf16, #ttnn_layout119> loc(#loc773)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout120>) -> () loc(#loc773)
        %18 = "ttnn.from_device"(%17) : (tensor<1024x512x1x1xbf16, #ttnn_layout119>) -> tensor<1024x512x1x1xbf16, #ttnn_layout117> loc(#loc773)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout119>) -> () loc(#loc773)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> tensor<1x1x1x1024xbf16, #ttnn_layout35> loc(#loc772)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc772)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> tensor<1x1x1x1024xbf16, #ttnn_layout36> loc(#loc772)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout35>) -> () loc(#loc772)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <896x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>, weights_format = "OIHW"}> : (tensor<1024x512x1x1xbf16, #ttnn_layout117>, !ttnn.device) -> tensor<1x1x512x1024xbf16, #ttnn_layout118> loc(#loc774)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout117>) -> () loc(#loc774)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 28 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <896x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>, !ttnn.device) -> tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc775)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout36>) -> () loc(#loc775)
        return %21, %22 : tensor<1x1x512x1024xbf16, #ttnn_layout118>, tensor<1x1x1x1024xbf16, #ttnn_layout30> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_54(%arg0: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<512xbf16, #ttnn_layout1> loc(unknown), %arg4: tensor<512x512x3x3xbf16, #ttnn_layout113> loc(unknown)) -> (tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4>) attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc106)
        %2 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc106)
        %3 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 9.99999974E-6 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xbf16, #ttnn_layout6> loc(#loc106)
        %4 = "ttnn.reshape"(%3) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xbf16, #ttnn_layout6>) -> tensor<1x1x1x1xbf16, #ttnn_layout7> loc(#loc776)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xbf16, #ttnn_layout6>) -> () loc(#loc776)
        %5 = "ttnn.add"(%1, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x1x1x1xbf16, #ttnn_layout7>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc106)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn_layout7>) -> () loc(#loc106)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc106)
        %6 = "ttnn.sqrt"(%5) : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc106)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc106)
        %7 = "ttnn.divide"(%2, %6) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>, tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x512x1x1xbf16, #ttnn_layout5> loc(#loc106)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc106)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc106)
        %8 = "ttnn.reshape"(%7) <{shape = [512 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<512x1x1x1xbf16, #ttnn_layout8> loc(#loc776)
        %9 = "ttnn.to_device"(%arg4, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>, !ttnn.device) -> tensor<512x512x3x3xbf16, #ttnn_layout115> loc(#loc777)
        %10 = "ttnn.to_layout"(%9) <{layout = #ttnn.layout<tile>}> : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> tensor<512x512x3x3xbf16, #ttnn_layout116> loc(#loc777)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> () loc(#loc777)
        %11 = "ttnn.multiply"(%10, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>, tensor<512x1x1x1xbf16, #ttnn_layout8>) -> tensor<512x512x3x3xbf16, #ttnn_layout116> loc(#loc778)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>) -> () loc(#loc778)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x1x1x1xbf16, #ttnn_layout8>) -> () loc(#loc778)
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1024)
        %13 = "ttnn.reshape"(%7) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc1025)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1025)
        %14 = "ttnn.multiply"(%12, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc106)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc106)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc106)
        %15 = "ttnn.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout1>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc918)
        %16 = "ttnn.subtract"(%15, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>, tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc106)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc106)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc106)
        %17 = "ttnn.to_layout"(%11) <{layout = #ttnn.layout<row_major>}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>) -> tensor<512x512x3x3xbf16, #ttnn_layout115> loc(#loc780)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout116>) -> () loc(#loc780)
        %18 = "ttnn.from_device"(%17) : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> tensor<512x512x3x3xbf16, #ttnn_layout113> loc(#loc780)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout115>) -> () loc(#loc780)
        %19 = "ttnn.to_layout"(%16) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> tensor<1x1x1x512xbf16, #ttnn_layout11> loc(#loc779)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc779)
        %20 = "ttnn.from_device"(%19) : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> tensor<1x1x1x512xbf16, #ttnn_layout12> loc(#loc779)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout11>) -> () loc(#loc779)
        %21 = "ttnn.prepare_conv2d_weights"(%18, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = true, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>, weights_format = "OIHW"}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>, !ttnn.device) -> tensor<1x1x4608x512xbf16, #ttnn_layout114> loc(#loc781)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>) -> () loc(#loc781)
        %22 = "ttnn.prepare_conv2d_bias"(%20, %0) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 512 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 7 : i32, input_memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <64x64>, <row_major>>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc782)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout12>) -> () loc(#loc782)
        return %21, %22 : tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<1000xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___classifier_1_bias"} loc("p0.3"), %arg1: tensor<1000x2048xbf16, #ttnn_layout121> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___classifier_1_weight"} loc("p1.10"), %arg2: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_running_var"} loc("p2.18"), %arg3: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_running_mean"} loc("p3.22"), %arg4: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_bias"} loc("p4.26"), %arg5: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_weight"} loc("p5.30"), %arg6: tensor<2048x1024x1x1xbf16, #ttnn_layout85> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_convolution_weight"} loc("p6.34"), %arg7: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_running_var"} loc("p7.42"), %arg8: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_running_mean"} loc("p8.46"), %arg9: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_bias"} loc("p9.50"), %arg10: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_weight"} loc("p10.54"), %arg11: tensor<1024x512x1x1xbf16, #ttnn_layout117> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_convolution_weight"} loc("p11.58"), %arg12: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_running_var"} loc("p12.64"), %arg13: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_running_mean"} loc("p13.68"), %arg14: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_bias"} loc("p14.72"), %arg15: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_weight"} loc("p15.76"), %arg16: tensor<512x256x1x1xbf16, #ttnn_layout108> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_convolution_weight"} loc("p16.80"), %arg17: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_normalization_running_var"} loc("p17.85"), %arg18: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_normalization_running_mean"} loc("p18.89"), %arg19: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_normalization_bias"} loc("p19.93"), %arg20: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_normalization_weight"} loc("p20.97"), %arg21: tensor<256x64x1x1xbf16, #ttnn_layout65> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_convolution_weight"} loc("p21.101"), %arg22: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_running_var"} loc("p22.103"), %arg23: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_running_mean"} loc("p23.107"), %arg24: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_bias"} loc("p24.111"), %arg25: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_weight"} loc("p25.115"), %arg26: tensor<64x3x7x7xbf16, #ttnn_layout100> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___resnet_embedder_embedder_convolution_weight"} loc("p26.119"), %arg27: tensor<8x3x224x224xbf16, #ttnn_layout122> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("p27.121"), %arg28: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___normalization_running_var"} loc("p28.310"), %arg29: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___normalization_running_mean"} loc("p29.314"), %arg30: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___normalization_bias"} loc("p30.318"), %arg31: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___normalization_weight"} loc("p31.322"), %arg32: tensor<256x64x1x1xbf16, #ttnn_layout65> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___convolution_weight"} loc("p32.326"), %arg33: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_running_var"} loc("p33.328"), %arg34: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_running_mean"} loc("p34.332"), %arg35: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_bias"} loc("p35.336"), %arg36: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_weight"} loc("p36.340"), %arg37: tensor<64x64x3x3xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___convolution_weight"} loc("p37.344"), %arg38: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_running_var"} loc("p38.346"), %arg39: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_running_mean"} loc("p39.350"), %arg40: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_bias"} loc("p40.354"), %arg41: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_weight"} loc("p41.358"), %arg42: tensor<64x64x1x1xbf16, #ttnn_layout77> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___convolution_weight"} loc("p42.362"), %arg43: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___normalization_running_var"} loc("p43.394"), %arg44: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___normalization_running_mean"} loc("p44.398"), %arg45: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___normalization_bias"} loc("p45.402"), %arg46: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___normalization_weight"} loc("p46.406"), %arg47: tensor<256x64x1x1xbf16, #ttnn_layout65> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___convolution_weight"} loc("p47.410"), %arg48: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_running_var"} loc("p48.412"), %arg49: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_running_mean"} loc("p49.416"), %arg50: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_bias"} loc("p50.420"), %arg51: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_weight"} loc("p51.424"), %arg52: tensor<64x64x3x3xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___convolution_weight"} loc("p52.428"), %arg53: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_running_var"} loc("p53.430"), %arg54: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_running_mean"} loc("p54.434"), %arg55: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_bias"} loc("p55.438"), %arg56: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_weight"} loc("p56.442"), %arg57: tensor<64x256x1x1xbf16, #ttnn_layout37> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___convolution_weight"} loc("p57.446"), %arg58: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___normalization_running_var"} loc("p58.478"), %arg59: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___normalization_running_mean"} loc("p59.482"), %arg60: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___normalization_bias"} loc("p60.486"), %arg61: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___normalization_weight"} loc("p61.490"), %arg62: tensor<256x64x1x1xbf16, #ttnn_layout65> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___convolution_weight"} loc("p62.494"), %arg63: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___normalization_running_var"} loc("p63.496"), %arg64: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___normalization_running_mean"} loc("p64.500"), %arg65: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___normalization_bias"} loc("p65.504"), %arg66: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___normalization_weight"} loc("p66.508"), %arg67: tensor<64x64x3x3xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___convolution_weight"} loc("p67.512"), %arg68: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___normalization_running_var"} loc("p68.514"), %arg69: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___normalization_running_mean"} loc("p69.518"), %arg70: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___normalization_bias"} loc("p70.522"), %arg71: tensor<64xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___normalization_weight"} loc("p71.526"), %arg72: tensor<64x256x1x1xbf16, #ttnn_layout37> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___convolution_weight"} loc("p72.530"), %arg73: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___normalization_running_var"} loc("p73.568"), %arg74: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___normalization_running_mean"} loc("p74.572"), %arg75: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___normalization_bias"} loc("p75.576"), %arg76: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___normalization_weight"} loc("p76.580"), %arg77: tensor<512x128x1x1xbf16, #ttnn_layout23> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___convolution_weight"} loc("p77.584"), %arg78: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_running_var"} loc("p78.586"), %arg79: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_running_mean"} loc("p79.590"), %arg80: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_bias"} loc("p80.594"), %arg81: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_weight"} loc("p81.598"), %arg82: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___convolution_weight"} loc("p82.602"), %arg83: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_running_var"} loc("p83.604"), %arg84: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_running_mean"} loc("p84.608"), %arg85: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_bias"} loc("p85.612"), %arg86: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_weight"} loc("p86.616"), %arg87: tensor<128x256x1x1xbf16, #ttnn_layout73> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___convolution_weight"} loc("p87.620"), %arg88: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___normalization_running_var"} loc("p88.652"), %arg89: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___normalization_running_mean"} loc("p89.656"), %arg90: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___normalization_bias"} loc("p90.660"), %arg91: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___normalization_weight"} loc("p91.664"), %arg92: tensor<512x128x1x1xbf16, #ttnn_layout23> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___convolution_weight"} loc("p92.668"), %arg93: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_running_var"} loc("p93.670"), %arg94: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_running_mean"} loc("p94.674"), %arg95: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_bias"} loc("p95.678"), %arg96: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_weight"} loc("p96.682"), %arg97: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___convolution_weight"} loc("p97.686"), %arg98: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_running_var"} loc("p98.688"), %arg99: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_running_mean"} loc("p99.692"), %arg100: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_bias"} loc("p100.696"), %arg101: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_weight"} loc("p101.700"), %arg102: tensor<128x512x1x1xbf16, #ttnn_layout69> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___convolution_weight"} loc("p102.704"), %arg103: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___normalization_running_var"} loc("p103.736"), %arg104: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___normalization_running_mean"} loc("p104.740"), %arg105: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___normalization_bias"} loc("p105.744"), %arg106: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___normalization_weight"} loc("p106.748"), %arg107: tensor<512x128x1x1xbf16, #ttnn_layout23> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___convolution_weight"} loc("p107.752"), %arg108: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___normalization_running_var"} loc("p108.754"), %arg109: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___normalization_running_mean"} loc("p109.758"), %arg110: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___normalization_bias"} loc("p110.762"), %arg111: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___normalization_weight"} loc("p111.766"), %arg112: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___convolution_weight"} loc("p112.770"), %arg113: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___normalization_running_var"} loc("p113.772"), %arg114: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___normalization_running_mean"} loc("p114.776"), %arg115: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___normalization_bias"} loc("p115.780"), %arg116: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___normalization_weight"} loc("p116.784"), %arg117: tensor<128x512x1x1xbf16, #ttnn_layout69> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___convolution_weight"} loc("p117.788"), %arg118: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___normalization_running_var"} loc("p118.820"), %arg119: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___normalization_running_mean"} loc("p119.824"), %arg120: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___normalization_bias"} loc("p120.828"), %arg121: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___normalization_weight"} loc("p121.832"), %arg122: tensor<512x128x1x1xbf16, #ttnn_layout23> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___convolution_weight"} loc("p122.836"), %arg123: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___normalization_running_var"} loc("p123.838"), %arg124: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___normalization_running_mean"} loc("p124.842"), %arg125: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___normalization_bias"} loc("p125.846"), %arg126: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___normalization_weight"} loc("p126.850"), %arg127: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___convolution_weight"} loc("p127.854"), %arg128: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___normalization_running_var"} loc("p128.856"), %arg129: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___normalization_running_mean"} loc("p129.860"), %arg130: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___normalization_bias"} loc("p130.864"), %arg131: tensor<128xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___normalization_weight"} loc("p131.868"), %arg132: tensor<128x512x1x1xbf16, #ttnn_layout69> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___convolution_weight"} loc("p132.872"), %arg133: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___normalization_running_var"} loc("p133.910"), %arg134: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___normalization_running_mean"} loc("p134.914"), %arg135: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___normalization_bias"} loc("p135.918"), %arg136: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___normalization_weight"} loc("p136.922"), %arg137: tensor<1024x256x1x1xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___convolution_weight"} loc("p137.926"), %arg138: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_running_var"} loc("p138.928"), %arg139: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_running_mean"} loc("p139.932"), %arg140: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_bias"} loc("p140.936"), %arg141: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_weight"} loc("p141.940"), %arg142: tensor<256x256x3x3xbf16, #ttnn_layout46> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___convolution_weight"} loc("p142.944"), %arg143: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_running_var"} loc("p143.946"), %arg144: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_running_mean"} loc("p144.950"), %arg145: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_bias"} loc("p145.954"), %arg146: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_weight"} loc("p146.958"), %arg147: tensor<256x512x1x1xbf16, #ttnn_layout104> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___convolution_weight"} loc("p147.962"), %arg148: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___normalization_running_var"} loc("p148.994"), %arg149: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___normalization_running_mean"} loc("p149.998"), %arg150: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___normalization_bias"} loc("p150.1002"), %arg151: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___normalization_weight"} loc("p151.1006"), %arg152: tensor<1024x256x1x1xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___convolution_weight"} loc("p152.1010"), %arg153: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_running_var"} loc("p153.1012"), %arg154: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_running_mean"} loc("p154.1016"), %arg155: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_bias"} loc("p155.1020"), %arg156: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_weight"} loc("p156.1024"), %arg157: tensor<256x256x3x3xbf16, #ttnn_layout46> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___convolution_weight"} loc("p157.1028"), %arg158: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_running_var"} loc("p158.1030"), %arg159: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_running_mean"} loc("p159.1034"), %arg160: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_bias"} loc("p160.1038"), %arg161: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_weight"} loc("p161.1042"), %arg162: tensor<256x1024x1x1xbf16, #ttnn_layout80> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___convolution_weight"} loc("p162.1046"), %arg163: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___normalization_running_var"} loc("p163.1078"), %arg164: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___normalization_running_mean"} loc("p164.1082"), %arg165: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___normalization_bias"} loc("p165.1086"), %arg166: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___normalization_weight"} loc("p166.1090"), %arg167: tensor<1024x256x1x1xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___convolution_weight"} loc("p167.1094"), %arg168: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___normalization_running_var"} loc("p168.1096"), %arg169: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___normalization_running_mean"} loc("p169.1100"), %arg170: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___normalization_bias"} loc("p170.1104"), %arg171: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___normalization_weight"} loc("p171.1108"), %arg172: tensor<256x256x3x3xbf16, #ttnn_layout46> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___convolution_weight"} loc("p172.1112"), %arg173: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___normalization_running_var"} loc("p173.1114"), %arg174: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___normalization_running_mean"} loc("p174.1118"), %arg175: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___normalization_bias"} loc("p175.1122"), %arg176: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___normalization_weight"} loc("p176.1126"), %arg177: tensor<256x1024x1x1xbf16, #ttnn_layout80> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___convolution_weight"} loc("p177.1130"), %arg178: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___normalization_running_var"} loc("p178.1162"), %arg179: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___normalization_running_mean"} loc("p179.1166"), %arg180: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___normalization_bias"} loc("p180.1170"), %arg181: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___normalization_weight"} loc("p181.1174"), %arg182: tensor<1024x256x1x1xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___convolution_weight"} loc("p182.1178"), %arg183: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___normalization_running_var"} loc("p183.1180"), %arg184: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___normalization_running_mean"} loc("p184.1184"), %arg185: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___normalization_bias"} loc("p185.1188"), %arg186: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___normalization_weight"} loc("p186.1192"), %arg187: tensor<256x256x3x3xbf16, #ttnn_layout46> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___convolution_weight"} loc("p187.1196"), %arg188: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___normalization_running_var"} loc("p188.1198"), %arg189: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___normalization_running_mean"} loc("p189.1202"), %arg190: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___normalization_bias"} loc("p190.1206"), %arg191: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___normalization_weight"} loc("p191.1210"), %arg192: tensor<256x1024x1x1xbf16, #ttnn_layout80> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___convolution_weight"} loc("p192.1214"), %arg193: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___normalization_running_var"} loc("p193.1246"), %arg194: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___normalization_running_mean"} loc("p194.1250"), %arg195: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___normalization_bias"} loc("p195.1254"), %arg196: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___normalization_weight"} loc("p196.1258"), %arg197: tensor<1024x256x1x1xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___convolution_weight"} loc("p197.1262"), %arg198: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___normalization_running_var"} loc("p198.1264"), %arg199: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___normalization_running_mean"} loc("p199.1268"), %arg200: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___normalization_bias"} loc("p200.1272"), %arg201: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___normalization_weight"} loc("p201.1276"), %arg202: tensor<256x256x3x3xbf16, #ttnn_layout46> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___convolution_weight"} loc("p202.1280"), %arg203: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___normalization_running_var"} loc("p203.1282"), %arg204: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___normalization_running_mean"} loc("p204.1286"), %arg205: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___normalization_bias"} loc("p205.1290"), %arg206: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___normalization_weight"} loc("p206.1294"), %arg207: tensor<256x1024x1x1xbf16, #ttnn_layout80> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___convolution_weight"} loc("p207.1298"), %arg208: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___normalization_running_var"} loc("p208.1330"), %arg209: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___normalization_running_mean"} loc("p209.1334"), %arg210: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___normalization_bias"} loc("p210.1338"), %arg211: tensor<1024xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___normalization_weight"} loc("p211.1342"), %arg212: tensor<1024x256x1x1xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___convolution_weight"} loc("p212.1346"), %arg213: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___normalization_running_var"} loc("p213.1348"), %arg214: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___normalization_running_mean"} loc("p214.1352"), %arg215: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___normalization_bias"} loc("p215.1356"), %arg216: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___normalization_weight"} loc("p216.1360"), %arg217: tensor<256x256x3x3xbf16, #ttnn_layout46> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___convolution_weight"} loc("p217.1364"), %arg218: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___normalization_running_var"} loc("p218.1366"), %arg219: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___normalization_running_mean"} loc("p219.1370"), %arg220: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___normalization_bias"} loc("p220.1374"), %arg221: tensor<256xbf16, #ttnn_layout45> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___normalization_weight"} loc("p221.1378"), %arg222: tensor<256x1024x1x1xbf16, #ttnn_layout80> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___convolution_weight"} loc("p222.1382"), %arg223: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___normalization_running_var"} loc("p223.1420"), %arg224: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___normalization_running_mean"} loc("p224.1424"), %arg225: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___normalization_bias"} loc("p225.1428"), %arg226: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___normalization_weight"} loc("p226.1432"), %arg227: tensor<2048x512x1x1xbf16, #ttnn_layout96> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___convolution_weight"} loc("p227.1436"), %arg228: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_running_var"} loc("p228.1438"), %arg229: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_running_mean"} loc("p229.1442"), %arg230: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_bias"} loc("p230.1446"), %arg231: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_weight"} loc("p231.1450"), %arg232: tensor<512x512x3x3xbf16, #ttnn_layout113> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___convolution_weight"} loc("p232.1454"), %arg233: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_running_var"} loc("p233.1456"), %arg234: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_running_mean"} loc("p234.1460"), %arg235: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_bias"} loc("p235.1464"), %arg236: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_weight"} loc("p236.1468"), %arg237: tensor<512x1024x1x1xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___convolution_weight"} loc("p237.1472"), %arg238: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___normalization_running_var"} loc("p238.1504"), %arg239: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___normalization_running_mean"} loc("p239.1508"), %arg240: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___normalization_bias"} loc("p240.1512"), %arg241: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___normalization_weight"} loc("p241.1516"), %arg242: tensor<2048x512x1x1xbf16, #ttnn_layout96> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___convolution_weight"} loc("p242.1520"), %arg243: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_running_var"} loc("p243.1522"), %arg244: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_running_mean"} loc("p244.1526"), %arg245: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_bias"} loc("p245.1530"), %arg246: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_weight"} loc("p246.1534"), %arg247: tensor<512x512x3x3xbf16, #ttnn_layout113> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___convolution_weight"} loc("p247.1538"), %arg248: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_running_var"} loc("p248.1540"), %arg249: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_running_mean"} loc("p249.1544"), %arg250: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_bias"} loc("p250.1548"), %arg251: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_weight"} loc("p251.1552"), %arg252: tensor<512x2048x1x1xbf16, #ttnn_layout41> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___convolution_weight"} loc("p252.1556"), %arg253: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___normalization_running_var"} loc("p253.1588"), %arg254: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___normalization_running_mean"} loc("p254.1592"), %arg255: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___normalization_bias"} loc("p255.1596"), %arg256: tensor<2048xbf16, #ttnn_layout84> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___normalization_weight"} loc("p256.1600"), %arg257: tensor<2048x512x1x1xbf16, #ttnn_layout96> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___convolution_weight"} loc("p257.1604"), %arg258: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___normalization_running_var"} loc("p258.1606"), %arg259: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___normalization_running_mean"} loc("p259.1610"), %arg260: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___normalization_bias"} loc("p260.1614"), %arg261: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___normalization_weight"} loc("p261.1618"), %arg262: tensor<512x512x3x3xbf16, #ttnn_layout113> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___convolution_weight"} loc("p262.1622"), %arg263: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___normalization_running_var"} loc("p263.1624"), %arg264: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___normalization_running_mean"} loc("p264.1628"), %arg265: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___normalization_bias"} loc("p265.1632"), %arg266: tensor<512xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___normalization_weight"} loc("p266.1636"), %arg267: tensor<512x2048x1x1xbf16, #ttnn_layout41> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___convolution_weight"} loc("p267.1640")) -> (tensor<8x1000xbf16, #ttnn_layout112> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<8x2048xbf16, #ttnn_layout> loc(#loc)
        %1:2 = ttcore.load_cached(@main_const_eval_1, [%arg233, %arg234, %arg235, %arg236, %arg237]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x1024x1x1xbf16, #ttnn_layout2>) -> (tensor<1x1x1024x512xbf16, #ttnn_layout3>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg237) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout2>) -> () loc(#loc)
        "ttnn.deallocate"(%arg236) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg235) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg234) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg233) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %2:2 = ttcore.load_cached(@main_const_eval_2, [%arg48, %arg49, %arg50, %arg51, %arg52]) : (tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64x64x3x3xbf16, #ttnn_layout14>) -> (tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16>) loc(#loc)
        "ttnn.deallocate"(%arg52) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>) -> () loc(#loc)
        "ttnn.deallocate"(%arg51) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg50) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg49) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg48) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3:2 = ttcore.load_cached(@main_const_eval_3, [%arg103, %arg104, %arg105, %arg106, %arg107]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x128x1x1xbf16, #ttnn_layout23>) -> (tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg107) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>) -> () loc(#loc)
        "ttnn.deallocate"(%arg106) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg105) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg104) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg103) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %4:2 = ttcore.load_cached(@main_const_eval_4, [%arg148, %arg149, %arg150, %arg151, %arg152]) : (tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) loc(#loc)
        "ttnn.deallocate"(%arg152) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc)
        "ttnn.deallocate"(%arg151) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg150) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg149) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg148) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        %5:2 = ttcore.load_cached(@main_const_eval_5, [%arg53, %arg54, %arg55, %arg56, %arg57]) : (tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64x256x1x1xbf16, #ttnn_layout37>) -> (tensor<1x1x256x64xbf16, #ttnn_layout38>, tensor<1x1x1x64xbf16, #ttnn_layout16>) loc(#loc)
        "ttnn.deallocate"(%arg57) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout37>) -> () loc(#loc)
        "ttnn.deallocate"(%arg56) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg55) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg54) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg53) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        %6:2 = ttcore.load_cached(@main_const_eval_6, [%arg248, %arg249, %arg250, %arg251, %arg252]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x2048x1x1xbf16, #ttnn_layout41>) -> (tensor<1x1x2048x512xbf16, #ttnn_layout42>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg252) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout41>) -> () loc(#loc)
        "ttnn.deallocate"(%arg251) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg250) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg249) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg248) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %7:2 = ttcore.load_cached(@main_const_eval_7, [%arg138, %arg139, %arg140, %arg141, %arg142]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x256x3x3xbf16, #ttnn_layout46>) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg142) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc)
        "ttnn.deallocate"(%arg141) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg140) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg139) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg138) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %8:2 = ttcore.load_cached(@main_const_eval_8, [%arg123, %arg124, %arg125, %arg126, %arg127]) : (tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128x128x3x3xbf16, #ttnn_layout56>) -> (tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>) loc(#loc)
        "ttnn.deallocate"(%arg127) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc)
        "ttnn.deallocate"(%arg126) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg125) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg124) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg123) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        %9:2 = ttcore.load_cached(@main_const_eval_9, [%arg43, %arg44, %arg45, %arg46, %arg47]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x64x1x1xbf16, #ttnn_layout65>) -> (tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg47) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>) -> () loc(#loc)
        "ttnn.deallocate"(%arg46) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg45) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg44) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg43) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %10:2 = ttcore.load_cached(@main_const_eval_10, [%arg213, %arg214, %arg215, %arg216, %arg217]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x256x3x3xbf16, #ttnn_layout46>) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg217) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc)
        "ttnn.deallocate"(%arg216) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg215) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg214) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg213) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %11:2 = ttcore.load_cached(@main_const_eval_11, [%arg198, %arg199, %arg200, %arg201, %arg202]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x256x3x3xbf16, #ttnn_layout46>) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg202) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc)
        "ttnn.deallocate"(%arg201) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg200) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg199) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg198) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %12:2 = ttcore.load_cached(@main_const_eval_12, [%arg113, %arg114, %arg115, %arg116, %arg117]) : (tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128x512x1x1xbf16, #ttnn_layout69>) -> (tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58>) loc(#loc)
        "ttnn.deallocate"(%arg117) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>) -> () loc(#loc)
        "ttnn.deallocate"(%arg116) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg115) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg114) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg113) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        %13:2 = ttcore.load_cached(@main_const_eval_13, [%arg83, %arg84, %arg85, %arg86, %arg87]) : (tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128x256x1x1xbf16, #ttnn_layout73>) -> (tensor<1x1x256x128xbf16, #ttnn_layout74>, tensor<1x1x1x128xbf16, #ttnn_layout58>) loc(#loc)
        "ttnn.deallocate"(%arg87) <{force = false}> : (tensor<128x256x1x1xbf16, #ttnn_layout73>) -> () loc(#loc)
        "ttnn.deallocate"(%arg86) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg85) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg84) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg83) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        %14:2 = ttcore.load_cached(@main_const_eval_14, [%arg63, %arg64, %arg65, %arg66, %arg67]) : (tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64x64x3x3xbf16, #ttnn_layout14>) -> (tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16>) loc(#loc)
        "ttnn.deallocate"(%arg67) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>) -> () loc(#loc)
        "ttnn.deallocate"(%arg66) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg65) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg64) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg63) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        %15:2 = ttcore.load_cached(@main_const_eval_15, [%arg73, %arg74, %arg75, %arg76, %arg77]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x128x1x1xbf16, #ttnn_layout23>) -> (tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg77) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>) -> () loc(#loc)
        "ttnn.deallocate"(%arg76) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg75) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg74) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg73) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %16:2 = ttcore.load_cached(@main_const_eval_16, [%arg38, %arg39, %arg40, %arg41, %arg42]) : (tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64x64x1x1xbf16, #ttnn_layout77>) -> (tensor<1x1x64x64xbf16, #ttnn_layout78>, tensor<1x1x1x64xbf16, #ttnn_layout16>) loc(#loc)
        "ttnn.deallocate"(%arg42) <{force = false}> : (tensor<64x64x1x1xbf16, #ttnn_layout77>) -> () loc(#loc)
        "ttnn.deallocate"(%arg41) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg40) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg39) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg38) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        %17:2 = ttcore.load_cached(@main_const_eval_17, [%arg17, %arg18, %arg19, %arg20, %arg21]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x64x1x1xbf16, #ttnn_layout65>) -> (tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg21) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>) -> () loc(#loc)
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %18:2 = ttcore.load_cached(@main_const_eval_18, [%arg263, %arg264, %arg265, %arg266, %arg267]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x2048x1x1xbf16, #ttnn_layout41>) -> (tensor<1x1x2048x512xbf16, #ttnn_layout42>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg267) <{force = false}> : (tensor<512x2048x1x1xbf16, #ttnn_layout41>) -> () loc(#loc)
        "ttnn.deallocate"(%arg266) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg265) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg264) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg263) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %19:2 = ttcore.load_cached(@main_const_eval_19, [%arg158, %arg159, %arg160, %arg161, %arg162]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg162) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc)
        "ttnn.deallocate"(%arg161) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg160) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg159) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg158) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %20:2 = ttcore.load_cached(@main_const_eval_20, [%arg98, %arg99, %arg100, %arg101, %arg102]) : (tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128x512x1x1xbf16, #ttnn_layout69>) -> (tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58>) loc(#loc)
        "ttnn.deallocate"(%arg102) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>) -> () loc(#loc)
        "ttnn.deallocate"(%arg101) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg100) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg99) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg98) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        %21:2 = ttcore.load_cached(@main_const_eval_21, [%arg118, %arg119, %arg120, %arg121, %arg122]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x128x1x1xbf16, #ttnn_layout23>) -> (tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg122) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>) -> () loc(#loc)
        "ttnn.deallocate"(%arg121) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg120) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg119) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg118) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %22:2 = ttcore.load_cached(@main_const_eval_22, [%arg68, %arg69, %arg70, %arg71, %arg72]) : (tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64x256x1x1xbf16, #ttnn_layout37>) -> (tensor<1x1x256x64xbf16, #ttnn_layout38>, tensor<1x1x1x64xbf16, #ttnn_layout16>) loc(#loc)
        "ttnn.deallocate"(%arg72) <{force = false}> : (tensor<64x256x1x1xbf16, #ttnn_layout37>) -> () loc(#loc)
        "ttnn.deallocate"(%arg71) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg70) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg69) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg68) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        %23:2 = ttcore.load_cached(@main_const_eval_23, [%arg2, %arg3, %arg4, %arg5, %arg6]) : (tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048x1024x1x1xbf16, #ttnn_layout85>) -> (tensor<1x1x1024x2048xbf16, #ttnn_layout86>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) loc(#loc)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout85>) -> () loc(#loc)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        %24:2 = ttcore.load_cached(@main_const_eval_24, [%arg173, %arg174, %arg175, %arg176, %arg177]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg177) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc)
        "ttnn.deallocate"(%arg176) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg175) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg174) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg173) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %25:2 = ttcore.load_cached(@main_const_eval_25, [%arg78, %arg79, %arg80, %arg81, %arg82]) : (tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128x128x3x3xbf16, #ttnn_layout56>) -> (tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>) loc(#loc)
        "ttnn.deallocate"(%arg82) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc)
        "ttnn.deallocate"(%arg81) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg80) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg79) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg78) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        %26:2 = ttcore.load_cached(@main_const_eval_26, [%arg253, %arg254, %arg255, %arg256, %arg257]) : (tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048x512x1x1xbf16, #ttnn_layout96>) -> (tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) loc(#loc)
        "ttnn.deallocate"(%arg257) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>) -> () loc(#loc)
        "ttnn.deallocate"(%arg256) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg255) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg254) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg253) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        %27:2 = ttcore.load_cached(@main_const_eval_27, [%arg183, %arg184, %arg185, %arg186, %arg187]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x256x3x3xbf16, #ttnn_layout46>) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg187) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc)
        "ttnn.deallocate"(%arg186) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg185) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg184) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg183) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %28:2 = ttcore.load_cached(@main_const_eval_28, [%arg208, %arg209, %arg210, %arg211, %arg212]) : (tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) loc(#loc)
        "ttnn.deallocate"(%arg212) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc)
        "ttnn.deallocate"(%arg211) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg210) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg209) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg208) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        %29:2 = ttcore.load_cached(@main_const_eval_29, [%arg58, %arg59, %arg60, %arg61, %arg62]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x64x1x1xbf16, #ttnn_layout65>) -> (tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg62) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>) -> () loc(#loc)
        "ttnn.deallocate"(%arg61) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg60) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg59) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg58) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %30:2 = ttcore.load_cached(@main_const_eval_30, [%arg22, %arg23, %arg24, %arg25, %arg26]) : (tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64x3x7x7xbf16, #ttnn_layout100>) -> (tensor<1x1x147x64xbf16, #ttnn_layout101>, tensor<1x1x1x64xbf16, #ttnn_layout16>) loc(#loc)
        "ttnn.deallocate"(%arg26) <{force = false}> : (tensor<64x3x7x7xbf16, #ttnn_layout100>) -> () loc(#loc)
        "ttnn.deallocate"(%arg25) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg24) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg23) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg22) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        %31:2 = ttcore.load_cached(@main_const_eval_31, [%arg223, %arg224, %arg225, %arg226, %arg227]) : (tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048x512x1x1xbf16, #ttnn_layout96>) -> (tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) loc(#loc)
        "ttnn.deallocate"(%arg227) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>) -> () loc(#loc)
        "ttnn.deallocate"(%arg226) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg225) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg224) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg223) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        %32:2 = ttcore.load_cached(@main_const_eval_32, [%arg33, %arg34, %arg35, %arg36, %arg37]) : (tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64xbf16, #ttnn_layout13>, tensor<64x64x3x3xbf16, #ttnn_layout14>) -> (tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16>) loc(#loc)
        "ttnn.deallocate"(%arg37) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout14>) -> () loc(#loc)
        "ttnn.deallocate"(%arg36) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg35) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg34) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        "ttnn.deallocate"(%arg33) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc)
        %33:2 = ttcore.load_cached(@main_const_eval_33, [%arg168, %arg169, %arg170, %arg171, %arg172]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x256x3x3xbf16, #ttnn_layout46>) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg172) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc)
        "ttnn.deallocate"(%arg171) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg170) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg169) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg168) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %34:2 = ttcore.load_cached(@main_const_eval_34, [%arg178, %arg179, %arg180, %arg181, %arg182]) : (tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) loc(#loc)
        "ttnn.deallocate"(%arg182) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc)
        "ttnn.deallocate"(%arg181) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg180) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg179) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg178) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        %35:2 = ttcore.load_cached(@main_const_eval_35, [%arg203, %arg204, %arg205, %arg206, %arg207]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg207) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc)
        "ttnn.deallocate"(%arg206) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg205) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg204) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg203) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %36:2 = ttcore.load_cached(@main_const_eval_36, [%arg93, %arg94, %arg95, %arg96, %arg97]) : (tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128x128x3x3xbf16, #ttnn_layout56>) -> (tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>) loc(#loc)
        "ttnn.deallocate"(%arg97) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc)
        "ttnn.deallocate"(%arg96) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg95) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg94) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg93) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        %37:2 = ttcore.load_cached(@main_const_eval_37, [%arg188, %arg189, %arg190, %arg191, %arg192]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg192) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc)
        "ttnn.deallocate"(%arg191) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg190) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg189) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg188) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %38:2 = ttcore.load_cached(@main_const_eval_38, [%arg143, %arg144, %arg145, %arg146, %arg147]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x512x1x1xbf16, #ttnn_layout104>) -> (tensor<1x1x512x256xbf16, #ttnn_layout105>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg147) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout104>) -> () loc(#loc)
        "ttnn.deallocate"(%arg146) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg145) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg144) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg143) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %39:2 = ttcore.load_cached(@main_const_eval_39, [%arg12, %arg13, %arg14, %arg15, %arg16]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x256x1x1xbf16, #ttnn_layout108>) -> (tensor<1x1x256x512xbf16, #ttnn_layout109>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout108>) -> () loc(#loc)
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %40:2 = ttcore.load_cached(@main_const_eval_40, [%arg28, %arg29, %arg30, %arg31, %arg32]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x64x1x1xbf16, #ttnn_layout65>) -> (tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg32) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout65>) -> () loc(#loc)
        "ttnn.deallocate"(%arg31) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg30) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg29) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg28) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %41:2 = ttcore.load_cached(@main_const_eval_41, [%arg193, %arg194, %arg195, %arg196, %arg197]) : (tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) loc(#loc)
        "ttnn.deallocate"(%arg197) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc)
        "ttnn.deallocate"(%arg196) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg195) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg194) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg193) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        %42 = ttcore.load_cached(@main_const_eval_42, [%arg0]) : (tensor<1000xbf16, #ttnn_layout27>) -> tensor<8x1000xbf16, #ttnn_layout112> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1000xbf16, #ttnn_layout27>) -> () loc(#loc)
        %43:2 = ttcore.load_cached(@main_const_eval_43, [%arg153, %arg154, %arg155, %arg156, %arg157]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x256x3x3xbf16, #ttnn_layout46>) -> (tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg157) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout46>) -> () loc(#loc)
        "ttnn.deallocate"(%arg156) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg155) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg154) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg153) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %44:2 = ttcore.load_cached(@main_const_eval_44, [%arg108, %arg109, %arg110, %arg111, %arg112]) : (tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128x128x3x3xbf16, #ttnn_layout56>) -> (tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>) loc(#loc)
        "ttnn.deallocate"(%arg112) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc)
        "ttnn.deallocate"(%arg111) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg110) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg109) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg108) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        %45:2 = ttcore.load_cached(@main_const_eval_45, [%arg218, %arg219, %arg220, %arg221, %arg222]) : (tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256xbf16, #ttnn_layout45>, tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> (tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>) loc(#loc)
        "ttnn.deallocate"(%arg222) <{force = false}> : (tensor<256x1024x1x1xbf16, #ttnn_layout80>) -> () loc(#loc)
        "ttnn.deallocate"(%arg221) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg220) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg219) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        "ttnn.deallocate"(%arg218) <{force = false}> : (tensor<256xbf16, #ttnn_layout45>) -> () loc(#loc)
        %46:2 = ttcore.load_cached(@main_const_eval_46, [%arg128, %arg129, %arg130, %arg131, %arg132]) : (tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128xbf16, #ttnn_layout55>, tensor<128x512x1x1xbf16, #ttnn_layout69>) -> (tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58>) loc(#loc)
        "ttnn.deallocate"(%arg132) <{force = false}> : (tensor<128x512x1x1xbf16, #ttnn_layout69>) -> () loc(#loc)
        "ttnn.deallocate"(%arg131) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg130) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg129) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        "ttnn.deallocate"(%arg128) <{force = false}> : (tensor<128xbf16, #ttnn_layout55>) -> () loc(#loc)
        %47:2 = ttcore.load_cached(@main_const_eval_47, [%arg228, %arg229, %arg230, %arg231, %arg232]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x512x3x3xbf16, #ttnn_layout113>) -> (tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg232) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>) -> () loc(#loc)
        "ttnn.deallocate"(%arg231) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg230) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg229) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg228) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %48:2 = ttcore.load_cached(@main_const_eval_48, [%arg133, %arg134, %arg135, %arg136, %arg137]) : (tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) loc(#loc)
        "ttnn.deallocate"(%arg137) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc)
        "ttnn.deallocate"(%arg136) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg135) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg134) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg133) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        %49:2 = ttcore.load_cached(@main_const_eval_49, [%arg243, %arg244, %arg245, %arg246, %arg247]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x512x3x3xbf16, #ttnn_layout113>) -> (tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg247) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>) -> () loc(#loc)
        "ttnn.deallocate"(%arg246) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg245) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg244) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg243) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %50:2 = ttcore.load_cached(@main_const_eval_50, [%arg163, %arg164, %arg165, %arg166, %arg167]) : (tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> (tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) loc(#loc)
        "ttnn.deallocate"(%arg167) <{force = false}> : (tensor<1024x256x1x1xbf16, #ttnn_layout28>) -> () loc(#loc)
        "ttnn.deallocate"(%arg166) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg165) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg164) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg163) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        %51:2 = ttcore.load_cached(@main_const_eval_51, [%arg88, %arg89, %arg90, %arg91, %arg92]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x128x1x1xbf16, #ttnn_layout23>) -> (tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg92) <{force = false}> : (tensor<512x128x1x1xbf16, #ttnn_layout23>) -> () loc(#loc)
        "ttnn.deallocate"(%arg91) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg90) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg89) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg88) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %52:2 = ttcore.load_cached(@main_const_eval_52, [%arg238, %arg239, %arg240, %arg241, %arg242]) : (tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048xbf16, #ttnn_layout84>, tensor<2048x512x1x1xbf16, #ttnn_layout96>) -> (tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87>) loc(#loc)
        "ttnn.deallocate"(%arg242) <{force = false}> : (tensor<2048x512x1x1xbf16, #ttnn_layout96>) -> () loc(#loc)
        "ttnn.deallocate"(%arg241) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg240) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg239) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        "ttnn.deallocate"(%arg238) <{force = false}> : (tensor<2048xbf16, #ttnn_layout84>) -> () loc(#loc)
        %53:2 = ttcore.load_cached(@main_const_eval_53, [%arg7, %arg8, %arg9, %arg10, %arg11]) : (tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024xbf16, #ttnn_layout27>, tensor<1024x512x1x1xbf16, #ttnn_layout117>) -> (tensor<1x1x512x1024xbf16, #ttnn_layout118>, tensor<1x1x1x1024xbf16, #ttnn_layout30>) loc(#loc)
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout117>) -> () loc(#loc)
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<1024xbf16, #ttnn_layout27>) -> () loc(#loc)
        %54:2 = ttcore.load_cached(@main_const_eval_54, [%arg258, %arg259, %arg260, %arg261, %arg262]) : (tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512xbf16, #ttnn_layout1>, tensor<512x512x3x3xbf16, #ttnn_layout113>) -> (tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4>) loc(#loc)
        "ttnn.deallocate"(%arg262) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout113>) -> () loc(#loc)
        "ttnn.deallocate"(%arg261) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg260) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg259) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg258) <{force = false}> : (tensor<512xbf16, #ttnn_layout1>) -> () loc(#loc)
        %55 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %56 = "ttnn.permute"(%arg27) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<8x3x224x224xbf16, #ttnn_layout122>) -> tensor<8x224x224x3xbf16, #ttnn_layout123> loc(#loc783)
        "ttnn.deallocate"(%arg27) <{force = false}> : (tensor<8x3x224x224xbf16, #ttnn_layout122>) -> () loc(#loc783)
        %57 = "ttnn.reshape"(%56) <{shape = [1 : i32, 1 : i32, 401408 : i32, 3 : i32]}> : (tensor<8x224x224x3xbf16, #ttnn_layout123>) -> tensor<1x1x401408x3xbf16, #ttnn_layout124> loc(#loc919)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<8x224x224x3xbf16, #ttnn_layout123>) -> () loc(#loc919)
        %58 = "ttnn.conv2d"(%57, %30#0, %30#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 3 : i32, input_height = 224 : i32, input_width = 224 : i32, kernel_size = array<i32: 7, 7>, out_channels = 64 : i32, padding = array<i32: 3, 3, 3, 3>, stride = array<i32: 2, 2>}> : (tensor<1x1x401408x3xbf16, #ttnn_layout124>, tensor<1x1x147x64xbf16, #ttnn_layout101>, tensor<1x1x1x64xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x100352x64xbf16, #ttnn_layout125> loc(#loc60)
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x1x401408x3xbf16, #ttnn_layout124>) -> () loc(#loc60)
        "ttnn.deallocate"(%30#1) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc60)
        "ttnn.deallocate"(%30#0) <{force = false}> : (tensor<1x1x147x64xbf16, #ttnn_layout101>) -> () loc(#loc60)
        %59 = "ttnn.max_pool2d"(%58) <{batch_size = 8 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, in_place_halo = false, input_height = 112 : si32, input_width = 112 : si32, kernel_size = array<i32: 3, 3>, padding = array<i32: 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x100352x64xbf16, #ttnn_layout125>) -> tensor<1x1x25088x64xbf16, #ttnn_layout126> loc(#loc376)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x1x100352x64xbf16, #ttnn_layout125>) -> () loc(#loc376)
        %60 = "ttnn.to_layout"(%59) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout126>) -> tensor<1x1x25088x64xbf16, #ttnn_layout127> loc(#loc784)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout126>) -> () loc(#loc784)
        %61 = "ttnn.to_memory_config"(%60) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout127>) -> tensor<1x1x25088x64xbf16, #ttnn_layout128> loc(#loc785)
        %62 = "ttnn.conv2d"(%61, %17#0, %17#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>, tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc34)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>) -> () loc(#loc34)
        "ttnn.deallocate"(%17#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc34)
        "ttnn.deallocate"(%17#0) <{force = false}> : (tensor<1x1x64x256xbf16, #ttnn_layout66>) -> () loc(#loc34)
        %63 = "ttnn.to_memory_config"(%62) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> tensor<1x1x25088x256xbf16, #ttnn_layout130> loc(#loc786)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc786)
        %64 = "ttnn.to_memory_config"(%60) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>, #ttnn.core_range<(0,7), (4,7)>]>, <416x64>, <row_major>>>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout127>) -> tensor<1x1x25088x64xbf16, #ttnn_layout128> loc(#loc787)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout127>) -> () loc(#loc787)
        %65 = "ttnn.conv2d"(%64, %16#0, %16#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 64 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>, tensor<1x1x64x64xbf16, #ttnn_layout78>, tensor<1x1x1x64xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x25088x64xbf16, #ttnn_layout128> loc(#loc32)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>) -> () loc(#loc32)
        "ttnn.deallocate"(%16#1) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc32)
        "ttnn.deallocate"(%16#0) <{force = false}> : (tensor<1x1x64x64xbf16, #ttnn_layout78>) -> () loc(#loc32)
        %66 = "ttnn.conv2d"(%65, %32#0, %32#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>, tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x25088x64xbf16, #ttnn_layout128> loc(#loc64)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>) -> () loc(#loc64)
        "ttnn.deallocate"(%32#1) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc64)
        "ttnn.deallocate"(%32#0) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout15>) -> () loc(#loc64)
        %67 = "ttnn.conv2d"(%66, %40#0, %40#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>, tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc80)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>) -> () loc(#loc80)
        "ttnn.deallocate"(%40#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc80)
        "ttnn.deallocate"(%40#0) <{force = false}> : (tensor<1x1x64x256xbf16, #ttnn_layout66>) -> () loc(#loc80)
        %68 = "ttnn.add"(%67, %63) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>, tensor<1x1x25088x256xbf16, #ttnn_layout130>) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc377)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc377)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout130>) -> () loc(#loc377)
        %69 = "ttnn.relu"(%68) : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc378)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc378)
        %70 = "ttnn.to_memory_config"(%69) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> tensor<1x1x25088x256xbf16, #ttnn_layout130> loc(#loc788)
        %71 = "ttnn.conv2d"(%69, %5#0, %5#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 64 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>, tensor<1x1x256x64xbf16, #ttnn_layout38>, tensor<1x1x1x64xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x25088x64xbf16, #ttnn_layout128> loc(#loc10)
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc10)
        "ttnn.deallocate"(%5#1) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc10)
        "ttnn.deallocate"(%5#0) <{force = false}> : (tensor<1x1x256x64xbf16, #ttnn_layout38>) -> () loc(#loc10)
        %72 = "ttnn.conv2d"(%71, %2#0, %2#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>, tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x25088x64xbf16, #ttnn_layout128> loc(#loc4)
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>) -> () loc(#loc4)
        "ttnn.deallocate"(%2#1) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc4)
        "ttnn.deallocate"(%2#0) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout15>) -> () loc(#loc4)
        %73 = "ttnn.conv2d"(%72, %9#0, %9#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>, tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc18)
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>) -> () loc(#loc18)
        "ttnn.deallocate"(%9#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc18)
        "ttnn.deallocate"(%9#0) <{force = false}> : (tensor<1x1x64x256xbf16, #ttnn_layout66>) -> () loc(#loc18)
        %74 = "ttnn.add"(%73, %70) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>, tensor<1x1x25088x256xbf16, #ttnn_layout130>) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc379)
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc379)
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout130>) -> () loc(#loc379)
        %75 = "ttnn.relu"(%74) : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc380)
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc380)
        %76 = "ttnn.to_memory_config"(%75) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> tensor<1x1x25088x256xbf16, #ttnn_layout130> loc(#loc789)
        %77 = "ttnn.conv2d"(%75, %22#0, %22#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 64 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>, tensor<1x1x256x64xbf16, #ttnn_layout38>, tensor<1x1x1x64xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x25088x64xbf16, #ttnn_layout128> loc(#loc44)
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc44)
        "ttnn.deallocate"(%22#1) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc44)
        "ttnn.deallocate"(%22#0) <{force = false}> : (tensor<1x1x256x64xbf16, #ttnn_layout38>) -> () loc(#loc44)
        %78 = "ttnn.conv2d"(%77, %14#0, %14#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>, tensor<1x1x576x64xbf16, #ttnn_layout15>, tensor<1x1x1x64xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x25088x64xbf16, #ttnn_layout128> loc(#loc28)
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>) -> () loc(#loc28)
        "ttnn.deallocate"(%14#1) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout16>) -> () loc(#loc28)
        "ttnn.deallocate"(%14#0) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout15>) -> () loc(#loc28)
        %79 = "ttnn.conv2d"(%78, %29#0, %29#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>, tensor<1x1x64x256xbf16, #ttnn_layout66>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc58)
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x1x25088x64xbf16, #ttnn_layout128>) -> () loc(#loc58)
        "ttnn.deallocate"(%29#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc58)
        "ttnn.deallocate"(%29#0) <{force = false}> : (tensor<1x1x64x256xbf16, #ttnn_layout66>) -> () loc(#loc58)
        %80 = "ttnn.add"(%79, %76) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>, tensor<1x1x25088x256xbf16, #ttnn_layout130>) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc381)
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc381)
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout130>) -> () loc(#loc381)
        %81 = "ttnn.relu"(%80) : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> tensor<1x1x25088x256xbf16, #ttnn_layout129> loc(#loc382)
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc382)
        %82 = "ttnn.to_memory_config"(%81) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,7)>]>, <3136x32>, <row_major>>>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> tensor<1x1x25088x256xbf16, #ttnn_layout131> loc(#loc790)
        %83 = "ttnn.conv2d"(%82, %39#0, %39#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout131>, tensor<1x1x256x512xbf16, #ttnn_layout109>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x6272x512xbf16, #ttnn_layout132> loc(#loc78)
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout131>) -> () loc(#loc78)
        "ttnn.deallocate"(%39#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc78)
        "ttnn.deallocate"(%39#0) <{force = false}> : (tensor<1x1x256x512xbf16, #ttnn_layout109>) -> () loc(#loc78)
        %84 = "ttnn.to_memory_config"(%83) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout132>) -> tensor<1x1x6272x512xbf16, #ttnn_layout133> loc(#loc791)
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout132>) -> () loc(#loc791)
        %85 = "ttnn.conv2d"(%81, %13#0, %13#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>, tensor<1x1x256x128xbf16, #ttnn_layout74>, tensor<1x1x1x128xbf16, #ttnn_layout58>, !ttnn.device) -> tensor<1x1x25088x128xbf16, #ttnn_layout134> loc(#loc26)
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x1x25088x256xbf16, #ttnn_layout129>) -> () loc(#loc26)
        "ttnn.deallocate"(%13#1) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc26)
        "ttnn.deallocate"(%13#0) <{force = false}> : (tensor<1x1x256x128xbf16, #ttnn_layout74>) -> () loc(#loc26)
        %86 = "ttnn.to_memory_config"(%85) <{memory_config = #ttnn.memory_config<#l1, <height_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,5)>, #ttnn.core_range<(0,6), (0,6)>]>, <512x128>, <row_major>>>}> : (tensor<1x1x25088x128xbf16, #ttnn_layout134>) -> tensor<1x1x25088x128xbf16, #ttnn_layout135> loc(#loc792)
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x1x25088x128xbf16, #ttnn_layout134>) -> () loc(#loc792)
        %87 = "ttnn.conv2d"(%86, %25#0, %25#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x25088x128xbf16, #ttnn_layout135>, tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>, !ttnn.device) -> tensor<1x1x6272x128xbf16, #ttnn_layout136> loc(#loc50)
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x1x25088x128xbf16, #ttnn_layout135>) -> () loc(#loc50)
        "ttnn.deallocate"(%25#1) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc50)
        "ttnn.deallocate"(%25#0) <{force = false}> : (tensor<1x1x1152x128xbf16, #ttnn_layout57>) -> () loc(#loc50)
        %88 = "ttnn.conv2d"(%87, %15#0, %15#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>, tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc30)
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>) -> () loc(#loc30)
        "ttnn.deallocate"(%15#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc30)
        "ttnn.deallocate"(%15#0) <{force = false}> : (tensor<1x1x128x512xbf16, #ttnn_layout24>) -> () loc(#loc30)
        %89 = "ttnn.add"(%88, %84) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>, tensor<1x1x6272x512xbf16, #ttnn_layout133>) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc383)
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc383)
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout133>) -> () loc(#loc383)
        %90 = "ttnn.relu"(%89) : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc384)
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc384)
        %91 = "ttnn.to_memory_config"(%90) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> tensor<1x1x6272x512xbf16, #ttnn_layout138> loc(#loc793)
        %92 = "ttnn.conv2d"(%90, %20#0, %20#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>, tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58>, !ttnn.device) -> tensor<1x1x6272x128xbf16, #ttnn_layout136> loc(#loc40)
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc40)
        "ttnn.deallocate"(%20#1) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc40)
        "ttnn.deallocate"(%20#0) <{force = false}> : (tensor<1x1x512x128xbf16, #ttnn_layout70>) -> () loc(#loc40)
        %93 = "ttnn.conv2d"(%92, %36#0, %36#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>, tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>, !ttnn.device) -> tensor<1x1x6272x128xbf16, #ttnn_layout136> loc(#loc72)
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>) -> () loc(#loc72)
        "ttnn.deallocate"(%36#1) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc72)
        "ttnn.deallocate"(%36#0) <{force = false}> : (tensor<1x1x1152x128xbf16, #ttnn_layout57>) -> () loc(#loc72)
        %94 = "ttnn.conv2d"(%93, %51#0, %51#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>, tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc101)
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>) -> () loc(#loc101)
        "ttnn.deallocate"(%51#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc101)
        "ttnn.deallocate"(%51#0) <{force = false}> : (tensor<1x1x128x512xbf16, #ttnn_layout24>) -> () loc(#loc101)
        %95 = "ttnn.add"(%94, %91) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>, tensor<1x1x6272x512xbf16, #ttnn_layout138>) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc385)
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc385)
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout138>) -> () loc(#loc385)
        %96 = "ttnn.relu"(%95) : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc386)
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc386)
        %97 = "ttnn.to_memory_config"(%96) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> tensor<1x1x6272x512xbf16, #ttnn_layout138> loc(#loc794)
        %98 = "ttnn.conv2d"(%96, %12#0, %12#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>, tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58>, !ttnn.device) -> tensor<1x1x6272x128xbf16, #ttnn_layout136> loc(#loc24)
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc24)
        "ttnn.deallocate"(%12#1) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc24)
        "ttnn.deallocate"(%12#0) <{force = false}> : (tensor<1x1x512x128xbf16, #ttnn_layout70>) -> () loc(#loc24)
        %99 = "ttnn.conv2d"(%98, %44#0, %44#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>, tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>, !ttnn.device) -> tensor<1x1x6272x128xbf16, #ttnn_layout136> loc(#loc87)
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>) -> () loc(#loc87)
        "ttnn.deallocate"(%44#1) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc87)
        "ttnn.deallocate"(%44#0) <{force = false}> : (tensor<1x1x1152x128xbf16, #ttnn_layout57>) -> () loc(#loc87)
        %100 = "ttnn.conv2d"(%99, %3#0, %3#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>, tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc6)
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>) -> () loc(#loc6)
        "ttnn.deallocate"(%3#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc6)
        "ttnn.deallocate"(%3#0) <{force = false}> : (tensor<1x1x128x512xbf16, #ttnn_layout24>) -> () loc(#loc6)
        %101 = "ttnn.add"(%100, %97) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>, tensor<1x1x6272x512xbf16, #ttnn_layout138>) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc387)
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc387)
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout138>) -> () loc(#loc387)
        %102 = "ttnn.relu"(%101) : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc388)
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc388)
        %103 = "ttnn.to_memory_config"(%102) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> tensor<1x1x6272x512xbf16, #ttnn_layout138> loc(#loc795)
        %104 = "ttnn.conv2d"(%102, %46#0, %46#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>, tensor<1x1x512x128xbf16, #ttnn_layout70>, tensor<1x1x1x128xbf16, #ttnn_layout58>, !ttnn.device) -> tensor<1x1x6272x128xbf16, #ttnn_layout136> loc(#loc91)
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc91)
        "ttnn.deallocate"(%46#1) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc91)
        "ttnn.deallocate"(%46#0) <{force = false}> : (tensor<1x1x512x128xbf16, #ttnn_layout70>) -> () loc(#loc91)
        %105 = "ttnn.conv2d"(%104, %8#0, %8#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>, tensor<1x1x1152x128xbf16, #ttnn_layout57>, tensor<1x1x1x128xbf16, #ttnn_layout58>, !ttnn.device) -> tensor<1x1x6272x128xbf16, #ttnn_layout136> loc(#loc16)
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>) -> () loc(#loc16)
        "ttnn.deallocate"(%8#1) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn_layout58>) -> () loc(#loc16)
        "ttnn.deallocate"(%8#0) <{force = false}> : (tensor<1x1x1152x128xbf16, #ttnn_layout57>) -> () loc(#loc16)
        %106 = "ttnn.conv2d"(%105, %21#0, %21#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>, tensor<1x1x128x512xbf16, #ttnn_layout24>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc42)
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x1x6272x128xbf16, #ttnn_layout136>) -> () loc(#loc42)
        "ttnn.deallocate"(%21#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc42)
        "ttnn.deallocate"(%21#0) <{force = false}> : (tensor<1x1x128x512xbf16, #ttnn_layout24>) -> () loc(#loc42)
        %107 = "ttnn.add"(%106, %103) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>, tensor<1x1x6272x512xbf16, #ttnn_layout138>) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc389)
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc389)
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout138>) -> () loc(#loc389)
        %108 = "ttnn.relu"(%107) : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> tensor<1x1x6272x512xbf16, #ttnn_layout137> loc(#loc390)
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc390)
        %109 = "ttnn.to_memory_config"(%108) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <896x64>, <row_major>>>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> tensor<1x1x6272x512xbf16, #ttnn_layout139> loc(#loc796)
        %110 = "ttnn.conv2d"(%109, %53#0, %53#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout139>, tensor<1x1x512x1024xbf16, #ttnn_layout118>, tensor<1x1x1x1024xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc105)
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout139>) -> () loc(#loc105)
        "ttnn.deallocate"(%53#1) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc105)
        "ttnn.deallocate"(%53#0) <{force = false}> : (tensor<1x1x512x1024xbf16, #ttnn_layout118>) -> () loc(#loc105)
        %111 = "ttnn.to_memory_config"(%110) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout141> loc(#loc797)
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc797)
        %112 = "ttnn.to_memory_config"(%108) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,7)>]>, <800x64>, <row_major>>>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> tensor<1x1x6272x512xbf16, #ttnn_layout132> loc(#loc798)
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout137>) -> () loc(#loc798)
        %113 = "ttnn.conv2d"(%112, %38#0, %38#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6272x512xbf16, #ttnn_layout132>, tensor<1x1x512x256xbf16, #ttnn_layout105>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x6272x256xbf16, #ttnn_layout142> loc(#loc76)
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x1x6272x512xbf16, #ttnn_layout132>) -> () loc(#loc76)
        "ttnn.deallocate"(%38#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc76)
        "ttnn.deallocate"(%38#0) <{force = false}> : (tensor<1x1x512x256xbf16, #ttnn_layout105>) -> () loc(#loc76)
        %114 = "ttnn.to_memory_config"(%113) <{memory_config = #ttnn.memory_config<#l1, <block_sharded>, #ttnn.shard_spec<<[#ttnn.core_range<(0,0), (7,6)>]>, <896x32>, <row_major>>>}> : (tensor<1x1x6272x256xbf16, #ttnn_layout142>) -> tensor<1x1x6272x256xbf16, #ttnn_layout143> loc(#loc799)
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x1x6272x256xbf16, #ttnn_layout142>) -> () loc(#loc799)
        %115 = "ttnn.conv2d"(%114, %7#0, %7#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x6272x256xbf16, #ttnn_layout143>, tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc14)
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x1x6272x256xbf16, #ttnn_layout143>) -> () loc(#loc14)
        "ttnn.deallocate"(%7#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc14)
        "ttnn.deallocate"(%7#0) <{force = false}> : (tensor<1x1x2304x256xbf16, #ttnn_layout47>) -> () loc(#loc14)
        %116 = "ttnn.conv2d"(%115, %48#0, %48#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc95)
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc95)
        "ttnn.deallocate"(%48#1) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc95)
        "ttnn.deallocate"(%48#0) <{force = false}> : (tensor<1x1x256x1024xbf16, #ttnn_layout29>) -> () loc(#loc95)
        %117 = "ttnn.add"(%116, %111) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc391)
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc391)
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> () loc(#loc391)
        %118 = "ttnn.relu"(%117) : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc392)
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc392)
        %119 = "ttnn.to_memory_config"(%118) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout141> loc(#loc800)
        %120 = "ttnn.conv2d"(%118, %19#0, %19#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc38)
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc38)
        "ttnn.deallocate"(%19#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc38)
        "ttnn.deallocate"(%19#0) <{force = false}> : (tensor<1x1x1024x256xbf16, #ttnn_layout81>) -> () loc(#loc38)
        %121 = "ttnn.conv2d"(%120, %43#0, %43#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc85)
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc85)
        "ttnn.deallocate"(%43#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc85)
        "ttnn.deallocate"(%43#0) <{force = false}> : (tensor<1x1x2304x256xbf16, #ttnn_layout47>) -> () loc(#loc85)
        %122 = "ttnn.conv2d"(%121, %4#0, %4#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc8)
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc8)
        "ttnn.deallocate"(%4#1) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc8)
        "ttnn.deallocate"(%4#0) <{force = false}> : (tensor<1x1x256x1024xbf16, #ttnn_layout29>) -> () loc(#loc8)
        %123 = "ttnn.add"(%122, %119) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc393)
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc393)
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> () loc(#loc393)
        %124 = "ttnn.relu"(%123) : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc394)
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc394)
        %125 = "ttnn.to_memory_config"(%124) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout141> loc(#loc801)
        %126 = "ttnn.conv2d"(%124, %24#0, %24#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc48)
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc48)
        "ttnn.deallocate"(%24#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc48)
        "ttnn.deallocate"(%24#0) <{force = false}> : (tensor<1x1x1024x256xbf16, #ttnn_layout81>) -> () loc(#loc48)
        %127 = "ttnn.conv2d"(%126, %33#0, %33#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc66)
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc66)
        "ttnn.deallocate"(%33#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc66)
        "ttnn.deallocate"(%33#0) <{force = false}> : (tensor<1x1x2304x256xbf16, #ttnn_layout47>) -> () loc(#loc66)
        %128 = "ttnn.conv2d"(%127, %50#0, %50#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc99)
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc99)
        "ttnn.deallocate"(%50#1) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc99)
        "ttnn.deallocate"(%50#0) <{force = false}> : (tensor<1x1x256x1024xbf16, #ttnn_layout29>) -> () loc(#loc99)
        %129 = "ttnn.add"(%128, %125) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc395)
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc395)
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> () loc(#loc395)
        %130 = "ttnn.relu"(%129) : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc396)
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc396)
        %131 = "ttnn.to_memory_config"(%130) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout141> loc(#loc802)
        %132 = "ttnn.conv2d"(%130, %37#0, %37#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc74)
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc74)
        "ttnn.deallocate"(%37#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc74)
        "ttnn.deallocate"(%37#0) <{force = false}> : (tensor<1x1x1024x256xbf16, #ttnn_layout81>) -> () loc(#loc74)
        %133 = "ttnn.conv2d"(%132, %27#0, %27#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc54)
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc54)
        "ttnn.deallocate"(%27#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc54)
        "ttnn.deallocate"(%27#0) <{force = false}> : (tensor<1x1x2304x256xbf16, #ttnn_layout47>) -> () loc(#loc54)
        %134 = "ttnn.conv2d"(%133, %34#0, %34#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc68)
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc68)
        "ttnn.deallocate"(%34#1) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc68)
        "ttnn.deallocate"(%34#0) <{force = false}> : (tensor<1x1x256x1024xbf16, #ttnn_layout29>) -> () loc(#loc68)
        %135 = "ttnn.add"(%134, %131) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc397)
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc397)
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> () loc(#loc397)
        %136 = "ttnn.relu"(%135) : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc398)
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc398)
        %137 = "ttnn.to_memory_config"(%136) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout141> loc(#loc803)
        %138 = "ttnn.conv2d"(%136, %35#0, %35#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc70)
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc70)
        "ttnn.deallocate"(%35#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc70)
        "ttnn.deallocate"(%35#0) <{force = false}> : (tensor<1x1x1024x256xbf16, #ttnn_layout81>) -> () loc(#loc70)
        %139 = "ttnn.conv2d"(%138, %11#0, %11#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc22)
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc22)
        "ttnn.deallocate"(%11#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc22)
        "ttnn.deallocate"(%11#0) <{force = false}> : (tensor<1x1x2304x256xbf16, #ttnn_layout47>) -> () loc(#loc22)
        %140 = "ttnn.conv2d"(%139, %41#0, %41#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc82)
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc82)
        "ttnn.deallocate"(%41#1) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc82)
        "ttnn.deallocate"(%41#0) <{force = false}> : (tensor<1x1x256x1024xbf16, #ttnn_layout29>) -> () loc(#loc82)
        %141 = "ttnn.add"(%140, %137) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc399)
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc399)
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> () loc(#loc399)
        %142 = "ttnn.relu"(%141) : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc400)
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc400)
        %143 = "ttnn.to_memory_config"(%142) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout141> loc(#loc804)
        %144 = "ttnn.conv2d"(%142, %45#0, %45#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1024x256xbf16, #ttnn_layout81>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc89)
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc89)
        "ttnn.deallocate"(%45#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc89)
        "ttnn.deallocate"(%45#0) <{force = false}> : (tensor<1x1x1024x256xbf16, #ttnn_layout81>) -> () loc(#loc89)
        %145 = "ttnn.conv2d"(%144, %10#0, %10#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x2304x256xbf16, #ttnn_layout47>, tensor<1x1x1x256xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x1568x256xbf16, #ttnn_layout144> loc(#loc20)
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc20)
        "ttnn.deallocate"(%10#1) <{force = false}> : (tensor<1x1x1x256xbf16, #ttnn_layout48>) -> () loc(#loc20)
        "ttnn.deallocate"(%10#0) <{force = false}> : (tensor<1x1x2304x256xbf16, #ttnn_layout47>) -> () loc(#loc20)
        %146 = "ttnn.conv2d"(%145, %28#0, %28#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>, tensor<1x1x256x1024xbf16, #ttnn_layout29>, tensor<1x1x1x1024xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc56)
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x1x1568x256xbf16, #ttnn_layout144>) -> () loc(#loc56)
        "ttnn.deallocate"(%28#1) <{force = false}> : (tensor<1x1x1x1024xbf16, #ttnn_layout30>) -> () loc(#loc56)
        "ttnn.deallocate"(%28#0) <{force = false}> : (tensor<1x1x256x1024xbf16, #ttnn_layout29>) -> () loc(#loc56)
        %147 = "ttnn.add"(%146, %143) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc401)
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc401)
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout141>) -> () loc(#loc401)
        %148 = "ttnn.relu"(%147) : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> tensor<1x1x1568x1024xbf16, #ttnn_layout140> loc(#loc402)
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc402)
        %149 = "ttnn.conv2d"(%148, %23#0, %23#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1024x2048xbf16, #ttnn_layout86>, tensor<1x1x1x2048xbf16, #ttnn_layout87>, !ttnn.device) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc46)
        "ttnn.deallocate"(%23#1) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc46)
        "ttnn.deallocate"(%23#0) <{force = false}> : (tensor<1x1x1024x2048xbf16, #ttnn_layout86>) -> () loc(#loc46)
        %150 = "ttnn.to_memory_config"(%149) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> tensor<1x1x392x2048xbf16, #ttnn_layout146> loc(#loc805)
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc805)
        %151 = "ttnn.conv2d"(%148, %1#0, %1#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>, tensor<1x1x1024x512xbf16, #ttnn_layout3>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x1568x512xbf16, #ttnn_layout147> loc(#loc2)
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x1x1568x1024xbf16, #ttnn_layout140>) -> () loc(#loc2)
        "ttnn.deallocate"(%1#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc2)
        "ttnn.deallocate"(%1#0) <{force = false}> : (tensor<1x1x1024x512xbf16, #ttnn_layout3>) -> () loc(#loc2)
        %152 = "ttnn.conv2d"(%151, %47#0, %47#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x1568x512xbf16, #ttnn_layout147>, tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x392x512xbf16, #ttnn_layout148> loc(#loc93)
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x1x1568x512xbf16, #ttnn_layout147>) -> () loc(#loc93)
        "ttnn.deallocate"(%47#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc93)
        "ttnn.deallocate"(%47#0) <{force = false}> : (tensor<1x1x4608x512xbf16, #ttnn_layout114>) -> () loc(#loc93)
        %153 = "ttnn.conv2d"(%152, %31#0, %31#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>, tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87>, !ttnn.device) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc62)
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>) -> () loc(#loc62)
        "ttnn.deallocate"(%31#1) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc62)
        "ttnn.deallocate"(%31#0) <{force = false}> : (tensor<1x1x512x2048xbf16, #ttnn_layout97>) -> () loc(#loc62)
        %154 = "ttnn.add"(%153, %150) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>, tensor<1x1x392x2048xbf16, #ttnn_layout146>) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc403)
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc403)
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout146>) -> () loc(#loc403)
        %155 = "ttnn.relu"(%154) : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc404)
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc404)
        %156 = "ttnn.to_memory_config"(%155) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> tensor<1x1x392x2048xbf16, #ttnn_layout146> loc(#loc806)
        %157 = "ttnn.conv2d"(%155, %6#0, %6#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 2048 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>, tensor<1x1x2048x512xbf16, #ttnn_layout42>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x392x512xbf16, #ttnn_layout148> loc(#loc12)
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc12)
        "ttnn.deallocate"(%6#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc12)
        "ttnn.deallocate"(%6#0) <{force = false}> : (tensor<1x1x2048x512xbf16, #ttnn_layout42>) -> () loc(#loc12)
        %158 = "ttnn.conv2d"(%157, %49#0, %49#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>, tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x392x512xbf16, #ttnn_layout148> loc(#loc97)
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>) -> () loc(#loc97)
        "ttnn.deallocate"(%49#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc97)
        "ttnn.deallocate"(%49#0) <{force = false}> : (tensor<1x1x4608x512xbf16, #ttnn_layout114>) -> () loc(#loc97)
        %159 = "ttnn.conv2d"(%158, %52#0, %52#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>, tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87>, !ttnn.device) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc103)
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>) -> () loc(#loc103)
        "ttnn.deallocate"(%52#1) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc103)
        "ttnn.deallocate"(%52#0) <{force = false}> : (tensor<1x1x512x2048xbf16, #ttnn_layout97>) -> () loc(#loc103)
        %160 = "ttnn.add"(%159, %156) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>, tensor<1x1x392x2048xbf16, #ttnn_layout146>) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc405)
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc405)
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout146>) -> () loc(#loc405)
        %161 = "ttnn.relu"(%160) : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc406)
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc406)
        %162 = "ttnn.to_memory_config"(%161) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> tensor<1x1x392x2048xbf16, #ttnn_layout146> loc(#loc807)
        %163 = "ttnn.conv2d"(%161, %18#0, %18#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 2048 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>, tensor<1x1x2048x512xbf16, #ttnn_layout42>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x392x512xbf16, #ttnn_layout148> loc(#loc36)
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc36)
        "ttnn.deallocate"(%18#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc36)
        "ttnn.deallocate"(%18#0) <{force = false}> : (tensor<1x1x2048x512xbf16, #ttnn_layout42>) -> () loc(#loc36)
        %164 = "ttnn.conv2d"(%163, %54#0, %54#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, activation = <op_type = relu>, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>, tensor<1x1x4608x512xbf16, #ttnn_layout114>, tensor<1x1x1x512xbf16, #ttnn_layout4>, !ttnn.device) -> tensor<1x1x392x512xbf16, #ttnn_layout148> loc(#loc107)
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>) -> () loc(#loc107)
        "ttnn.deallocate"(%54#1) <{force = false}> : (tensor<1x1x1x512xbf16, #ttnn_layout4>) -> () loc(#loc107)
        "ttnn.deallocate"(%54#0) <{force = false}> : (tensor<1x1x4608x512xbf16, #ttnn_layout114>) -> () loc(#loc107)
        %165 = "ttnn.conv2d"(%164, %26#0, %26#1, %55) <{batch_size = 8 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>, tensor<1x1x512x2048xbf16, #ttnn_layout97>, tensor<1x1x1x2048xbf16, #ttnn_layout87>, !ttnn.device) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc52)
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x1x392x512xbf16, #ttnn_layout148>) -> () loc(#loc52)
        "ttnn.deallocate"(%26#1) <{force = false}> : (tensor<1x1x1x2048xbf16, #ttnn_layout87>) -> () loc(#loc52)
        "ttnn.deallocate"(%26#0) <{force = false}> : (tensor<1x1x512x2048xbf16, #ttnn_layout97>) -> () loc(#loc52)
        %166 = "ttnn.add"(%165, %162) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>, tensor<1x1x392x2048xbf16, #ttnn_layout146>) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc407)
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc407)
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout146>) -> () loc(#loc407)
        %167 = "ttnn.relu"(%166) : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> tensor<1x1x392x2048xbf16, #ttnn_layout145> loc(#loc408)
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc408)
        %168 = "ttnn.to_memory_config"(%167) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> tensor<1x1x392x2048xbf16, #ttnn_layout146> loc(#loc808)
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout145>) -> () loc(#loc808)
        %169 = "ttnn.reshape"(%168) <{shape = [8 : i32, 7 : i32, 7 : i32, 2048 : i32]}> : (tensor<1x1x392x2048xbf16, #ttnn_layout146>) -> tensor<8x7x7x2048xbf16, #ttnn_layout149> loc(#loc408)
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x1x392x2048xbf16, #ttnn_layout146>) -> () loc(#loc408)
        %170 = "ttnn.permute"(%169) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<8x7x7x2048xbf16, #ttnn_layout149>) -> tensor<8x2048x7x7xbf16, #ttnn_layout150> loc(#loc408)
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<8x7x7x2048xbf16, #ttnn_layout149>) -> () loc(#loc408)
        %171 = "ttnn.sum"(%170) <{dim_arg = [2 : i32, 3 : i32], keep_dim = false}> : (tensor<8x2048x7x7xbf16, #ttnn_layout150>) -> tensor<8x2048xbf16, #ttnn_layout> loc(#loc409)
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<8x2048x7x7xbf16, #ttnn_layout150>) -> () loc(#loc409)
        %172 = "ttnn.multiply"(%171, %0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<8x2048xbf16, #ttnn_layout>, tensor<8x2048xbf16, #ttnn_layout>) -> tensor<8x2048xbf16, #ttnn_layout151> loc(#loc410)
        "ttnn.deallocate"(%171) <{force = false}> : (tensor<8x2048xbf16, #ttnn_layout>) -> () loc(#loc410)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<8x2048xbf16, #ttnn_layout>) -> () loc(#loc410)
        %173 = "ttnn.to_memory_config"(%172) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<8x2048xbf16, #ttnn_layout151>) -> tensor<8x2048xbf16, #ttnn_layout152> loc(#loc809)
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<8x2048xbf16, #ttnn_layout151>) -> () loc(#loc809)
        %174 = "ttnn.linear"(%173, %arg1, %42) <{transpose_a = false, transpose_b = true}> : (tensor<8x2048xbf16, #ttnn_layout152>, tensor<1000x2048xbf16, #ttnn_layout121>, tensor<8x1000xbf16, #ttnn_layout112>) -> tensor<8x1000xbf16, #ttnn_layout112> loc(#loc411)
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<8x2048xbf16, #ttnn_layout152>) -> () loc(#loc411)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<8x1000xbf16, #ttnn_layout112>) -> () loc(#loc411)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<1000x2048xbf16, #ttnn_layout121>) -> () loc(#loc411)
        return %174 : tensor<8x1000xbf16, #ttnn_layout112> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("batch-norm-inference.1475")
#loc2 = loc("convolution.1474")
#loc3 = loc("batch-norm-inference.458")
#loc4 = loc("convolution.457")
#loc5 = loc("batch-norm-inference.809")
#loc6 = loc("convolution.808")
#loc7 = loc("batch-norm-inference.1067")
#loc8 = loc("convolution.1066")
#loc9 = loc("batch-norm-inference.449")
#loc10 = loc("convolution.448")
#loc11 = loc("batch-norm-inference.1559")
#loc12 = loc("convolution.1558")
#loc13 = loc("batch-norm-inference.974")
#loc14 = loc("convolution.973")
#loc15 = loc("batch-norm-inference.884")
#loc16 = loc("convolution.883")
#loc17 = loc("batch-norm-inference.467")
#loc18 = loc("convolution.466")
#loc19 = loc("batch-norm-inference.1394")
#loc20 = loc("convolution.1393")
#loc21 = loc("batch-norm-inference.1310")
#loc22 = loc("convolution.1309")
#loc23 = loc("batch-norm-inference.791")
#loc24 = loc("convolution.790")
#loc25 = loc("batch-norm-inference.623")
#loc26 = loc("convolution.622")
#loc27 = loc("batch-norm-inference.542")
#loc28 = loc("convolution.541")
#loc29 = loc("batch-norm-inference.641")
#loc30 = loc("convolution.640")
#loc31 = loc("batch-norm-inference.365")
#loc32 = loc("convolution.364")
#loc33 = loc("batch-norm-inference.305")
#loc34 = loc("convolution.304")
#loc35 = loc("batch-norm-inference.1643")
#loc36 = loc("convolution.1642")
#loc37 = loc("batch-norm-inference.1049")
#loc38 = loc("convolution.1048")
#loc39 = loc("batch-norm-inference.707")
#loc40 = loc("convolution.706")
#loc41 = loc("batch-norm-inference.893")
#loc42 = loc("convolution.892")
#loc43 = loc("batch-norm-inference.533")
#loc44 = loc("convolution.532")
#loc45 = loc("batch-norm-inference.1415")
#loc46 = loc("convolution.1414")
#loc47 = loc("batch-norm-inference.1133")
#loc48 = loc("convolution.1132")
#loc49 = loc("batch-norm-inference.632")
#loc50 = loc("convolution.631")
#loc51 = loc("batch-norm-inference.1661")
#loc52 = loc("convolution.1660")
#loc53 = loc("batch-norm-inference.1226")
#loc54 = loc("convolution.1225")
#loc55 = loc("batch-norm-inference.1403")
#loc56 = loc("convolution.1402")
#loc57 = loc("batch-norm-inference.551")
#loc58 = loc("convolution.550")
#loc59 = loc("batch-norm-inference.124")
#loc60 = loc("convolution.123")
#loc61 = loc("batch-norm-inference.1493")
#loc62 = loc("convolution.1492")
#loc63 = loc("batch-norm-inference.374")
#loc64 = loc("convolution.373")
#loc65 = loc("batch-norm-inference.1142")
#loc66 = loc("convolution.1141")
#loc67 = loc("batch-norm-inference.1235")
#loc68 = loc("convolution.1234")
#loc69 = loc("batch-norm-inference.1301")
#loc70 = loc("convolution.1300")
#loc71 = loc("batch-norm-inference.716")
#loc72 = loc("convolution.715")
#loc73 = loc("batch-norm-inference.1217")
#loc74 = loc("convolution.1216")
#loc75 = loc("batch-norm-inference.965")
#loc76 = loc("convolution.964")
#loc77 = loc("batch-norm-inference.563")
#loc78 = loc("convolution.562")
#loc79 = loc("batch-norm-inference.383")
#loc80 = loc("convolution.382")
#loc81 = loc("batch-norm-inference.1319")
#loc82 = loc("convolution.1318")
#loc83 = loc("broadcast.1695")
#loc84 = loc("batch-norm-inference.1058")
#loc85 = loc("convolution.1057")
#loc86 = loc("batch-norm-inference.800")
#loc87 = loc("convolution.799")
#loc88 = loc("batch-norm-inference.1385")
#loc89 = loc("convolution.1384")
#loc90 = loc("batch-norm-inference.875")
#loc91 = loc("convolution.874")
#loc92 = loc("batch-norm-inference.1484")
#loc93 = loc("convolution.1483")
#loc94 = loc("batch-norm-inference.983")
#loc95 = loc("convolution.982")
#loc96 = loc("batch-norm-inference.1568")
#loc97 = loc("convolution.1567")
#loc98 = loc("batch-norm-inference.1151")
#loc99 = loc("convolution.1150")
#loc100 = loc("batch-norm-inference.725")
#loc101 = loc("convolution.724")
#loc102 = loc("batch-norm-inference.1577")
#loc103 = loc("convolution.1576")
#loc104 = loc("batch-norm-inference.905")
#loc105 = loc("convolution.904")
#loc106 = loc("batch-norm-inference.1652")
#loc107 = loc("convolution.1651")
#loc376 = loc("reduce-window.143")
#loc377 = loc("add.390")
#loc378 = loc("maximum.393")
#loc379 = loc("add.474")
#loc380 = loc("maximum.477")
#loc381 = loc("add.558")
#loc382 = loc("maximum.561")
#loc383 = loc("add.648")
#loc384 = loc("maximum.651")
#loc385 = loc("add.732")
#loc386 = loc("maximum.735")
#loc387 = loc("add.816")
#loc388 = loc("maximum.819")
#loc389 = loc("add.900")
#loc390 = loc("maximum.903")
#loc391 = loc("add.990")
#loc392 = loc("maximum.993")
#loc393 = loc("add.1074")
#loc394 = loc("maximum.1077")
#loc395 = loc("add.1158")
#loc396 = loc("maximum.1161")
#loc397 = loc("add.1242")
#loc398 = loc("maximum.1245")
#loc399 = loc("add.1326")
#loc400 = loc("maximum.1329")
#loc401 = loc("add.1410")
#loc402 = loc("maximum.1413")
#loc403 = loc("add.1500")
#loc404 = loc("maximum.1503")
#loc405 = loc("add.1584")
#loc406 = loc("maximum.1587")
#loc407 = loc("add.1668")
#loc408 = loc("maximum.1671")
#loc409 = loc("reduce.1678")
#loc410 = loc("multiply.1687")
#loc411 = loc("add.1696")
#loc412 = loc("batch-norm-inference.1475_reshape"(#loc1))
#loc413 = loc("convolution.1474_multiply_in_0_layout"(#loc2))
#loc414 = loc("convolution.1474_multiply"(#loc2))
#loc415 = loc("convolution.1474_bias"(#loc2))
#loc416 = loc("convolution.1474_weight"(#loc2))
#loc417 = loc("convolution.1474_prepare_conv2d_weight"(#loc2))
#loc418 = loc("convolution.1474_prepare_conv2d_bias"(#loc2))
#loc419 = loc("batch-norm-inference.458_reshape"(#loc3))
#loc420 = loc("convolution.457_multiply_in_0_layout"(#loc4))
#loc421 = loc("convolution.457_multiply"(#loc4))
#loc422 = loc("convolution.457_bias"(#loc4))
#loc423 = loc("convolution.457_weight"(#loc4))
#loc424 = loc("convolution.457_prepare_conv2d_weight"(#loc4))
#loc425 = loc("convolution.457_prepare_conv2d_bias"(#loc4))
#loc426 = loc("batch-norm-inference.809_reshape"(#loc5))
#loc427 = loc("convolution.808_multiply_in_0_layout"(#loc6))
#loc428 = loc("convolution.808_multiply"(#loc6))
#loc429 = loc("convolution.808_bias"(#loc6))
#loc430 = loc("convolution.808_weight"(#loc6))
#loc431 = loc("convolution.808_prepare_conv2d_weight"(#loc6))
#loc432 = loc("convolution.808_prepare_conv2d_bias"(#loc6))
#loc433 = loc("batch-norm-inference.1067_reshape"(#loc7))
#loc434 = loc("convolution.1066_multiply_in_0_layout"(#loc8))
#loc435 = loc("convolution.1066_multiply"(#loc8))
#loc436 = loc("convolution.1066_bias"(#loc8))
#loc437 = loc("convolution.1066_weight"(#loc8))
#loc438 = loc("convolution.1066_prepare_conv2d_weight"(#loc8))
#loc439 = loc("convolution.1066_prepare_conv2d_bias"(#loc8))
#loc440 = loc("batch-norm-inference.449_reshape"(#loc9))
#loc441 = loc("convolution.448_multiply_in_0_layout"(#loc10))
#loc442 = loc("convolution.448_multiply"(#loc10))
#loc443 = loc("convolution.448_bias"(#loc10))
#loc444 = loc("convolution.448_weight"(#loc10))
#loc445 = loc("convolution.448_prepare_conv2d_weight"(#loc10))
#loc446 = loc("convolution.448_prepare_conv2d_bias"(#loc10))
#loc447 = loc("batch-norm-inference.1559_reshape"(#loc11))
#loc448 = loc("convolution.1558_multiply_in_0_layout"(#loc12))
#loc449 = loc("convolution.1558_multiply"(#loc12))
#loc450 = loc("convolution.1558_bias"(#loc12))
#loc451 = loc("convolution.1558_weight"(#loc12))
#loc452 = loc("convolution.1558_prepare_conv2d_weight"(#loc12))
#loc453 = loc("convolution.1558_prepare_conv2d_bias"(#loc12))
#loc454 = loc("batch-norm-inference.974_reshape"(#loc13))
#loc455 = loc("convolution.973_multiply_in_0_layout"(#loc14))
#loc456 = loc("convolution.973_multiply"(#loc14))
#loc457 = loc("convolution.973_bias"(#loc14))
#loc458 = loc("convolution.973_weight"(#loc14))
#loc459 = loc("convolution.973_prepare_conv2d_weight"(#loc14))
#loc460 = loc("convolution.973_prepare_conv2d_bias"(#loc14))
#loc461 = loc("batch-norm-inference.884_reshape"(#loc15))
#loc462 = loc("convolution.883_multiply_in_0_layout"(#loc16))
#loc463 = loc("convolution.883_multiply"(#loc16))
#loc464 = loc("convolution.883_bias"(#loc16))
#loc465 = loc("convolution.883_weight"(#loc16))
#loc466 = loc("convolution.883_prepare_conv2d_weight"(#loc16))
#loc467 = loc("convolution.883_prepare_conv2d_bias"(#loc16))
#loc468 = loc("batch-norm-inference.467_reshape"(#loc17))
#loc469 = loc("convolution.466_multiply_in_0_layout"(#loc18))
#loc470 = loc("convolution.466_multiply"(#loc18))
#loc471 = loc("convolution.466_bias"(#loc18))
#loc472 = loc("convolution.466_weight"(#loc18))
#loc473 = loc("convolution.466_prepare_conv2d_weight"(#loc18))
#loc474 = loc("convolution.466_prepare_conv2d_bias"(#loc18))
#loc475 = loc("batch-norm-inference.1394_reshape"(#loc19))
#loc476 = loc("convolution.1393_multiply_in_0_layout"(#loc20))
#loc477 = loc("convolution.1393_multiply"(#loc20))
#loc478 = loc("convolution.1393_bias"(#loc20))
#loc479 = loc("convolution.1393_weight"(#loc20))
#loc480 = loc("convolution.1393_prepare_conv2d_weight"(#loc20))
#loc481 = loc("convolution.1393_prepare_conv2d_bias"(#loc20))
#loc482 = loc("batch-norm-inference.1310_reshape"(#loc21))
#loc483 = loc("convolution.1309_multiply_in_0_layout"(#loc22))
#loc484 = loc("convolution.1309_multiply"(#loc22))
#loc485 = loc("convolution.1309_bias"(#loc22))
#loc486 = loc("convolution.1309_weight"(#loc22))
#loc487 = loc("convolution.1309_prepare_conv2d_weight"(#loc22))
#loc488 = loc("convolution.1309_prepare_conv2d_bias"(#loc22))
#loc489 = loc("batch-norm-inference.791_reshape"(#loc23))
#loc490 = loc("convolution.790_multiply_in_0_layout"(#loc24))
#loc491 = loc("convolution.790_multiply"(#loc24))
#loc492 = loc("convolution.790_bias"(#loc24))
#loc493 = loc("convolution.790_weight"(#loc24))
#loc494 = loc("convolution.790_prepare_conv2d_weight"(#loc24))
#loc495 = loc("convolution.790_prepare_conv2d_bias"(#loc24))
#loc496 = loc("batch-norm-inference.623_reshape"(#loc25))
#loc497 = loc("convolution.622_multiply_in_0_layout"(#loc26))
#loc498 = loc("convolution.622_multiply"(#loc26))
#loc499 = loc("convolution.622_bias"(#loc26))
#loc500 = loc("convolution.622_weight"(#loc26))
#loc501 = loc("convolution.622_prepare_conv2d_weight"(#loc26))
#loc502 = loc("convolution.622_prepare_conv2d_bias"(#loc26))
#loc503 = loc("batch-norm-inference.542_reshape"(#loc27))
#loc504 = loc("convolution.541_multiply_in_0_layout"(#loc28))
#loc505 = loc("convolution.541_multiply"(#loc28))
#loc506 = loc("convolution.541_bias"(#loc28))
#loc507 = loc("convolution.541_weight"(#loc28))
#loc508 = loc("convolution.541_prepare_conv2d_weight"(#loc28))
#loc509 = loc("convolution.541_prepare_conv2d_bias"(#loc28))
#loc510 = loc("batch-norm-inference.641_reshape"(#loc29))
#loc511 = loc("convolution.640_multiply_in_0_layout"(#loc30))
#loc512 = loc("convolution.640_multiply"(#loc30))
#loc513 = loc("convolution.640_bias"(#loc30))
#loc514 = loc("convolution.640_weight"(#loc30))
#loc515 = loc("convolution.640_prepare_conv2d_weight"(#loc30))
#loc516 = loc("convolution.640_prepare_conv2d_bias"(#loc30))
#loc517 = loc("batch-norm-inference.365_reshape"(#loc31))
#loc518 = loc("convolution.364_multiply_in_0_layout"(#loc32))
#loc519 = loc("convolution.364_multiply"(#loc32))
#loc520 = loc("convolution.364_bias"(#loc32))
#loc521 = loc("convolution.364_weight"(#loc32))
#loc522 = loc("convolution.364_prepare_conv2d_weight"(#loc32))
#loc523 = loc("convolution.364_prepare_conv2d_bias"(#loc32))
#loc524 = loc("batch-norm-inference.305_reshape"(#loc33))
#loc525 = loc("convolution.304_multiply_in_0_layout"(#loc34))
#loc526 = loc("convolution.304_multiply"(#loc34))
#loc527 = loc("convolution.304_bias"(#loc34))
#loc528 = loc("convolution.304_weight"(#loc34))
#loc529 = loc("convolution.304_prepare_conv2d_weight"(#loc34))
#loc530 = loc("convolution.304_prepare_conv2d_bias"(#loc34))
#loc531 = loc("batch-norm-inference.1643_reshape"(#loc35))
#loc532 = loc("convolution.1642_multiply_in_0_layout"(#loc36))
#loc533 = loc("convolution.1642_multiply"(#loc36))
#loc534 = loc("convolution.1642_bias"(#loc36))
#loc535 = loc("convolution.1642_weight"(#loc36))
#loc536 = loc("convolution.1642_prepare_conv2d_weight"(#loc36))
#loc537 = loc("convolution.1642_prepare_conv2d_bias"(#loc36))
#loc538 = loc("batch-norm-inference.1049_reshape"(#loc37))
#loc539 = loc("convolution.1048_multiply_in_0_layout"(#loc38))
#loc540 = loc("convolution.1048_multiply"(#loc38))
#loc541 = loc("convolution.1048_bias"(#loc38))
#loc542 = loc("convolution.1048_weight"(#loc38))
#loc543 = loc("convolution.1048_prepare_conv2d_weight"(#loc38))
#loc544 = loc("convolution.1048_prepare_conv2d_bias"(#loc38))
#loc545 = loc("batch-norm-inference.707_reshape"(#loc39))
#loc546 = loc("convolution.706_multiply_in_0_layout"(#loc40))
#loc547 = loc("convolution.706_multiply"(#loc40))
#loc548 = loc("convolution.706_bias"(#loc40))
#loc549 = loc("convolution.706_weight"(#loc40))
#loc550 = loc("convolution.706_prepare_conv2d_weight"(#loc40))
#loc551 = loc("convolution.706_prepare_conv2d_bias"(#loc40))
#loc552 = loc("batch-norm-inference.893_reshape"(#loc41))
#loc553 = loc("convolution.892_multiply_in_0_layout"(#loc42))
#loc554 = loc("convolution.892_multiply"(#loc42))
#loc555 = loc("convolution.892_bias"(#loc42))
#loc556 = loc("convolution.892_weight"(#loc42))
#loc557 = loc("convolution.892_prepare_conv2d_weight"(#loc42))
#loc558 = loc("convolution.892_prepare_conv2d_bias"(#loc42))
#loc559 = loc("batch-norm-inference.533_reshape"(#loc43))
#loc560 = loc("convolution.532_multiply_in_0_layout"(#loc44))
#loc561 = loc("convolution.532_multiply"(#loc44))
#loc562 = loc("convolution.532_bias"(#loc44))
#loc563 = loc("convolution.532_weight"(#loc44))
#loc564 = loc("convolution.532_prepare_conv2d_weight"(#loc44))
#loc565 = loc("convolution.532_prepare_conv2d_bias"(#loc44))
#loc566 = loc("batch-norm-inference.1415_reshape"(#loc45))
#loc567 = loc("convolution.1414_multiply"(#loc46))
#loc568 = loc("convolution.1414_multiply_in_0_layout"(#loc46))
#loc569 = loc("convolution.1414_bias"(#loc46))
#loc570 = loc("convolution.1414_weight"(#loc46))
#loc571 = loc("convolution.1414_prepare_conv2d_weight"(#loc46))
#loc572 = loc("convolution.1414_prepare_conv2d_bias"(#loc46))
#loc573 = loc("batch-norm-inference.1133_reshape"(#loc47))
#loc574 = loc("convolution.1132_multiply_in_0_layout"(#loc48))
#loc575 = loc("convolution.1132_multiply"(#loc48))
#loc576 = loc("convolution.1132_bias"(#loc48))
#loc577 = loc("convolution.1132_weight"(#loc48))
#loc578 = loc("convolution.1132_prepare_conv2d_weight"(#loc48))
#loc579 = loc("convolution.1132_prepare_conv2d_bias"(#loc48))
#loc580 = loc("batch-norm-inference.632_reshape"(#loc49))
#loc581 = loc("convolution.631_multiply_in_0_layout"(#loc50))
#loc582 = loc("convolution.631_multiply"(#loc50))
#loc583 = loc("convolution.631_bias"(#loc50))
#loc584 = loc("convolution.631_weight"(#loc50))
#loc585 = loc("convolution.631_prepare_conv2d_weight"(#loc50))
#loc586 = loc("convolution.631_prepare_conv2d_bias"(#loc50))
#loc587 = loc("batch-norm-inference.1661_reshape"(#loc51))
#loc588 = loc("convolution.1660_multiply_in_0_layout"(#loc52))
#loc589 = loc("convolution.1660_multiply"(#loc52))
#loc590 = loc("convolution.1660_bias"(#loc52))
#loc591 = loc("convolution.1660_weight"(#loc52))
#loc592 = loc("convolution.1660_prepare_conv2d_weight"(#loc52))
#loc593 = loc("convolution.1660_prepare_conv2d_bias"(#loc52))
#loc594 = loc("batch-norm-inference.1226_reshape"(#loc53))
#loc595 = loc("convolution.1225_multiply_in_0_layout"(#loc54))
#loc596 = loc("convolution.1225_multiply"(#loc54))
#loc597 = loc("convolution.1225_bias"(#loc54))
#loc598 = loc("convolution.1225_weight"(#loc54))
#loc599 = loc("convolution.1225_prepare_conv2d_weight"(#loc54))
#loc600 = loc("convolution.1225_prepare_conv2d_bias"(#loc54))
#loc601 = loc("batch-norm-inference.1403_reshape"(#loc55))
#loc602 = loc("convolution.1402_multiply_in_0_layout"(#loc56))
#loc603 = loc("convolution.1402_multiply"(#loc56))
#loc604 = loc("convolution.1402_bias"(#loc56))
#loc605 = loc("convolution.1402_weight"(#loc56))
#loc606 = loc("convolution.1402_prepare_conv2d_weight"(#loc56))
#loc607 = loc("convolution.1402_prepare_conv2d_bias"(#loc56))
#loc608 = loc("batch-norm-inference.551_reshape"(#loc57))
#loc609 = loc("convolution.550_multiply_in_0_layout"(#loc58))
#loc610 = loc("convolution.550_multiply"(#loc58))
#loc611 = loc("convolution.550_bias"(#loc58))
#loc612 = loc("convolution.550_weight"(#loc58))
#loc613 = loc("convolution.550_prepare_conv2d_weight"(#loc58))
#loc614 = loc("convolution.550_prepare_conv2d_bias"(#loc58))
#loc615 = loc("batch-norm-inference.124_reshape"(#loc59))
#loc616 = loc("convolution.123_multiply_in_0_layout"(#loc60))
#loc617 = loc("convolution.123_multiply"(#loc60))
#loc618 = loc("convolution.123_bias"(#loc60))
#loc619 = loc("convolution.123_weight"(#loc60))
#loc620 = loc("convolution.123_prepare_conv2d_weight"(#loc60))
#loc621 = loc("convolution.123_prepare_conv2d_bias"(#loc60))
#loc622 = loc("batch-norm-inference.1493_reshape"(#loc61))
#loc623 = loc("convolution.1492_multiply_in_0_layout"(#loc62))
#loc624 = loc("convolution.1492_multiply"(#loc62))
#loc625 = loc("convolution.1492_bias"(#loc62))
#loc626 = loc("convolution.1492_weight"(#loc62))
#loc627 = loc("convolution.1492_prepare_conv2d_weight"(#loc62))
#loc628 = loc("convolution.1492_prepare_conv2d_bias"(#loc62))
#loc629 = loc("batch-norm-inference.374_reshape"(#loc63))
#loc630 = loc("convolution.373_multiply_in_0_layout"(#loc64))
#loc631 = loc("convolution.373_multiply"(#loc64))
#loc632 = loc("convolution.373_bias"(#loc64))
#loc633 = loc("convolution.373_weight"(#loc64))
#loc634 = loc("convolution.373_prepare_conv2d_weight"(#loc64))
#loc635 = loc("convolution.373_prepare_conv2d_bias"(#loc64))
#loc636 = loc("batch-norm-inference.1142_reshape"(#loc65))
#loc637 = loc("convolution.1141_multiply_in_0_layout"(#loc66))
#loc638 = loc("convolution.1141_multiply"(#loc66))
#loc639 = loc("convolution.1141_bias"(#loc66))
#loc640 = loc("convolution.1141_weight"(#loc66))
#loc641 = loc("convolution.1141_prepare_conv2d_weight"(#loc66))
#loc642 = loc("convolution.1141_prepare_conv2d_bias"(#loc66))
#loc643 = loc("batch-norm-inference.1235_reshape"(#loc67))
#loc644 = loc("convolution.1234_multiply_in_0_layout"(#loc68))
#loc645 = loc("convolution.1234_multiply"(#loc68))
#loc646 = loc("convolution.1234_bias"(#loc68))
#loc647 = loc("convolution.1234_weight"(#loc68))
#loc648 = loc("convolution.1234_prepare_conv2d_weight"(#loc68))
#loc649 = loc("convolution.1234_prepare_conv2d_bias"(#loc68))
#loc650 = loc("batch-norm-inference.1301_reshape"(#loc69))
#loc651 = loc("convolution.1300_multiply_in_0_layout"(#loc70))
#loc652 = loc("convolution.1300_multiply"(#loc70))
#loc653 = loc("convolution.1300_bias"(#loc70))
#loc654 = loc("convolution.1300_weight"(#loc70))
#loc655 = loc("convolution.1300_prepare_conv2d_weight"(#loc70))
#loc656 = loc("convolution.1300_prepare_conv2d_bias"(#loc70))
#loc657 = loc("batch-norm-inference.716_reshape"(#loc71))
#loc658 = loc("convolution.715_multiply_in_0_layout"(#loc72))
#loc659 = loc("convolution.715_multiply"(#loc72))
#loc660 = loc("convolution.715_bias"(#loc72))
#loc661 = loc("convolution.715_weight"(#loc72))
#loc662 = loc("convolution.715_prepare_conv2d_weight"(#loc72))
#loc663 = loc("convolution.715_prepare_conv2d_bias"(#loc72))
#loc664 = loc("batch-norm-inference.1217_reshape"(#loc73))
#loc665 = loc("convolution.1216_multiply_in_0_layout"(#loc74))
#loc666 = loc("convolution.1216_multiply"(#loc74))
#loc667 = loc("convolution.1216_bias"(#loc74))
#loc668 = loc("convolution.1216_weight"(#loc74))
#loc669 = loc("convolution.1216_prepare_conv2d_weight"(#loc74))
#loc670 = loc("convolution.1216_prepare_conv2d_bias"(#loc74))
#loc671 = loc("batch-norm-inference.965_reshape"(#loc75))
#loc672 = loc("convolution.964_multiply_in_0_layout"(#loc76))
#loc673 = loc("convolution.964_multiply"(#loc76))
#loc674 = loc("convolution.964_bias"(#loc76))
#loc675 = loc("convolution.964_weight"(#loc76))
#loc676 = loc("convolution.964_prepare_conv2d_weight"(#loc76))
#loc677 = loc("convolution.964_prepare_conv2d_bias"(#loc76))
#loc678 = loc("batch-norm-inference.563_reshape"(#loc77))
#loc679 = loc("convolution.562_multiply_in_0_layout"(#loc78))
#loc680 = loc("convolution.562_multiply"(#loc78))
#loc681 = loc("convolution.562_bias"(#loc78))
#loc682 = loc("convolution.562_weight"(#loc78))
#loc683 = loc("convolution.562_prepare_conv2d_weight"(#loc78))
#loc684 = loc("convolution.562_prepare_conv2d_bias"(#loc78))
#loc685 = loc("batch-norm-inference.383_reshape"(#loc79))
#loc686 = loc("convolution.382_multiply_in_0_layout"(#loc80))
#loc687 = loc("convolution.382_multiply"(#loc80))
#loc688 = loc("convolution.382_bias"(#loc80))
#loc689 = loc("convolution.382_weight"(#loc80))
#loc690 = loc("convolution.382_prepare_conv2d_weight"(#loc80))
#loc691 = loc("convolution.382_prepare_conv2d_bias"(#loc80))
#loc692 = loc("batch-norm-inference.1319_reshape"(#loc81))
#loc693 = loc("convolution.1318_multiply_in_0_layout"(#loc82))
#loc694 = loc("convolution.1318_multiply"(#loc82))
#loc695 = loc("convolution.1318_bias"(#loc82))
#loc696 = loc("convolution.1318_weight"(#loc82))
#loc697 = loc("convolution.1318_prepare_conv2d_weight"(#loc82))
#loc698 = loc("convolution.1318_prepare_conv2d_bias"(#loc82))
#loc699 = loc("batch-norm-inference.1058_reshape"(#loc84))
#loc700 = loc("convolution.1057_multiply_in_0_layout"(#loc85))
#loc701 = loc("convolution.1057_multiply"(#loc85))
#loc702 = loc("convolution.1057_bias"(#loc85))
#loc703 = loc("convolution.1057_weight"(#loc85))
#loc704 = loc("convolution.1057_prepare_conv2d_weight"(#loc85))
#loc705 = loc("convolution.1057_prepare_conv2d_bias"(#loc85))
#loc706 = loc("batch-norm-inference.800_reshape"(#loc86))
#loc707 = loc("convolution.799_multiply_in_0_layout"(#loc87))
#loc708 = loc("convolution.799_multiply"(#loc87))
#loc709 = loc("convolution.799_bias"(#loc87))
#loc710 = loc("convolution.799_weight"(#loc87))
#loc711 = loc("convolution.799_prepare_conv2d_weight"(#loc87))
#loc712 = loc("convolution.799_prepare_conv2d_bias"(#loc87))
#loc713 = loc("batch-norm-inference.1385_reshape"(#loc88))
#loc714 = loc("convolution.1384_multiply_in_0_layout"(#loc89))
#loc715 = loc("convolution.1384_multiply"(#loc89))
#loc716 = loc("convolution.1384_bias"(#loc89))
#loc717 = loc("convolution.1384_weight"(#loc89))
#loc718 = loc("convolution.1384_prepare_conv2d_weight"(#loc89))
#loc719 = loc("convolution.1384_prepare_conv2d_bias"(#loc89))
#loc720 = loc("batch-norm-inference.875_reshape"(#loc90))
#loc721 = loc("convolution.874_multiply_in_0_layout"(#loc91))
#loc722 = loc("convolution.874_multiply"(#loc91))
#loc723 = loc("convolution.874_bias"(#loc91))
#loc724 = loc("convolution.874_weight"(#loc91))
#loc725 = loc("convolution.874_prepare_conv2d_weight"(#loc91))
#loc726 = loc("convolution.874_prepare_conv2d_bias"(#loc91))
#loc727 = loc("batch-norm-inference.1484_reshape"(#loc92))
#loc728 = loc("convolution.1483_multiply_in_0_layout"(#loc93))
#loc729 = loc("convolution.1483_multiply"(#loc93))
#loc730 = loc("convolution.1483_bias"(#loc93))
#loc731 = loc("convolution.1483_weight"(#loc93))
#loc732 = loc("convolution.1483_prepare_conv2d_weight"(#loc93))
#loc733 = loc("convolution.1483_prepare_conv2d_bias"(#loc93))
#loc734 = loc("batch-norm-inference.983_reshape"(#loc94))
#loc735 = loc("convolution.982_multiply_in_0_layout"(#loc95))
#loc736 = loc("convolution.982_multiply"(#loc95))
#loc737 = loc("convolution.982_bias"(#loc95))
#loc738 = loc("convolution.982_weight"(#loc95))
#loc739 = loc("convolution.982_prepare_conv2d_weight"(#loc95))
#loc740 = loc("convolution.982_prepare_conv2d_bias"(#loc95))
#loc741 = loc("batch-norm-inference.1568_reshape"(#loc96))
#loc742 = loc("convolution.1567_multiply_in_0_layout"(#loc97))
#loc743 = loc("convolution.1567_multiply"(#loc97))
#loc744 = loc("convolution.1567_bias"(#loc97))
#loc745 = loc("convolution.1567_weight"(#loc97))
#loc746 = loc("convolution.1567_prepare_conv2d_weight"(#loc97))
#loc747 = loc("convolution.1567_prepare_conv2d_bias"(#loc97))
#loc748 = loc("batch-norm-inference.1151_reshape"(#loc98))
#loc749 = loc("convolution.1150_multiply_in_0_layout"(#loc99))
#loc750 = loc("convolution.1150_multiply"(#loc99))
#loc751 = loc("convolution.1150_bias"(#loc99))
#loc752 = loc("convolution.1150_weight"(#loc99))
#loc753 = loc("convolution.1150_prepare_conv2d_weight"(#loc99))
#loc754 = loc("convolution.1150_prepare_conv2d_bias"(#loc99))
#loc755 = loc("batch-norm-inference.725_reshape"(#loc100))
#loc756 = loc("convolution.724_multiply_in_0_layout"(#loc101))
#loc757 = loc("convolution.724_multiply"(#loc101))
#loc758 = loc("convolution.724_bias"(#loc101))
#loc759 = loc("convolution.724_weight"(#loc101))
#loc760 = loc("convolution.724_prepare_conv2d_weight"(#loc101))
#loc761 = loc("convolution.724_prepare_conv2d_bias"(#loc101))
#loc762 = loc("batch-norm-inference.1577_reshape"(#loc102))
#loc763 = loc("convolution.1576_multiply_in_0_layout"(#loc103))
#loc764 = loc("convolution.1576_multiply"(#loc103))
#loc765 = loc("convolution.1576_bias"(#loc103))
#loc766 = loc("convolution.1576_weight"(#loc103))
#loc767 = loc("convolution.1576_prepare_conv2d_weight"(#loc103))
#loc768 = loc("convolution.1576_prepare_conv2d_bias"(#loc103))
#loc769 = loc("batch-norm-inference.905_reshape"(#loc104))
#loc770 = loc("convolution.904_multiply_in_0_layout"(#loc105))
#loc771 = loc("convolution.904_multiply"(#loc105))
#loc772 = loc("convolution.904_bias"(#loc105))
#loc773 = loc("convolution.904_weight"(#loc105))
#loc774 = loc("convolution.904_prepare_conv2d_weight"(#loc105))
#loc775 = loc("convolution.904_prepare_conv2d_bias"(#loc105))
#loc776 = loc("batch-norm-inference.1652_reshape"(#loc106))
#loc777 = loc("convolution.1651_multiply_in_0_layout"(#loc107))
#loc778 = loc("convolution.1651_multiply"(#loc107))
#loc779 = loc("convolution.1651_bias"(#loc107))
#loc780 = loc("convolution.1651_weight"(#loc107))
#loc781 = loc("convolution.1651_prepare_conv2d_weight"(#loc107))
#loc782 = loc("convolution.1651_prepare_conv2d_bias"(#loc107))
#loc783 = loc("convolution.123_input"(#loc60))
#loc784 = loc("reduce-window.143_revert_layout"(#loc376))
#loc785 = loc("convolution.304_mem_reconfig"(#loc34))
#loc786 = loc("convolution.304_spill"(#loc34))
#loc787 = loc("convolution.364_mem_reconfig"(#loc32))
#loc788 = loc("maximum.393_spill"(#loc378))
#loc789 = loc("maximum.477_spill"(#loc380))
#loc790 = loc("convolution.562_mem_reconfig"(#loc78))
#loc791 = loc("convolution.562_spill"(#loc78))
#loc792 = loc("convolution.631_mem_reconfig"(#loc50))
#loc793 = loc("maximum.651_spill"(#loc384))
#loc794 = loc("maximum.735_spill"(#loc386))
#loc795 = loc("maximum.819_spill"(#loc388))
#loc796 = loc("convolution.904_mem_reconfig"(#loc105))
#loc797 = loc("convolution.904_spill"(#loc105))
#loc798 = loc("convolution.964_mem_reconfig"(#loc76))
#loc799 = loc("convolution.973_mem_reconfig"(#loc14))
#loc800 = loc("maximum.993_spill"(#loc392))
#loc801 = loc("maximum.1077_spill"(#loc394))
#loc802 = loc("maximum.1161_spill"(#loc396))
#loc803 = loc("maximum.1245_spill"(#loc398))
#loc804 = loc("maximum.1329_spill"(#loc400))
#loc805 = loc("convolution.1414_spill"(#loc46))
#loc806 = loc("maximum.1503_spill"(#loc404))
#loc807 = loc("maximum.1587_spill"(#loc406))
#loc808 = loc("maximum.1671_spill"(#loc408))
#loc809 = loc("multiply.1687_spill"(#loc410))
#loc810 = loc("convolution.1474_bias_tm1"(#loc415))
#loc811 = loc("convolution.1474_bias_tm0"(#loc415))
#loc812 = loc("convolution.457_bias_tm1"(#loc422))
#loc813 = loc("convolution.457_bias_tm0"(#loc422))
#loc814 = loc("convolution.808_bias_tm1"(#loc429))
#loc815 = loc("convolution.808_bias_tm0"(#loc429))
#loc816 = loc("convolution.1066_bias_tm1"(#loc436))
#loc817 = loc("convolution.1066_bias_tm0"(#loc436))
#loc818 = loc("convolution.448_bias_tm1"(#loc443))
#loc819 = loc("convolution.448_bias_tm0"(#loc443))
#loc820 = loc("convolution.1558_bias_tm1"(#loc450))
#loc821 = loc("convolution.1558_bias_tm0"(#loc450))
#loc822 = loc("convolution.973_bias_tm1"(#loc457))
#loc823 = loc("convolution.973_bias_tm0"(#loc457))
#loc824 = loc("convolution.883_bias_tm1"(#loc464))
#loc825 = loc("convolution.883_bias_tm0"(#loc464))
#loc826 = loc("convolution.466_bias_tm1"(#loc471))
#loc827 = loc("convolution.466_bias_tm0"(#loc471))
#loc828 = loc("convolution.1393_bias_tm1"(#loc478))
#loc829 = loc("convolution.1393_bias_tm0"(#loc478))
#loc830 = loc("convolution.1309_bias_tm1"(#loc485))
#loc831 = loc("convolution.1309_bias_tm0"(#loc485))
#loc832 = loc("convolution.790_bias_tm1"(#loc492))
#loc833 = loc("convolution.790_bias_tm0"(#loc492))
#loc834 = loc("convolution.622_bias_tm1"(#loc499))
#loc835 = loc("convolution.622_bias_tm0"(#loc499))
#loc836 = loc("convolution.541_bias_tm1"(#loc506))
#loc837 = loc("convolution.541_bias_tm0"(#loc506))
#loc838 = loc("convolution.640_bias_tm1"(#loc513))
#loc839 = loc("convolution.640_bias_tm0"(#loc513))
#loc840 = loc("convolution.364_bias_tm1"(#loc520))
#loc841 = loc("convolution.364_bias_tm0"(#loc520))
#loc842 = loc("convolution.304_bias_tm1"(#loc527))
#loc843 = loc("convolution.304_bias_tm0"(#loc527))
#loc844 = loc("convolution.1642_bias_tm1"(#loc534))
#loc845 = loc("convolution.1642_bias_tm0"(#loc534))
#loc846 = loc("convolution.1048_bias_tm1"(#loc541))
#loc847 = loc("convolution.1048_bias_tm0"(#loc541))
#loc848 = loc("convolution.706_bias_tm1"(#loc548))
#loc849 = loc("convolution.706_bias_tm0"(#loc548))
#loc850 = loc("convolution.892_bias_tm1"(#loc555))
#loc851 = loc("convolution.892_bias_tm0"(#loc555))
#loc852 = loc("convolution.532_bias_tm1"(#loc562))
#loc853 = loc("convolution.532_bias_tm0"(#loc562))
#loc854 = loc("convolution.1414_multiply_rhs_permute"(#loc567))
#loc855 = loc("convolution.1414_multiply_lhs_permute"(#loc567))
#loc856 = loc("convolution.1414_multiply_output_permute"(#loc567))
#loc857 = loc("convolution.1414_bias_tm1"(#loc569))
#loc858 = loc("convolution.1414_bias_tm0"(#loc569))
#loc859 = loc("convolution.1132_bias_tm1"(#loc576))
#loc860 = loc("convolution.1132_bias_tm0"(#loc576))
#loc861 = loc("convolution.631_bias_tm1"(#loc583))
#loc862 = loc("convolution.631_bias_tm0"(#loc583))
#loc863 = loc("convolution.1660_bias_tm1"(#loc590))
#loc864 = loc("convolution.1660_bias_tm0"(#loc590))
#loc865 = loc("convolution.1225_bias_tm1"(#loc597))
#loc866 = loc("convolution.1225_bias_tm0"(#loc597))
#loc867 = loc("convolution.1402_bias_tm1"(#loc604))
#loc868 = loc("convolution.1402_bias_tm0"(#loc604))
#loc869 = loc("convolution.550_bias_tm1"(#loc611))
#loc870 = loc("convolution.550_bias_tm0"(#loc611))
#loc871 = loc("convolution.123_bias_tm1"(#loc618))
#loc872 = loc("convolution.123_bias_tm0"(#loc618))
#loc873 = loc("convolution.1492_bias_tm1"(#loc625))
#loc874 = loc("convolution.1492_bias_tm0"(#loc625))
#loc875 = loc("convolution.373_bias_tm1"(#loc632))
#loc876 = loc("convolution.373_bias_tm0"(#loc632))
#loc877 = loc("convolution.1141_bias_tm1"(#loc639))
#loc878 = loc("convolution.1141_bias_tm0"(#loc639))
#loc879 = loc("convolution.1234_bias_tm1"(#loc646))
#loc880 = loc("convolution.1234_bias_tm0"(#loc646))
#loc881 = loc("convolution.1300_bias_tm1"(#loc653))
#loc882 = loc("convolution.1300_bias_tm0"(#loc653))
#loc883 = loc("convolution.715_bias_tm1"(#loc660))
#loc884 = loc("convolution.715_bias_tm0"(#loc660))
#loc885 = loc("convolution.1216_bias_tm1"(#loc667))
#loc886 = loc("convolution.1216_bias_tm0"(#loc667))
#loc887 = loc("convolution.964_bias_tm1"(#loc674))
#loc888 = loc("convolution.964_bias_tm0"(#loc674))
#loc889 = loc("convolution.562_bias_tm1"(#loc681))
#loc890 = loc("convolution.562_bias_tm0"(#loc681))
#loc891 = loc("convolution.382_bias_tm1"(#loc688))
#loc892 = loc("convolution.382_bias_tm0"(#loc688))
#loc893 = loc("convolution.1318_bias_tm1"(#loc695))
#loc894 = loc("convolution.1318_bias_tm0"(#loc695))
#loc895 = loc("convolution.1057_bias_tm1"(#loc702))
#loc896 = loc("convolution.1057_bias_tm0"(#loc702))
#loc897 = loc("convolution.799_bias_tm1"(#loc709))
#loc898 = loc("convolution.799_bias_tm0"(#loc709))
#loc899 = loc("convolution.1384_bias_tm1"(#loc716))
#loc900 = loc("convolution.1384_bias_tm0"(#loc716))
#loc901 = loc("convolution.874_bias_tm1"(#loc723))
#loc902 = loc("convolution.874_bias_tm0"(#loc723))
#loc903 = loc("convolution.1483_bias_tm1"(#loc730))
#loc904 = loc("convolution.1483_bias_tm0"(#loc730))
#loc905 = loc("convolution.982_bias_tm1"(#loc737))
#loc906 = loc("convolution.982_bias_tm0"(#loc737))
#loc907 = loc("convolution.1567_bias_tm1"(#loc744))
#loc908 = loc("convolution.1567_bias_tm0"(#loc744))
#loc909 = loc("convolution.1150_bias_tm1"(#loc751))
#loc910 = loc("convolution.1150_bias_tm0"(#loc751))
#loc911 = loc("convolution.724_bias_tm1"(#loc758))
#loc912 = loc("convolution.724_bias_tm0"(#loc758))
#loc913 = loc("convolution.1576_bias_tm1"(#loc765))
#loc914 = loc("convolution.1576_bias_tm0"(#loc765))
#loc915 = loc("convolution.904_bias_tm1"(#loc772))
#loc916 = loc("convolution.904_bias_tm0"(#loc772))
#loc917 = loc("convolution.1651_bias_tm1"(#loc779))
#loc918 = loc("convolution.1651_bias_tm0"(#loc779))
#loc919 = loc("convolution.123_input_reshape"(#loc783))
#loc920 = loc("convolution.1474_bias_tm1_tm0"(#loc810))
#loc921 = loc("convolution.1474_bias_tm1_tm1"(#loc810))
#loc922 = loc("convolution.457_bias_tm1_tm0"(#loc812))
#loc923 = loc("convolution.457_bias_tm1_tm1"(#loc812))
#loc924 = loc("convolution.808_bias_tm1_tm0"(#loc814))
#loc925 = loc("convolution.808_bias_tm1_tm1"(#loc814))
#loc926 = loc("convolution.1066_bias_tm1_tm0"(#loc816))
#loc927 = loc("convolution.1066_bias_tm1_tm1"(#loc816))
#loc928 = loc("convolution.448_bias_tm1_tm0"(#loc818))
#loc929 = loc("convolution.448_bias_tm1_tm1"(#loc818))
#loc930 = loc("convolution.1558_bias_tm1_tm0"(#loc820))
#loc931 = loc("convolution.1558_bias_tm1_tm1"(#loc820))
#loc932 = loc("convolution.973_bias_tm1_tm0"(#loc822))
#loc933 = loc("convolution.973_bias_tm1_tm1"(#loc822))
#loc934 = loc("convolution.883_bias_tm1_tm0"(#loc824))
#loc935 = loc("convolution.883_bias_tm1_tm1"(#loc824))
#loc936 = loc("convolution.466_bias_tm1_tm0"(#loc826))
#loc937 = loc("convolution.466_bias_tm1_tm1"(#loc826))
#loc938 = loc("convolution.1393_bias_tm1_tm0"(#loc828))
#loc939 = loc("convolution.1393_bias_tm1_tm1"(#loc828))
#loc940 = loc("convolution.1309_bias_tm1_tm0"(#loc830))
#loc941 = loc("convolution.1309_bias_tm1_tm1"(#loc830))
#loc942 = loc("convolution.790_bias_tm1_tm0"(#loc832))
#loc943 = loc("convolution.790_bias_tm1_tm1"(#loc832))
#loc944 = loc("convolution.622_bias_tm1_tm0"(#loc834))
#loc945 = loc("convolution.622_bias_tm1_tm1"(#loc834))
#loc946 = loc("convolution.541_bias_tm1_tm0"(#loc836))
#loc947 = loc("convolution.541_bias_tm1_tm1"(#loc836))
#loc948 = loc("convolution.640_bias_tm1_tm0"(#loc838))
#loc949 = loc("convolution.640_bias_tm1_tm1"(#loc838))
#loc950 = loc("convolution.364_bias_tm1_tm0"(#loc840))
#loc951 = loc("convolution.364_bias_tm1_tm1"(#loc840))
#loc952 = loc("convolution.304_bias_tm1_tm0"(#loc842))
#loc953 = loc("convolution.304_bias_tm1_tm1"(#loc842))
#loc954 = loc("convolution.1642_bias_tm1_tm0"(#loc844))
#loc955 = loc("convolution.1642_bias_tm1_tm1"(#loc844))
#loc956 = loc("convolution.1048_bias_tm1_tm0"(#loc846))
#loc957 = loc("convolution.1048_bias_tm1_tm1"(#loc846))
#loc958 = loc("convolution.706_bias_tm1_tm0"(#loc848))
#loc959 = loc("convolution.706_bias_tm1_tm1"(#loc848))
#loc960 = loc("convolution.892_bias_tm1_tm0"(#loc850))
#loc961 = loc("convolution.892_bias_tm1_tm1"(#loc850))
#loc962 = loc("convolution.532_bias_tm1_tm0"(#loc852))
#loc963 = loc("convolution.532_bias_tm1_tm1"(#loc852))
#loc964 = loc("convolution.1414_bias_tm1_tm0"(#loc857))
#loc965 = loc("convolution.1414_bias_tm1_tm1"(#loc857))
#loc966 = loc("convolution.1132_bias_tm1_tm0"(#loc859))
#loc967 = loc("convolution.1132_bias_tm1_tm1"(#loc859))
#loc968 = loc("convolution.631_bias_tm1_tm0"(#loc861))
#loc969 = loc("convolution.631_bias_tm1_tm1"(#loc861))
#loc970 = loc("convolution.1660_bias_tm1_tm0"(#loc863))
#loc971 = loc("convolution.1660_bias_tm1_tm1"(#loc863))
#loc972 = loc("convolution.1225_bias_tm1_tm0"(#loc865))
#loc973 = loc("convolution.1225_bias_tm1_tm1"(#loc865))
#loc974 = loc("convolution.1402_bias_tm1_tm0"(#loc867))
#loc975 = loc("convolution.1402_bias_tm1_tm1"(#loc867))
#loc976 = loc("convolution.550_bias_tm1_tm0"(#loc869))
#loc977 = loc("convolution.550_bias_tm1_tm1"(#loc869))
#loc978 = loc("convolution.123_bias_tm1_tm0"(#loc871))
#loc979 = loc("convolution.123_bias_tm1_tm1"(#loc871))
#loc980 = loc("convolution.1492_bias_tm1_tm0"(#loc873))
#loc981 = loc("convolution.1492_bias_tm1_tm1"(#loc873))
#loc982 = loc("convolution.373_bias_tm1_tm0"(#loc875))
#loc983 = loc("convolution.373_bias_tm1_tm1"(#loc875))
#loc984 = loc("convolution.1141_bias_tm1_tm0"(#loc877))
#loc985 = loc("convolution.1141_bias_tm1_tm1"(#loc877))
#loc986 = loc("convolution.1234_bias_tm1_tm0"(#loc879))
#loc987 = loc("convolution.1234_bias_tm1_tm1"(#loc879))
#loc988 = loc("convolution.1300_bias_tm1_tm0"(#loc881))
#loc989 = loc("convolution.1300_bias_tm1_tm1"(#loc881))
#loc990 = loc("convolution.715_bias_tm1_tm0"(#loc883))
#loc991 = loc("convolution.715_bias_tm1_tm1"(#loc883))
#loc992 = loc("convolution.1216_bias_tm1_tm0"(#loc885))
#loc993 = loc("convolution.1216_bias_tm1_tm1"(#loc885))
#loc994 = loc("convolution.964_bias_tm1_tm0"(#loc887))
#loc995 = loc("convolution.964_bias_tm1_tm1"(#loc887))
#loc996 = loc("convolution.562_bias_tm1_tm0"(#loc889))
#loc997 = loc("convolution.562_bias_tm1_tm1"(#loc889))
#loc998 = loc("convolution.382_bias_tm1_tm0"(#loc891))
#loc999 = loc("convolution.382_bias_tm1_tm1"(#loc891))
#loc1000 = loc("convolution.1318_bias_tm1_tm0"(#loc893))
#loc1001 = loc("convolution.1318_bias_tm1_tm1"(#loc893))
#loc1002 = loc("convolution.1057_bias_tm1_tm0"(#loc895))
#loc1003 = loc("convolution.1057_bias_tm1_tm1"(#loc895))
#loc1004 = loc("convolution.799_bias_tm1_tm0"(#loc897))
#loc1005 = loc("convolution.799_bias_tm1_tm1"(#loc897))
#loc1006 = loc("convolution.1384_bias_tm1_tm0"(#loc899))
#loc1007 = loc("convolution.1384_bias_tm1_tm1"(#loc899))
#loc1008 = loc("convolution.874_bias_tm1_tm0"(#loc901))
#loc1009 = loc("convolution.874_bias_tm1_tm1"(#loc901))
#loc1010 = loc("convolution.1483_bias_tm1_tm0"(#loc903))
#loc1011 = loc("convolution.1483_bias_tm1_tm1"(#loc903))
#loc1012 = loc("convolution.982_bias_tm1_tm0"(#loc905))
#loc1013 = loc("convolution.982_bias_tm1_tm1"(#loc905))
#loc1014 = loc("convolution.1567_bias_tm1_tm0"(#loc907))
#loc1015 = loc("convolution.1567_bias_tm1_tm1"(#loc907))
#loc1016 = loc("convolution.1150_bias_tm1_tm0"(#loc909))
#loc1017 = loc("convolution.1150_bias_tm1_tm1"(#loc909))
#loc1018 = loc("convolution.724_bias_tm1_tm0"(#loc911))
#loc1019 = loc("convolution.724_bias_tm1_tm1"(#loc911))
#loc1020 = loc("convolution.1576_bias_tm1_tm0"(#loc913))
#loc1021 = loc("convolution.1576_bias_tm1_tm1"(#loc913))
#loc1022 = loc("convolution.904_bias_tm1_tm0"(#loc915))
#loc1023 = loc("convolution.904_bias_tm1_tm1"(#loc915))
#loc1024 = loc("convolution.1651_bias_tm1_tm0"(#loc917))
#loc1025 = loc("convolution.1651_bias_tm1_tm1"(#loc917))
