#loc1 = loc("p0.3")
#loc2 = loc("p1.10")
#loc3 = loc("p2.18")
#loc4 = loc("p3.22")
#loc5 = loc("p4.26")
#loc6 = loc("p5.30")
#loc7 = loc("p6.34")
#loc8 = loc("p7.42")
#loc9 = loc("p8.46")
#loc10 = loc("p9.50")
#loc11 = loc("p10.54")
#loc12 = loc("p11.58")
#loc13 = loc("p12.64")
#loc14 = loc("p13.68")
#loc15 = loc("p14.72")
#loc16 = loc("p15.76")
#loc17 = loc("p16.80")
#loc18 = loc("p17.85")
#loc19 = loc("p18.89")
#loc20 = loc("p19.93")
#loc21 = loc("p20.97")
#loc22 = loc("p21.101")
#loc23 = loc("p22.103")
#loc24 = loc("p23.107")
#loc25 = loc("p24.111")
#loc26 = loc("p25.115")
#loc27 = loc("p26.119")
#loc28 = loc("p27.121")
#loc29 = loc("p28.310")
#loc30 = loc("p29.314")
#loc31 = loc("p30.318")
#loc32 = loc("p31.322")
#loc33 = loc("p32.326")
#loc34 = loc("p33.328")
#loc35 = loc("p34.332")
#loc36 = loc("p35.336")
#loc37 = loc("p36.340")
#loc38 = loc("p37.344")
#loc39 = loc("p38.346")
#loc40 = loc("p39.350")
#loc41 = loc("p40.354")
#loc42 = loc("p41.358")
#loc43 = loc("p42.362")
#loc44 = loc("p43.394")
#loc45 = loc("p44.398")
#loc46 = loc("p45.402")
#loc47 = loc("p46.406")
#loc48 = loc("p47.410")
#loc49 = loc("p48.412")
#loc50 = loc("p49.416")
#loc51 = loc("p50.420")
#loc52 = loc("p51.424")
#loc53 = loc("p52.428")
#loc54 = loc("p53.430")
#loc55 = loc("p54.434")
#loc56 = loc("p55.438")
#loc57 = loc("p56.442")
#loc58 = loc("p57.446")
#loc59 = loc("p58.478")
#loc60 = loc("p59.482")
#loc61 = loc("p60.486")
#loc62 = loc("p61.490")
#loc63 = loc("p62.494")
#loc64 = loc("p63.496")
#loc65 = loc("p64.500")
#loc66 = loc("p65.504")
#loc67 = loc("p66.508")
#loc68 = loc("p67.512")
#loc69 = loc("p68.514")
#loc70 = loc("p69.518")
#loc71 = loc("p70.522")
#loc72 = loc("p71.526")
#loc73 = loc("p72.530")
#loc74 = loc("p73.568")
#loc75 = loc("p74.572")
#loc76 = loc("p75.576")
#loc77 = loc("p76.580")
#loc78 = loc("p77.584")
#loc79 = loc("p78.586")
#loc80 = loc("p79.590")
#loc81 = loc("p80.594")
#loc82 = loc("p81.598")
#loc83 = loc("p82.602")
#loc84 = loc("p83.604")
#loc85 = loc("p84.608")
#loc86 = loc("p85.612")
#loc87 = loc("p86.616")
#loc88 = loc("p87.620")
#loc89 = loc("p88.652")
#loc90 = loc("p89.656")
#loc91 = loc("p90.660")
#loc92 = loc("p91.664")
#loc93 = loc("p92.668")
#loc94 = loc("p93.670")
#loc95 = loc("p94.674")
#loc96 = loc("p95.678")
#loc97 = loc("p96.682")
#loc98 = loc("p97.686")
#loc99 = loc("p98.688")
#loc100 = loc("p99.692")
#loc101 = loc("p100.696")
#loc102 = loc("p101.700")
#loc103 = loc("p102.704")
#loc104 = loc("p103.736")
#loc105 = loc("p104.740")
#loc106 = loc("p105.744")
#loc107 = loc("p106.748")
#loc108 = loc("p107.752")
#loc109 = loc("p108.754")
#loc110 = loc("p109.758")
#loc111 = loc("p110.762")
#loc112 = loc("p111.766")
#loc113 = loc("p112.770")
#loc114 = loc("p113.772")
#loc115 = loc("p114.776")
#loc116 = loc("p115.780")
#loc117 = loc("p116.784")
#loc118 = loc("p117.788")
#loc119 = loc("p118.820")
#loc120 = loc("p119.824")
#loc121 = loc("p120.828")
#loc122 = loc("p121.832")
#loc123 = loc("p122.836")
#loc124 = loc("p123.838")
#loc125 = loc("p124.842")
#loc126 = loc("p125.846")
#loc127 = loc("p126.850")
#loc128 = loc("p127.854")
#loc129 = loc("p128.856")
#loc130 = loc("p129.860")
#loc131 = loc("p130.864")
#loc132 = loc("p131.868")
#loc133 = loc("p132.872")
#loc134 = loc("p133.910")
#loc135 = loc("p134.914")
#loc136 = loc("p135.918")
#loc137 = loc("p136.922")
#loc138 = loc("p137.926")
#loc139 = loc("p138.928")
#loc140 = loc("p139.932")
#loc141 = loc("p140.936")
#loc142 = loc("p141.940")
#loc143 = loc("p142.944")
#loc144 = loc("p143.946")
#loc145 = loc("p144.950")
#loc146 = loc("p145.954")
#loc147 = loc("p146.958")
#loc148 = loc("p147.962")
#loc149 = loc("p148.994")
#loc150 = loc("p149.998")
#loc151 = loc("p150.1002")
#loc152 = loc("p151.1006")
#loc153 = loc("p152.1010")
#loc154 = loc("p153.1012")
#loc155 = loc("p154.1016")
#loc156 = loc("p155.1020")
#loc157 = loc("p156.1024")
#loc158 = loc("p157.1028")
#loc159 = loc("p158.1030")
#loc160 = loc("p159.1034")
#loc161 = loc("p160.1038")
#loc162 = loc("p161.1042")
#loc163 = loc("p162.1046")
#loc164 = loc("p163.1078")
#loc165 = loc("p164.1082")
#loc166 = loc("p165.1086")
#loc167 = loc("p166.1090")
#loc168 = loc("p167.1094")
#loc169 = loc("p168.1096")
#loc170 = loc("p169.1100")
#loc171 = loc("p170.1104")
#loc172 = loc("p171.1108")
#loc173 = loc("p172.1112")
#loc174 = loc("p173.1114")
#loc175 = loc("p174.1118")
#loc176 = loc("p175.1122")
#loc177 = loc("p176.1126")
#loc178 = loc("p177.1130")
#loc179 = loc("p178.1162")
#loc180 = loc("p179.1166")
#loc181 = loc("p180.1170")
#loc182 = loc("p181.1174")
#loc183 = loc("p182.1178")
#loc184 = loc("p183.1180")
#loc185 = loc("p184.1184")
#loc186 = loc("p185.1188")
#loc187 = loc("p186.1192")
#loc188 = loc("p187.1196")
#loc189 = loc("p188.1198")
#loc190 = loc("p189.1202")
#loc191 = loc("p190.1206")
#loc192 = loc("p191.1210")
#loc193 = loc("p192.1214")
#loc194 = loc("p193.1246")
#loc195 = loc("p194.1250")
#loc196 = loc("p195.1254")
#loc197 = loc("p196.1258")
#loc198 = loc("p197.1262")
#loc199 = loc("p198.1264")
#loc200 = loc("p199.1268")
#loc201 = loc("p200.1272")
#loc202 = loc("p201.1276")
#loc203 = loc("p202.1280")
#loc204 = loc("p203.1282")
#loc205 = loc("p204.1286")
#loc206 = loc("p205.1290")
#loc207 = loc("p206.1294")
#loc208 = loc("p207.1298")
#loc209 = loc("p208.1330")
#loc210 = loc("p209.1334")
#loc211 = loc("p210.1338")
#loc212 = loc("p211.1342")
#loc213 = loc("p212.1346")
#loc214 = loc("p213.1348")
#loc215 = loc("p214.1352")
#loc216 = loc("p215.1356")
#loc217 = loc("p216.1360")
#loc218 = loc("p217.1364")
#loc219 = loc("p218.1366")
#loc220 = loc("p219.1370")
#loc221 = loc("p220.1374")
#loc222 = loc("p221.1378")
#loc223 = loc("p222.1382")
#loc224 = loc("p223.1420")
#loc225 = loc("p224.1424")
#loc226 = loc("p225.1428")
#loc227 = loc("p226.1432")
#loc228 = loc("p227.1436")
#loc229 = loc("p228.1438")
#loc230 = loc("p229.1442")
#loc231 = loc("p230.1446")
#loc232 = loc("p231.1450")
#loc233 = loc("p232.1454")
#loc234 = loc("p233.1456")
#loc235 = loc("p234.1460")
#loc236 = loc("p235.1464")
#loc237 = loc("p236.1468")
#loc238 = loc("p237.1472")
#loc239 = loc("p238.1504")
#loc240 = loc("p239.1508")
#loc241 = loc("p240.1512")
#loc242 = loc("p241.1516")
#loc243 = loc("p242.1520")
#loc244 = loc("p243.1522")
#loc245 = loc("p244.1526")
#loc246 = loc("p245.1530")
#loc247 = loc("p246.1534")
#loc248 = loc("p247.1538")
#loc249 = loc("p248.1540")
#loc250 = loc("p249.1544")
#loc251 = loc("p250.1548")
#loc252 = loc("p251.1552")
#loc253 = loc("p252.1556")
#loc254 = loc("p253.1588")
#loc255 = loc("p254.1592")
#loc256 = loc("p255.1596")
#loc257 = loc("p256.1600")
#loc258 = loc("p257.1604")
#loc259 = loc("p258.1606")
#loc260 = loc("p259.1610")
#loc261 = loc("p260.1614")
#loc262 = loc("p261.1618")
#loc263 = loc("p262.1622")
#loc264 = loc("p263.1624")
#loc265 = loc("p264.1628")
#loc266 = loc("p265.1632")
#loc267 = loc("p266.1636")
#loc268 = loc("p267.1640")
module @SyncTensorsGraph.1698 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  func.func @main(%arg0: tensor<1000xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___classifier_1_bias"} loc("p0.3"), %arg1: tensor<1000x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___classifier_1_weight"} loc("p1.10"), %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_running_var"} loc("p2.18"), %arg3: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_running_mean"} loc("p3.22"), %arg4: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_bias"} loc("p4.26"), %arg5: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_weight"} loc("p5.30"), %arg6: tensor<2048x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_convolution_weight"} loc("p6.34"), %arg7: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_running_var"} loc("p7.42"), %arg8: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_running_mean"} loc("p8.46"), %arg9: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_bias"} loc("p9.50"), %arg10: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_weight"} loc("p10.54"), %arg11: tensor<1024x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_convolution_weight"} loc("p11.58"), %arg12: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_running_var"} loc("p12.64"), %arg13: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_running_mean"} loc("p13.68"), %arg14: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_bias"} loc("p14.72"), %arg15: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_weight"} loc("p15.76"), %arg16: tensor<512x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_convolution_weight"} loc("p16.80"), %arg17: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_normalization_running_var"} loc("p17.85"), %arg18: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_normalization_running_mean"} loc("p18.89"), %arg19: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_normalization_bias"} loc("p19.93"), %arg20: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_normalization_weight"} loc("p20.97"), %arg21: tensor<256x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_0_layers_0_shortcut_convolution_weight"} loc("p21.101"), %arg22: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_running_var"} loc("p22.103"), %arg23: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_running_mean"} loc("p23.107"), %arg24: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_bias"} loc("p24.111"), %arg25: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_weight"} loc("p25.115"), %arg26: tensor<64x3x7x7xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_convolution_weight"} loc("p26.119"), %arg27: tensor<8x3x224x224xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("p27.121"), %arg28: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___normalization_running_var"} loc("p28.310"), %arg29: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___normalization_running_mean"} loc("p29.314"), %arg30: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___normalization_bias"} loc("p30.318"), %arg31: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___normalization_weight"} loc("p31.322"), %arg32: tensor<256x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___2___convolution_weight"} loc("p32.326"), %arg33: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_running_var"} loc("p33.328"), %arg34: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_running_mean"} loc("p34.332"), %arg35: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_bias"} loc("p35.336"), %arg36: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_weight"} loc("p36.340"), %arg37: tensor<64x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___convolution_weight"} loc("p37.344"), %arg38: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_running_var"} loc("p38.346"), %arg39: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_running_mean"} loc("p39.350"), %arg40: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_bias"} loc("p40.354"), %arg41: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_weight"} loc("p41.358"), %arg42: tensor<64x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___convolution_weight"} loc("p42.362"), %arg43: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___normalization_running_var"} loc("p43.394"), %arg44: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___normalization_running_mean"} loc("p44.398"), %arg45: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___normalization_bias"} loc("p45.402"), %arg46: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___normalization_weight"} loc("p46.406"), %arg47: tensor<256x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___2___convolution_weight"} loc("p47.410"), %arg48: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_running_var"} loc("p48.412"), %arg49: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_running_mean"} loc("p49.416"), %arg50: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_bias"} loc("p50.420"), %arg51: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_weight"} loc("p51.424"), %arg52: tensor<64x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___convolution_weight"} loc("p52.428"), %arg53: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_running_var"} loc("p53.430"), %arg54: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_running_mean"} loc("p54.434"), %arg55: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_bias"} loc("p55.438"), %arg56: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_weight"} loc("p56.442"), %arg57: tensor<64x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___convolution_weight"} loc("p57.446"), %arg58: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___normalization_running_var"} loc("p58.478"), %arg59: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___normalization_running_mean"} loc("p59.482"), %arg60: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___normalization_bias"} loc("p60.486"), %arg61: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___normalization_weight"} loc("p61.490"), %arg62: tensor<256x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___2___convolution_weight"} loc("p62.494"), %arg63: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___normalization_running_var"} loc("p63.496"), %arg64: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___normalization_running_mean"} loc("p64.500"), %arg65: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___normalization_bias"} loc("p65.504"), %arg66: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___normalization_weight"} loc("p66.508"), %arg67: tensor<64x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___1___convolution_weight"} loc("p67.512"), %arg68: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___normalization_running_var"} loc("p68.514"), %arg69: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___normalization_running_mean"} loc("p69.518"), %arg70: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___normalization_bias"} loc("p70.522"), %arg71: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___normalization_weight"} loc("p71.526"), %arg72: tensor<64x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_2_layer___0___convolution_weight"} loc("p72.530"), %arg73: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___normalization_running_var"} loc("p73.568"), %arg74: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___normalization_running_mean"} loc("p74.572"), %arg75: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___normalization_bias"} loc("p75.576"), %arg76: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___normalization_weight"} loc("p76.580"), %arg77: tensor<512x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___2___convolution_weight"} loc("p77.584"), %arg78: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_running_var"} loc("p78.586"), %arg79: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_running_mean"} loc("p79.590"), %arg80: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_bias"} loc("p80.594"), %arg81: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_weight"} loc("p81.598"), %arg82: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___convolution_weight"} loc("p82.602"), %arg83: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_running_var"} loc("p83.604"), %arg84: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_running_mean"} loc("p84.608"), %arg85: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_bias"} loc("p85.612"), %arg86: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_weight"} loc("p86.616"), %arg87: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___convolution_weight"} loc("p87.620"), %arg88: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___normalization_running_var"} loc("p88.652"), %arg89: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___normalization_running_mean"} loc("p89.656"), %arg90: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___normalization_bias"} loc("p90.660"), %arg91: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___normalization_weight"} loc("p91.664"), %arg92: tensor<512x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___2___convolution_weight"} loc("p92.668"), %arg93: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_running_var"} loc("p93.670"), %arg94: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_running_mean"} loc("p94.674"), %arg95: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_bias"} loc("p95.678"), %arg96: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_weight"} loc("p96.682"), %arg97: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___convolution_weight"} loc("p97.686"), %arg98: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_running_var"} loc("p98.688"), %arg99: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_running_mean"} loc("p99.692"), %arg100: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_bias"} loc("p100.696"), %arg101: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_weight"} loc("p101.700"), %arg102: tensor<128x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___convolution_weight"} loc("p102.704"), %arg103: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___normalization_running_var"} loc("p103.736"), %arg104: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___normalization_running_mean"} loc("p104.740"), %arg105: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___normalization_bias"} loc("p105.744"), %arg106: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___normalization_weight"} loc("p106.748"), %arg107: tensor<512x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___2___convolution_weight"} loc("p107.752"), %arg108: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___normalization_running_var"} loc("p108.754"), %arg109: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___normalization_running_mean"} loc("p109.758"), %arg110: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___normalization_bias"} loc("p110.762"), %arg111: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___normalization_weight"} loc("p111.766"), %arg112: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___1___convolution_weight"} loc("p112.770"), %arg113: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___normalization_running_var"} loc("p113.772"), %arg114: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___normalization_running_mean"} loc("p114.776"), %arg115: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___normalization_bias"} loc("p115.780"), %arg116: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___normalization_weight"} loc("p116.784"), %arg117: tensor<128x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_2_layer___0___convolution_weight"} loc("p117.788"), %arg118: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___normalization_running_var"} loc("p118.820"), %arg119: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___normalization_running_mean"} loc("p119.824"), %arg120: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___normalization_bias"} loc("p120.828"), %arg121: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___normalization_weight"} loc("p121.832"), %arg122: tensor<512x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___2___convolution_weight"} loc("p122.836"), %arg123: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___normalization_running_var"} loc("p123.838"), %arg124: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___normalization_running_mean"} loc("p124.842"), %arg125: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___normalization_bias"} loc("p125.846"), %arg126: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___normalization_weight"} loc("p126.850"), %arg127: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___1___convolution_weight"} loc("p127.854"), %arg128: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___normalization_running_var"} loc("p128.856"), %arg129: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___normalization_running_mean"} loc("p129.860"), %arg130: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___normalization_bias"} loc("p130.864"), %arg131: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___normalization_weight"} loc("p131.868"), %arg132: tensor<128x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_3_layer___0___convolution_weight"} loc("p132.872"), %arg133: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___normalization_running_var"} loc("p133.910"), %arg134: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___normalization_running_mean"} loc("p134.914"), %arg135: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___normalization_bias"} loc("p135.918"), %arg136: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___normalization_weight"} loc("p136.922"), %arg137: tensor<1024x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___2___convolution_weight"} loc("p137.926"), %arg138: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_running_var"} loc("p138.928"), %arg139: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_running_mean"} loc("p139.932"), %arg140: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_bias"} loc("p140.936"), %arg141: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_weight"} loc("p141.940"), %arg142: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___convolution_weight"} loc("p142.944"), %arg143: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_running_var"} loc("p143.946"), %arg144: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_running_mean"} loc("p144.950"), %arg145: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_bias"} loc("p145.954"), %arg146: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_weight"} loc("p146.958"), %arg147: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___convolution_weight"} loc("p147.962"), %arg148: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___normalization_running_var"} loc("p148.994"), %arg149: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___normalization_running_mean"} loc("p149.998"), %arg150: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___normalization_bias"} loc("p150.1002"), %arg151: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___normalization_weight"} loc("p151.1006"), %arg152: tensor<1024x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___2___convolution_weight"} loc("p152.1010"), %arg153: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_running_var"} loc("p153.1012"), %arg154: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_running_mean"} loc("p154.1016"), %arg155: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_bias"} loc("p155.1020"), %arg156: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_weight"} loc("p156.1024"), %arg157: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___convolution_weight"} loc("p157.1028"), %arg158: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_running_var"} loc("p158.1030"), %arg159: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_running_mean"} loc("p159.1034"), %arg160: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_bias"} loc("p160.1038"), %arg161: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_weight"} loc("p161.1042"), %arg162: tensor<256x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___convolution_weight"} loc("p162.1046"), %arg163: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___normalization_running_var"} loc("p163.1078"), %arg164: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___normalization_running_mean"} loc("p164.1082"), %arg165: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___normalization_bias"} loc("p165.1086"), %arg166: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___normalization_weight"} loc("p166.1090"), %arg167: tensor<1024x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___2___convolution_weight"} loc("p167.1094"), %arg168: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___normalization_running_var"} loc("p168.1096"), %arg169: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___normalization_running_mean"} loc("p169.1100"), %arg170: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___normalization_bias"} loc("p170.1104"), %arg171: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___normalization_weight"} loc("p171.1108"), %arg172: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___1___convolution_weight"} loc("p172.1112"), %arg173: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___normalization_running_var"} loc("p173.1114"), %arg174: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___normalization_running_mean"} loc("p174.1118"), %arg175: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___normalization_bias"} loc("p175.1122"), %arg176: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___normalization_weight"} loc("p176.1126"), %arg177: tensor<256x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_2_layer___0___convolution_weight"} loc("p177.1130"), %arg178: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___normalization_running_var"} loc("p178.1162"), %arg179: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___normalization_running_mean"} loc("p179.1166"), %arg180: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___normalization_bias"} loc("p180.1170"), %arg181: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___normalization_weight"} loc("p181.1174"), %arg182: tensor<1024x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___2___convolution_weight"} loc("p182.1178"), %arg183: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___normalization_running_var"} loc("p183.1180"), %arg184: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___normalization_running_mean"} loc("p184.1184"), %arg185: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___normalization_bias"} loc("p185.1188"), %arg186: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___normalization_weight"} loc("p186.1192"), %arg187: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___1___convolution_weight"} loc("p187.1196"), %arg188: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___normalization_running_var"} loc("p188.1198"), %arg189: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___normalization_running_mean"} loc("p189.1202"), %arg190: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___normalization_bias"} loc("p190.1206"), %arg191: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___normalization_weight"} loc("p191.1210"), %arg192: tensor<256x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_3_layer___0___convolution_weight"} loc("p192.1214"), %arg193: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___normalization_running_var"} loc("p193.1246"), %arg194: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___normalization_running_mean"} loc("p194.1250"), %arg195: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___normalization_bias"} loc("p195.1254"), %arg196: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___normalization_weight"} loc("p196.1258"), %arg197: tensor<1024x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___2___convolution_weight"} loc("p197.1262"), %arg198: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___normalization_running_var"} loc("p198.1264"), %arg199: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___normalization_running_mean"} loc("p199.1268"), %arg200: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___normalization_bias"} loc("p200.1272"), %arg201: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___normalization_weight"} loc("p201.1276"), %arg202: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___1___convolution_weight"} loc("p202.1280"), %arg203: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___normalization_running_var"} loc("p203.1282"), %arg204: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___normalization_running_mean"} loc("p204.1286"), %arg205: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___normalization_bias"} loc("p205.1290"), %arg206: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___normalization_weight"} loc("p206.1294"), %arg207: tensor<256x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_4_layer___0___convolution_weight"} loc("p207.1298"), %arg208: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___normalization_running_var"} loc("p208.1330"), %arg209: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___normalization_running_mean"} loc("p209.1334"), %arg210: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___normalization_bias"} loc("p210.1338"), %arg211: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___normalization_weight"} loc("p211.1342"), %arg212: tensor<1024x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___2___convolution_weight"} loc("p212.1346"), %arg213: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___normalization_running_var"} loc("p213.1348"), %arg214: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___normalization_running_mean"} loc("p214.1352"), %arg215: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___normalization_bias"} loc("p215.1356"), %arg216: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___normalization_weight"} loc("p216.1360"), %arg217: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___1___convolution_weight"} loc("p217.1364"), %arg218: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___normalization_running_var"} loc("p218.1366"), %arg219: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___normalization_running_mean"} loc("p219.1370"), %arg220: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___normalization_bias"} loc("p220.1374"), %arg221: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___normalization_weight"} loc("p221.1378"), %arg222: tensor<256x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_5_layer___0___convolution_weight"} loc("p222.1382"), %arg223: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___normalization_running_var"} loc("p223.1420"), %arg224: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___normalization_running_mean"} loc("p224.1424"), %arg225: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___normalization_bias"} loc("p225.1428"), %arg226: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___normalization_weight"} loc("p226.1432"), %arg227: tensor<2048x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___2___convolution_weight"} loc("p227.1436"), %arg228: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_running_var"} loc("p228.1438"), %arg229: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_running_mean"} loc("p229.1442"), %arg230: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_bias"} loc("p230.1446"), %arg231: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_weight"} loc("p231.1450"), %arg232: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___convolution_weight"} loc("p232.1454"), %arg233: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_running_var"} loc("p233.1456"), %arg234: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_running_mean"} loc("p234.1460"), %arg235: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_bias"} loc("p235.1464"), %arg236: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_weight"} loc("p236.1468"), %arg237: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___convolution_weight"} loc("p237.1472"), %arg238: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___normalization_running_var"} loc("p238.1504"), %arg239: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___normalization_running_mean"} loc("p239.1508"), %arg240: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___normalization_bias"} loc("p240.1512"), %arg241: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___normalization_weight"} loc("p241.1516"), %arg242: tensor<2048x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___2___convolution_weight"} loc("p242.1520"), %arg243: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_running_var"} loc("p243.1522"), %arg244: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_running_mean"} loc("p244.1526"), %arg245: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_bias"} loc("p245.1530"), %arg246: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_weight"} loc("p246.1534"), %arg247: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___convolution_weight"} loc("p247.1538"), %arg248: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_running_var"} loc("p248.1540"), %arg249: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_running_mean"} loc("p249.1544"), %arg250: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_bias"} loc("p250.1548"), %arg251: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_weight"} loc("p251.1552"), %arg252: tensor<512x2048x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___convolution_weight"} loc("p252.1556"), %arg253: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___normalization_running_var"} loc("p253.1588"), %arg254: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___normalization_running_mean"} loc("p254.1592"), %arg255: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___normalization_bias"} loc("p255.1596"), %arg256: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___normalization_weight"} loc("p256.1600"), %arg257: tensor<2048x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___2___convolution_weight"} loc("p257.1604"), %arg258: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___normalization_running_var"} loc("p258.1606"), %arg259: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___normalization_running_mean"} loc("p259.1610"), %arg260: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___normalization_bias"} loc("p260.1614"), %arg261: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___normalization_weight"} loc("p261.1618"), %arg262: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___1___convolution_weight"} loc("p262.1622"), %arg263: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___normalization_running_var"} loc("p263.1624"), %arg264: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___normalization_running_mean"} loc("p264.1628"), %arg265: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___normalization_bias"} loc("p265.1632"), %arg266: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___normalization_weight"} loc("p266.1636"), %arg267: tensor<512x2048x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_2_layer___0___convolution_weight"} loc("p267.1640")) -> (tensor<8x1000xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<2.038570e-02> : tensor<8x2048xbf16>}> : () -> tensor<8x2048xbf16> loc(#loc)
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x2048x7x7xbf16>}> : () -> tensor<8x2048x7x7xbf16> loc(#loc)
    %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x512x7x7xbf16>}> : () -> tensor<8x512x7x7xbf16> loc(#loc)
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x512x14x14xbf16>}> : () -> tensor<8x512x14x14xbf16> loc(#loc)
    %4 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x1024x14x14xbf16>}> : () -> tensor<8x1024x14x14xbf16> loc(#loc)
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x256x14x14xbf16>}> : () -> tensor<8x256x14x14xbf16> loc(#loc)
    %6 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x256x28x28xbf16>}> : () -> tensor<8x256x28x28xbf16> loc(#loc)
    %7 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x512x28x28xbf16>}> : () -> tensor<8x512x28x28xbf16> loc(#loc)
    %8 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x128x28x28xbf16>}> : () -> tensor<8x128x28x28xbf16> loc(#loc)
    %9 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x128x56x56xbf16>}> : () -> tensor<8x128x56x56xbf16> loc(#loc)
    %10 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x256x56x56xbf16>}> : () -> tensor<8x256x56x56xbf16> loc(#loc)
    %11 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x64x56x56xbf16>}> : () -> tensor<8x64x56x56xbf16> loc(#loc)
    %12 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x64x112x112xbf16>}> : () -> tensor<8x64x112x112xbf16> loc(#loc)
    %13 = "ttir.constant"() <{value = dense<0xFF80> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
    %14 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
    %15 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc269)
    %16 = "ttir.reshape"(%arg22, %15) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc269)
    %17 = ttir.empty() : tensor<64xbf16> loc(#loc270)
    %18 = "ttir.reshape"(%16, %17) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc270)
    %19 = ttir.empty() : tensor<8x64x112x112xbf16> loc(#loc271)
    %20 = "ttir.convolution"(%arg27, %arg26, %19) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 3, 3, 3, 3>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<8x3x224x224xbf16>, tensor<64x3x7x7xbf16>, tensor<8x64x112x112xbf16>) -> tensor<8x64x112x112xbf16> loc(#loc271)
    %21 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc272)
    %22 = "ttir.reshape"(%arg25, %21) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc272)
    %23 = ttir.empty() : tensor<64xbf16> loc(#loc273)
    %24 = "ttir.reshape"(%22, %23) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc273)
    %25 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc274)
    %26 = "ttir.reshape"(%arg24, %25) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc274)
    %27 = ttir.empty() : tensor<64xbf16> loc(#loc275)
    %28 = "ttir.reshape"(%26, %27) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc275)
    %29 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc276)
    %30 = "ttir.reshape"(%arg23, %29) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc276)
    %31 = ttir.empty() : tensor<64xbf16> loc(#loc277)
    %32 = "ttir.reshape"(%30, %31) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc277)
    %33 = ttir.empty() : tensor<8x64x112x112xbf16> loc(#loc278)
    %34 = "ttir.batch_norm_inference"(%20, %24, %28, %32, %18, %33) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x64x112x112xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<8x64x112x112xbf16>) -> tensor<8x64x112x112xbf16> loc(#loc278)
    %35 = ttir.empty() : tensor<8x64x112x112xbf16> loc(#loc279)
    %36 = "ttir.maximum"(%34, %12, %35) : (tensor<8x64x112x112xbf16>, tensor<8x64x112x112xbf16>, tensor<8x64x112x112xbf16>) -> tensor<8x64x112x112xbf16> loc(#loc279)
    %37 = ttir.empty() : tensor<8x64x114x114xbf16> loc(#loc280)
    %38 = "ttir.pad"(%36, %37) <{padding = array<i32: 0, 0, 0, 0, 1, 1, 1, 1>, value = 0xFF800000 : f32}> : (tensor<8x64x112x112xbf16>, tensor<8x64x114x114xbf16>) -> tensor<8x64x114x114xbf16> loc(#loc280)
    %39 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc281)
    %40 = "ttir.pooling"(%38, %39) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 3, 3>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<8x64x114x114xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc281)
    %41 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc282)
    %42 = "ttir.reshape"(%arg17, %41) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc282)
    %43 = ttir.empty() : tensor<256xbf16> loc(#loc283)
    %44 = "ttir.reshape"(%42, %43) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc283)
    %45 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc284)
    %46 = "ttir.reshape"(%arg38, %45) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc284)
    %47 = ttir.empty() : tensor<64xbf16> loc(#loc285)
    %48 = "ttir.reshape"(%46, %47) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc285)
    %49 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc286)
    %50 = "ttir.reshape"(%arg33, %49) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc286)
    %51 = ttir.empty() : tensor<64xbf16> loc(#loc287)
    %52 = "ttir.reshape"(%50, %51) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc287)
    %53 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc288)
    %54 = "ttir.reshape"(%arg28, %53) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc288)
    %55 = ttir.empty() : tensor<256xbf16> loc(#loc289)
    %56 = "ttir.reshape"(%54, %55) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc289)
    %57 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc290)
    %58 = "ttir.reshape"(%arg53, %57) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc290)
    %59 = ttir.empty() : tensor<64xbf16> loc(#loc291)
    %60 = "ttir.reshape"(%58, %59) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc291)
    %61 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc292)
    %62 = "ttir.reshape"(%arg48, %61) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc292)
    %63 = ttir.empty() : tensor<64xbf16> loc(#loc293)
    %64 = "ttir.reshape"(%62, %63) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc293)
    %65 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc294)
    %66 = "ttir.reshape"(%arg43, %65) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc294)
    %67 = ttir.empty() : tensor<256xbf16> loc(#loc295)
    %68 = "ttir.reshape"(%66, %67) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc295)
    %69 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc296)
    %70 = "ttir.reshape"(%arg68, %69) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc296)
    %71 = ttir.empty() : tensor<64xbf16> loc(#loc297)
    %72 = "ttir.reshape"(%70, %71) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc297)
    %73 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc298)
    %74 = "ttir.reshape"(%arg63, %73) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc298)
    %75 = ttir.empty() : tensor<64xbf16> loc(#loc299)
    %76 = "ttir.reshape"(%74, %75) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc299)
    %77 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc300)
    %78 = "ttir.reshape"(%arg58, %77) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc300)
    %79 = ttir.empty() : tensor<256xbf16> loc(#loc301)
    %80 = "ttir.reshape"(%78, %79) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc301)
    %81 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc302)
    %82 = "ttir.reshape"(%arg12, %81) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc302)
    %83 = ttir.empty() : tensor<512xbf16> loc(#loc303)
    %84 = "ttir.reshape"(%82, %83) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc303)
    %85 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc304)
    %86 = "ttir.reshape"(%arg83, %85) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc304)
    %87 = ttir.empty() : tensor<128xbf16> loc(#loc305)
    %88 = "ttir.reshape"(%86, %87) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc305)
    %89 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc306)
    %90 = "ttir.reshape"(%arg78, %89) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc306)
    %91 = ttir.empty() : tensor<128xbf16> loc(#loc307)
    %92 = "ttir.reshape"(%90, %91) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc307)
    %93 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc308)
    %94 = "ttir.reshape"(%arg73, %93) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc308)
    %95 = ttir.empty() : tensor<512xbf16> loc(#loc309)
    %96 = "ttir.reshape"(%94, %95) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc309)
    %97 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc310)
    %98 = "ttir.reshape"(%arg98, %97) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc310)
    %99 = ttir.empty() : tensor<128xbf16> loc(#loc311)
    %100 = "ttir.reshape"(%98, %99) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc311)
    %101 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc312)
    %102 = "ttir.reshape"(%arg93, %101) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc312)
    %103 = ttir.empty() : tensor<128xbf16> loc(#loc313)
    %104 = "ttir.reshape"(%102, %103) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc313)
    %105 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc314)
    %106 = "ttir.reshape"(%arg88, %105) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc314)
    %107 = ttir.empty() : tensor<512xbf16> loc(#loc315)
    %108 = "ttir.reshape"(%106, %107) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc315)
    %109 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc316)
    %110 = "ttir.reshape"(%arg113, %109) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc316)
    %111 = ttir.empty() : tensor<128xbf16> loc(#loc317)
    %112 = "ttir.reshape"(%110, %111) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc317)
    %113 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc318)
    %114 = "ttir.reshape"(%arg108, %113) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc318)
    %115 = ttir.empty() : tensor<128xbf16> loc(#loc319)
    %116 = "ttir.reshape"(%114, %115) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc319)
    %117 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc320)
    %118 = "ttir.reshape"(%arg103, %117) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc320)
    %119 = ttir.empty() : tensor<512xbf16> loc(#loc321)
    %120 = "ttir.reshape"(%118, %119) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc321)
    %121 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc322)
    %122 = "ttir.reshape"(%arg128, %121) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc322)
    %123 = ttir.empty() : tensor<128xbf16> loc(#loc323)
    %124 = "ttir.reshape"(%122, %123) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc323)
    %125 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc324)
    %126 = "ttir.reshape"(%arg123, %125) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc324)
    %127 = ttir.empty() : tensor<128xbf16> loc(#loc325)
    %128 = "ttir.reshape"(%126, %127) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc325)
    %129 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc326)
    %130 = "ttir.reshape"(%arg118, %129) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc326)
    %131 = ttir.empty() : tensor<512xbf16> loc(#loc327)
    %132 = "ttir.reshape"(%130, %131) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc327)
    %133 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc328)
    %134 = "ttir.reshape"(%arg7, %133) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc328)
    %135 = ttir.empty() : tensor<1024xbf16> loc(#loc329)
    %136 = "ttir.reshape"(%134, %135) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc329)
    %137 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc330)
    %138 = "ttir.reshape"(%arg143, %137) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc330)
    %139 = ttir.empty() : tensor<256xbf16> loc(#loc331)
    %140 = "ttir.reshape"(%138, %139) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc331)
    %141 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc332)
    %142 = "ttir.reshape"(%arg138, %141) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc332)
    %143 = ttir.empty() : tensor<256xbf16> loc(#loc333)
    %144 = "ttir.reshape"(%142, %143) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc333)
    %145 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc334)
    %146 = "ttir.reshape"(%arg133, %145) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc334)
    %147 = ttir.empty() : tensor<1024xbf16> loc(#loc335)
    %148 = "ttir.reshape"(%146, %147) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc335)
    %149 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc336)
    %150 = "ttir.reshape"(%arg158, %149) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc336)
    %151 = ttir.empty() : tensor<256xbf16> loc(#loc337)
    %152 = "ttir.reshape"(%150, %151) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc337)
    %153 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc338)
    %154 = "ttir.reshape"(%arg153, %153) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc338)
    %155 = ttir.empty() : tensor<256xbf16> loc(#loc339)
    %156 = "ttir.reshape"(%154, %155) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc339)
    %157 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc340)
    %158 = "ttir.reshape"(%arg148, %157) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc340)
    %159 = ttir.empty() : tensor<1024xbf16> loc(#loc341)
    %160 = "ttir.reshape"(%158, %159) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc341)
    %161 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc342)
    %162 = "ttir.reshape"(%arg173, %161) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc342)
    %163 = ttir.empty() : tensor<256xbf16> loc(#loc343)
    %164 = "ttir.reshape"(%162, %163) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc343)
    %165 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc344)
    %166 = "ttir.reshape"(%arg168, %165) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc344)
    %167 = ttir.empty() : tensor<256xbf16> loc(#loc345)
    %168 = "ttir.reshape"(%166, %167) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc345)
    %169 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc346)
    %170 = "ttir.reshape"(%arg163, %169) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc346)
    %171 = ttir.empty() : tensor<1024xbf16> loc(#loc347)
    %172 = "ttir.reshape"(%170, %171) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc347)
    %173 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc348)
    %174 = "ttir.reshape"(%arg188, %173) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc348)
    %175 = ttir.empty() : tensor<256xbf16> loc(#loc349)
    %176 = "ttir.reshape"(%174, %175) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc349)
    %177 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc350)
    %178 = "ttir.reshape"(%arg183, %177) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc350)
    %179 = ttir.empty() : tensor<256xbf16> loc(#loc351)
    %180 = "ttir.reshape"(%178, %179) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc351)
    %181 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc352)
    %182 = "ttir.reshape"(%arg178, %181) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc352)
    %183 = ttir.empty() : tensor<1024xbf16> loc(#loc353)
    %184 = "ttir.reshape"(%182, %183) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc353)
    %185 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc354)
    %186 = "ttir.reshape"(%arg203, %185) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc354)
    %187 = ttir.empty() : tensor<256xbf16> loc(#loc355)
    %188 = "ttir.reshape"(%186, %187) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc355)
    %189 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc356)
    %190 = "ttir.reshape"(%arg198, %189) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc356)
    %191 = ttir.empty() : tensor<256xbf16> loc(#loc357)
    %192 = "ttir.reshape"(%190, %191) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc357)
    %193 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc358)
    %194 = "ttir.reshape"(%arg193, %193) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc358)
    %195 = ttir.empty() : tensor<1024xbf16> loc(#loc359)
    %196 = "ttir.reshape"(%194, %195) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc359)
    %197 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc360)
    %198 = "ttir.reshape"(%arg218, %197) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc360)
    %199 = ttir.empty() : tensor<256xbf16> loc(#loc361)
    %200 = "ttir.reshape"(%198, %199) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc361)
    %201 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc362)
    %202 = "ttir.reshape"(%arg213, %201) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc362)
    %203 = ttir.empty() : tensor<256xbf16> loc(#loc363)
    %204 = "ttir.reshape"(%202, %203) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc363)
    %205 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc364)
    %206 = "ttir.reshape"(%arg208, %205) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc364)
    %207 = ttir.empty() : tensor<1024xbf16> loc(#loc365)
    %208 = "ttir.reshape"(%206, %207) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc365)
    %209 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc366)
    %210 = "ttir.reshape"(%arg2, %209) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc366)
    %211 = ttir.empty() : tensor<2048xbf16> loc(#loc367)
    %212 = "ttir.reshape"(%210, %211) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc367)
    %213 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc368)
    %214 = "ttir.reshape"(%arg233, %213) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc368)
    %215 = ttir.empty() : tensor<512xbf16> loc(#loc369)
    %216 = "ttir.reshape"(%214, %215) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc369)
    %217 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc370)
    %218 = "ttir.reshape"(%arg228, %217) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc370)
    %219 = ttir.empty() : tensor<512xbf16> loc(#loc371)
    %220 = "ttir.reshape"(%218, %219) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc371)
    %221 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc372)
    %222 = "ttir.reshape"(%arg223, %221) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc372)
    %223 = ttir.empty() : tensor<2048xbf16> loc(#loc373)
    %224 = "ttir.reshape"(%222, %223) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc373)
    %225 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc374)
    %226 = "ttir.reshape"(%arg248, %225) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc374)
    %227 = ttir.empty() : tensor<512xbf16> loc(#loc375)
    %228 = "ttir.reshape"(%226, %227) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc375)
    %229 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc376)
    %230 = "ttir.reshape"(%arg243, %229) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc376)
    %231 = ttir.empty() : tensor<512xbf16> loc(#loc377)
    %232 = "ttir.reshape"(%230, %231) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc377)
    %233 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc378)
    %234 = "ttir.reshape"(%arg238, %233) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc378)
    %235 = ttir.empty() : tensor<2048xbf16> loc(#loc379)
    %236 = "ttir.reshape"(%234, %235) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc379)
    %237 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc380)
    %238 = "ttir.reshape"(%arg263, %237) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc380)
    %239 = ttir.empty() : tensor<512xbf16> loc(#loc381)
    %240 = "ttir.reshape"(%238, %239) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc381)
    %241 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc382)
    %242 = "ttir.reshape"(%arg258, %241) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc382)
    %243 = ttir.empty() : tensor<512xbf16> loc(#loc383)
    %244 = "ttir.reshape"(%242, %243) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc383)
    %245 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc384)
    %246 = "ttir.reshape"(%arg253, %245) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc384)
    %247 = ttir.empty() : tensor<2048xbf16> loc(#loc385)
    %248 = "ttir.reshape"(%246, %247) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc385)
    %249 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc386)
    %250 = "ttir.convolution"(%40, %arg42, %249) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x64x56x56xbf16>, tensor<64x64x1x1xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc386)
    %251 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc387)
    %252 = "ttir.reshape"(%arg41, %251) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc387)
    %253 = ttir.empty() : tensor<64xbf16> loc(#loc388)
    %254 = "ttir.reshape"(%252, %253) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc388)
    %255 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc389)
    %256 = "ttir.reshape"(%arg40, %255) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc389)
    %257 = ttir.empty() : tensor<64xbf16> loc(#loc390)
    %258 = "ttir.reshape"(%256, %257) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc390)
    %259 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc391)
    %260 = "ttir.reshape"(%arg39, %259) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc391)
    %261 = ttir.empty() : tensor<64xbf16> loc(#loc392)
    %262 = "ttir.reshape"(%260, %261) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc392)
    %263 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc393)
    %264 = "ttir.batch_norm_inference"(%250, %254, %258, %262, %48, %263) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x64x56x56xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc393)
    %265 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc394)
    %266 = "ttir.maximum"(%264, %11, %265) : (tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc394)
    %267 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc395)
    %268 = "ttir.convolution"(%266, %arg37, %267) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x64x56x56xbf16>, tensor<64x64x3x3xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc395)
    %269 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc396)
    %270 = "ttir.reshape"(%arg36, %269) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc396)
    %271 = ttir.empty() : tensor<64xbf16> loc(#loc397)
    %272 = "ttir.reshape"(%270, %271) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc397)
    %273 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc398)
    %274 = "ttir.reshape"(%arg35, %273) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc398)
    %275 = ttir.empty() : tensor<64xbf16> loc(#loc399)
    %276 = "ttir.reshape"(%274, %275) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc399)
    %277 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc400)
    %278 = "ttir.reshape"(%arg34, %277) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc400)
    %279 = ttir.empty() : tensor<64xbf16> loc(#loc401)
    %280 = "ttir.reshape"(%278, %279) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc401)
    %281 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc402)
    %282 = "ttir.batch_norm_inference"(%268, %272, %276, %280, %52, %281) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x64x56x56xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc402)
    %283 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc403)
    %284 = "ttir.maximum"(%282, %11, %283) : (tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc403)
    %285 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc404)
    %286 = "ttir.convolution"(%284, %arg32, %285) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x64x56x56xbf16>, tensor<256x64x1x1xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc404)
    %287 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc405)
    %288 = "ttir.reshape"(%arg31, %287) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc405)
    %289 = ttir.empty() : tensor<256xbf16> loc(#loc406)
    %290 = "ttir.reshape"(%288, %289) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc406)
    %291 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc407)
    %292 = "ttir.reshape"(%arg30, %291) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc407)
    %293 = ttir.empty() : tensor<256xbf16> loc(#loc408)
    %294 = "ttir.reshape"(%292, %293) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc408)
    %295 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc409)
    %296 = "ttir.reshape"(%arg29, %295) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc409)
    %297 = ttir.empty() : tensor<256xbf16> loc(#loc410)
    %298 = "ttir.reshape"(%296, %297) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc410)
    %299 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc411)
    %300 = "ttir.batch_norm_inference"(%286, %290, %294, %298, %56, %299) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x56x56xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc411)
    %301 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc412)
    %302 = "ttir.convolution"(%40, %arg21, %301) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x64x56x56xbf16>, tensor<256x64x1x1xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc412)
    %303 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc413)
    %304 = "ttir.reshape"(%arg20, %303) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc413)
    %305 = ttir.empty() : tensor<256xbf16> loc(#loc414)
    %306 = "ttir.reshape"(%304, %305) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc414)
    %307 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc415)
    %308 = "ttir.reshape"(%arg19, %307) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc415)
    %309 = ttir.empty() : tensor<256xbf16> loc(#loc416)
    %310 = "ttir.reshape"(%308, %309) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc416)
    %311 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc417)
    %312 = "ttir.reshape"(%arg18, %311) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc417)
    %313 = ttir.empty() : tensor<256xbf16> loc(#loc418)
    %314 = "ttir.reshape"(%312, %313) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc418)
    %315 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc419)
    %316 = "ttir.batch_norm_inference"(%302, %306, %310, %314, %44, %315) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x56x56xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc419)
    %317 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc420)
    %318 = "ttir.add"(%300, %316, %317) : (tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc420)
    %319 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc421)
    %320 = "ttir.maximum"(%318, %10, %319) : (tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc421)
    %321 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc422)
    %322 = "ttir.convolution"(%320, %arg57, %321) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x56x56xbf16>, tensor<64x256x1x1xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc422)
    %323 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc423)
    %324 = "ttir.reshape"(%arg56, %323) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc423)
    %325 = ttir.empty() : tensor<64xbf16> loc(#loc424)
    %326 = "ttir.reshape"(%324, %325) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc424)
    %327 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc425)
    %328 = "ttir.reshape"(%arg55, %327) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc425)
    %329 = ttir.empty() : tensor<64xbf16> loc(#loc426)
    %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc426)
    %331 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc427)
    %332 = "ttir.reshape"(%arg54, %331) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc427)
    %333 = ttir.empty() : tensor<64xbf16> loc(#loc428)
    %334 = "ttir.reshape"(%332, %333) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc428)
    %335 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc429)
    %336 = "ttir.batch_norm_inference"(%322, %326, %330, %334, %60, %335) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x64x56x56xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc429)
    %337 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc430)
    %338 = "ttir.maximum"(%336, %11, %337) : (tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc430)
    %339 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc431)
    %340 = "ttir.convolution"(%338, %arg52, %339) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x64x56x56xbf16>, tensor<64x64x3x3xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc431)
    %341 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc432)
    %342 = "ttir.reshape"(%arg51, %341) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc432)
    %343 = ttir.empty() : tensor<64xbf16> loc(#loc433)
    %344 = "ttir.reshape"(%342, %343) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc433)
    %345 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc434)
    %346 = "ttir.reshape"(%arg50, %345) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc434)
    %347 = ttir.empty() : tensor<64xbf16> loc(#loc435)
    %348 = "ttir.reshape"(%346, %347) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc435)
    %349 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc436)
    %350 = "ttir.reshape"(%arg49, %349) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc436)
    %351 = ttir.empty() : tensor<64xbf16> loc(#loc437)
    %352 = "ttir.reshape"(%350, %351) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc437)
    %353 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc438)
    %354 = "ttir.batch_norm_inference"(%340, %344, %348, %352, %64, %353) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x64x56x56xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc438)
    %355 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc439)
    %356 = "ttir.maximum"(%354, %11, %355) : (tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc439)
    %357 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc440)
    %358 = "ttir.convolution"(%356, %arg47, %357) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x64x56x56xbf16>, tensor<256x64x1x1xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc440)
    %359 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc441)
    %360 = "ttir.reshape"(%arg46, %359) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc441)
    %361 = ttir.empty() : tensor<256xbf16> loc(#loc442)
    %362 = "ttir.reshape"(%360, %361) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc442)
    %363 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc443)
    %364 = "ttir.reshape"(%arg45, %363) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc443)
    %365 = ttir.empty() : tensor<256xbf16> loc(#loc444)
    %366 = "ttir.reshape"(%364, %365) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc444)
    %367 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc445)
    %368 = "ttir.reshape"(%arg44, %367) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc445)
    %369 = ttir.empty() : tensor<256xbf16> loc(#loc446)
    %370 = "ttir.reshape"(%368, %369) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc446)
    %371 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc447)
    %372 = "ttir.batch_norm_inference"(%358, %362, %366, %370, %68, %371) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x56x56xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc447)
    %373 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc448)
    %374 = "ttir.add"(%372, %320, %373) : (tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc448)
    %375 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc449)
    %376 = "ttir.maximum"(%374, %10, %375) : (tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc449)
    %377 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc450)
    %378 = "ttir.convolution"(%376, %arg72, %377) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x56x56xbf16>, tensor<64x256x1x1xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc450)
    %379 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc451)
    %380 = "ttir.reshape"(%arg71, %379) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc451)
    %381 = ttir.empty() : tensor<64xbf16> loc(#loc452)
    %382 = "ttir.reshape"(%380, %381) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc452)
    %383 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc453)
    %384 = "ttir.reshape"(%arg70, %383) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc453)
    %385 = ttir.empty() : tensor<64xbf16> loc(#loc454)
    %386 = "ttir.reshape"(%384, %385) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc454)
    %387 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc455)
    %388 = "ttir.reshape"(%arg69, %387) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc455)
    %389 = ttir.empty() : tensor<64xbf16> loc(#loc456)
    %390 = "ttir.reshape"(%388, %389) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc456)
    %391 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc457)
    %392 = "ttir.batch_norm_inference"(%378, %382, %386, %390, %72, %391) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x64x56x56xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc457)
    %393 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc458)
    %394 = "ttir.maximum"(%392, %11, %393) : (tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc458)
    %395 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc459)
    %396 = "ttir.convolution"(%394, %arg67, %395) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x64x56x56xbf16>, tensor<64x64x3x3xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc459)
    %397 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc460)
    %398 = "ttir.reshape"(%arg66, %397) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc460)
    %399 = ttir.empty() : tensor<64xbf16> loc(#loc461)
    %400 = "ttir.reshape"(%398, %399) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc461)
    %401 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc462)
    %402 = "ttir.reshape"(%arg65, %401) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc462)
    %403 = ttir.empty() : tensor<64xbf16> loc(#loc463)
    %404 = "ttir.reshape"(%402, %403) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc463)
    %405 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc464)
    %406 = "ttir.reshape"(%arg64, %405) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc464)
    %407 = ttir.empty() : tensor<64xbf16> loc(#loc465)
    %408 = "ttir.reshape"(%406, %407) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc465)
    %409 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc466)
    %410 = "ttir.batch_norm_inference"(%396, %400, %404, %408, %76, %409) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x64x56x56xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc466)
    %411 = ttir.empty() : tensor<8x64x56x56xbf16> loc(#loc467)
    %412 = "ttir.maximum"(%410, %11, %411) : (tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>, tensor<8x64x56x56xbf16>) -> tensor<8x64x56x56xbf16> loc(#loc467)
    %413 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc468)
    %414 = "ttir.convolution"(%412, %arg62, %413) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x64x56x56xbf16>, tensor<256x64x1x1xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc468)
    %415 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc469)
    %416 = "ttir.reshape"(%arg61, %415) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc469)
    %417 = ttir.empty() : tensor<256xbf16> loc(#loc470)
    %418 = "ttir.reshape"(%416, %417) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc470)
    %419 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc471)
    %420 = "ttir.reshape"(%arg60, %419) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc471)
    %421 = ttir.empty() : tensor<256xbf16> loc(#loc472)
    %422 = "ttir.reshape"(%420, %421) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc472)
    %423 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc473)
    %424 = "ttir.reshape"(%arg59, %423) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc473)
    %425 = ttir.empty() : tensor<256xbf16> loc(#loc474)
    %426 = "ttir.reshape"(%424, %425) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc474)
    %427 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc475)
    %428 = "ttir.batch_norm_inference"(%414, %418, %422, %426, %80, %427) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x56x56xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc475)
    %429 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc476)
    %430 = "ttir.add"(%428, %376, %429) : (tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc476)
    %431 = ttir.empty() : tensor<8x256x56x56xbf16> loc(#loc477)
    %432 = "ttir.maximum"(%430, %10, %431) : (tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>, tensor<8x256x56x56xbf16>) -> tensor<8x256x56x56xbf16> loc(#loc477)
    %433 = ttir.empty() : tensor<8x128x56x56xbf16> loc(#loc478)
    %434 = "ttir.convolution"(%432, %arg87, %433) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x56x56xbf16>, tensor<128x256x1x1xbf16>, tensor<8x128x56x56xbf16>) -> tensor<8x128x56x56xbf16> loc(#loc478)
    %435 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc479)
    %436 = "ttir.reshape"(%arg86, %435) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc479)
    %437 = ttir.empty() : tensor<128xbf16> loc(#loc480)
    %438 = "ttir.reshape"(%436, %437) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc480)
    %439 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc481)
    %440 = "ttir.reshape"(%arg85, %439) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc481)
    %441 = ttir.empty() : tensor<128xbf16> loc(#loc482)
    %442 = "ttir.reshape"(%440, %441) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc482)
    %443 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc483)
    %444 = "ttir.reshape"(%arg84, %443) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc483)
    %445 = ttir.empty() : tensor<128xbf16> loc(#loc484)
    %446 = "ttir.reshape"(%444, %445) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc484)
    %447 = ttir.empty() : tensor<8x128x56x56xbf16> loc(#loc485)
    %448 = "ttir.batch_norm_inference"(%434, %438, %442, %446, %88, %447) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x128x56x56xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<8x128x56x56xbf16>) -> tensor<8x128x56x56xbf16> loc(#loc485)
    %449 = ttir.empty() : tensor<8x128x56x56xbf16> loc(#loc486)
    %450 = "ttir.maximum"(%448, %9, %449) : (tensor<8x128x56x56xbf16>, tensor<8x128x56x56xbf16>, tensor<8x128x56x56xbf16>) -> tensor<8x128x56x56xbf16> loc(#loc486)
    %451 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc487)
    %452 = "ttir.convolution"(%450, %arg82, %451) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<8x128x56x56xbf16>, tensor<128x128x3x3xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc487)
    %453 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc488)
    %454 = "ttir.reshape"(%arg81, %453) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc488)
    %455 = ttir.empty() : tensor<128xbf16> loc(#loc489)
    %456 = "ttir.reshape"(%454, %455) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc489)
    %457 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc490)
    %458 = "ttir.reshape"(%arg80, %457) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc490)
    %459 = ttir.empty() : tensor<128xbf16> loc(#loc491)
    %460 = "ttir.reshape"(%458, %459) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc491)
    %461 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc492)
    %462 = "ttir.reshape"(%arg79, %461) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc492)
    %463 = ttir.empty() : tensor<128xbf16> loc(#loc493)
    %464 = "ttir.reshape"(%462, %463) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc493)
    %465 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc494)
    %466 = "ttir.batch_norm_inference"(%452, %456, %460, %464, %92, %465) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x128x28x28xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc494)
    %467 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc495)
    %468 = "ttir.maximum"(%466, %8, %467) : (tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc495)
    %469 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc496)
    %470 = "ttir.convolution"(%468, %arg77, %469) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x128x28x28xbf16>, tensor<512x128x1x1xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc496)
    %471 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc497)
    %472 = "ttir.reshape"(%arg76, %471) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc497)
    %473 = ttir.empty() : tensor<512xbf16> loc(#loc498)
    %474 = "ttir.reshape"(%472, %473) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc498)
    %475 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc499)
    %476 = "ttir.reshape"(%arg75, %475) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc499)
    %477 = ttir.empty() : tensor<512xbf16> loc(#loc500)
    %478 = "ttir.reshape"(%476, %477) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc500)
    %479 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc501)
    %480 = "ttir.reshape"(%arg74, %479) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc501)
    %481 = ttir.empty() : tensor<512xbf16> loc(#loc502)
    %482 = "ttir.reshape"(%480, %481) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc502)
    %483 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc503)
    %484 = "ttir.batch_norm_inference"(%470, %474, %478, %482, %96, %483) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc503)
    %485 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc504)
    %486 = "ttir.convolution"(%432, %arg16, %485) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<8x256x56x56xbf16>, tensor<512x256x1x1xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc504)
    %487 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc505)
    %488 = "ttir.reshape"(%arg15, %487) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc505)
    %489 = ttir.empty() : tensor<512xbf16> loc(#loc506)
    %490 = "ttir.reshape"(%488, %489) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc506)
    %491 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc507)
    %492 = "ttir.reshape"(%arg14, %491) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc507)
    %493 = ttir.empty() : tensor<512xbf16> loc(#loc508)
    %494 = "ttir.reshape"(%492, %493) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc508)
    %495 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc509)
    %496 = "ttir.reshape"(%arg13, %495) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc509)
    %497 = ttir.empty() : tensor<512xbf16> loc(#loc510)
    %498 = "ttir.reshape"(%496, %497) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc510)
    %499 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc511)
    %500 = "ttir.batch_norm_inference"(%486, %490, %494, %498, %84, %499) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc511)
    %501 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc512)
    %502 = "ttir.add"(%484, %500, %501) : (tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc512)
    %503 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc513)
    %504 = "ttir.maximum"(%502, %7, %503) : (tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc513)
    %505 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc514)
    %506 = "ttir.convolution"(%504, %arg102, %505) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x512x28x28xbf16>, tensor<128x512x1x1xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc514)
    %507 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc515)
    %508 = "ttir.reshape"(%arg101, %507) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc515)
    %509 = ttir.empty() : tensor<128xbf16> loc(#loc516)
    %510 = "ttir.reshape"(%508, %509) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc516)
    %511 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc517)
    %512 = "ttir.reshape"(%arg100, %511) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc517)
    %513 = ttir.empty() : tensor<128xbf16> loc(#loc518)
    %514 = "ttir.reshape"(%512, %513) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc518)
    %515 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc519)
    %516 = "ttir.reshape"(%arg99, %515) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc519)
    %517 = ttir.empty() : tensor<128xbf16> loc(#loc520)
    %518 = "ttir.reshape"(%516, %517) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc520)
    %519 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc521)
    %520 = "ttir.batch_norm_inference"(%506, %510, %514, %518, %100, %519) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x128x28x28xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc521)
    %521 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc522)
    %522 = "ttir.maximum"(%520, %8, %521) : (tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc522)
    %523 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc523)
    %524 = "ttir.convolution"(%522, %arg97, %523) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x128x28x28xbf16>, tensor<128x128x3x3xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc523)
    %525 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc524)
    %526 = "ttir.reshape"(%arg96, %525) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc524)
    %527 = ttir.empty() : tensor<128xbf16> loc(#loc525)
    %528 = "ttir.reshape"(%526, %527) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc525)
    %529 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc526)
    %530 = "ttir.reshape"(%arg95, %529) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc526)
    %531 = ttir.empty() : tensor<128xbf16> loc(#loc527)
    %532 = "ttir.reshape"(%530, %531) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc527)
    %533 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc528)
    %534 = "ttir.reshape"(%arg94, %533) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc528)
    %535 = ttir.empty() : tensor<128xbf16> loc(#loc529)
    %536 = "ttir.reshape"(%534, %535) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc529)
    %537 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc530)
    %538 = "ttir.batch_norm_inference"(%524, %528, %532, %536, %104, %537) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x128x28x28xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc530)
    %539 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc531)
    %540 = "ttir.maximum"(%538, %8, %539) : (tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc531)
    %541 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc532)
    %542 = "ttir.convolution"(%540, %arg92, %541) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x128x28x28xbf16>, tensor<512x128x1x1xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc532)
    %543 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc533)
    %544 = "ttir.reshape"(%arg91, %543) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc533)
    %545 = ttir.empty() : tensor<512xbf16> loc(#loc534)
    %546 = "ttir.reshape"(%544, %545) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc534)
    %547 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc535)
    %548 = "ttir.reshape"(%arg90, %547) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc535)
    %549 = ttir.empty() : tensor<512xbf16> loc(#loc536)
    %550 = "ttir.reshape"(%548, %549) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc536)
    %551 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc537)
    %552 = "ttir.reshape"(%arg89, %551) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc537)
    %553 = ttir.empty() : tensor<512xbf16> loc(#loc538)
    %554 = "ttir.reshape"(%552, %553) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc538)
    %555 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc539)
    %556 = "ttir.batch_norm_inference"(%542, %546, %550, %554, %108, %555) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc539)
    %557 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc540)
    %558 = "ttir.add"(%556, %504, %557) : (tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc540)
    %559 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc541)
    %560 = "ttir.maximum"(%558, %7, %559) : (tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc541)
    %561 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc542)
    %562 = "ttir.convolution"(%560, %arg117, %561) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x512x28x28xbf16>, tensor<128x512x1x1xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc542)
    %563 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc543)
    %564 = "ttir.reshape"(%arg116, %563) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc543)
    %565 = ttir.empty() : tensor<128xbf16> loc(#loc544)
    %566 = "ttir.reshape"(%564, %565) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc544)
    %567 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc545)
    %568 = "ttir.reshape"(%arg115, %567) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc545)
    %569 = ttir.empty() : tensor<128xbf16> loc(#loc546)
    %570 = "ttir.reshape"(%568, %569) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc546)
    %571 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc547)
    %572 = "ttir.reshape"(%arg114, %571) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc547)
    %573 = ttir.empty() : tensor<128xbf16> loc(#loc548)
    %574 = "ttir.reshape"(%572, %573) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc548)
    %575 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc549)
    %576 = "ttir.batch_norm_inference"(%562, %566, %570, %574, %112, %575) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x128x28x28xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc549)
    %577 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc550)
    %578 = "ttir.maximum"(%576, %8, %577) : (tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc550)
    %579 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc551)
    %580 = "ttir.convolution"(%578, %arg112, %579) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x128x28x28xbf16>, tensor<128x128x3x3xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc551)
    %581 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc552)
    %582 = "ttir.reshape"(%arg111, %581) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc552)
    %583 = ttir.empty() : tensor<128xbf16> loc(#loc553)
    %584 = "ttir.reshape"(%582, %583) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc553)
    %585 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc554)
    %586 = "ttir.reshape"(%arg110, %585) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc554)
    %587 = ttir.empty() : tensor<128xbf16> loc(#loc555)
    %588 = "ttir.reshape"(%586, %587) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc555)
    %589 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc556)
    %590 = "ttir.reshape"(%arg109, %589) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc556)
    %591 = ttir.empty() : tensor<128xbf16> loc(#loc557)
    %592 = "ttir.reshape"(%590, %591) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc557)
    %593 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc558)
    %594 = "ttir.batch_norm_inference"(%580, %584, %588, %592, %116, %593) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x128x28x28xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc558)
    %595 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc559)
    %596 = "ttir.maximum"(%594, %8, %595) : (tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc559)
    %597 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc560)
    %598 = "ttir.convolution"(%596, %arg107, %597) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x128x28x28xbf16>, tensor<512x128x1x1xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc560)
    %599 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc561)
    %600 = "ttir.reshape"(%arg106, %599) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc561)
    %601 = ttir.empty() : tensor<512xbf16> loc(#loc562)
    %602 = "ttir.reshape"(%600, %601) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc562)
    %603 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc563)
    %604 = "ttir.reshape"(%arg105, %603) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc563)
    %605 = ttir.empty() : tensor<512xbf16> loc(#loc564)
    %606 = "ttir.reshape"(%604, %605) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc564)
    %607 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc565)
    %608 = "ttir.reshape"(%arg104, %607) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc565)
    %609 = ttir.empty() : tensor<512xbf16> loc(#loc566)
    %610 = "ttir.reshape"(%608, %609) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc566)
    %611 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc567)
    %612 = "ttir.batch_norm_inference"(%598, %602, %606, %610, %120, %611) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc567)
    %613 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc568)
    %614 = "ttir.add"(%612, %560, %613) : (tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc568)
    %615 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc569)
    %616 = "ttir.maximum"(%614, %7, %615) : (tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc569)
    %617 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc570)
    %618 = "ttir.convolution"(%616, %arg132, %617) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x512x28x28xbf16>, tensor<128x512x1x1xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc570)
    %619 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc571)
    %620 = "ttir.reshape"(%arg131, %619) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc571)
    %621 = ttir.empty() : tensor<128xbf16> loc(#loc572)
    %622 = "ttir.reshape"(%620, %621) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc572)
    %623 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc573)
    %624 = "ttir.reshape"(%arg130, %623) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc573)
    %625 = ttir.empty() : tensor<128xbf16> loc(#loc574)
    %626 = "ttir.reshape"(%624, %625) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc574)
    %627 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc575)
    %628 = "ttir.reshape"(%arg129, %627) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc575)
    %629 = ttir.empty() : tensor<128xbf16> loc(#loc576)
    %630 = "ttir.reshape"(%628, %629) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc576)
    %631 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc577)
    %632 = "ttir.batch_norm_inference"(%618, %622, %626, %630, %124, %631) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x128x28x28xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc577)
    %633 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc578)
    %634 = "ttir.maximum"(%632, %8, %633) : (tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc578)
    %635 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc579)
    %636 = "ttir.convolution"(%634, %arg127, %635) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x128x28x28xbf16>, tensor<128x128x3x3xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc579)
    %637 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc580)
    %638 = "ttir.reshape"(%arg126, %637) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc580)
    %639 = ttir.empty() : tensor<128xbf16> loc(#loc581)
    %640 = "ttir.reshape"(%638, %639) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc581)
    %641 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc582)
    %642 = "ttir.reshape"(%arg125, %641) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc582)
    %643 = ttir.empty() : tensor<128xbf16> loc(#loc583)
    %644 = "ttir.reshape"(%642, %643) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc583)
    %645 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc584)
    %646 = "ttir.reshape"(%arg124, %645) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc584)
    %647 = ttir.empty() : tensor<128xbf16> loc(#loc585)
    %648 = "ttir.reshape"(%646, %647) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc585)
    %649 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc586)
    %650 = "ttir.batch_norm_inference"(%636, %640, %644, %648, %128, %649) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x128x28x28xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc586)
    %651 = ttir.empty() : tensor<8x128x28x28xbf16> loc(#loc587)
    %652 = "ttir.maximum"(%650, %8, %651) : (tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>, tensor<8x128x28x28xbf16>) -> tensor<8x128x28x28xbf16> loc(#loc587)
    %653 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc588)
    %654 = "ttir.convolution"(%652, %arg122, %653) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x128x28x28xbf16>, tensor<512x128x1x1xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc588)
    %655 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc589)
    %656 = "ttir.reshape"(%arg121, %655) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc589)
    %657 = ttir.empty() : tensor<512xbf16> loc(#loc590)
    %658 = "ttir.reshape"(%656, %657) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc590)
    %659 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc591)
    %660 = "ttir.reshape"(%arg120, %659) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc591)
    %661 = ttir.empty() : tensor<512xbf16> loc(#loc592)
    %662 = "ttir.reshape"(%660, %661) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc592)
    %663 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc593)
    %664 = "ttir.reshape"(%arg119, %663) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc593)
    %665 = ttir.empty() : tensor<512xbf16> loc(#loc594)
    %666 = "ttir.reshape"(%664, %665) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc594)
    %667 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc595)
    %668 = "ttir.batch_norm_inference"(%654, %658, %662, %666, %132, %667) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc595)
    %669 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc596)
    %670 = "ttir.add"(%668, %616, %669) : (tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc596)
    %671 = ttir.empty() : tensor<8x512x28x28xbf16> loc(#loc597)
    %672 = "ttir.maximum"(%670, %7, %671) : (tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>, tensor<8x512x28x28xbf16>) -> tensor<8x512x28x28xbf16> loc(#loc597)
    %673 = ttir.empty() : tensor<8x256x28x28xbf16> loc(#loc598)
    %674 = "ttir.convolution"(%672, %arg147, %673) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x512x28x28xbf16>, tensor<256x512x1x1xbf16>, tensor<8x256x28x28xbf16>) -> tensor<8x256x28x28xbf16> loc(#loc598)
    %675 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc599)
    %676 = "ttir.reshape"(%arg146, %675) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc599)
    %677 = ttir.empty() : tensor<256xbf16> loc(#loc600)
    %678 = "ttir.reshape"(%676, %677) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc600)
    %679 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc601)
    %680 = "ttir.reshape"(%arg145, %679) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc601)
    %681 = ttir.empty() : tensor<256xbf16> loc(#loc602)
    %682 = "ttir.reshape"(%680, %681) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc602)
    %683 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc603)
    %684 = "ttir.reshape"(%arg144, %683) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc603)
    %685 = ttir.empty() : tensor<256xbf16> loc(#loc604)
    %686 = "ttir.reshape"(%684, %685) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc604)
    %687 = ttir.empty() : tensor<8x256x28x28xbf16> loc(#loc605)
    %688 = "ttir.batch_norm_inference"(%674, %678, %682, %686, %140, %687) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x28x28xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x28x28xbf16>) -> tensor<8x256x28x28xbf16> loc(#loc605)
    %689 = ttir.empty() : tensor<8x256x28x28xbf16> loc(#loc606)
    %690 = "ttir.maximum"(%688, %6, %689) : (tensor<8x256x28x28xbf16>, tensor<8x256x28x28xbf16>, tensor<8x256x28x28xbf16>) -> tensor<8x256x28x28xbf16> loc(#loc606)
    %691 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc607)
    %692 = "ttir.convolution"(%690, %arg142, %691) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<8x256x28x28xbf16>, tensor<256x256x3x3xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc607)
    %693 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc608)
    %694 = "ttir.reshape"(%arg141, %693) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc608)
    %695 = ttir.empty() : tensor<256xbf16> loc(#loc609)
    %696 = "ttir.reshape"(%694, %695) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc609)
    %697 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc610)
    %698 = "ttir.reshape"(%arg140, %697) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc610)
    %699 = ttir.empty() : tensor<256xbf16> loc(#loc611)
    %700 = "ttir.reshape"(%698, %699) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc611)
    %701 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc612)
    %702 = "ttir.reshape"(%arg139, %701) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc612)
    %703 = ttir.empty() : tensor<256xbf16> loc(#loc613)
    %704 = "ttir.reshape"(%702, %703) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc613)
    %705 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc614)
    %706 = "ttir.batch_norm_inference"(%692, %696, %700, %704, %144, %705) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc614)
    %707 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc615)
    %708 = "ttir.maximum"(%706, %5, %707) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc615)
    %709 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc616)
    %710 = "ttir.convolution"(%708, %arg137, %709) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<1024x256x1x1xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc616)
    %711 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc617)
    %712 = "ttir.reshape"(%arg136, %711) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc617)
    %713 = ttir.empty() : tensor<1024xbf16> loc(#loc618)
    %714 = "ttir.reshape"(%712, %713) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc618)
    %715 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc619)
    %716 = "ttir.reshape"(%arg135, %715) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc619)
    %717 = ttir.empty() : tensor<1024xbf16> loc(#loc620)
    %718 = "ttir.reshape"(%716, %717) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc620)
    %719 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc621)
    %720 = "ttir.reshape"(%arg134, %719) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc621)
    %721 = ttir.empty() : tensor<1024xbf16> loc(#loc622)
    %722 = "ttir.reshape"(%720, %721) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc622)
    %723 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc623)
    %724 = "ttir.batch_norm_inference"(%710, %714, %718, %722, %148, %723) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc623)
    %725 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc624)
    %726 = "ttir.convolution"(%672, %arg11, %725) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<8x512x28x28xbf16>, tensor<1024x512x1x1xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc624)
    %727 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc625)
    %728 = "ttir.reshape"(%arg10, %727) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc625)
    %729 = ttir.empty() : tensor<1024xbf16> loc(#loc626)
    %730 = "ttir.reshape"(%728, %729) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc626)
    %731 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc627)
    %732 = "ttir.reshape"(%arg9, %731) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc627)
    %733 = ttir.empty() : tensor<1024xbf16> loc(#loc628)
    %734 = "ttir.reshape"(%732, %733) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc628)
    %735 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc629)
    %736 = "ttir.reshape"(%arg8, %735) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc629)
    %737 = ttir.empty() : tensor<1024xbf16> loc(#loc630)
    %738 = "ttir.reshape"(%736, %737) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc630)
    %739 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc631)
    %740 = "ttir.batch_norm_inference"(%726, %730, %734, %738, %136, %739) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc631)
    %741 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc632)
    %742 = "ttir.add"(%724, %740, %741) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc632)
    %743 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc633)
    %744 = "ttir.maximum"(%742, %4, %743) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc633)
    %745 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc634)
    %746 = "ttir.convolution"(%744, %arg162, %745) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x1024x14x14xbf16>, tensor<256x1024x1x1xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc634)
    %747 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc635)
    %748 = "ttir.reshape"(%arg161, %747) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc635)
    %749 = ttir.empty() : tensor<256xbf16> loc(#loc636)
    %750 = "ttir.reshape"(%748, %749) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc636)
    %751 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc637)
    %752 = "ttir.reshape"(%arg160, %751) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc637)
    %753 = ttir.empty() : tensor<256xbf16> loc(#loc638)
    %754 = "ttir.reshape"(%752, %753) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc638)
    %755 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc639)
    %756 = "ttir.reshape"(%arg159, %755) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc639)
    %757 = ttir.empty() : tensor<256xbf16> loc(#loc640)
    %758 = "ttir.reshape"(%756, %757) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc640)
    %759 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc641)
    %760 = "ttir.batch_norm_inference"(%746, %750, %754, %758, %152, %759) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc641)
    %761 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc642)
    %762 = "ttir.maximum"(%760, %5, %761) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc642)
    %763 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc643)
    %764 = "ttir.convolution"(%762, %arg157, %763) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<256x256x3x3xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc643)
    %765 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc644)
    %766 = "ttir.reshape"(%arg156, %765) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc644)
    %767 = ttir.empty() : tensor<256xbf16> loc(#loc645)
    %768 = "ttir.reshape"(%766, %767) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc645)
    %769 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc646)
    %770 = "ttir.reshape"(%arg155, %769) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc646)
    %771 = ttir.empty() : tensor<256xbf16> loc(#loc647)
    %772 = "ttir.reshape"(%770, %771) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc647)
    %773 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc648)
    %774 = "ttir.reshape"(%arg154, %773) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc648)
    %775 = ttir.empty() : tensor<256xbf16> loc(#loc649)
    %776 = "ttir.reshape"(%774, %775) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc649)
    %777 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc650)
    %778 = "ttir.batch_norm_inference"(%764, %768, %772, %776, %156, %777) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc650)
    %779 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc651)
    %780 = "ttir.maximum"(%778, %5, %779) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc651)
    %781 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc652)
    %782 = "ttir.convolution"(%780, %arg152, %781) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<1024x256x1x1xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc652)
    %783 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc653)
    %784 = "ttir.reshape"(%arg151, %783) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc653)
    %785 = ttir.empty() : tensor<1024xbf16> loc(#loc654)
    %786 = "ttir.reshape"(%784, %785) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc654)
    %787 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc655)
    %788 = "ttir.reshape"(%arg150, %787) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc655)
    %789 = ttir.empty() : tensor<1024xbf16> loc(#loc656)
    %790 = "ttir.reshape"(%788, %789) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc656)
    %791 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc657)
    %792 = "ttir.reshape"(%arg149, %791) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc657)
    %793 = ttir.empty() : tensor<1024xbf16> loc(#loc658)
    %794 = "ttir.reshape"(%792, %793) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc658)
    %795 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc659)
    %796 = "ttir.batch_norm_inference"(%782, %786, %790, %794, %160, %795) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc659)
    %797 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc660)
    %798 = "ttir.add"(%796, %744, %797) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc660)
    %799 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc661)
    %800 = "ttir.maximum"(%798, %4, %799) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc661)
    %801 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc662)
    %802 = "ttir.convolution"(%800, %arg177, %801) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x1024x14x14xbf16>, tensor<256x1024x1x1xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc662)
    %803 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc663)
    %804 = "ttir.reshape"(%arg176, %803) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc663)
    %805 = ttir.empty() : tensor<256xbf16> loc(#loc664)
    %806 = "ttir.reshape"(%804, %805) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc664)
    %807 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc665)
    %808 = "ttir.reshape"(%arg175, %807) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc665)
    %809 = ttir.empty() : tensor<256xbf16> loc(#loc666)
    %810 = "ttir.reshape"(%808, %809) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc666)
    %811 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc667)
    %812 = "ttir.reshape"(%arg174, %811) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc667)
    %813 = ttir.empty() : tensor<256xbf16> loc(#loc668)
    %814 = "ttir.reshape"(%812, %813) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc668)
    %815 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc669)
    %816 = "ttir.batch_norm_inference"(%802, %806, %810, %814, %164, %815) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc669)
    %817 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc670)
    %818 = "ttir.maximum"(%816, %5, %817) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc670)
    %819 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc671)
    %820 = "ttir.convolution"(%818, %arg172, %819) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<256x256x3x3xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc671)
    %821 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc672)
    %822 = "ttir.reshape"(%arg171, %821) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc672)
    %823 = ttir.empty() : tensor<256xbf16> loc(#loc673)
    %824 = "ttir.reshape"(%822, %823) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc673)
    %825 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc674)
    %826 = "ttir.reshape"(%arg170, %825) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc674)
    %827 = ttir.empty() : tensor<256xbf16> loc(#loc675)
    %828 = "ttir.reshape"(%826, %827) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc675)
    %829 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc676)
    %830 = "ttir.reshape"(%arg169, %829) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc676)
    %831 = ttir.empty() : tensor<256xbf16> loc(#loc677)
    %832 = "ttir.reshape"(%830, %831) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc677)
    %833 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc678)
    %834 = "ttir.batch_norm_inference"(%820, %824, %828, %832, %168, %833) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc678)
    %835 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc679)
    %836 = "ttir.maximum"(%834, %5, %835) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc679)
    %837 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc680)
    %838 = "ttir.convolution"(%836, %arg167, %837) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<1024x256x1x1xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc680)
    %839 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc681)
    %840 = "ttir.reshape"(%arg166, %839) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc681)
    %841 = ttir.empty() : tensor<1024xbf16> loc(#loc682)
    %842 = "ttir.reshape"(%840, %841) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc682)
    %843 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc683)
    %844 = "ttir.reshape"(%arg165, %843) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc683)
    %845 = ttir.empty() : tensor<1024xbf16> loc(#loc684)
    %846 = "ttir.reshape"(%844, %845) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc684)
    %847 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc685)
    %848 = "ttir.reshape"(%arg164, %847) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc685)
    %849 = ttir.empty() : tensor<1024xbf16> loc(#loc686)
    %850 = "ttir.reshape"(%848, %849) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc686)
    %851 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc687)
    %852 = "ttir.batch_norm_inference"(%838, %842, %846, %850, %172, %851) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc687)
    %853 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc688)
    %854 = "ttir.add"(%852, %800, %853) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc688)
    %855 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc689)
    %856 = "ttir.maximum"(%854, %4, %855) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc689)
    %857 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc690)
    %858 = "ttir.convolution"(%856, %arg192, %857) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x1024x14x14xbf16>, tensor<256x1024x1x1xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc690)
    %859 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc691)
    %860 = "ttir.reshape"(%arg191, %859) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc691)
    %861 = ttir.empty() : tensor<256xbf16> loc(#loc692)
    %862 = "ttir.reshape"(%860, %861) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc692)
    %863 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc693)
    %864 = "ttir.reshape"(%arg190, %863) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc693)
    %865 = ttir.empty() : tensor<256xbf16> loc(#loc694)
    %866 = "ttir.reshape"(%864, %865) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc694)
    %867 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc695)
    %868 = "ttir.reshape"(%arg189, %867) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc695)
    %869 = ttir.empty() : tensor<256xbf16> loc(#loc696)
    %870 = "ttir.reshape"(%868, %869) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc696)
    %871 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc697)
    %872 = "ttir.batch_norm_inference"(%858, %862, %866, %870, %176, %871) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc697)
    %873 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc698)
    %874 = "ttir.maximum"(%872, %5, %873) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc698)
    %875 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc699)
    %876 = "ttir.convolution"(%874, %arg187, %875) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<256x256x3x3xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc699)
    %877 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc700)
    %878 = "ttir.reshape"(%arg186, %877) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc700)
    %879 = ttir.empty() : tensor<256xbf16> loc(#loc701)
    %880 = "ttir.reshape"(%878, %879) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc701)
    %881 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc702)
    %882 = "ttir.reshape"(%arg185, %881) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc702)
    %883 = ttir.empty() : tensor<256xbf16> loc(#loc703)
    %884 = "ttir.reshape"(%882, %883) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc703)
    %885 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc704)
    %886 = "ttir.reshape"(%arg184, %885) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc704)
    %887 = ttir.empty() : tensor<256xbf16> loc(#loc705)
    %888 = "ttir.reshape"(%886, %887) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc705)
    %889 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc706)
    %890 = "ttir.batch_norm_inference"(%876, %880, %884, %888, %180, %889) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc706)
    %891 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc707)
    %892 = "ttir.maximum"(%890, %5, %891) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc707)
    %893 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc708)
    %894 = "ttir.convolution"(%892, %arg182, %893) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<1024x256x1x1xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc708)
    %895 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc709)
    %896 = "ttir.reshape"(%arg181, %895) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc709)
    %897 = ttir.empty() : tensor<1024xbf16> loc(#loc710)
    %898 = "ttir.reshape"(%896, %897) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc710)
    %899 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc711)
    %900 = "ttir.reshape"(%arg180, %899) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc711)
    %901 = ttir.empty() : tensor<1024xbf16> loc(#loc712)
    %902 = "ttir.reshape"(%900, %901) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc712)
    %903 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc713)
    %904 = "ttir.reshape"(%arg179, %903) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc713)
    %905 = ttir.empty() : tensor<1024xbf16> loc(#loc714)
    %906 = "ttir.reshape"(%904, %905) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc714)
    %907 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc715)
    %908 = "ttir.batch_norm_inference"(%894, %898, %902, %906, %184, %907) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc715)
    %909 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc716)
    %910 = "ttir.add"(%908, %856, %909) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc716)
    %911 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc717)
    %912 = "ttir.maximum"(%910, %4, %911) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc717)
    %913 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc718)
    %914 = "ttir.convolution"(%912, %arg207, %913) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x1024x14x14xbf16>, tensor<256x1024x1x1xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc718)
    %915 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc719)
    %916 = "ttir.reshape"(%arg206, %915) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc719)
    %917 = ttir.empty() : tensor<256xbf16> loc(#loc720)
    %918 = "ttir.reshape"(%916, %917) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc720)
    %919 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc721)
    %920 = "ttir.reshape"(%arg205, %919) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc721)
    %921 = ttir.empty() : tensor<256xbf16> loc(#loc722)
    %922 = "ttir.reshape"(%920, %921) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc722)
    %923 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc723)
    %924 = "ttir.reshape"(%arg204, %923) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc723)
    %925 = ttir.empty() : tensor<256xbf16> loc(#loc724)
    %926 = "ttir.reshape"(%924, %925) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc724)
    %927 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc725)
    %928 = "ttir.batch_norm_inference"(%914, %918, %922, %926, %188, %927) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc725)
    %929 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc726)
    %930 = "ttir.maximum"(%928, %5, %929) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc726)
    %931 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc727)
    %932 = "ttir.convolution"(%930, %arg202, %931) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<256x256x3x3xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc727)
    %933 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc728)
    %934 = "ttir.reshape"(%arg201, %933) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc728)
    %935 = ttir.empty() : tensor<256xbf16> loc(#loc729)
    %936 = "ttir.reshape"(%934, %935) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc729)
    %937 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc730)
    %938 = "ttir.reshape"(%arg200, %937) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc730)
    %939 = ttir.empty() : tensor<256xbf16> loc(#loc731)
    %940 = "ttir.reshape"(%938, %939) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc731)
    %941 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc732)
    %942 = "ttir.reshape"(%arg199, %941) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc732)
    %943 = ttir.empty() : tensor<256xbf16> loc(#loc733)
    %944 = "ttir.reshape"(%942, %943) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc733)
    %945 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc734)
    %946 = "ttir.batch_norm_inference"(%932, %936, %940, %944, %192, %945) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc734)
    %947 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc735)
    %948 = "ttir.maximum"(%946, %5, %947) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc735)
    %949 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc736)
    %950 = "ttir.convolution"(%948, %arg197, %949) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<1024x256x1x1xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc736)
    %951 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc737)
    %952 = "ttir.reshape"(%arg196, %951) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc737)
    %953 = ttir.empty() : tensor<1024xbf16> loc(#loc738)
    %954 = "ttir.reshape"(%952, %953) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc738)
    %955 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc739)
    %956 = "ttir.reshape"(%arg195, %955) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc739)
    %957 = ttir.empty() : tensor<1024xbf16> loc(#loc740)
    %958 = "ttir.reshape"(%956, %957) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc740)
    %959 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc741)
    %960 = "ttir.reshape"(%arg194, %959) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc741)
    %961 = ttir.empty() : tensor<1024xbf16> loc(#loc742)
    %962 = "ttir.reshape"(%960, %961) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc742)
    %963 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc743)
    %964 = "ttir.batch_norm_inference"(%950, %954, %958, %962, %196, %963) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc743)
    %965 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc744)
    %966 = "ttir.add"(%964, %912, %965) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc744)
    %967 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc745)
    %968 = "ttir.maximum"(%966, %4, %967) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc745)
    %969 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc746)
    %970 = "ttir.convolution"(%968, %arg222, %969) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x1024x14x14xbf16>, tensor<256x1024x1x1xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc746)
    %971 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc747)
    %972 = "ttir.reshape"(%arg221, %971) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc747)
    %973 = ttir.empty() : tensor<256xbf16> loc(#loc748)
    %974 = "ttir.reshape"(%972, %973) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc748)
    %975 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc749)
    %976 = "ttir.reshape"(%arg220, %975) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc749)
    %977 = ttir.empty() : tensor<256xbf16> loc(#loc750)
    %978 = "ttir.reshape"(%976, %977) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc750)
    %979 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc751)
    %980 = "ttir.reshape"(%arg219, %979) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc751)
    %981 = ttir.empty() : tensor<256xbf16> loc(#loc752)
    %982 = "ttir.reshape"(%980, %981) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc752)
    %983 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc753)
    %984 = "ttir.batch_norm_inference"(%970, %974, %978, %982, %200, %983) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc753)
    %985 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc754)
    %986 = "ttir.maximum"(%984, %5, %985) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc754)
    %987 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc755)
    %988 = "ttir.convolution"(%986, %arg217, %987) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<256x256x3x3xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc755)
    %989 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc756)
    %990 = "ttir.reshape"(%arg216, %989) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc756)
    %991 = ttir.empty() : tensor<256xbf16> loc(#loc757)
    %992 = "ttir.reshape"(%990, %991) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc757)
    %993 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc758)
    %994 = "ttir.reshape"(%arg215, %993) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc758)
    %995 = ttir.empty() : tensor<256xbf16> loc(#loc759)
    %996 = "ttir.reshape"(%994, %995) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc759)
    %997 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc760)
    %998 = "ttir.reshape"(%arg214, %997) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc760)
    %999 = ttir.empty() : tensor<256xbf16> loc(#loc761)
    %1000 = "ttir.reshape"(%998, %999) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc761)
    %1001 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc762)
    %1002 = "ttir.batch_norm_inference"(%988, %992, %996, %1000, %204, %1001) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x256x14x14xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc762)
    %1003 = ttir.empty() : tensor<8x256x14x14xbf16> loc(#loc763)
    %1004 = "ttir.maximum"(%1002, %5, %1003) : (tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>, tensor<8x256x14x14xbf16>) -> tensor<8x256x14x14xbf16> loc(#loc763)
    %1005 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc764)
    %1006 = "ttir.convolution"(%1004, %arg212, %1005) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x256x14x14xbf16>, tensor<1024x256x1x1xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc764)
    %1007 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc765)
    %1008 = "ttir.reshape"(%arg211, %1007) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc765)
    %1009 = ttir.empty() : tensor<1024xbf16> loc(#loc766)
    %1010 = "ttir.reshape"(%1008, %1009) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc766)
    %1011 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc767)
    %1012 = "ttir.reshape"(%arg210, %1011) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc767)
    %1013 = ttir.empty() : tensor<1024xbf16> loc(#loc768)
    %1014 = "ttir.reshape"(%1012, %1013) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc768)
    %1015 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc769)
    %1016 = "ttir.reshape"(%arg209, %1015) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc769)
    %1017 = ttir.empty() : tensor<1024xbf16> loc(#loc770)
    %1018 = "ttir.reshape"(%1016, %1017) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc770)
    %1019 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc771)
    %1020 = "ttir.batch_norm_inference"(%1006, %1010, %1014, %1018, %208, %1019) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc771)
    %1021 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc772)
    %1022 = "ttir.add"(%1020, %968, %1021) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc772)
    %1023 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc773)
    %1024 = "ttir.maximum"(%1022, %4, %1023) : (tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc773)
    %1025 = ttir.empty() : tensor<8x512x14x14xbf16> loc(#loc774)
    %1026 = "ttir.convolution"(%1024, %arg237, %1025) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x1024x14x14xbf16>, tensor<512x1024x1x1xbf16>, tensor<8x512x14x14xbf16>) -> tensor<8x512x14x14xbf16> loc(#loc774)
    %1027 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc775)
    %1028 = "ttir.reshape"(%arg236, %1027) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc775)
    %1029 = ttir.empty() : tensor<512xbf16> loc(#loc776)
    %1030 = "ttir.reshape"(%1028, %1029) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc776)
    %1031 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc777)
    %1032 = "ttir.reshape"(%arg235, %1031) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc777)
    %1033 = ttir.empty() : tensor<512xbf16> loc(#loc778)
    %1034 = "ttir.reshape"(%1032, %1033) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc778)
    %1035 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc779)
    %1036 = "ttir.reshape"(%arg234, %1035) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc779)
    %1037 = ttir.empty() : tensor<512xbf16> loc(#loc780)
    %1038 = "ttir.reshape"(%1036, %1037) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc780)
    %1039 = ttir.empty() : tensor<8x512x14x14xbf16> loc(#loc781)
    %1040 = "ttir.batch_norm_inference"(%1026, %1030, %1034, %1038, %216, %1039) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x14x14xbf16>) -> tensor<8x512x14x14xbf16> loc(#loc781)
    %1041 = ttir.empty() : tensor<8x512x14x14xbf16> loc(#loc782)
    %1042 = "ttir.maximum"(%1040, %3, %1041) : (tensor<8x512x14x14xbf16>, tensor<8x512x14x14xbf16>, tensor<8x512x14x14xbf16>) -> tensor<8x512x14x14xbf16> loc(#loc782)
    %1043 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc783)
    %1044 = "ttir.convolution"(%1042, %arg232, %1043) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<8x512x14x14xbf16>, tensor<512x512x3x3xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc783)
    %1045 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc784)
    %1046 = "ttir.reshape"(%arg231, %1045) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc784)
    %1047 = ttir.empty() : tensor<512xbf16> loc(#loc785)
    %1048 = "ttir.reshape"(%1046, %1047) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc785)
    %1049 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc786)
    %1050 = "ttir.reshape"(%arg230, %1049) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc786)
    %1051 = ttir.empty() : tensor<512xbf16> loc(#loc787)
    %1052 = "ttir.reshape"(%1050, %1051) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc787)
    %1053 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc788)
    %1054 = "ttir.reshape"(%arg229, %1053) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc788)
    %1055 = ttir.empty() : tensor<512xbf16> loc(#loc789)
    %1056 = "ttir.reshape"(%1054, %1055) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc789)
    %1057 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc790)
    %1058 = "ttir.batch_norm_inference"(%1044, %1048, %1052, %1056, %220, %1057) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x7x7xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc790)
    %1059 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc791)
    %1060 = "ttir.maximum"(%1058, %2, %1059) : (tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc791)
    %1061 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc792)
    %1062 = "ttir.convolution"(%1060, %arg227, %1061) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x512x7x7xbf16>, tensor<2048x512x1x1xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc792)
    %1063 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc793)
    %1064 = "ttir.reshape"(%arg226, %1063) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc793)
    %1065 = ttir.empty() : tensor<2048xbf16> loc(#loc794)
    %1066 = "ttir.reshape"(%1064, %1065) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc794)
    %1067 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc795)
    %1068 = "ttir.reshape"(%arg225, %1067) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc795)
    %1069 = ttir.empty() : tensor<2048xbf16> loc(#loc796)
    %1070 = "ttir.reshape"(%1068, %1069) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc796)
    %1071 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc797)
    %1072 = "ttir.reshape"(%arg224, %1071) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc797)
    %1073 = ttir.empty() : tensor<2048xbf16> loc(#loc798)
    %1074 = "ttir.reshape"(%1072, %1073) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc798)
    %1075 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc799)
    %1076 = "ttir.batch_norm_inference"(%1062, %1066, %1070, %1074, %224, %1075) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x2048x7x7xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc799)
    %1077 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc800)
    %1078 = "ttir.convolution"(%1024, %arg6, %1077) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<8x1024x14x14xbf16>, tensor<2048x1024x1x1xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc800)
    %1079 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc801)
    %1080 = "ttir.reshape"(%arg5, %1079) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc801)
    %1081 = ttir.empty() : tensor<2048xbf16> loc(#loc802)
    %1082 = "ttir.reshape"(%1080, %1081) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc802)
    %1083 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc803)
    %1084 = "ttir.reshape"(%arg4, %1083) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc803)
    %1085 = ttir.empty() : tensor<2048xbf16> loc(#loc804)
    %1086 = "ttir.reshape"(%1084, %1085) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc804)
    %1087 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc805)
    %1088 = "ttir.reshape"(%arg3, %1087) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc805)
    %1089 = ttir.empty() : tensor<2048xbf16> loc(#loc806)
    %1090 = "ttir.reshape"(%1088, %1089) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc806)
    %1091 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc807)
    %1092 = "ttir.batch_norm_inference"(%1078, %1082, %1086, %1090, %212, %1091) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x2048x7x7xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc807)
    %1093 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc808)
    %1094 = "ttir.add"(%1076, %1092, %1093) : (tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc808)
    %1095 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc809)
    %1096 = "ttir.maximum"(%1094, %1, %1095) : (tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc809)
    %1097 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc810)
    %1098 = "ttir.convolution"(%1096, %arg252, %1097) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x2048x7x7xbf16>, tensor<512x2048x1x1xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc810)
    %1099 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc811)
    %1100 = "ttir.reshape"(%arg251, %1099) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc811)
    %1101 = ttir.empty() : tensor<512xbf16> loc(#loc812)
    %1102 = "ttir.reshape"(%1100, %1101) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc812)
    %1103 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc813)
    %1104 = "ttir.reshape"(%arg250, %1103) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc813)
    %1105 = ttir.empty() : tensor<512xbf16> loc(#loc814)
    %1106 = "ttir.reshape"(%1104, %1105) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc814)
    %1107 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc815)
    %1108 = "ttir.reshape"(%arg249, %1107) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc815)
    %1109 = ttir.empty() : tensor<512xbf16> loc(#loc816)
    %1110 = "ttir.reshape"(%1108, %1109) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc816)
    %1111 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc817)
    %1112 = "ttir.batch_norm_inference"(%1098, %1102, %1106, %1110, %228, %1111) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x7x7xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc817)
    %1113 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc818)
    %1114 = "ttir.maximum"(%1112, %2, %1113) : (tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc818)
    %1115 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc819)
    %1116 = "ttir.convolution"(%1114, %arg247, %1115) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x512x7x7xbf16>, tensor<512x512x3x3xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc819)
    %1117 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc820)
    %1118 = "ttir.reshape"(%arg246, %1117) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc820)
    %1119 = ttir.empty() : tensor<512xbf16> loc(#loc821)
    %1120 = "ttir.reshape"(%1118, %1119) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc821)
    %1121 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc822)
    %1122 = "ttir.reshape"(%arg245, %1121) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc822)
    %1123 = ttir.empty() : tensor<512xbf16> loc(#loc823)
    %1124 = "ttir.reshape"(%1122, %1123) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc823)
    %1125 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc824)
    %1126 = "ttir.reshape"(%arg244, %1125) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc824)
    %1127 = ttir.empty() : tensor<512xbf16> loc(#loc825)
    %1128 = "ttir.reshape"(%1126, %1127) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc825)
    %1129 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc826)
    %1130 = "ttir.batch_norm_inference"(%1116, %1120, %1124, %1128, %232, %1129) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x7x7xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc826)
    %1131 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc827)
    %1132 = "ttir.maximum"(%1130, %2, %1131) : (tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc827)
    %1133 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc828)
    %1134 = "ttir.convolution"(%1132, %arg242, %1133) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x512x7x7xbf16>, tensor<2048x512x1x1xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc828)
    %1135 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc829)
    %1136 = "ttir.reshape"(%arg241, %1135) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc829)
    %1137 = ttir.empty() : tensor<2048xbf16> loc(#loc830)
    %1138 = "ttir.reshape"(%1136, %1137) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc830)
    %1139 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc831)
    %1140 = "ttir.reshape"(%arg240, %1139) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc831)
    %1141 = ttir.empty() : tensor<2048xbf16> loc(#loc832)
    %1142 = "ttir.reshape"(%1140, %1141) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc832)
    %1143 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc833)
    %1144 = "ttir.reshape"(%arg239, %1143) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc833)
    %1145 = ttir.empty() : tensor<2048xbf16> loc(#loc834)
    %1146 = "ttir.reshape"(%1144, %1145) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc834)
    %1147 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc835)
    %1148 = "ttir.batch_norm_inference"(%1134, %1138, %1142, %1146, %236, %1147) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x2048x7x7xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc835)
    %1149 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc836)
    %1150 = "ttir.add"(%1148, %1096, %1149) : (tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc836)
    %1151 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc837)
    %1152 = "ttir.maximum"(%1150, %1, %1151) : (tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc837)
    %1153 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc838)
    %1154 = "ttir.convolution"(%1152, %arg267, %1153) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x2048x7x7xbf16>, tensor<512x2048x1x1xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc838)
    %1155 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc839)
    %1156 = "ttir.reshape"(%arg266, %1155) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc839)
    %1157 = ttir.empty() : tensor<512xbf16> loc(#loc840)
    %1158 = "ttir.reshape"(%1156, %1157) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc840)
    %1159 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc841)
    %1160 = "ttir.reshape"(%arg265, %1159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc841)
    %1161 = ttir.empty() : tensor<512xbf16> loc(#loc842)
    %1162 = "ttir.reshape"(%1160, %1161) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc842)
    %1163 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc843)
    %1164 = "ttir.reshape"(%arg264, %1163) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc843)
    %1165 = ttir.empty() : tensor<512xbf16> loc(#loc844)
    %1166 = "ttir.reshape"(%1164, %1165) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc844)
    %1167 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc845)
    %1168 = "ttir.batch_norm_inference"(%1154, %1158, %1162, %1166, %240, %1167) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x7x7xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc845)
    %1169 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc846)
    %1170 = "ttir.maximum"(%1168, %2, %1169) : (tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc846)
    %1171 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc847)
    %1172 = "ttir.convolution"(%1170, %arg262, %1171) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x512x7x7xbf16>, tensor<512x512x3x3xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc847)
    %1173 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc848)
    %1174 = "ttir.reshape"(%arg261, %1173) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc848)
    %1175 = ttir.empty() : tensor<512xbf16> loc(#loc849)
    %1176 = "ttir.reshape"(%1174, %1175) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc849)
    %1177 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc850)
    %1178 = "ttir.reshape"(%arg260, %1177) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc850)
    %1179 = ttir.empty() : tensor<512xbf16> loc(#loc851)
    %1180 = "ttir.reshape"(%1178, %1179) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc851)
    %1181 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc852)
    %1182 = "ttir.reshape"(%arg259, %1181) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc852)
    %1183 = ttir.empty() : tensor<512xbf16> loc(#loc853)
    %1184 = "ttir.reshape"(%1182, %1183) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc853)
    %1185 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc854)
    %1186 = "ttir.batch_norm_inference"(%1172, %1176, %1180, %1184, %244, %1185) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x512x7x7xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc854)
    %1187 = ttir.empty() : tensor<8x512x7x7xbf16> loc(#loc855)
    %1188 = "ttir.maximum"(%1186, %2, %1187) : (tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>, tensor<8x512x7x7xbf16>) -> tensor<8x512x7x7xbf16> loc(#loc855)
    %1189 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc856)
    %1190 = "ttir.convolution"(%1188, %arg257, %1189) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<8x512x7x7xbf16>, tensor<2048x512x1x1xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc856)
    %1191 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc857)
    %1192 = "ttir.reshape"(%arg256, %1191) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc857)
    %1193 = ttir.empty() : tensor<2048xbf16> loc(#loc858)
    %1194 = "ttir.reshape"(%1192, %1193) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc858)
    %1195 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc859)
    %1196 = "ttir.reshape"(%arg255, %1195) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc859)
    %1197 = ttir.empty() : tensor<2048xbf16> loc(#loc860)
    %1198 = "ttir.reshape"(%1196, %1197) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc860)
    %1199 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc861)
    %1200 = "ttir.reshape"(%arg254, %1199) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc861)
    %1201 = ttir.empty() : tensor<2048xbf16> loc(#loc862)
    %1202 = "ttir.reshape"(%1200, %1201) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc862)
    %1203 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc863)
    %1204 = "ttir.batch_norm_inference"(%1190, %1194, %1198, %1202, %248, %1203) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<8x2048x7x7xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc863)
    %1205 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc864)
    %1206 = "ttir.add"(%1204, %1152, %1205) : (tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc864)
    %1207 = ttir.empty() : tensor<8x2048x7x7xbf16> loc(#loc865)
    %1208 = "ttir.maximum"(%1206, %1, %1207) : (tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>, tensor<8x2048x7x7xbf16>) -> tensor<8x2048x7x7xbf16> loc(#loc865)
    %1209 = ttir.empty() : tensor<8x2048xbf16> loc(#loc866)
    %1210 = "ttir.sum"(%1208, %1209) <{dim_arg = [2 : i32, 3 : i32], keep_dim = false}> : (tensor<8x2048x7x7xbf16>, tensor<8x2048xbf16>) -> tensor<8x2048xbf16> loc(#loc866)
    %1211 = ttir.empty() : tensor<8x2048xbf16> loc(#loc867)
    %1212 = "ttir.multiply"(%1210, %0, %1211) : (tensor<8x2048xbf16>, tensor<8x2048xbf16>, tensor<8x2048xbf16>) -> tensor<8x2048xbf16> loc(#loc867)
    %1213 = ttir.empty() : tensor<1x1000x2048xbf16> loc(#loc868)
    %1214 = "ttir.reshape"(%arg1, %1213) <{shape = [1 : i32, 1000 : i32, 2048 : i32]}> : (tensor<1000x2048xbf16>, tensor<1x1000x2048xbf16>) -> tensor<1x1000x2048xbf16> loc(#loc868)
    %1215 = ttir.empty() : tensor<1000x2048xbf16> loc(#loc869)
    %1216 = "ttir.reshape"(%1214, %1215) <{shape = [1000 : i32, 2048 : i32]}> : (tensor<1x1000x2048xbf16>, tensor<1000x2048xbf16>) -> tensor<1000x2048xbf16> loc(#loc869)
    %1217 = ttir.empty() : tensor<2048x1000xbf16> loc(#loc870)
    %1218 = "ttir.permute"(%1216, %1217) <{permutation = array<i64: 1, 0>}> : (tensor<1000x2048xbf16>, tensor<2048x1000xbf16>) -> tensor<2048x1000xbf16> loc(#loc870)
    %1219 = "ttir.dot_general"(%1212, %1218) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<8x2048xbf16>, tensor<2048x1000xbf16>) -> tensor<8x1000xbf16> loc(#loc871)
    %1220 = ttir.empty() : tensor<1x1x1000xbf16> loc(#loc872)
    %1221 = "ttir.reshape"(%arg0, %1220) <{shape = [1 : i32, 1 : i32, 1000 : i32]}> : (tensor<1000xbf16>, tensor<1x1x1000xbf16>) -> tensor<1x1x1000xbf16> loc(#loc872)
    %1222 = ttir.empty() : tensor<1000xbf16> loc(#loc873)
    %1223 = "ttir.reshape"(%1221, %1222) <{shape = [1000 : i32]}> : (tensor<1x1x1000xbf16>, tensor<1000xbf16>) -> tensor<1000xbf16> loc(#loc873)
    %1224 = ttir.empty() : tensor<1x1000xbf16> loc(#loc874)
    %1225 = "ttir.reshape"(%1223, %1224) <{shape = [1 : i32, 1000 : i32]}> : (tensor<1000xbf16>, tensor<1x1000xbf16>) -> tensor<1x1000xbf16> loc(#loc874)
    %1226 = ttir.empty() : tensor<8x1000xbf16> loc(#loc874)
    %1227 = "ttir.broadcast"(%1225, %1226) <{broadcast_dimensions = array<i64: 8, 1>}> : (tensor<1x1000xbf16>, tensor<8x1000xbf16>) -> tensor<8x1000xbf16> loc(#loc874)
    %1228 = ttir.empty() : tensor<8x1000xbf16> loc(#loc875)
    %1229 = "ttir.add"(%1219, %1227, %1228) : (tensor<8x1000xbf16>, tensor<8x1000xbf16>, tensor<8x1000xbf16>) -> tensor<8x1000xbf16> loc(#loc875)
    return %1229 : tensor<8x1000xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc269 = loc("reshape.104")
#loc270 = loc("reshape.106")
#loc271 = loc("convolution.123")
#loc272 = loc("reshape.116")
#loc273 = loc("reshape.118")
#loc274 = loc("reshape.112")
#loc275 = loc("reshape.114")
#loc276 = loc("reshape.108")
#loc277 = loc("reshape.110")
#loc278 = loc("batch-norm-inference.124")
#loc279 = loc("maximum.131")
#loc280 = loc("pad.133")
#loc281 = loc("reduce-window.143")
#loc282 = loc("reshape.86")
#loc283 = loc("reshape.88")
#loc284 = loc("reshape.347")
#loc285 = loc("reshape.349")
#loc286 = loc("reshape.329")
#loc287 = loc("reshape.331")
#loc288 = loc("reshape.311")
#loc289 = loc("reshape.313")
#loc290 = loc("reshape.431")
#loc291 = loc("reshape.433")
#loc292 = loc("reshape.413")
#loc293 = loc("reshape.415")
#loc294 = loc("reshape.395")
#loc295 = loc("reshape.397")
#loc296 = loc("reshape.515")
#loc297 = loc("reshape.517")
#loc298 = loc("reshape.497")
#loc299 = loc("reshape.499")
#loc300 = loc("reshape.479")
#loc301 = loc("reshape.481")
#loc302 = loc("reshape.65")
#loc303 = loc("reshape.67")
#loc304 = loc("reshape.605")
#loc305 = loc("reshape.607")
#loc306 = loc("reshape.587")
#loc307 = loc("reshape.589")
#loc308 = loc("reshape.569")
#loc309 = loc("reshape.571")
#loc310 = loc("reshape.689")
#loc311 = loc("reshape.691")
#loc312 = loc("reshape.671")
#loc313 = loc("reshape.673")
#loc314 = loc("reshape.653")
#loc315 = loc("reshape.655")
#loc316 = loc("reshape.773")
#loc317 = loc("reshape.775")
#loc318 = loc("reshape.755")
#loc319 = loc("reshape.757")
#loc320 = loc("reshape.737")
#loc321 = loc("reshape.739")
#loc322 = loc("reshape.857")
#loc323 = loc("reshape.859")
#loc324 = loc("reshape.839")
#loc325 = loc("reshape.841")
#loc326 = loc("reshape.821")
#loc327 = loc("reshape.823")
#loc328 = loc("reshape.43")
#loc329 = loc("reshape.45")
#loc330 = loc("reshape.947")
#loc331 = loc("reshape.949")
#loc332 = loc("reshape.929")
#loc333 = loc("reshape.931")
#loc334 = loc("reshape.911")
#loc335 = loc("reshape.913")
#loc336 = loc("reshape.1031")
#loc337 = loc("reshape.1033")
#loc338 = loc("reshape.1013")
#loc339 = loc("reshape.1015")
#loc340 = loc("reshape.995")
#loc341 = loc("reshape.997")
#loc342 = loc("reshape.1115")
#loc343 = loc("reshape.1117")
#loc344 = loc("reshape.1097")
#loc345 = loc("reshape.1099")
#loc346 = loc("reshape.1079")
#loc347 = loc("reshape.1081")
#loc348 = loc("reshape.1199")
#loc349 = loc("reshape.1201")
#loc350 = loc("reshape.1181")
#loc351 = loc("reshape.1183")
#loc352 = loc("reshape.1163")
#loc353 = loc("reshape.1165")
#loc354 = loc("reshape.1283")
#loc355 = loc("reshape.1285")
#loc356 = loc("reshape.1265")
#loc357 = loc("reshape.1267")
#loc358 = loc("reshape.1247")
#loc359 = loc("reshape.1249")
#loc360 = loc("reshape.1367")
#loc361 = loc("reshape.1369")
#loc362 = loc("reshape.1349")
#loc363 = loc("reshape.1351")
#loc364 = loc("reshape.1331")
#loc365 = loc("reshape.1333")
#loc366 = loc("reshape.19")
#loc367 = loc("reshape.21")
#loc368 = loc("reshape.1457")
#loc369 = loc("reshape.1459")
#loc370 = loc("reshape.1439")
#loc371 = loc("reshape.1441")
#loc372 = loc("reshape.1421")
#loc373 = loc("reshape.1423")
#loc374 = loc("reshape.1541")
#loc375 = loc("reshape.1543")
#loc376 = loc("reshape.1523")
#loc377 = loc("reshape.1525")
#loc378 = loc("reshape.1505")
#loc379 = loc("reshape.1507")
#loc380 = loc("reshape.1625")
#loc381 = loc("reshape.1627")
#loc382 = loc("reshape.1607")
#loc383 = loc("reshape.1609")
#loc384 = loc("reshape.1589")
#loc385 = loc("reshape.1591")
#loc386 = loc("convolution.364")
#loc387 = loc("reshape.359")
#loc388 = loc("reshape.361")
#loc389 = loc("reshape.355")
#loc390 = loc("reshape.357")
#loc391 = loc("reshape.351")
#loc392 = loc("reshape.353")
#loc393 = loc("batch-norm-inference.365")
#loc394 = loc("maximum.372")
#loc395 = loc("convolution.373")
#loc396 = loc("reshape.341")
#loc397 = loc("reshape.343")
#loc398 = loc("reshape.337")
#loc399 = loc("reshape.339")
#loc400 = loc("reshape.333")
#loc401 = loc("reshape.335")
#loc402 = loc("batch-norm-inference.374")
#loc403 = loc("maximum.381")
#loc404 = loc("convolution.382")
#loc405 = loc("reshape.323")
#loc406 = loc("reshape.325")
#loc407 = loc("reshape.319")
#loc408 = loc("reshape.321")
#loc409 = loc("reshape.315")
#loc410 = loc("reshape.317")
#loc411 = loc("batch-norm-inference.383")
#loc412 = loc("convolution.304")
#loc413 = loc("reshape.98")
#loc414 = loc("reshape.100")
#loc415 = loc("reshape.94")
#loc416 = loc("reshape.96")
#loc417 = loc("reshape.90")
#loc418 = loc("reshape.92")
#loc419 = loc("batch-norm-inference.305")
#loc420 = loc("add.390")
#loc421 = loc("maximum.393")
#loc422 = loc("convolution.448")
#loc423 = loc("reshape.443")
#loc424 = loc("reshape.445")
#loc425 = loc("reshape.439")
#loc426 = loc("reshape.441")
#loc427 = loc("reshape.435")
#loc428 = loc("reshape.437")
#loc429 = loc("batch-norm-inference.449")
#loc430 = loc("maximum.456")
#loc431 = loc("convolution.457")
#loc432 = loc("reshape.425")
#loc433 = loc("reshape.427")
#loc434 = loc("reshape.421")
#loc435 = loc("reshape.423")
#loc436 = loc("reshape.417")
#loc437 = loc("reshape.419")
#loc438 = loc("batch-norm-inference.458")
#loc439 = loc("maximum.465")
#loc440 = loc("convolution.466")
#loc441 = loc("reshape.407")
#loc442 = loc("reshape.409")
#loc443 = loc("reshape.403")
#loc444 = loc("reshape.405")
#loc445 = loc("reshape.399")
#loc446 = loc("reshape.401")
#loc447 = loc("batch-norm-inference.467")
#loc448 = loc("add.474")
#loc449 = loc("maximum.477")
#loc450 = loc("convolution.532")
#loc451 = loc("reshape.527")
#loc452 = loc("reshape.529")
#loc453 = loc("reshape.523")
#loc454 = loc("reshape.525")
#loc455 = loc("reshape.519")
#loc456 = loc("reshape.521")
#loc457 = loc("batch-norm-inference.533")
#loc458 = loc("maximum.540")
#loc459 = loc("convolution.541")
#loc460 = loc("reshape.509")
#loc461 = loc("reshape.511")
#loc462 = loc("reshape.505")
#loc463 = loc("reshape.507")
#loc464 = loc("reshape.501")
#loc465 = loc("reshape.503")
#loc466 = loc("batch-norm-inference.542")
#loc467 = loc("maximum.549")
#loc468 = loc("convolution.550")
#loc469 = loc("reshape.491")
#loc470 = loc("reshape.493")
#loc471 = loc("reshape.487")
#loc472 = loc("reshape.489")
#loc473 = loc("reshape.483")
#loc474 = loc("reshape.485")
#loc475 = loc("batch-norm-inference.551")
#loc476 = loc("add.558")
#loc477 = loc("maximum.561")
#loc478 = loc("convolution.622")
#loc479 = loc("reshape.617")
#loc480 = loc("reshape.619")
#loc481 = loc("reshape.613")
#loc482 = loc("reshape.615")
#loc483 = loc("reshape.609")
#loc484 = loc("reshape.611")
#loc485 = loc("batch-norm-inference.623")
#loc486 = loc("maximum.630")
#loc487 = loc("convolution.631")
#loc488 = loc("reshape.599")
#loc489 = loc("reshape.601")
#loc490 = loc("reshape.595")
#loc491 = loc("reshape.597")
#loc492 = loc("reshape.591")
#loc493 = loc("reshape.593")
#loc494 = loc("batch-norm-inference.632")
#loc495 = loc("maximum.639")
#loc496 = loc("convolution.640")
#loc497 = loc("reshape.581")
#loc498 = loc("reshape.583")
#loc499 = loc("reshape.577")
#loc500 = loc("reshape.579")
#loc501 = loc("reshape.573")
#loc502 = loc("reshape.575")
#loc503 = loc("batch-norm-inference.641")
#loc504 = loc("convolution.562")
#loc505 = loc("reshape.77")
#loc506 = loc("reshape.79")
#loc507 = loc("reshape.73")
#loc508 = loc("reshape.75")
#loc509 = loc("reshape.69")
#loc510 = loc("reshape.71")
#loc511 = loc("batch-norm-inference.563")
#loc512 = loc("add.648")
#loc513 = loc("maximum.651")
#loc514 = loc("convolution.706")
#loc515 = loc("reshape.701")
#loc516 = loc("reshape.703")
#loc517 = loc("reshape.697")
#loc518 = loc("reshape.699")
#loc519 = loc("reshape.693")
#loc520 = loc("reshape.695")
#loc521 = loc("batch-norm-inference.707")
#loc522 = loc("maximum.714")
#loc523 = loc("convolution.715")
#loc524 = loc("reshape.683")
#loc525 = loc("reshape.685")
#loc526 = loc("reshape.679")
#loc527 = loc("reshape.681")
#loc528 = loc("reshape.675")
#loc529 = loc("reshape.677")
#loc530 = loc("batch-norm-inference.716")
#loc531 = loc("maximum.723")
#loc532 = loc("convolution.724")
#loc533 = loc("reshape.665")
#loc534 = loc("reshape.667")
#loc535 = loc("reshape.661")
#loc536 = loc("reshape.663")
#loc537 = loc("reshape.657")
#loc538 = loc("reshape.659")
#loc539 = loc("batch-norm-inference.725")
#loc540 = loc("add.732")
#loc541 = loc("maximum.735")
#loc542 = loc("convolution.790")
#loc543 = loc("reshape.785")
#loc544 = loc("reshape.787")
#loc545 = loc("reshape.781")
#loc546 = loc("reshape.783")
#loc547 = loc("reshape.777")
#loc548 = loc("reshape.779")
#loc549 = loc("batch-norm-inference.791")
#loc550 = loc("maximum.798")
#loc551 = loc("convolution.799")
#loc552 = loc("reshape.767")
#loc553 = loc("reshape.769")
#loc554 = loc("reshape.763")
#loc555 = loc("reshape.765")
#loc556 = loc("reshape.759")
#loc557 = loc("reshape.761")
#loc558 = loc("batch-norm-inference.800")
#loc559 = loc("maximum.807")
#loc560 = loc("convolution.808")
#loc561 = loc("reshape.749")
#loc562 = loc("reshape.751")
#loc563 = loc("reshape.745")
#loc564 = loc("reshape.747")
#loc565 = loc("reshape.741")
#loc566 = loc("reshape.743")
#loc567 = loc("batch-norm-inference.809")
#loc568 = loc("add.816")
#loc569 = loc("maximum.819")
#loc570 = loc("convolution.874")
#loc571 = loc("reshape.869")
#loc572 = loc("reshape.871")
#loc573 = loc("reshape.865")
#loc574 = loc("reshape.867")
#loc575 = loc("reshape.861")
#loc576 = loc("reshape.863")
#loc577 = loc("batch-norm-inference.875")
#loc578 = loc("maximum.882")
#loc579 = loc("convolution.883")
#loc580 = loc("reshape.851")
#loc581 = loc("reshape.853")
#loc582 = loc("reshape.847")
#loc583 = loc("reshape.849")
#loc584 = loc("reshape.843")
#loc585 = loc("reshape.845")
#loc586 = loc("batch-norm-inference.884")
#loc587 = loc("maximum.891")
#loc588 = loc("convolution.892")
#loc589 = loc("reshape.833")
#loc590 = loc("reshape.835")
#loc591 = loc("reshape.829")
#loc592 = loc("reshape.831")
#loc593 = loc("reshape.825")
#loc594 = loc("reshape.827")
#loc595 = loc("batch-norm-inference.893")
#loc596 = loc("add.900")
#loc597 = loc("maximum.903")
#loc598 = loc("convolution.964")
#loc599 = loc("reshape.959")
#loc600 = loc("reshape.961")
#loc601 = loc("reshape.955")
#loc602 = loc("reshape.957")
#loc603 = loc("reshape.951")
#loc604 = loc("reshape.953")
#loc605 = loc("batch-norm-inference.965")
#loc606 = loc("maximum.972")
#loc607 = loc("convolution.973")
#loc608 = loc("reshape.941")
#loc609 = loc("reshape.943")
#loc610 = loc("reshape.937")
#loc611 = loc("reshape.939")
#loc612 = loc("reshape.933")
#loc613 = loc("reshape.935")
#loc614 = loc("batch-norm-inference.974")
#loc615 = loc("maximum.981")
#loc616 = loc("convolution.982")
#loc617 = loc("reshape.923")
#loc618 = loc("reshape.925")
#loc619 = loc("reshape.919")
#loc620 = loc("reshape.921")
#loc621 = loc("reshape.915")
#loc622 = loc("reshape.917")
#loc623 = loc("batch-norm-inference.983")
#loc624 = loc("convolution.904")
#loc625 = loc("reshape.55")
#loc626 = loc("reshape.57")
#loc627 = loc("reshape.51")
#loc628 = loc("reshape.53")
#loc629 = loc("reshape.47")
#loc630 = loc("reshape.49")
#loc631 = loc("batch-norm-inference.905")
#loc632 = loc("add.990")
#loc633 = loc("maximum.993")
#loc634 = loc("convolution.1048")
#loc635 = loc("reshape.1043")
#loc636 = loc("reshape.1045")
#loc637 = loc("reshape.1039")
#loc638 = loc("reshape.1041")
#loc639 = loc("reshape.1035")
#loc640 = loc("reshape.1037")
#loc641 = loc("batch-norm-inference.1049")
#loc642 = loc("maximum.1056")
#loc643 = loc("convolution.1057")
#loc644 = loc("reshape.1025")
#loc645 = loc("reshape.1027")
#loc646 = loc("reshape.1021")
#loc647 = loc("reshape.1023")
#loc648 = loc("reshape.1017")
#loc649 = loc("reshape.1019")
#loc650 = loc("batch-norm-inference.1058")
#loc651 = loc("maximum.1065")
#loc652 = loc("convolution.1066")
#loc653 = loc("reshape.1007")
#loc654 = loc("reshape.1009")
#loc655 = loc("reshape.1003")
#loc656 = loc("reshape.1005")
#loc657 = loc("reshape.999")
#loc658 = loc("reshape.1001")
#loc659 = loc("batch-norm-inference.1067")
#loc660 = loc("add.1074")
#loc661 = loc("maximum.1077")
#loc662 = loc("convolution.1132")
#loc663 = loc("reshape.1127")
#loc664 = loc("reshape.1129")
#loc665 = loc("reshape.1123")
#loc666 = loc("reshape.1125")
#loc667 = loc("reshape.1119")
#loc668 = loc("reshape.1121")
#loc669 = loc("batch-norm-inference.1133")
#loc670 = loc("maximum.1140")
#loc671 = loc("convolution.1141")
#loc672 = loc("reshape.1109")
#loc673 = loc("reshape.1111")
#loc674 = loc("reshape.1105")
#loc675 = loc("reshape.1107")
#loc676 = loc("reshape.1101")
#loc677 = loc("reshape.1103")
#loc678 = loc("batch-norm-inference.1142")
#loc679 = loc("maximum.1149")
#loc680 = loc("convolution.1150")
#loc681 = loc("reshape.1091")
#loc682 = loc("reshape.1093")
#loc683 = loc("reshape.1087")
#loc684 = loc("reshape.1089")
#loc685 = loc("reshape.1083")
#loc686 = loc("reshape.1085")
#loc687 = loc("batch-norm-inference.1151")
#loc688 = loc("add.1158")
#loc689 = loc("maximum.1161")
#loc690 = loc("convolution.1216")
#loc691 = loc("reshape.1211")
#loc692 = loc("reshape.1213")
#loc693 = loc("reshape.1207")
#loc694 = loc("reshape.1209")
#loc695 = loc("reshape.1203")
#loc696 = loc("reshape.1205")
#loc697 = loc("batch-norm-inference.1217")
#loc698 = loc("maximum.1224")
#loc699 = loc("convolution.1225")
#loc700 = loc("reshape.1193")
#loc701 = loc("reshape.1195")
#loc702 = loc("reshape.1189")
#loc703 = loc("reshape.1191")
#loc704 = loc("reshape.1185")
#loc705 = loc("reshape.1187")
#loc706 = loc("batch-norm-inference.1226")
#loc707 = loc("maximum.1233")
#loc708 = loc("convolution.1234")
#loc709 = loc("reshape.1175")
#loc710 = loc("reshape.1177")
#loc711 = loc("reshape.1171")
#loc712 = loc("reshape.1173")
#loc713 = loc("reshape.1167")
#loc714 = loc("reshape.1169")
#loc715 = loc("batch-norm-inference.1235")
#loc716 = loc("add.1242")
#loc717 = loc("maximum.1245")
#loc718 = loc("convolution.1300")
#loc719 = loc("reshape.1295")
#loc720 = loc("reshape.1297")
#loc721 = loc("reshape.1291")
#loc722 = loc("reshape.1293")
#loc723 = loc("reshape.1287")
#loc724 = loc("reshape.1289")
#loc725 = loc("batch-norm-inference.1301")
#loc726 = loc("maximum.1308")
#loc727 = loc("convolution.1309")
#loc728 = loc("reshape.1277")
#loc729 = loc("reshape.1279")
#loc730 = loc("reshape.1273")
#loc731 = loc("reshape.1275")
#loc732 = loc("reshape.1269")
#loc733 = loc("reshape.1271")
#loc734 = loc("batch-norm-inference.1310")
#loc735 = loc("maximum.1317")
#loc736 = loc("convolution.1318")
#loc737 = loc("reshape.1259")
#loc738 = loc("reshape.1261")
#loc739 = loc("reshape.1255")
#loc740 = loc("reshape.1257")
#loc741 = loc("reshape.1251")
#loc742 = loc("reshape.1253")
#loc743 = loc("batch-norm-inference.1319")
#loc744 = loc("add.1326")
#loc745 = loc("maximum.1329")
#loc746 = loc("convolution.1384")
#loc747 = loc("reshape.1379")
#loc748 = loc("reshape.1381")
#loc749 = loc("reshape.1375")
#loc750 = loc("reshape.1377")
#loc751 = loc("reshape.1371")
#loc752 = loc("reshape.1373")
#loc753 = loc("batch-norm-inference.1385")
#loc754 = loc("maximum.1392")
#loc755 = loc("convolution.1393")
#loc756 = loc("reshape.1361")
#loc757 = loc("reshape.1363")
#loc758 = loc("reshape.1357")
#loc759 = loc("reshape.1359")
#loc760 = loc("reshape.1353")
#loc761 = loc("reshape.1355")
#loc762 = loc("batch-norm-inference.1394")
#loc763 = loc("maximum.1401")
#loc764 = loc("convolution.1402")
#loc765 = loc("reshape.1343")
#loc766 = loc("reshape.1345")
#loc767 = loc("reshape.1339")
#loc768 = loc("reshape.1341")
#loc769 = loc("reshape.1335")
#loc770 = loc("reshape.1337")
#loc771 = loc("batch-norm-inference.1403")
#loc772 = loc("add.1410")
#loc773 = loc("maximum.1413")
#loc774 = loc("convolution.1474")
#loc775 = loc("reshape.1469")
#loc776 = loc("reshape.1471")
#loc777 = loc("reshape.1465")
#loc778 = loc("reshape.1467")
#loc779 = loc("reshape.1461")
#loc780 = loc("reshape.1463")
#loc781 = loc("batch-norm-inference.1475")
#loc782 = loc("maximum.1482")
#loc783 = loc("convolution.1483")
#loc784 = loc("reshape.1451")
#loc785 = loc("reshape.1453")
#loc786 = loc("reshape.1447")
#loc787 = loc("reshape.1449")
#loc788 = loc("reshape.1443")
#loc789 = loc("reshape.1445")
#loc790 = loc("batch-norm-inference.1484")
#loc791 = loc("maximum.1491")
#loc792 = loc("convolution.1492")
#loc793 = loc("reshape.1433")
#loc794 = loc("reshape.1435")
#loc795 = loc("reshape.1429")
#loc796 = loc("reshape.1431")
#loc797 = loc("reshape.1425")
#loc798 = loc("reshape.1427")
#loc799 = loc("batch-norm-inference.1493")
#loc800 = loc("convolution.1414")
#loc801 = loc("reshape.31")
#loc802 = loc("reshape.33")
#loc803 = loc("reshape.27")
#loc804 = loc("reshape.29")
#loc805 = loc("reshape.23")
#loc806 = loc("reshape.25")
#loc807 = loc("batch-norm-inference.1415")
#loc808 = loc("add.1500")
#loc809 = loc("maximum.1503")
#loc810 = loc("convolution.1558")
#loc811 = loc("reshape.1553")
#loc812 = loc("reshape.1555")
#loc813 = loc("reshape.1549")
#loc814 = loc("reshape.1551")
#loc815 = loc("reshape.1545")
#loc816 = loc("reshape.1547")
#loc817 = loc("batch-norm-inference.1559")
#loc818 = loc("maximum.1566")
#loc819 = loc("convolution.1567")
#loc820 = loc("reshape.1535")
#loc821 = loc("reshape.1537")
#loc822 = loc("reshape.1531")
#loc823 = loc("reshape.1533")
#loc824 = loc("reshape.1527")
#loc825 = loc("reshape.1529")
#loc826 = loc("batch-norm-inference.1568")
#loc827 = loc("maximum.1575")
#loc828 = loc("convolution.1576")
#loc829 = loc("reshape.1517")
#loc830 = loc("reshape.1519")
#loc831 = loc("reshape.1513")
#loc832 = loc("reshape.1515")
#loc833 = loc("reshape.1509")
#loc834 = loc("reshape.1511")
#loc835 = loc("batch-norm-inference.1577")
#loc836 = loc("add.1584")
#loc837 = loc("maximum.1587")
#loc838 = loc("convolution.1642")
#loc839 = loc("reshape.1637")
#loc840 = loc("reshape.1639")
#loc841 = loc("reshape.1633")
#loc842 = loc("reshape.1635")
#loc843 = loc("reshape.1629")
#loc844 = loc("reshape.1631")
#loc845 = loc("batch-norm-inference.1643")
#loc846 = loc("maximum.1650")
#loc847 = loc("convolution.1651")
#loc848 = loc("reshape.1619")
#loc849 = loc("reshape.1621")
#loc850 = loc("reshape.1615")
#loc851 = loc("reshape.1617")
#loc852 = loc("reshape.1611")
#loc853 = loc("reshape.1613")
#loc854 = loc("batch-norm-inference.1652")
#loc855 = loc("maximum.1659")
#loc856 = loc("convolution.1660")
#loc857 = loc("reshape.1601")
#loc858 = loc("reshape.1603")
#loc859 = loc("reshape.1597")
#loc860 = loc("reshape.1599")
#loc861 = loc("reshape.1593")
#loc862 = loc("reshape.1595")
#loc863 = loc("batch-norm-inference.1661")
#loc864 = loc("add.1668")
#loc865 = loc("maximum.1671")
#loc866 = loc("reduce.1678")
#loc867 = loc("multiply.1687")
#loc868 = loc("reshape.11")
#loc869 = loc("reshape.13")
#loc870 = loc("transpose.14")
#loc871 = loc("dot.1690")
#loc872 = loc("reshape.4")
#loc873 = loc("reshape.6")
#loc874 = loc("broadcast.1695")
#loc875 = loc("add.1696")
