WARNING:root:Defaulting to PJRT_DEVICE=CPU
Using TT-Metal from the source tree: /localdev/hshah/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /localdev/hshah/tt-xla/venv/bin/python
cachedir: .pytest_cache
rootdir: /localdev/hshah/tt-xla
configfile: pytest.ini
plugins: anyio-4.11.0, jaxtyping-0.3.3, forked-1.6.0, split-0.10.0
collecting ... collected 1 item

tests/torch/single_chip/graphs/test_simple_matmul.py::test_matmul_incorrect_sharding 2025-10-23 04:30:13.548 (   0.000s) [        2146C480]      dylib_platform.cc:47       1| DylibPlatform::SubclassInitialize
2025-10-23 04:30:13.556 (   0.008s) [        2146C480]     client_instance.cc:44       1| ClientInstance::ClientInstance
2025-10-23 04:30:13.556 (   0.008s) [        2146C480]              client.cc:18       1| TTClientInstance::TTClientInstance
2025-10-23 04:30:13.556 (   0.008s) [        2146C480]     client_instance.cc:73       1| ClientInstance::Initialize
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]              stubs.inc:106   WARN| STUB: PJRT_Client_TopologyDescription
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     client_instance.cc:510      1| ClientInstance::PJRT_Client_PlatformVersion
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     client_instance.cc:490      1| ClientInstance::PJRT_Client_PlatformName
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     client_instance.cc:522      1| ClientInstance::PJRT_Client_Devices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     client_instance.cc:535      1| ClientInstance::PJRT_Client_AddressableDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     client_instance.cc:585      1| ClientInstance::PJRT_Client_AddressableMemories
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-23 04:30:27.360 (  13.812s) [        2146C480]        api_bindings.cc:76       1| PJRT_Plugin_Attributes
2025-10-23 04:30:27.361059: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.812s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.361 (  13.813s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.648 (  14.100s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.648 (  14.100s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.648 (  14.100s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.648 (  14.100s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.648 (  14.100s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.648 (  14.100s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.648 (  14.100s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.648 (  14.100s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.659 (  14.111s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.659 (  14.111s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.659 (  14.111s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.659 (  14.111s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.659 (  14.111s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.659 (  14.111s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.659 (  14.111s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.659 (  14.111s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.661 (  14.113s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.661 (  14.113s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.661 (  14.113s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.662 (  14.114s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.663 (  14.115s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.664 (  14.116s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.664 (  14.116s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.664 (  14.116s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.664 (  14.116s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.664 (  14.116s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.664 (  14.116s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.664 (  14.116s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.664 (  14.116s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.665 (  14.117s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.666 (  14.118s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.666 (  14.118s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.666 (  14.118s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.666 (  14.118s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.666 (  14.118s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.666 (  14.118s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.666 (  14.118s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.666 (  14.118s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.670 (  14.122s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.671 (  14.122s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.671 (  14.123s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.672 (  14.123s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     client_instance.cc:641      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:27.672 (  14.124s) [        2146C480]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.124s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.673 (  14.125s) [        2146C480]     buffer_instance.cc:483      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-23 04:30:27.680 (  14.131s) [        2146C480]     client_instance.cc:598      1| ClientInstance::PJRT_Client_Compile
2025-10-23 04:30:27.680 (  14.132s) [        2146C480]      module_builder.cc:221      1| ModuleBuilder::buildModule
2025-10-23 04:30:27.681 (  14.133s) [        2146C480]      module_builder.cc:963      1| MLIR Module vhlo:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc("p0.1"), %arg1: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc("p1.2"), %arg2: !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc("p2.4"), %arg3: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc("p3.5"), %arg4: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc("p4.7")) -> (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.transpose_v1"(%arg3) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc6)
    %3 = "vhlo.dot_general_v2"(%arg4, %2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc7)
    %4 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc8)
    %5 = "vhlo.add_v1"(%3, %4) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc9)
    %6 = "vhlo.maximum_v1"(%5, %1) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc10)
    %7 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc11)
    %8 = "vhlo.dot_general_v2"(%6, %7) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc12)
    %9 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc13)
    %10 = "vhlo.add_v1"(%8, %9) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc14)
    %11 = "vhlo.maximum_v1"(%10, %1) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1> loc(#loc15)
    "vhlo.return_v1"(%11) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("maximum.16")
#loc11 = loc("transpose.3")
#loc12 = loc("dot.17")
#loc13 = loc("broadcast.21")
#loc14 = loc("add.22")
#loc15 = loc("maximum.25")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<2048x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<2048x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %2 = "vhlo.transpose_v1"(%arg3) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %3 = "vhlo.dot_general_v2"(%arg4, %2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %5 = "vhlo.add_v1"(%3, %4) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %6 = "vhlo.maximum_v1"(%5, %1) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %7 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %8 = "vhlo.dot_general_v2"(%6, %7) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %9 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %10 = "vhlo.add_v1"(%8, %9) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %11 = "vhlo.maximum_v1"(%10, %1) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    "vhlo.return_v1"(%11) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<2048x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<2048x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %2 = "vhlo.transpose_v1"(%arg3) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %3 = "vhlo.dot_general_v2"(%arg4, %2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %5 = "vhlo.add_v1"(%3, %4) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %6 = "vhlo.maximum_v1"(%5, %1) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %7 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2048,2048]{0,1}">} : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %8 = "vhlo.dot_general_v2"(%6, %7) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %9 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %10 = "vhlo.add_v1"(%8, %9) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    %11 = "vhlo.maximum_v1"(%10, %1) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>
    "vhlo.return_v1"(%11) : (!vhlo.tensor_v1<2048x2048x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


2025-10-23 04:30:27.687 (  14.139s) [        2146C480]      module_builder.cc:963      1| MLIR Module shlo:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p0.1"), %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"} loc("p1.2"), %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"} loc("p2.4"), %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"} loc("p3.5"), %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p4.7")) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16> loc(#loc)
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc6)
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc7)
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc8)
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16> loc(#loc9)
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16> loc(#loc10)
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc11)
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc13)
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16> loc(#loc14)
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16> loc(#loc15)
    return %10 : tensor<2048x2048xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("maximum.16")
#loc11 = loc("transpose.3")
#loc12 = loc("dot.17")
#loc13 = loc("broadcast.21")
#loc14 = loc("add.22")
#loc15 = loc("maximum.25")
------------------ END OF MLIR MODULE ------------------
2025-10-23 04:30:27.689 (  14.141s) [        2146C480]      module_builder.cc:963      1| MLIR Module shlo_frontend:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p0.1"), %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p1.2"), %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p2.4"), %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p3.5"), %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p4.7")) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16> loc(#loc)
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc6)
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc7)
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc8)
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16> loc(#loc9)
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16> loc(#loc10)
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc11)
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc13)
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16> loc(#loc14)
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16> loc(#loc15)
    return %10 : tensor<2048x2048xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("maximum.16")
#loc11 = loc("transpose.3")
#loc12 = loc("dot.17")
#loc13 = loc("broadcast.21")
#loc14 = loc("add.22")
#loc15 = loc("maximum.25")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>}) -> tensor<2048x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %9 = stablehlo.add %7, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %10 = stablehlo.maximum %9, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    return %10 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = sdy.all_reduce {"_axis_0"} %7 out_sharding=<@mesh, [{?}, {?}]> : tensor<2048x2048xbf16>
    %9 = sdy.reshard %8 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<2048x2048xbf16>
    %10 = sdy.reshard %arg0 <@mesh, [{"_axis_0"}]> : tensor<2048xbf16>
    %11 = stablehlo.broadcast_in_dim %10, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %12 = stablehlo.add %9, %11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %13 = stablehlo.maximum %12, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    return %13 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<2048x2048xbf16>
    %1 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %2 = stablehlo.dot_general %arg4, %1, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %3 = stablehlo.broadcast_in_dim %arg2, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %4 = stablehlo.add %2, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %5 = stablehlo.maximum %4, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %6 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %8 = sdy.all_reduce {"_axis_0"} %7 out_sharding=<@mesh, [{?}, {?}]> : tensor<2048x2048xbf16>
    %9 = sdy.reshard %8 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<2048x2048xbf16>
    %10 = sdy.reshard %arg0 <@mesh, [{"_axis_0"}]> : tensor<2048xbf16>
    %11 = stablehlo.broadcast_in_dim %10, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
    %12 = stablehlo.add %9, %11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    %13 = stablehlo.maximum %12, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
    return %13 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x2048xbf16>, %arg7: tensor<2048xbf16>, %arg8: tensor<2048x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %5 = stablehlo.add %3, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %6 = stablehlo.maximum %5, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = sdy.all_reduce {"_axis_0"} %8 out_sharding=<@mesh, [{?}, {?}]> : tensor<2048x2048xbf16>
      %10 = sdy.reshard %9 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<2048x2048xbf16>
      %11 = sdy.reshard %arg5 <@mesh, [{"_axis_0"}]> : tensor<2048xbf16>
      %12 = stablehlo.broadcast_in_dim %11, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %13 = stablehlo.add %10, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %14 = stablehlo.maximum %13, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      sdy.return %14 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x2048xbf16>, %arg7: tensor<2048xbf16>, %arg8: tensor<2048x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %5 = stablehlo.add %3, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %6 = stablehlo.maximum %5, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = sdy.all_reduce {"_axis_0"} %8 out_sharding=<@mesh, [{?}, {?}]> : tensor<2048x2048xbf16>
      %10 = sdy.reshard %9 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<2048x2048xbf16>
      %11 = sdy.reshard %arg5 <@mesh, [{"_axis_0"}]> : tensor<2048xbf16>
      %12 = stablehlo.broadcast_in_dim %11, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %13 = stablehlo.add %10, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %14 = stablehlo.maximum %13, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      sdy.return %14 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x2048xbf16>, %arg7: tensor<2048xbf16>, %arg8: tensor<2048x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %5 = stablehlo.add %3, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %6 = stablehlo.maximum %5, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = sdy.all_reduce {"_axis_0"} %8 out_sharding=<@mesh, [{?}, {?}]> : tensor<2048x2048xbf16>
      %10 = sdy.all_slice [{}, {"_axis_0"}] %9 out_sharding=<@mesh, [{}, {"_axis_0"}]> : tensor<2048x2048xbf16>
      %11 = sdy.all_slice [{"_axis_0"}] %arg5 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<2048xbf16>
      %12 = stablehlo.broadcast_in_dim %11, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %13 = stablehlo.add %10, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %14 = stablehlo.maximum %13, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      sdy.return %14 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x2048xbf16>, %arg7: tensor<2048xbf16>, %arg8: tensor<2048x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<2048x2048xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %5 = stablehlo.add %3, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %6 = stablehlo.maximum %5, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}]>]>} : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = sdy.all_reduce {"_axis_0"} %8 out_sharding=<@mesh, [{?}, {?}]> : tensor<2048x2048xbf16>
      %10 = sdy.all_slice [{}, {"_axis_0"}] %9 out_sharding=<@mesh, [{}, {"_axis_0"}]> : tensor<2048x2048xbf16>
      %11 = sdy.all_slice [{"_axis_0"}] %arg5 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<2048xbf16>
      %12 = stablehlo.broadcast_in_dim %11, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<2048xbf16>) -> tensor<2048x2048xbf16>
      %13 = stablehlo.add %10, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      %14 = stablehlo.maximum %13, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : tensor<2048x2048xbf16>
      sdy.return %14 : tensor<2048x2048xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


2025-10-23 04:30:27.708 (  14.160s) [        2146C480]      module_builder.cc:963      1| MLIR Module shlo_compiler:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
#loc12 = loc("dot.17")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]> loc(#loc)
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p0.1"), %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p1.2"), %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p2.4"), %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p3.5"), %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p4.7")) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16> loc("p0.1"), %arg6: tensor<2048x256xbf16> loc("p1.2"), %arg7: tensor<256xbf16> loc("p2.4"), %arg8: tensor<256x2048xbf16> loc("p3.5"), %arg9: tensor<2048x2048xbf16> loc("p4.7")) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16> loc(#loc)
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16> loc(#loc6)
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc7)
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16> loc(#loc8)
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16> loc(#loc9)
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16> loc(#loc10)
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16> loc(#loc11)
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16> loc("dot.17"), %arg11: tensor<bf16> loc("dot.17")):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16> loc(#loc12)
        stablehlo.return %21 : tensor<bf16> loc(#loc12)
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16> loc(#loc12)
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16> loc(#loc12)
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16> loc(#loc12)
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16> loc(#loc12)
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16> loc(#loc1)
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16> loc(#loc1)
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16> loc(#loc1)
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16> loc(#loc1)
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16> loc(#loc13)
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16> loc(#loc14)
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16> loc(#loc15)
      sdy.return %20 : tensor<2048x256xbf16> loc(#loc)
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc)
    return %0 : tensor<2048x2048xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("maximum.16")
#loc11 = loc("transpose.3")
#loc13 = loc("broadcast.21")
#loc14 = loc("add.22")
#loc15 = loc("maximum.25")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before LegalizeStableHLOCompositeToTTIR (legalize-stablehlo-composite-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg5: tensor<2048xbf16>, %arg6: tensor<2048x256xbf16>, %arg7: tensor<256xbf16>, %arg8: tensor<256x2048xbf16>, %arg9: tensor<2048x2048xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<2048x256xbf16>
      %2 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<256x2048xbf16>) -> tensor<2048x256xbf16>
      %3 = stablehlo.dot_general %arg9, %2, contracting_dims = [1] x [0] : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
      %4 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %5 = stablehlo.add %3, %4 : tensor<2048x256xbf16>
      %6 = stablehlo.maximum %5, %1 : tensor<2048x256xbf16>
      %7 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2048,2048]{0,1}"} : (tensor<2048x256xbf16>) -> tensor<256x2048xbf16>
      %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
      %9 = "stablehlo.all_reduce"(%8) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg10: tensor<bf16>, %arg11: tensor<bf16>):
        %21 = stablehlo.add %arg10, %arg11 : tensor<bf16>
        stablehlo.return %21 : tensor<bf16>
      }) : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
      %10 = stablehlo.reshape %9 : (tensor<2048x2048xbf16>) -> tensor<2048x8x256xbf16>
      %11 = "stablehlo.all_to_all"(%10) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
      %12 = stablehlo.slice %11 [0:2048, 0:1, 0:256] : (tensor<2048x8x256xbf16>) -> tensor<2048x1x256xbf16>
      %13 = stablehlo.reshape %12 : (tensor<2048x1x256xbf16>) -> tensor<2048x256xbf16>
      %14 = stablehlo.reshape %arg5 : (tensor<2048xbf16>) -> tensor<8x256xbf16>
      %15 = "stablehlo.all_to_all"(%14) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x256xbf16>) -> tensor<8x256xbf16>
      %16 = stablehlo.slice %15 [0:1, 0:256] : (tensor<8x256xbf16>) -> tensor<1x256xbf16>
      %17 = stablehlo.reshape %16 : (tensor<1x256xbf16>) -> tensor<256xbf16>
      %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<256xbf16>) -> tensor<2048x256xbf16>
      %19 = stablehlo.add %13, %18 : tensor<2048x256xbf16>
      %20 = stablehlo.maximum %19, %1 : tensor<2048x256xbf16>
      sdy.return %20 : tensor<2048x256xbf16>
    } : (tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048xbf16>, tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    return %0 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<2048xbf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %6 = ttir.empty() : tensor<1x1xbf16>
    %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = ttir.empty() : tensor<2048x256xbf16>
    %11 = "ttir.permute"(%3, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %12 = "ttir.dot_general"(%4, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %13 = ttir.empty() : tensor<1x256xbf16>
    %14 = "ttir.reshape"(%2, %13) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<2048x256xbf16>
    %18 = "ttir.add"(%12, %16, %17) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %19 = ttir.empty() : tensor<2048x256xbf16>
    %20 = "ttir.maximum"(%18, %9, %19) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %21 = ttir.empty() : tensor<256x2048xbf16>
    %22 = "ttir.permute"(%1, %21) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %23 = "ttir.dot_general"(%20, %22) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %24 = ttir.empty() : tensor<2048x2048xbf16>
    %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %26 = ttir.empty() : tensor<2048x8x256xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %28 = ttir.empty() : tensor<2048x8x256xbf16>
    %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %30 = ttir.empty() : tensor<2048x1x256xbf16>
    %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
    %32 = ttir.empty() : tensor<2048x256xbf16>
    %33 = "ttir.reshape"(%31, %32) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %34 = ttir.empty() : tensor<8x256xbf16>
    %35 = "ttir.reshape"(%0, %34) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %36 = ttir.empty() : tensor<8x256xbf16>
    %37 = "ttir.all_to_all"(%35, %36) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %38 = ttir.empty() : tensor<1x256xbf16>
    %39 = "ttir.slice_static"(%37, %38) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %40 = ttir.empty() : tensor<256xbf16>
    %41 = "ttir.reshape"(%39, %40) <{shape = [256 : i32]}> : (tensor<1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16>
    %42 = ttir.empty() : tensor<1x256xbf16>
    %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %44 = ttir.empty() : tensor<2048x256xbf16>
    %45 = "ttir.broadcast"(%43, %44) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %46 = ttir.empty() : tensor<2048x256xbf16>
    %47 = "ttir.add"(%33, %45, %46) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %48 = ttir.empty() : tensor<2048x256xbf16>
    %49 = "ttir.maximum"(%47, %9, %48) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %50 = "ttir.mesh_shard"(%49) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
    return %50 : tensor<2048x2048xbf16>
  }
}


2025-10-23 04:30:27.718 (  14.170s) [        2146C480]      module_builder.cc:963      1| MLIR Module ttir:
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p0.1"), %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p1.2"), %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p2.4"), %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p3.5"), %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p4.7")) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc)
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16> loc(#loc)
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16> loc(#loc)
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16> loc(#loc)
    %4 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc)
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
    %6 = ttir.empty() : tensor<1x1xbf16> loc(#loc)
    %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16> loc(#loc)
    %8 = ttir.empty() : tensor<2048x256xbf16> loc(#loc)
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc)
    %10 = ttir.empty() : tensor<2048x256xbf16> loc(#loc6)
    %11 = "ttir.permute"(%3, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc6)
    %12 = "ttir.dot_general"(%4, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc7)
    %13 = ttir.empty() : tensor<1x256xbf16> loc(#loc8)
    %14 = "ttir.reshape"(%2, %13) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16> loc(#loc8)
    %15 = ttir.empty() : tensor<2048x256xbf16> loc(#loc8)
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc8)
    %17 = ttir.empty() : tensor<2048x256xbf16> loc(#loc9)
    %18 = "ttir.add"(%12, %16, %17) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc9)
    %19 = ttir.empty() : tensor<2048x256xbf16> loc(#loc10)
    %20 = "ttir.maximum"(%18, %9, %19) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc10)
    %21 = ttir.empty() : tensor<256x2048xbf16> loc(#loc11)
    %22 = "ttir.permute"(%1, %21) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16> loc(#loc11)
    %23 = "ttir.dot_general"(%20, %22) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
    %24 = ttir.empty() : tensor<2048x2048xbf16> loc(#loc12)
    %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16> loc(#loc12)
    %26 = ttir.empty() : tensor<2048x8x256xbf16> loc(#loc12)
    %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16> loc(#loc12)
    %28 = ttir.empty() : tensor<2048x8x256xbf16> loc(#loc12)
    %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16> loc(#loc12)
    %30 = ttir.empty() : tensor<2048x1x256xbf16> loc(#loc12)
    %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16> loc(#loc12)
    %32 = ttir.empty() : tensor<2048x256xbf16> loc(#loc12)
    %33 = "ttir.reshape"(%31, %32) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc12)
    %34 = ttir.empty() : tensor<8x256xbf16> loc(#loc1)
    %35 = "ttir.reshape"(%0, %34) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16> loc(#loc1)
    %36 = ttir.empty() : tensor<8x256xbf16> loc(#loc1)
    %37 = "ttir.all_to_all"(%35, %36) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16> loc(#loc1)
    %38 = ttir.empty() : tensor<1x256xbf16> loc(#loc1)
    %39 = "ttir.slice_static"(%37, %38) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16> loc(#loc1)
    %40 = ttir.empty() : tensor<256xbf16> loc(#loc1)
    %41 = "ttir.reshape"(%39, %40) <{shape = [256 : i32]}> : (tensor<1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc1)
    %42 = ttir.empty() : tensor<1x256xbf16> loc(#loc13)
    %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16> loc(#loc13)
    %44 = ttir.empty() : tensor<2048x256xbf16> loc(#loc13)
    %45 = "ttir.broadcast"(%43, %44) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc13)
    %46 = ttir.empty() : tensor<2048x256xbf16> loc(#loc14)
    %47 = "ttir.add"(%33, %45, %46) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc14)
    %48 = ttir.empty() : tensor<2048x256xbf16> loc(#loc15)
    %49 = "ttir.maximum"(%47, %9, %48) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16> loc(#loc15)
    %50 = "ttir.mesh_shard"(%49) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16> loc(#loc)
    return %50 : tensor<2048x2048xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("transpose.6")
#loc7 = loc("dot.8")
#loc8 = loc("broadcast.12")
#loc9 = loc("add.13")
#loc10 = loc("maximum.16")
#loc11 = loc("transpose.3")
#loc12 = loc("dot.17")
#loc13 = loc("broadcast.21")
#loc14 = loc("add.22")
#loc15 = loc("maximum.25")
------------------ END OF MLIR MODULE ------------------
2025-10-23 04:30:27.719 (  14.171s) [        2146C480]      module_builder.cc:772   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-10-23 04:30:27.719 (  14.171s) [        2146C480]      module_builder.cc:786   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-10-23 04:30:27.719 (  14.171s) [        2146C480]      module_builder.cc:798   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2025-10-23 04:30:27.720 (  14.172s) [        2146C480]     client_instance.cc:373      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 8]
2025-10-23 04:30:27.720 (  14.172s) [        2146C480]     client_instance.cc:471      1| ClientInstance::getOrCreateOptimizerSubmesh - creating optimizer submesh
// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.mesh_shard"(%arg0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<2048xbf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = "ttir.mesh_shard"(%arg4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %6 = ttir.empty() : tensor<1x1xbf16>
    %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = ttir.empty() : tensor<2048x256xbf16>
    %11 = "ttir.permute"(%3, %10) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %12 = "ttir.dot_general"(%4, %11) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %13 = ttir.empty() : tensor<1x256xbf16>
    %14 = "ttir.reshape"(%2, %13) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<2048x256xbf16>
    %18 = "ttir.add"(%12, %16, %17) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %19 = ttir.empty() : tensor<2048x256xbf16>
    %20 = "ttir.maximum"(%18, %9, %19) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %21 = ttir.empty() : tensor<256x2048xbf16>
    %22 = "ttir.permute"(%1, %21) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %23 = "ttir.dot_general"(%20, %22) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %24 = ttir.empty() : tensor<2048x2048xbf16>
    %25 = "ttir.all_reduce"(%23, %24) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %26 = ttir.empty() : tensor<2048x8x256xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %28 = ttir.empty() : tensor<2048x8x256xbf16>
    %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %30 = ttir.empty() : tensor<2048x1x256xbf16>
    %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
    %32 = ttir.empty() : tensor<2048x256xbf16>
    %33 = "ttir.reshape"(%31, %32) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %34 = ttir.empty() : tensor<8x256xbf16>
    %35 = "ttir.reshape"(%0, %34) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %36 = ttir.empty() : tensor<8x256xbf16>
    %37 = "ttir.all_to_all"(%35, %36) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %38 = ttir.empty() : tensor<1x256xbf16>
    %39 = "ttir.slice_static"(%37, %38) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %40 = ttir.empty() : tensor<256xbf16>
    %41 = "ttir.reshape"(%39, %40) <{shape = [256 : i32]}> : (tensor<1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16>
    %42 = ttir.empty() : tensor<1x256xbf16>
    %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %44 = ttir.empty() : tensor<2048x256xbf16>
    %45 = "ttir.broadcast"(%43, %44) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %46 = ttir.empty() : tensor<2048x256xbf16>
    %47 = "ttir.add"(%33, %45, %46) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %48 = ttir.empty() : tensor<2048x256xbf16>
    %49 = "ttir.maximum"(%47, %9, %48) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %50 = "ttir.mesh_shard"(%49) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
    return %50 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x256xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = "ttir.dot_general"(%arg4, %9) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %11 = ttir.empty() : tensor<1x256xbf16>
    %12 = "ttir.reshape"(%2, %11) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %13 = ttir.empty() : tensor<2048x256xbf16>
    %14 = "ttir.broadcast"(%12, %13) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.add"(%10, %14, %15) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<2048x256xbf16>
    %18 = "ttir.maximum"(%16, %7, %17) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %19 = ttir.empty() : tensor<256x2048xbf16>
    %20 = "ttir.permute"(%1, %19) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %21 = "ttir.dot_general"(%18, %20) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %22 = ttir.empty() : tensor<2048x2048xbf16>
    %23 = "ttir.all_reduce"(%21, %22) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %24 = ttir.empty() : tensor<2048x8x256xbf16>
    %25 = "ttir.reshape"(%23, %24) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %26 = ttir.empty() : tensor<2048x8x256xbf16>
    %27 = "ttir.all_to_all"(%25, %26) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %28 = ttir.empty() : tensor<2048x1x256xbf16>
    %29 = "ttir.slice_static"(%27, %28) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
    %30 = ttir.empty() : tensor<2048x256xbf16>
    %31 = "ttir.reshape"(%29, %30) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %32 = ttir.empty() : tensor<8x256xbf16>
    %33 = "ttir.reshape"(%arg0, %32) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %34 = ttir.empty() : tensor<8x256xbf16>
    %35 = "ttir.all_to_all"(%33, %34) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %36 = ttir.empty() : tensor<1x256xbf16>
    %37 = "ttir.slice_static"(%35, %36) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %38 = ttir.empty() : tensor<2048x256xbf16>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %40 = ttir.empty() : tensor<2048x256xbf16>
    %41 = "ttir.add"(%31, %39, %40) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %42 = ttir.empty() : tensor<2048x256xbf16>
    %43 = "ttir.maximum"(%41, %7, %42) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
    return %44 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x256xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = "ttir.dot_general"(%arg4, %9) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %11 = ttir.empty() : tensor<1x256xbf16>
    %12 = "ttir.reshape"(%2, %11) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %13 = ttir.empty() : tensor<2048x256xbf16>
    %14 = "ttir.broadcast"(%12, %13) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.add"(%10, %14, %15) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<2048x256xbf16>
    %18 = "ttir.maximum"(%16, %7, %17) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %19 = ttir.empty() : tensor<256x2048xbf16>
    %20 = "ttir.permute"(%1, %19) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %21 = "ttir.dot_general"(%18, %20) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %22 = ttir.empty() : tensor<2048x2048xbf16>
    %23 = "ttir.all_reduce"(%21, %22) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %24 = ttir.empty() : tensor<2048x8x256xbf16>
    %25 = "ttir.reshape"(%23, %24) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %26 = ttir.empty() : tensor<2048x8x256xbf16>
    %27 = "ttir.all_to_all"(%25, %26) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %28 = ttir.empty() : tensor<2048x1x256xbf16>
    %29 = "ttir.slice_static"(%27, %28) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
    %30 = ttir.empty() : tensor<2048x256xbf16>
    %31 = "ttir.reshape"(%29, %30) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %32 = ttir.empty() : tensor<8x256xbf16>
    %33 = "ttir.reshape"(%arg0, %32) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %34 = ttir.empty() : tensor<8x256xbf16>
    %35 = "ttir.all_to_all"(%33, %34) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %36 = ttir.empty() : tensor<1x256xbf16>
    %37 = "ttir.slice_static"(%35, %36) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %38 = ttir.empty() : tensor<2048x256xbf16>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %40 = ttir.empty() : tensor<2048x256xbf16>
    %41 = "ttir.add"(%31, %39, %40) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %42 = ttir.empty() : tensor<2048x256xbf16>
    %43 = "ttir.maximum"(%41, %7, %42) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
    return %44 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x256xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = "ttir.dot_general"(%arg4, %9) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %11 = ttir.empty() : tensor<1x256xbf16>
    %12 = "ttir.reshape"(%2, %11) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %13 = ttir.empty() : tensor<2048x256xbf16>
    %14 = "ttir.broadcast"(%12, %13) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %15 = ttir.empty() : tensor<2048x256xbf16>
    %16 = "ttir.add"(%10, %14, %15) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %17 = ttir.empty() : tensor<2048x256xbf16>
    %18 = "ttir.maximum"(%16, %7, %17) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %19 = ttir.empty() : tensor<256x2048xbf16>
    %20 = "ttir.permute"(%1, %19) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %21 = "ttir.dot_general"(%18, %20) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<2048x2048xbf16>
    %22 = ttir.empty() : tensor<2048x2048xbf16>
    %23 = "ttir.all_reduce"(%21, %22) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %24 = ttir.empty() : tensor<2048x8x256xbf16>
    %25 = "ttir.reshape"(%23, %24) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %26 = ttir.empty() : tensor<2048x8x256xbf16>
    %27 = "ttir.all_to_all"(%25, %26) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %28 = ttir.empty() : tensor<2048x1x256xbf16>
    %29 = "ttir.slice_static"(%27, %28) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
    %30 = ttir.empty() : tensor<2048x256xbf16>
    %31 = "ttir.reshape"(%29, %30) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %32 = ttir.empty() : tensor<8x256xbf16>
    %33 = "ttir.reshape"(%arg0, %32) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %34 = ttir.empty() : tensor<8x256xbf16>
    %35 = "ttir.all_to_all"(%33, %34) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %36 = ttir.empty() : tensor<1x256xbf16>
    %37 = "ttir.slice_static"(%35, %36) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %38 = ttir.empty() : tensor<2048x256xbf16>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %40 = ttir.empty() : tensor<2048x256xbf16>
    %41 = "ttir.add"(%31, %39, %40) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %42 = ttir.empty() : tensor<2048x256xbf16>
    %43 = "ttir.maximum"(%41, %7, %42) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %44 = "ttir.mesh_shard"(%43) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
    return %44 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x256xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = ttir.empty() : tensor<2048x2048xbf16>
    %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %12 = ttir.empty() : tensor<2048x256xbf16>
    %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %14 = ttir.empty() : tensor<2048x2048xbf16>
    %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %16 = ttir.empty() : tensor<2048x256xbf16>
    %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %18 = ttir.empty() : tensor<2048x256xbf16>
    %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %20 = ttir.empty() : tensor<2048x256xbf16>
    %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %22 = ttir.empty() : tensor<1x256xbf16>
    %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %24 = ttir.empty() : tensor<2048x256xbf16>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %26 = ttir.empty() : tensor<2048x256xbf16>
    %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %28 = ttir.empty() : tensor<2048x256xbf16>
    %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %30 = ttir.empty() : tensor<256x2048xbf16>
    %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %32 = ttir.empty() : tensor<2048x256xbf16>
    %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %34 = ttir.empty() : tensor<256x2048xbf16>
    %35 = "ttir.permute"(%31, %34) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %36 = ttir.empty() : tensor<2048x256xbf16>
    %37 = "ttir.reshape"(%33, %36) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %38 = ttir.empty() : tensor<256x2048xbf16>
    %39 = "ttir.reshape"(%35, %38) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %40 = ttir.empty() : tensor<2048x2048xbf16>
    %41 = "ttir.matmul"(%37, %39, %40) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %42 = ttir.empty() : tensor<2048x2048xbf16>
    %43 = "ttir.reshape"(%41, %42) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %44 = ttir.empty() : tensor<2048x2048xbf16>
    %45 = "ttir.all_reduce"(%43, %44) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %46 = ttir.empty() : tensor<2048x8x256xbf16>
    %47 = "ttir.reshape"(%45, %46) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %48 = ttir.empty() : tensor<2048x8x256xbf16>
    %49 = "ttir.all_to_all"(%47, %48) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %50 = ttir.empty() : tensor<2048x1x256xbf16>
    %51 = "ttir.slice_static"(%49, %50) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
    %52 = ttir.empty() : tensor<2048x256xbf16>
    %53 = "ttir.reshape"(%51, %52) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %54 = ttir.empty() : tensor<8x256xbf16>
    %55 = "ttir.reshape"(%arg0, %54) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %56 = ttir.empty() : tensor<8x256xbf16>
    %57 = "ttir.all_to_all"(%55, %56) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %58 = ttir.empty() : tensor<1x256xbf16>
    %59 = "ttir.slice_static"(%57, %58) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %60 = ttir.empty() : tensor<2048x256xbf16>
    %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %62 = ttir.empty() : tensor<2048x256xbf16>
    %63 = "ttir.add"(%53, %61, %62) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %64 = ttir.empty() : tensor<2048x256xbf16>
    %65 = "ttir.maximum"(%63, %7, %64) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
    return %66 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump Before TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
    %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
    %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
    %4 = ttir.empty() : tensor<1x1xbf16>
    %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %6 = ttir.empty() : tensor<2048x256xbf16>
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %8 = ttir.empty() : tensor<2048x256xbf16>
    %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %10 = ttir.empty() : tensor<2048x2048xbf16>
    %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %12 = ttir.empty() : tensor<2048x256xbf16>
    %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %14 = ttir.empty() : tensor<2048x2048xbf16>
    %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %16 = ttir.empty() : tensor<2048x256xbf16>
    %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %18 = ttir.empty() : tensor<2048x256xbf16>
    %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %20 = ttir.empty() : tensor<2048x256xbf16>
    %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %22 = ttir.empty() : tensor<1x256xbf16>
    %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %24 = ttir.empty() : tensor<2048x256xbf16>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %26 = ttir.empty() : tensor<2048x256xbf16>
    %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %28 = ttir.empty() : tensor<2048x256xbf16>
    %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %30 = ttir.empty() : tensor<256x2048xbf16>
    %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %32 = ttir.empty() : tensor<2048x256xbf16>
    %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %34 = ttir.empty() : tensor<256x2048xbf16>
    %35 = "ttir.permute"(%31, %34) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %36 = ttir.empty() : tensor<2048x256xbf16>
    %37 = "ttir.reshape"(%33, %36) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %38 = ttir.empty() : tensor<256x2048xbf16>
    %39 = "ttir.reshape"(%35, %38) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
    %40 = ttir.empty() : tensor<2048x2048xbf16>
    %41 = "ttir.matmul"(%37, %39, %40) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %42 = ttir.empty() : tensor<2048x2048xbf16>
    %43 = "ttir.reshape"(%41, %42) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %44 = ttir.empty() : tensor<2048x2048xbf16>
    %45 = "ttir.all_reduce"(%43, %44) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
    %46 = ttir.empty() : tensor<2048x8x256xbf16>
    %47 = "ttir.reshape"(%45, %46) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %48 = ttir.empty() : tensor<2048x8x256xbf16>
    %49 = "ttir.all_to_all"(%47, %48) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
    %50 = ttir.empty() : tensor<2048x1x256xbf16>
    %51 = "ttir.slice_static"(%49, %50) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
    %52 = ttir.empty() : tensor<2048x256xbf16>
    %53 = "ttir.reshape"(%51, %52) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %54 = ttir.empty() : tensor<8x256xbf16>
    %55 = "ttir.reshape"(%arg0, %54) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %56 = ttir.empty() : tensor<8x256xbf16>
    %57 = "ttir.all_to_all"(%55, %56) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
    %58 = ttir.empty() : tensor<1x256xbf16>
    %59 = "ttir.slice_static"(%57, %58) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
    %60 = ttir.empty() : tensor<2048x256xbf16>
    %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %62 = ttir.empty() : tensor<2048x256xbf16>
    %63 = "ttir.add"(%53, %61, %62) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %64 = ttir.empty() : tensor<2048x256xbf16>
    %65 = "ttir.maximum"(%63, %7, %64) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
    %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
    return %66 : tensor<2048x2048xbf16>
  }
}


// -----// IR Dump After TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<2048x256xbf16>
        %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %30 = ttir.empty() : tensor<256x2048xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %32 = ttir.empty() : tensor<2048x256xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %34 = ttir.empty() : tensor<256x2048xbf16>
        %35 = "ttir.permute"(%31, %34) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<256x2048xbf16>
        %39 = "ttir.reshape"(%35, %38) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.matmul"(%37, %39, %40) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<2048x2048xbf16>
        %45 = "ttir.all_reduce"(%43, %44) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %46 = ttir.empty() : tensor<2048x8x256xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %48 = ttir.empty() : tensor<2048x8x256xbf16>
        %49 = "ttir.all_to_all"(%47, %48) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %50 = ttir.empty() : tensor<2048x1x256xbf16>
        %51 = "ttir.slice_static"(%49, %50) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %52 = ttir.empty() : tensor<2048x256xbf16>
        %53 = "ttir.reshape"(%51, %52) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %54 = ttir.empty() : tensor<8x256xbf16>
        %55 = "ttir.reshape"(%arg0, %54) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %56 = ttir.empty() : tensor<8x256xbf16>
        %57 = "ttir.all_to_all"(%55, %56) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %58 = ttir.empty() : tensor<1x256xbf16>
        %59 = "ttir.slice_static"(%57, %58) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %60 = ttir.empty() : tensor<2048x256xbf16>
        %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %62 = ttir.empty() : tensor<2048x256xbf16>
        %63 = "ttir.add"(%53, %61, %62) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %64 = ttir.empty() : tensor<2048x256xbf16>
        %65 = "ttir.maximum"(%63, %7, %64) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %66 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRHoistTransform (ttir-cpu-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<2048x256xbf16>
        %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %30 = ttir.empty() : tensor<256x2048xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %32 = ttir.empty() : tensor<2048x256xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %34 = ttir.empty() : tensor<256x2048xbf16>
        %35 = "ttir.permute"(%31, %34) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<256x2048xbf16>
        %39 = "ttir.reshape"(%35, %38) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.matmul"(%37, %39, %40) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<2048x2048xbf16>
        %45 = "ttir.all_reduce"(%43, %44) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %46 = ttir.empty() : tensor<2048x8x256xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %48 = ttir.empty() : tensor<2048x8x256xbf16>
        %49 = "ttir.all_to_all"(%47, %48) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %50 = ttir.empty() : tensor<2048x1x256xbf16>
        %51 = "ttir.slice_static"(%49, %50) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %52 = ttir.empty() : tensor<2048x256xbf16>
        %53 = "ttir.reshape"(%51, %52) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %54 = ttir.empty() : tensor<8x256xbf16>
        %55 = "ttir.reshape"(%arg0, %54) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %56 = ttir.empty() : tensor<8x256xbf16>
        %57 = "ttir.all_to_all"(%55, %56) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %58 = ttir.empty() : tensor<1x256xbf16>
        %59 = "ttir.slice_static"(%57, %58) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %60 = ttir.empty() : tensor<2048x256xbf16>
        %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %62 = ttir.empty() : tensor<2048x256xbf16>
        %63 = "ttir.add"(%53, %61, %62) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %64 = ttir.empty() : tensor<2048x256xbf16>
        %65 = "ttir.maximum"(%63, %7, %64) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %66 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<2048x256xbf16>
        %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %30 = ttir.empty() : tensor<256x2048xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %32 = ttir.empty() : tensor<2048x256xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %34 = ttir.empty() : tensor<256x2048xbf16>
        %35 = "ttir.permute"(%31, %34) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<256x2048xbf16>
        %39 = "ttir.reshape"(%35, %38) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.matmul"(%37, %39, %40) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<2048x2048xbf16>
        %45 = "ttir.all_reduce"(%43, %44) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %46 = ttir.empty() : tensor<2048x8x256xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %48 = ttir.empty() : tensor<2048x8x256xbf16>
        %49 = "ttir.all_to_all"(%47, %48) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %50 = ttir.empty() : tensor<2048x1x256xbf16>
        %51 = "ttir.slice_static"(%49, %50) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %52 = ttir.empty() : tensor<2048x256xbf16>
        %53 = "ttir.reshape"(%51, %52) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %54 = ttir.empty() : tensor<8x256xbf16>
        %55 = "ttir.reshape"(%arg0, %54) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %56 = ttir.empty() : tensor<8x256xbf16>
        %57 = "ttir.all_to_all"(%55, %56) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %58 = ttir.empty() : tensor<1x256xbf16>
        %59 = "ttir.slice_static"(%57, %58) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %60 = ttir.empty() : tensor<2048x256xbf16>
        %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %62 = ttir.empty() : tensor<2048x256xbf16>
        %63 = "ttir.add"(%53, %61, %62) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %64 = ttir.empty() : tensor<2048x256xbf16>
        %65 = "ttir.maximum"(%63, %7, %64) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %66 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<2048x256xbf16>
        %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %30 = ttir.empty() : tensor<256x2048xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %32 = ttir.empty() : tensor<2048x256xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %34 = ttir.empty() : tensor<256x2048xbf16>
        %35 = "ttir.permute"(%31, %34) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<256x2048xbf16>
        %39 = "ttir.reshape"(%35, %38) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.matmul"(%37, %39, %40) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<2048x2048xbf16>
        %45 = "ttir.all_reduce"(%43, %44) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %46 = ttir.empty() : tensor<2048x8x256xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %48 = ttir.empty() : tensor<2048x8x256xbf16>
        %49 = "ttir.all_to_all"(%47, %48) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %50 = ttir.empty() : tensor<2048x1x256xbf16>
        %51 = "ttir.slice_static"(%49, %50) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %52 = ttir.empty() : tensor<2048x256xbf16>
        %53 = "ttir.reshape"(%51, %52) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %54 = ttir.empty() : tensor<8x256xbf16>
        %55 = "ttir.reshape"(%arg0, %54) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %56 = ttir.empty() : tensor<8x256xbf16>
        %57 = "ttir.all_to_all"(%55, %56) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %58 = ttir.empty() : tensor<1x256xbf16>
        %59 = "ttir.slice_static"(%57, %58) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %60 = ttir.empty() : tensor<2048x256xbf16>
        %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %62 = ttir.empty() : tensor<2048x256xbf16>
        %63 = "ttir.add"(%53, %61, %62) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %64 = ttir.empty() : tensor<2048x256xbf16>
        %65 = "ttir.maximum"(%63, %7, %64) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %66 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<2048x256xbf16>
        %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %30 = ttir.empty() : tensor<256x2048xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %32 = ttir.empty() : tensor<2048x256xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %34 = ttir.empty() : tensor<256x2048xbf16>
        %35 = "ttir.permute"(%31, %34) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<256x2048xbf16>
        %39 = "ttir.reshape"(%35, %38) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.matmul"(%37, %39, %40) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<2048x2048xbf16>
        %45 = "ttir.all_reduce"(%43, %44) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %46 = ttir.empty() : tensor<2048x8x256xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %48 = ttir.empty() : tensor<2048x8x256xbf16>
        %49 = "ttir.all_to_all"(%47, %48) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %50 = ttir.empty() : tensor<2048x1x256xbf16>
        %51 = "ttir.slice_static"(%49, %50) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %52 = ttir.empty() : tensor<2048x256xbf16>
        %53 = "ttir.reshape"(%51, %52) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %54 = ttir.empty() : tensor<8x256xbf16>
        %55 = "ttir.reshape"(%arg0, %54) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %56 = ttir.empty() : tensor<8x256xbf16>
        %57 = "ttir.all_to_all"(%55, %56) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %58 = ttir.empty() : tensor<1x256xbf16>
        %59 = "ttir.slice_static"(%57, %58) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %60 = ttir.empty() : tensor<2048x256xbf16>
        %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %62 = ttir.empty() : tensor<2048x256xbf16>
        %63 = "ttir.add"(%53, %61, %62) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %64 = ttir.empty() : tensor<2048x256xbf16>
        %65 = "ttir.maximum"(%63, %7, %64) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %66 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.permute"(%3, %8) <{permutation = array<i64: 1, 0>}> : (tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x2048xbf16>
        %11 = "ttir.permute"(%arg4, %10) <{permutation = array<i64: 0, 1>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.permute"(%9, %12) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.reshape"(%11, %14) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.reshape"(%13, %16) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x256xbf16>
        %19 = "ttir.matmul"(%15, %17, %18) <{transpose_a = false, transpose_b = false}> : (tensor<2048x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %20 = ttir.empty() : tensor<2048x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %22 = ttir.empty() : tensor<1x256xbf16>
        %23 = "ttir.reshape"(%2, %22) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.add"(%21, %25, %26) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<2048x256xbf16>
        %29 = "ttir.maximum"(%27, %7, %28) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %30 = ttir.empty() : tensor<256x2048xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 1, 0>}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %32 = ttir.empty() : tensor<2048x256xbf16>
        %33 = "ttir.permute"(%29, %32) <{permutation = array<i64: 0, 1>}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %34 = ttir.empty() : tensor<256x2048xbf16>
        %35 = "ttir.permute"(%31, %34) <{permutation = array<i64: 0, 1>}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.reshape"(%33, %36) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<256x2048xbf16>
        %39 = "ttir.reshape"(%35, %38) <{shape = [256 : i32, 2048 : i32]}> : (tensor<256x2048xbf16>, tensor<256x2048xbf16>) -> tensor<256x2048xbf16>
        %40 = ttir.empty() : tensor<2048x2048xbf16>
        %41 = "ttir.matmul"(%37, %39, %40) <{transpose_a = false, transpose_b = false}> : (tensor<2048x256xbf16>, tensor<256x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %42 = ttir.empty() : tensor<2048x2048xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %44 = ttir.empty() : tensor<2048x2048xbf16>
        %45 = "ttir.all_reduce"(%43, %44) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %46 = ttir.empty() : tensor<2048x8x256xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %48 = ttir.empty() : tensor<2048x8x256xbf16>
        %49 = "ttir.all_to_all"(%47, %48) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %50 = ttir.empty() : tensor<2048x1x256xbf16>
        %51 = "ttir.slice_static"(%49, %50) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %52 = ttir.empty() : tensor<2048x256xbf16>
        %53 = "ttir.reshape"(%51, %52) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %54 = ttir.empty() : tensor<8x256xbf16>
        %55 = "ttir.reshape"(%arg0, %54) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %56 = ttir.empty() : tensor<8x256xbf16>
        %57 = "ttir.all_to_all"(%55, %56) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %58 = ttir.empty() : tensor<1x256xbf16>
        %59 = "ttir.slice_static"(%57, %58) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %60 = ttir.empty() : tensor<2048x256xbf16>
        %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %62 = ttir.empty() : tensor<2048x256xbf16>
        %63 = "ttir.add"(%53, %61, %62) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %64 = ttir.empty() : tensor<2048x256xbf16>
        %65 = "ttir.maximum"(%63, %7, %64) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %66 = "ttir.mesh_shard"(%65) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %66 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.matmul"(%arg4, %3, %8) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<1x256xbf16>
        %11 = "ttir.reshape"(%2, %10) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.add"(%9, %13, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.maximum"(%15, %7, %16) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.matmul"(%17, %1, %18) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.reshape"(%21, %22) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x8x256xbf16>
        %25 = "ttir.all_to_all"(%23, %24) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %26 = ttir.empty() : tensor<2048x1x256xbf16>
        %27 = "ttir.slice_static"(%25, %26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %28 = ttir.empty() : tensor<2048x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.reshape"(%arg0, %30) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<8x256xbf16>
        %33 = "ttir.all_to_all"(%31, %32) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.slice_static"(%33, %34) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.add"(%29, %37, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = ttir.empty() : tensor<2048x256xbf16>
        %41 = "ttir.maximum"(%39, %7, %40) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %42 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.matmul"(%arg4, %3, %8) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<1x256xbf16>
        %11 = "ttir.reshape"(%2, %10) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.add"(%9, %13, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x256xbf16>
        %17 = "ttir.maximum"(%15, %7, %16) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.matmul"(%17, %1, %18) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x2048xbf16>
        %21 = "ttir.all_reduce"(%19, %20) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.reshape"(%21, %22) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x8x256xbf16>
        %25 = "ttir.all_to_all"(%23, %24) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %26 = ttir.empty() : tensor<2048x1x256xbf16>
        %27 = "ttir.slice_static"(%25, %26) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %28 = ttir.empty() : tensor<2048x256xbf16>
        %29 = "ttir.reshape"(%27, %28) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.reshape"(%arg0, %30) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<8x256xbf16>
        %33 = "ttir.all_to_all"(%31, %32) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %34 = ttir.empty() : tensor<1x256xbf16>
        %35 = "ttir.slice_static"(%33, %34) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.add"(%29, %37, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = ttir.empty() : tensor<2048x256xbf16>
        %41 = "ttir.maximum"(%39, %7, %40) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %42 = "ttir.mesh_shard"(%41) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %42 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDequantConversion (ttir-quant-dequant-conversion) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRExplicateTMs (ttir-explicate-tms) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<2048x256xbf16>
        %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 2048, 256>}> : (tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %8 = ttir.empty() : tensor<1x256xbf16>
        %9 = "ttir.reshape"(%2, %8) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.linear"(%arg4, %3, %11, %12) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x256xbf16>
        %15 = "ttir.maximum"(%13, %7, %14) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.matmul"(%15, %1, %16) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x2048xbf16>
        %19 = "ttir.all_reduce"(%17, %18) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x8x256xbf16>
        %23 = "ttir.all_to_all"(%21, %22) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %24 = ttir.empty() : tensor<2048x1x256xbf16>
        %25 = "ttir.slice_static"(%23, %24) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %26 = ttir.empty() : tensor<2048x256xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.reshape"(%arg0, %28) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<8x256xbf16>
        %31 = "ttir.all_to_all"(%29, %30) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %32 = ttir.empty() : tensor<1x256xbf16>
        %33 = "ttir.slice_static"(%31, %32) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.broadcast"(%33, %34) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = ttir.empty() : tensor<2048x256xbf16>
        %37 = "ttir.add"(%27, %35, %36) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %38 = ttir.empty() : tensor<2048x256xbf16>
        %39 = "ttir.maximum"(%37, %7, %38) : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %40 = "ttir.mesh_shard"(%39) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %40 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<1x256xbf16>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.maximum"(%11, %5, %12) : (tensor<2048x256xbf16>, tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x8x256xbf16>
        %19 = "ttir.reshape"(%17, %18) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.all_to_all"(%19, %20) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x1x256xbf16>
        %23 = "ttir.slice_static"(%21, %22) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.reshape"(%23, %24) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<8x256xbf16>
        %27 = "ttir.reshape"(%arg0, %26) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %32 = ttir.empty() : tensor<2048x256xbf16>
        %33 = "ttir.add"(%25, %31, %32) : (tensor<2048x256xbf16>, tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.maximum"(%33, %5, %34) : (tensor<2048x256xbf16>, tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = "ttir.mesh_shard"(%35) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %36 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDataTypeConversionPass (ttir-quant-data-type-conversion) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<1x256xbf16>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.maximum"(%11, %5, %12) : (tensor<2048x256xbf16>, tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x8x256xbf16>
        %19 = "ttir.reshape"(%17, %18) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.all_to_all"(%19, %20) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x1x256xbf16>
        %23 = "ttir.slice_static"(%21, %22) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.reshape"(%23, %24) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<8x256xbf16>
        %27 = "ttir.reshape"(%arg0, %26) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %32 = ttir.empty() : tensor<2048x256xbf16>
        %33 = "ttir.add"(%25, %31, %32) : (tensor<2048x256xbf16>, tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.maximum"(%33, %5, %34) : (tensor<2048x256xbf16>, tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = "ttir.mesh_shard"(%35) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %36 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<2048x256xbf16>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16>) -> tensor<256xbf16>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16>) -> tensor<256x2048xbf16>
        %4 = ttir.empty() : tensor<1x1xbf16>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %6 = ttir.empty() : tensor<1x256xbf16>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %8 = ttir.empty() : tensor<2048x256xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %10 = ttir.empty() : tensor<2048x256xbf16>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16>, tensor<256x2048xbf16>, tensor<2048x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %12 = ttir.empty() : tensor<2048x256xbf16>
        %13 = "ttir.maximum"(%11, %5, %12) : (tensor<2048x256xbf16>, tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %14 = ttir.empty() : tensor<2048x2048xbf16>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16>, tensor<2048x256xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %16 = ttir.empty() : tensor<2048x2048xbf16>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16>, tensor<2048x2048xbf16>) -> tensor<2048x2048xbf16>
        %18 = ttir.empty() : tensor<2048x8x256xbf16>
        %19 = "ttir.reshape"(%17, %18) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %20 = ttir.empty() : tensor<2048x8x256xbf16>
        %21 = "ttir.all_to_all"(%19, %20) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16>, tensor<2048x8x256xbf16>) -> tensor<2048x8x256xbf16>
        %22 = ttir.empty() : tensor<2048x1x256xbf16>
        %23 = "ttir.slice_static"(%21, %22) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16>, tensor<2048x1x256xbf16>) -> tensor<2048x1x256xbf16>
        %24 = ttir.empty() : tensor<2048x256xbf16>
        %25 = "ttir.reshape"(%23, %24) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %26 = ttir.empty() : tensor<8x256xbf16>
        %27 = "ttir.reshape"(%arg0, %26) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %28 = ttir.empty() : tensor<8x256xbf16>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16>, tensor<8x256xbf16>) -> tensor<8x256xbf16>
        %30 = ttir.empty() : tensor<1x256xbf16>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16>
        %32 = ttir.empty() : tensor<2048x256xbf16>
        %33 = "ttir.add"(%25, %31, %32) : (tensor<2048x256xbf16>, tensor<1x256xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %34 = ttir.empty() : tensor<2048x256xbf16>
        %35 = "ttir.maximum"(%33, %5, %34) : (tensor<2048x256xbf16>, tensor<1x1xbf16>, tensor<2048x256xbf16>) -> tensor<2048x256xbf16>
        %36 = "ttir.mesh_shard"(%35) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16>) -> tensor<2048x2048xbf16>
        return %36 : tensor<2048x2048xbf16>
      }
    }
  }
}


// -----// IR Dump After TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout3>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<256xbf16, #ttnn_layout5>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<256x2048xbf16, #ttnn_layout6>
        %4 = ttir.empty() : tensor<1x1xbf16, #ttnn_layout7>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %8 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %12 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %13 = "ttir.maximum"(%11, %5, %12) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %14 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %18 = ttir.empty() : tensor<2048x8x256xbf16, #ttnn_layout9>
        %19 = "ttir.reshape"(%17, %18) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %20 = ttir.empty() : tensor<2048x8x256xbf16, #ttnn_layout9>
        %21 = "ttir.all_to_all"(%19, %20) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16, #ttnn_layout9>, tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %22 = ttir.empty() : tensor<2048x1x256xbf16, #ttnn_layout9>
        %23 = "ttir.slice_static"(%21, %22) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %24 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %25 = "ttir.reshape"(%23, %24) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %26 = ttir.empty() : tensor<8x256xbf16, #ttnn_layout8>
        %27 = "ttir.reshape"(%arg0, %26) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout>, tensor<8x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %28 = ttir.empty() : tensor<8x256xbf16, #ttnn_layout8>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16, #ttnn_layout8>, tensor<8x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %30 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout8>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %32 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %33 = "ttir.add"(%25, %31, %32) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x256xbf16, #ttnn_layout8>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %34 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %35 = "ttir.maximum"(%33, %5, %34) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %36 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout10>
        %37 = ttir.to_layout %35, %36 : tensor<2048x256xbf16, #ttnn_layout4> into tensor<2048x256xbf16, #ttnn_layout10> -> tensor<2048x256xbf16, #ttnn_layout10>
        %38 = "ttir.mesh_shard"(%37) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout10>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %38 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout3>
        %1 = "ttir.mesh_shard"(%arg1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %2 = "ttir.mesh_shard"(%arg2) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<256xbf16, #ttnn_layout5>
        %3 = "ttir.mesh_shard"(%arg3) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<256x2048xbf16, #ttnn_layout6>
        %4 = ttir.empty() : tensor<1x1xbf16, #ttnn_layout7>
        %5 = "ttir.reshape"(%0, %4) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttir.reshape"(%2, %6) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %8 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 2048, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %11 = "ttir.linear"(%arg4, %3, %9, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %12 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %13 = "ttir.maximum"(%11, %5, %12) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %14 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %15 = "ttir.matmul"(%13, %1, %14) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = ttir.empty() : tensor<2048x2048xbf16, #ttnn_layout1>
        %17 = "ttir.all_reduce"(%15, %16) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %18 = ttir.empty() : tensor<2048x8x256xbf16, #ttnn_layout9>
        %19 = "ttir.reshape"(%17, %18) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %20 = ttir.empty() : tensor<2048x8x256xbf16, #ttnn_layout9>
        %21 = "ttir.all_to_all"(%19, %20) <{concat_dim = 1 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 1 : si32}> : (tensor<2048x8x256xbf16, #ttnn_layout9>, tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %22 = ttir.empty() : tensor<2048x1x256xbf16, #ttnn_layout9>
        %23 = "ttir.slice_static"(%21, %22) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %24 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %25 = "ttir.reshape"(%23, %24) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %26 = ttir.empty() : tensor<8x256xbf16, #ttnn_layout8>
        %27 = "ttir.reshape"(%arg0, %26) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout>, tensor<8x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %28 = ttir.empty() : tensor<8x256xbf16, #ttnn_layout8>
        %29 = "ttir.all_to_all"(%27, %28) <{concat_dim = 0 : si32, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : si32, split_dim = 0 : si32}> : (tensor<8x256xbf16, #ttnn_layout8>, tensor<8x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %30 = ttir.empty() : tensor<1x256xbf16, #ttnn_layout8>
        %31 = "ttir.slice_static"(%29, %30) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %32 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %33 = "ttir.add"(%25, %31, %32) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x256xbf16, #ttnn_layout8>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %34 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout4>
        %35 = "ttir.maximum"(%33, %5, %34) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %36 = ttir.empty() : tensor<2048x256xbf16, #ttnn_layout10>
        %37 = ttir.to_layout %35, %36 : tensor<2048x256xbf16, #ttnn_layout4> into tensor<2048x256xbf16, #ttnn_layout10> -> tensor<2048x256xbf16, #ttnn_layout10>
        %38 = "ttir.mesh_shard"(%37) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout10>) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %38 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %7 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout8>
        %8 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %9 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = "ttnn.repeat"(%8) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %11 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %12 = "ttnn.linear"(%arg4, %4, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %13 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %14 = "ttnn.maximum"(%12, %6) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %15 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = "ttnn.matmul"(%14, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %17 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %18 = "ttnn.all_reduce"(%16, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %19 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x8x256>}> : (!ttnn.device) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %20 = "ttnn.reshape"(%18) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %21 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x8x256>}> : (!ttnn.device) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %22 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %23 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %24 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %25 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %26 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %27 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %28 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %29 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %30 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %31 = "ttnn.point_to_point"(%23, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %32 = "ttnn.point_to_point"(%24, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %33 = "ttnn.point_to_point"(%25, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %34 = "ttnn.point_to_point"(%26, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %35 = "ttnn.point_to_point"(%27, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %36 = "ttnn.point_to_point"(%28, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %37 = "ttnn.point_to_point"(%29, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %38 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %39 = "ttnn.point_to_point"(%23, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %40 = "ttnn.point_to_point"(%24, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %41 = "ttnn.point_to_point"(%25, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %42 = "ttnn.point_to_point"(%26, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %43 = "ttnn.point_to_point"(%27, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %44 = "ttnn.point_to_point"(%28, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %45 = "ttnn.point_to_point"(%29, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %46 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %47 = "ttnn.point_to_point"(%23, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %48 = "ttnn.point_to_point"(%24, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %49 = "ttnn.point_to_point"(%25, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %50 = "ttnn.point_to_point"(%26, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %51 = "ttnn.point_to_point"(%27, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %52 = "ttnn.point_to_point"(%28, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %53 = "ttnn.point_to_point"(%29, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %54 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %55 = "ttnn.point_to_point"(%23, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %56 = "ttnn.point_to_point"(%24, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %57 = "ttnn.point_to_point"(%25, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %58 = "ttnn.point_to_point"(%26, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %59 = "ttnn.point_to_point"(%27, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %60 = "ttnn.point_to_point"(%28, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %61 = "ttnn.point_to_point"(%29, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %62 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %63 = "ttnn.point_to_point"(%23, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %64 = "ttnn.point_to_point"(%24, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %65 = "ttnn.point_to_point"(%25, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %66 = "ttnn.point_to_point"(%26, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %67 = "ttnn.point_to_point"(%27, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %68 = "ttnn.point_to_point"(%28, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %69 = "ttnn.point_to_point"(%29, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %70 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %71 = "ttnn.point_to_point"(%23, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %72 = "ttnn.point_to_point"(%24, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %73 = "ttnn.point_to_point"(%25, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %74 = "ttnn.point_to_point"(%26, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %75 = "ttnn.point_to_point"(%27, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %76 = "ttnn.point_to_point"(%28, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %77 = "ttnn.point_to_point"(%29, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %78 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %79 = "ttnn.point_to_point"(%23, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %80 = "ttnn.point_to_point"(%24, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %81 = "ttnn.point_to_point"(%25, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %82 = "ttnn.point_to_point"(%26, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %83 = "ttnn.point_to_point"(%27, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %84 = "ttnn.point_to_point"(%28, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %85 = "ttnn.point_to_point"(%29, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %86 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %87 = "ttnn.point_to_point"(%23, %86) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %88 = "ttnn.point_to_point"(%24, %87) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %89 = "ttnn.point_to_point"(%25, %88) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %90 = "ttnn.point_to_point"(%26, %89) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %91 = "ttnn.point_to_point"(%27, %90) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %92 = "ttnn.point_to_point"(%28, %91) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %93 = "ttnn.point_to_point"(%29, %92) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %94 = "ttnn.concat"(%37, %45, %53, %61, %69, %77, %85, %93) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %95 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x1x256>}> : (!ttnn.device) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %96 = "ttnn.slice_static"(%94) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %97 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %98 = "ttnn.reshape"(%96) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %99 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<8x256>}> : (!ttnn.device) -> tensor<8x256xbf16, #ttnn_layout8>
        %100 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<8x256xbf16, #ttnn_layout8>
        %101 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<8x256>}> : (!ttnn.device) -> tensor<8x256xbf16, #ttnn_layout8>
        %102 = "ttnn.slice_static"(%100) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.slice_static"(%100) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.slice_static"(%100) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.slice_static"(%100) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.slice_static"(%100) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.slice_static"(%100) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.slice_static"(%100) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.slice_static"(%100) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%103, %110) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%104, %111) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%105, %112) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%106, %113) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%107, %114) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%108, %115) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%109, %116) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%103, %118) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%104, %119) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%105, %120) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%106, %121) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%107, %122) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%108, %123) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%109, %124) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%103, %126) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%104, %127) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%105, %128) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%106, %129) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%107, %130) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%108, %131) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%109, %132) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%103, %134) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%104, %135) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%105, %136) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%106, %137) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%107, %138) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%108, %139) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%109, %140) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%103, %142) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%104, %143) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%105, %144) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%106, %145) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%107, %146) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%108, %147) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%109, %148) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%103, %150) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%104, %151) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%105, %152) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%106, %153) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%107, %154) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%108, %155) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%109, %156) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%103, %158) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%104, %159) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%105, %160) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.point_to_point"(%106, %161) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %163 = "ttnn.point_to_point"(%107, %162) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %164 = "ttnn.point_to_point"(%108, %163) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %165 = "ttnn.point_to_point"(%109, %164) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %166 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %167 = "ttnn.point_to_point"(%103, %166) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %168 = "ttnn.point_to_point"(%104, %167) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %169 = "ttnn.point_to_point"(%105, %168) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %170 = "ttnn.point_to_point"(%106, %169) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %171 = "ttnn.point_to_point"(%107, %170) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %172 = "ttnn.point_to_point"(%108, %171) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %173 = "ttnn.point_to_point"(%109, %172) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %174 = "ttnn.concat"(%117, %125, %133, %141, %149, %157, %165, %173) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %175 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout8>
        %176 = "ttnn.slice_static"(%174) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %177 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %178 = "ttnn.add"(%98, %176) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %179 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %180 = "ttnn.maximum"(%178, %6) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %181 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<2048x256>}> : () -> tensor<2048x256xbf16, #ttnn_layout10>
        %182 = "ttnn.to_layout"(%180) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout10>
        %183 = "ttnn.mesh_shard"(%182, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %183 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before TTNNFusing (ttnn-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %7 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout8>
        %8 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %9 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = "ttnn.repeat"(%8) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %11 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %12 = "ttnn.linear"(%arg4, %4, %10) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %13 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %14 = "ttnn.maximum"(%12, %6) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %15 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %16 = "ttnn.matmul"(%14, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %17 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x2048>}> : (!ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %18 = "ttnn.all_reduce"(%16, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %19 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x8x256>}> : (!ttnn.device) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %20 = "ttnn.reshape"(%18) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %21 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x8x256>}> : (!ttnn.device) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %22 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %23 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %24 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %25 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %26 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %27 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %28 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %29 = "ttnn.slice_static"(%20) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %30 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %31 = "ttnn.point_to_point"(%23, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %32 = "ttnn.point_to_point"(%24, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %33 = "ttnn.point_to_point"(%25, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %34 = "ttnn.point_to_point"(%26, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %35 = "ttnn.point_to_point"(%27, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %36 = "ttnn.point_to_point"(%28, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %37 = "ttnn.point_to_point"(%29, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %38 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %39 = "ttnn.point_to_point"(%23, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %40 = "ttnn.point_to_point"(%24, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %41 = "ttnn.point_to_point"(%25, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %42 = "ttnn.point_to_point"(%26, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %43 = "ttnn.point_to_point"(%27, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %44 = "ttnn.point_to_point"(%28, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %45 = "ttnn.point_to_point"(%29, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %46 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %47 = "ttnn.point_to_point"(%23, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %48 = "ttnn.point_to_point"(%24, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %49 = "ttnn.point_to_point"(%25, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %50 = "ttnn.point_to_point"(%26, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %51 = "ttnn.point_to_point"(%27, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %52 = "ttnn.point_to_point"(%28, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %53 = "ttnn.point_to_point"(%29, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %54 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %55 = "ttnn.point_to_point"(%23, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %56 = "ttnn.point_to_point"(%24, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %57 = "ttnn.point_to_point"(%25, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %58 = "ttnn.point_to_point"(%26, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %59 = "ttnn.point_to_point"(%27, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %60 = "ttnn.point_to_point"(%28, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %61 = "ttnn.point_to_point"(%29, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %62 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %63 = "ttnn.point_to_point"(%23, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %64 = "ttnn.point_to_point"(%24, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %65 = "ttnn.point_to_point"(%25, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %66 = "ttnn.point_to_point"(%26, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %67 = "ttnn.point_to_point"(%27, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %68 = "ttnn.point_to_point"(%28, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %69 = "ttnn.point_to_point"(%29, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %70 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %71 = "ttnn.point_to_point"(%23, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %72 = "ttnn.point_to_point"(%24, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %73 = "ttnn.point_to_point"(%25, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %74 = "ttnn.point_to_point"(%26, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %75 = "ttnn.point_to_point"(%27, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %76 = "ttnn.point_to_point"(%28, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %77 = "ttnn.point_to_point"(%29, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %78 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %79 = "ttnn.point_to_point"(%23, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %80 = "ttnn.point_to_point"(%24, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %81 = "ttnn.point_to_point"(%25, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %82 = "ttnn.point_to_point"(%26, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %83 = "ttnn.point_to_point"(%27, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %84 = "ttnn.point_to_point"(%28, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %85 = "ttnn.point_to_point"(%29, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %86 = "ttnn.point_to_point"(%22) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %87 = "ttnn.point_to_point"(%23, %86) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %88 = "ttnn.point_to_point"(%24, %87) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %89 = "ttnn.point_to_point"(%25, %88) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %90 = "ttnn.point_to_point"(%26, %89) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %91 = "ttnn.point_to_point"(%27, %90) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %92 = "ttnn.point_to_point"(%28, %91) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %93 = "ttnn.point_to_point"(%29, %92) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %94 = "ttnn.concat"(%37, %45, %53, %61, %69, %77, %85, %93) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %95 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x1x256>}> : (!ttnn.device) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %96 = "ttnn.slice_static"(%94) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %97 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %98 = "ttnn.reshape"(%96) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %99 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<8x256>}> : (!ttnn.device) -> tensor<8x256xbf16, #ttnn_layout8>
        %100 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<8x256xbf16, #ttnn_layout8>
        %101 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<8x256>}> : (!ttnn.device) -> tensor<8x256xbf16, #ttnn_layout8>
        %102 = "ttnn.slice_static"(%100) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.slice_static"(%100) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.slice_static"(%100) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.slice_static"(%100) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.slice_static"(%100) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.slice_static"(%100) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.slice_static"(%100) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.slice_static"(%100) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%103, %110) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%104, %111) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%105, %112) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%106, %113) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%107, %114) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%108, %115) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%109, %116) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%103, %118) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%104, %119) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%105, %120) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%106, %121) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%107, %122) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%108, %123) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%109, %124) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%103, %126) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%104, %127) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%105, %128) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%106, %129) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%107, %130) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%108, %131) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%109, %132) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%103, %134) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%104, %135) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%105, %136) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%106, %137) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%107, %138) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%108, %139) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%109, %140) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%103, %142) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%104, %143) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%105, %144) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%106, %145) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%107, %146) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%108, %147) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%109, %148) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%103, %150) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%104, %151) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%105, %152) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%106, %153) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%107, %154) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%108, %155) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%109, %156) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%103, %158) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%104, %159) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%105, %160) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.point_to_point"(%106, %161) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %163 = "ttnn.point_to_point"(%107, %162) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %164 = "ttnn.point_to_point"(%108, %163) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %165 = "ttnn.point_to_point"(%109, %164) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %166 = "ttnn.point_to_point"(%102) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %167 = "ttnn.point_to_point"(%103, %166) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %168 = "ttnn.point_to_point"(%104, %167) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %169 = "ttnn.point_to_point"(%105, %168) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %170 = "ttnn.point_to_point"(%106, %169) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %171 = "ttnn.point_to_point"(%107, %170) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %172 = "ttnn.point_to_point"(%108, %171) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %173 = "ttnn.point_to_point"(%109, %172) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %174 = "ttnn.concat"(%117, %125, %133, %141, %149, %157, %165, %173) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %175 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<1x256>}> : (!ttnn.device) -> tensor<1x256xbf16, #ttnn_layout8>
        %176 = "ttnn.slice_static"(%174) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %177 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %178 = "ttnn.add"(%98, %176) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %179 = "ttnn.empty"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, shape = #ttnn.shape<2048x256>}> : (!ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %180 = "ttnn.maximum"(%178, %6) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %181 = "ttnn.zeros"() <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, shape = #ttnn.shape<2048x256>}> : () -> tensor<2048x256xbf16, #ttnn_layout10>
        %182 = "ttnn.to_layout"(%180) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout10>
        %183 = "ttnn.mesh_shard"(%182, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %183 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After TTNNFusing (ttnn-fusing) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.maximum"(%8, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = "ttnn.matmul"(%9, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %11 = "ttnn.all_reduce"(%10, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %13 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %14 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %15 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %16 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %17 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %18 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %19 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %20 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %21 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %22 = "ttnn.point_to_point"(%14, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %29 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %30 = "ttnn.point_to_point"(%14, %29) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %37 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %38 = "ttnn.point_to_point"(%14, %37) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %45 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %46 = "ttnn.point_to_point"(%14, %45) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %53 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %54 = "ttnn.point_to_point"(%14, %53) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %61 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %62 = "ttnn.point_to_point"(%14, %61) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %69 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %70 = "ttnn.point_to_point"(%14, %69) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %77 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %78 = "ttnn.point_to_point"(%14, %77) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %85 = "ttnn.concat"(%28, %36, %44, %52, %60, %68, %76, %84) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %86 = "ttnn.slice_static"(%85) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %87 = "ttnn.reshape"(%86) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %88 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<8x256xbf16, #ttnn_layout8>
        %89 = "ttnn.slice_static"(%88) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %90 = "ttnn.slice_static"(%88) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%88) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%88) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%88) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%88) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%88) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%88) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.point_to_point"(%90, %97) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%90, %105) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%90, %113) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%90, %121) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%90, %129) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%90, %137) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%90, %145) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%90, %153) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.concat"(%104, %112, %120, %128, %136, %144, %152, %160) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %162 = "ttnn.slice_static"(%161) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %163 = "ttnn.add"(%87, %162) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %164 = "ttnn.maximum"(%163, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %165 = "ttnn.to_layout"(%164) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout10>
        %166 = "ttnn.mesh_shard"(%165, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %166 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.maximum"(%8, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = "ttnn.matmul"(%9, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %11 = "ttnn.all_reduce"(%10, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %12 = "ttnn.reshape"(%11) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %13 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %14 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %15 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %16 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %17 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %18 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %19 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %20 = "ttnn.slice_static"(%12) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %21 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %22 = "ttnn.point_to_point"(%14, %21) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %29 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %30 = "ttnn.point_to_point"(%14, %29) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %37 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %38 = "ttnn.point_to_point"(%14, %37) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %45 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %46 = "ttnn.point_to_point"(%14, %45) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %53 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %54 = "ttnn.point_to_point"(%14, %53) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %61 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %62 = "ttnn.point_to_point"(%14, %61) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %69 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %70 = "ttnn.point_to_point"(%14, %69) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %77 = "ttnn.point_to_point"(%13) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %78 = "ttnn.point_to_point"(%14, %77) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %85 = "ttnn.concat"(%28, %36, %44, %52, %60, %68, %76, %84) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>, tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout9>
        %86 = "ttnn.slice_static"(%85) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout9>) -> tensor<2048x1x256xbf16, #ttnn_layout9>
        %87 = "ttnn.reshape"(%86) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout9>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %88 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<8x256xbf16, #ttnn_layout8>
        %89 = "ttnn.slice_static"(%88) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %90 = "ttnn.slice_static"(%88) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%88) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%88) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%88) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%88) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%88) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%88) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.point_to_point"(%90, %97) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%90, %105) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%90, %113) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%90, %121) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%90, %129) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%90, %137) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%90, %145) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%89) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%90, %153) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.concat"(%104, %112, %120, %128, %136, %144, %152, %160) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %162 = "ttnn.slice_static"(%161) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %163 = "ttnn.add"(%87, %162) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %164 = "ttnn.maximum"(%163, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %165 = "ttnn.to_layout"(%164) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout10>
        %166 = "ttnn.mesh_shard"(%165, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %166 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.maximum"(%8, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = "ttnn.matmul"(%9, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %11 = "ttnn.reshape"(%10) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reduce_scatter"(%11, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %13 = "ttnn.all_gather"(%12, %0) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %14 = "ttnn.reshape"(%13) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%22, %29) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%22, %37) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%22, %45) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%22, %53) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%22, %61) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%22, %69) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%22, %77) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.point_to_point"(%22, %85) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %87 = "ttnn.concat"(%30, %38, %46, %54, %62, %70, %78, %86) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %88 = "ttnn.slice_static"(%87) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %89 = "ttnn.reshape"(%88) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %90 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<8x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%90) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%90) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%90) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%90) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%90) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%90) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%90) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.slice_static"(%90) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%98, %105) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%98, %113) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%98, %121) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%98, %129) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%98, %137) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%98, %145) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%98, %153) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.point_to_point"(%98, %161) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %163 = "ttnn.concat"(%106, %114, %122, %130, %138, %146, %154, %162) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %164 = "ttnn.slice_static"(%163) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %165 = "ttnn.add"(%89, %164) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %166 = "ttnn.maximum"(%165, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %167 = "ttnn.to_layout"(%166) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %168 = "ttnn.mesh_shard"(%167, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %168 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.maximum"(%8, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = "ttnn.matmul"(%9, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %11 = "ttnn.reshape"(%10) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reduce_scatter"(%11, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %13 = "ttnn.all_gather"(%12, %0) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %14 = "ttnn.reshape"(%13) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%22, %29) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%22, %37) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%22, %45) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%22, %53) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%22, %61) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%22, %69) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%22, %77) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.point_to_point"(%22, %85) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %87 = "ttnn.concat"(%30, %38, %46, %54, %62, %70, %78, %86) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %88 = "ttnn.slice_static"(%87) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %89 = "ttnn.reshape"(%88) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %90 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<8x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%90) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%90) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%90) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%90) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%90) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%90) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%90) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.slice_static"(%90) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%98, %105) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%98, %113) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%98, %121) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%98, %129) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%98, %137) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%98, %145) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%98, %153) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.point_to_point"(%98, %161) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %163 = "ttnn.concat"(%106, %114, %122, %130, %138, %146, %154, %162) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %164 = "ttnn.slice_static"(%163) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %165 = "ttnn.add"(%89, %164) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %166 = "ttnn.maximum"(%165, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %167 = "ttnn.to_layout"(%166) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %168 = "ttnn.mesh_shard"(%167, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %168 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump Before ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout3>
        %2 = "ttnn.mesh_shard"(%arg1, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout4>
        %3 = "ttnn.mesh_shard"(%arg2, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout5>
        %4 = "ttnn.mesh_shard"(%arg3, %0) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout6>
        %5 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout3>) -> tensor<1x1xbf16, #ttnn_layout7>
        %6 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout5>) -> tensor<1x256xbf16, #ttnn_layout8>
        %7 = "ttnn.repeat"(%6) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %8 = "ttnn.linear"(%arg4, %4, %7) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout1>, tensor<256x2048xbf16, #ttnn_layout6>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %9 = "ttnn.maximum"(%8, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %10 = "ttnn.matmul"(%9, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x2048xbf16, #ttnn_layout1>
        %11 = "ttnn.reshape"(%10) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout1>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %12 = "ttnn.reduce_scatter"(%11, %0) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %13 = "ttnn.all_gather"(%12, %0) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %14 = "ttnn.reshape"(%13) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.slice_static"(%14) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%22, %29) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%22, %37) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%22, %45) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%22, %53) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%22, %61) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%22, %69) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%22, %77) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.point_to_point"(%22, %85) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %87 = "ttnn.concat"(%30, %38, %46, %54, %62, %70, %78, %86) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %88 = "ttnn.slice_static"(%87) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %89 = "ttnn.reshape"(%88) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %90 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout>) -> tensor<8x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%90) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%90) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%90) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%90) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%90) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%90) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%90) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.slice_static"(%90) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%98, %105) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%98, %113) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%98, %121) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%98, %129) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%98, %137) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%98, %145) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%98, %153) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.point_to_point"(%98, %161) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %163 = "ttnn.concat"(%106, %114, %122, %130, %138, %146, %154, %162) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %164 = "ttnn.slice_static"(%163) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %165 = "ttnn.add"(%89, %164) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %166 = "ttnn.maximum"(%165, %5) : (tensor<2048x256xbf16, #ttnn_layout4>, tensor<1x1xbf16, #ttnn_layout7>) -> tensor<2048x256xbf16, #ttnn_layout4>
        %167 = "ttnn.to_layout"(%166) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout4>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %168 = "ttnn.mesh_shard"(%167, %0) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout2>
        return %168 : tensor<2048x2048xbf16, #ttnn_layout2>
      }
    }
  }
}


// -----// IR Dump After ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.maximum"(%7, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %14 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.concat"(%29, %37, %45, %53, %61, %69, %77, %85) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %87 = "ttnn.slice_static"(%86) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %88 = "ttnn.reshape"(%87) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %89 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<8x256xbf16, #ttnn_layout8>
        %90 = "ttnn.slice_static"(%89) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%89) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%89) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%89) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%89) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%89) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%89) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%89) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.concat"(%105, %113, %121, %129, %137, %145, %153, %161) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %163 = "ttnn.slice_static"(%162) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %164 = "ttnn.add"(%88, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %165 = "ttnn.maximum"(%164, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %166 = "ttnn.to_layout"(%165) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %167 = "ttnn.mesh_shard"(%166, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %167 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump Before ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.maximum"(%7, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %14 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.concat"(%29, %37, %45, %53, %61, %69, %77, %85) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %87 = "ttnn.slice_static"(%86) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %88 = "ttnn.reshape"(%87) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %89 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<8x256xbf16, #ttnn_layout8>
        %90 = "ttnn.slice_static"(%89) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%89) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%89) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%89) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%89) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%89) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%89) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%89) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.concat"(%105, %113, %121, %129, %137, %145, %153, %161) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %163 = "ttnn.slice_static"(%162) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %164 = "ttnn.add"(%88, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %165 = "ttnn.maximum"(%164, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %166 = "ttnn.to_layout"(%165) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %167 = "ttnn.mesh_shard"(%166, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %167 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump After ConstEvalHoistTransform (const-eval-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.maximum"(%7, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %14 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.concat"(%29, %37, %45, %53, %61, %69, %77, %85) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %87 = "ttnn.slice_static"(%86) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %88 = "ttnn.reshape"(%87) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %89 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<8x256xbf16, #ttnn_layout8>
        %90 = "ttnn.slice_static"(%89) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%89) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%89) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%89) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%89) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%89) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%89) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%89) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.concat"(%105, %113, %121, %129, %137, %145, %153, %161) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %163 = "ttnn.slice_static"(%162) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %164 = "ttnn.add"(%88, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %165 = "ttnn.maximum"(%164, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %166 = "ttnn.to_layout"(%165) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %167 = "ttnn.mesh_shard"(%166, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %167 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump Before TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.maximum"(%7, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %14 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.concat"(%29, %37, %45, %53, %61, %69, %77, %85) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %87 = "ttnn.slice_static"(%86) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %88 = "ttnn.reshape"(%87) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %89 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<8x256xbf16, #ttnn_layout8>
        %90 = "ttnn.slice_static"(%89) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%89) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%89) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%89) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%89) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%89) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%89) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%89) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.concat"(%105, %113, %121, %129, %137, %145, %153, %161) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %163 = "ttnn.slice_static"(%162) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %164 = "ttnn.add"(%88, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %165 = "ttnn.maximum"(%164, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %166 = "ttnn.to_layout"(%165) <{dtype = #ttcore.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#system_memory>}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %167 = "ttnn.mesh_shard"(%166, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout12>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %167 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump After TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.maximum"(%7, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %14 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.concat"(%29, %37, %45, %53, %61, %69, %77, %85) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %87 = "ttnn.slice_static"(%86) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %88 = "ttnn.reshape"(%87) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %89 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<8x256xbf16, #ttnn_layout8>
        %90 = "ttnn.slice_static"(%89) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%89) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%89) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%89) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%89) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%89) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%89) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%89) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.concat"(%105, %113, %121, %129, %137, %145, %153, %161) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %163 = "ttnn.slice_static"(%162) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %164 = "ttnn.add"(%88, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %165 = "ttnn.maximum"(%164, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %166 = "ttnn.to_layout"(%165) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %167 = "ttnn.from_device"(%166) : (tensor<2048x256xbf16, #ttnn_layout12>) -> tensor<2048x256xbf16, #ttnn_layout13>
        %168 = "ttnn.mesh_shard"(%167, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %168 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump Before TTCoreOptimizationBarrierFold (ttcore-optimization-barrier-fold) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.maximum"(%7, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %14 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.concat"(%29, %37, %45, %53, %61, %69, %77, %85) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %87 = "ttnn.slice_static"(%86) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %88 = "ttnn.reshape"(%87) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %89 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<8x256xbf16, #ttnn_layout8>
        %90 = "ttnn.slice_static"(%89) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%89) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%89) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%89) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%89) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%89) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%89) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%89) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.concat"(%105, %113, %121, %129, %137, %145, %153, %161) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %163 = "ttnn.slice_static"(%162) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %164 = "ttnn.add"(%88, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %165 = "ttnn.maximum"(%164, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %166 = "ttnn.to_layout"(%165) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %167 = "ttnn.from_device"(%166) : (tensor<2048x256xbf16, #ttnn_layout12>) -> tensor<2048x256xbf16, #ttnn_layout13>
        %168 = "ttnn.mesh_shard"(%167, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %168 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump Before TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %8 = "ttnn.maximum"(%7, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %11 = "ttnn.reduce_scatter"(%10, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        %12 = "ttnn.all_gather"(%11, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %14 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %22 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %30 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %38 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %46 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %54 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %62 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %70 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %78 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %86 = "ttnn.concat"(%29, %37, %45, %53, %61, %69, %77, %85) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        %87 = "ttnn.slice_static"(%86) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %88 = "ttnn.reshape"(%87) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %89 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<8x256xbf16, #ttnn_layout8>
        %90 = "ttnn.slice_static"(%89) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%89) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%89) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%89) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%89) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%89) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%89) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%89) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %98 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %106 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %114 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %122 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %130 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %138 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %146 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %154 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %162 = "ttnn.concat"(%105, %113, %121, %129, %137, %145, %153, %161) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        %163 = "ttnn.slice_static"(%162) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %164 = "ttnn.add"(%88, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %165 = "ttnn.maximum"(%164, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        %166 = "ttnn.to_layout"(%165) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout12>
        %167 = "ttnn.from_device"(%166) : (tensor<2048x256xbf16, #ttnn_layout12>) -> tensor<2048x256xbf16, #ttnn_layout13>
        %168 = "ttnn.mesh_shard"(%167, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        return %168 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


// -----// IR Dump After TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation: @SyncTensorsGraph.27) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn_layout1>) -> ()
        return %2 : tensor<1x1xbf16, #ttnn_layout>
      }
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<2048xbf16, #ttnn_layout2>) -> ()
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> ()
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<256x2048xbf16, #ttnn_layout7>) -> ()
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %8 = "ttnn.maximum"(%7, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3>
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> ()
        %11 = "ttnn.reduce_scatter"(%10, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> ()
        %12 = "ttnn.all_gather"(%11, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>) -> ()
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> ()
        %14 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %15 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %16 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %17 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %18 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %19 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %20 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %21 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> ()
        %22 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %30 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %38 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %46 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %54 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %62 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %70 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %78 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %86 = "ttnn.concat"(%29, %37, %45, %53, %61, %69, %77, %85) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %87 = "ttnn.slice_static"(%86) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> ()
        %88 = "ttnn.reshape"(%87) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> ()
        %89 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<8x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<2048xbf16, #ttnn_layout2>) -> ()
        %90 = "ttnn.slice_static"(%89) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %91 = "ttnn.slice_static"(%89) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %92 = "ttnn.slice_static"(%89) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %93 = "ttnn.slice_static"(%89) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %94 = "ttnn.slice_static"(%89) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %95 = "ttnn.slice_static"(%89) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %96 = "ttnn.slice_static"(%89) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %97 = "ttnn.slice_static"(%89) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<8x256xbf16, #ttnn_layout8>) -> ()
        %98 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %106 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %114 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %122 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %130 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %138 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %146 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %154 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %162 = "ttnn.concat"(%105, %113, %121, %129, %137, %145, %153, %161) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        %163 = "ttnn.slice_static"(%162) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8>
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<8x256xbf16, #ttnn_layout8>) -> ()
        %164 = "ttnn.add"(%88, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> ()
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        %165 = "ttnn.maximum"(%164, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5>
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1xbf16, #ttnn_layout>) -> ()
        %166 = "ttnn.to_layout"(%165) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout12>
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> ()
        %167 = "ttnn.from_device"(%166) : (tensor<2048x256xbf16, #ttnn_layout12>) -> tensor<2048x256xbf16, #ttnn_layout13>
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout12>) -> ()
        %168 = "ttnn.mesh_shard"(%167, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4>
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout13>) -> ()
        return %168 : tensor<2048x2048xbf16, #ttnn_layout4>
      }
    }
  }
}


2025-10-23 04:30:27.951 (  14.403s) [        2146C480]      module_builder.cc:963      1| MLIR Module ttnn:
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("p0.1")
#loc2 = loc("p1.2")
#loc3 = loc("p2.4")
#loc4 = loc("p3.5")
#loc5 = loc("p4.7")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073151744, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101152, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073168608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x2048xbf16, #system_memory>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), <1x1>, memref<64x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<2048x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2048x256xbf16, #system_memory>>
module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.27 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]> loc(#loc)
      func.func @main_const_eval_0() -> tensor<1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn_layout1> loc(#loc)
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout1>) -> tensor<1x1xbf16, #ttnn_layout> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn_layout1>) -> () loc(#loc)
        return %2 : tensor<1x1xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p0.1"), %arg1: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p1.2"), %arg2: tensor<2048xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p2.4"), %arg3: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p3.5"), %arg4: tensor<2048x2048xbf16, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p4.7")) -> (tensor<2048x2048xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1xbf16, #ttnn_layout> loc(#loc)
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x8>}> : () -> !ttnn.device loc(#loc)
        %2 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<2048xbf16, #ttnn_layout2>) -> () loc(#loc)
        %4 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<2048x2048xbf16, #ttnn_layout3>, !ttnn.device) -> tensor<256x2048xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.reshape"(%3) <{shape = [1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc6)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc6)
        %6 = "ttnn.repeat"(%5) <{repeat_dims = #ttnn.shape<2048x1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc6)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc6)
        %7 = "ttnn.linear"(%arg4, %4, %6) <{transpose_a = false, transpose_b = true}> : (tensor<2048x2048xbf16, #ttnn_layout3>, tensor<256x2048xbf16, #ttnn_layout7>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc7)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc7)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<256x2048xbf16, #ttnn_layout7>) -> () loc(#loc7)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc7)
        %8 = "ttnn.maximum"(%7, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc8)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc8)
        %9 = "ttnn.matmul"(%8, %2) <{transpose_a = false, transpose_b = true}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x2048xbf16, #ttnn_layout3> loc(#loc9)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc9)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc9)
        %10 = "ttnn.reshape"(%9) <{shape = [1 : i32, 1 : i32, 2048 : i32, 2048 : i32]}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9> loc(#loc14)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<2048x2048xbf16, #ttnn_layout3>) -> () loc(#loc14)
        %11 = "ttnn.reduce_scatter"(%10, %1) <{cluster_axis = 1 : ui32, num_links = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>, scatter_dim = 2 : si32}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>, !ttnn.device) -> tensor<1x1x256x2048xbf16, #ttnn_layout10> loc(#loc15)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> () loc(#loc15)
        %12 = "ttnn.all_gather"(%11, %1) <{all_gather_dim = 2 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<1x1x2048x2048xbf16, #ttnn_layout9> loc(#loc13)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x256x2048xbf16, #ttnn_layout10>) -> () loc(#loc13)
        %13 = "ttnn.reshape"(%12) <{shape = [2048 : i32, 8 : i32, 256 : i32]}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> tensor<2048x8x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x2048x2048xbf16, #ttnn_layout9>) -> () loc(#loc9)
        %14 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %15 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 1 : i32, 0 : i32], ends = [2048 : i32, 2 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %16 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 2 : i32, 0 : i32], ends = [2048 : i32, 3 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %17 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 3 : i32, 0 : i32], ends = [2048 : i32, 4 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %18 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 4 : i32, 0 : i32], ends = [2048 : i32, 5 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %19 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 5 : i32, 0 : i32], ends = [2048 : i32, 6 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %20 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 6 : i32, 0 : i32], ends = [2048 : i32, 7 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %21 = "ttnn.slice_static"(%13) <{begins = [0 : i32, 7 : i32, 0 : i32], ends = [2048 : i32, 8 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %22 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %23 = "ttnn.point_to_point"(%15, %22) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %24 = "ttnn.point_to_point"(%16, %23) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %25 = "ttnn.point_to_point"(%17, %24) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %26 = "ttnn.point_to_point"(%18, %25) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %27 = "ttnn.point_to_point"(%19, %26) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %28 = "ttnn.point_to_point"(%20, %27) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %29 = "ttnn.point_to_point"(%21, %28) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %30 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %31 = "ttnn.point_to_point"(%15, %30) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %32 = "ttnn.point_to_point"(%16, %31) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %33 = "ttnn.point_to_point"(%17, %32) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %34 = "ttnn.point_to_point"(%18, %33) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %35 = "ttnn.point_to_point"(%19, %34) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %36 = "ttnn.point_to_point"(%20, %35) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %37 = "ttnn.point_to_point"(%21, %36) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %38 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %39 = "ttnn.point_to_point"(%15, %38) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %40 = "ttnn.point_to_point"(%16, %39) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %41 = "ttnn.point_to_point"(%17, %40) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %42 = "ttnn.point_to_point"(%18, %41) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %43 = "ttnn.point_to_point"(%19, %42) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %44 = "ttnn.point_to_point"(%20, %43) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %45 = "ttnn.point_to_point"(%21, %44) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %46 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %47 = "ttnn.point_to_point"(%15, %46) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %48 = "ttnn.point_to_point"(%16, %47) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %49 = "ttnn.point_to_point"(%17, %48) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %50 = "ttnn.point_to_point"(%18, %49) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %51 = "ttnn.point_to_point"(%19, %50) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %52 = "ttnn.point_to_point"(%20, %51) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %53 = "ttnn.point_to_point"(%21, %52) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %54 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %55 = "ttnn.point_to_point"(%15, %54) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %56 = "ttnn.point_to_point"(%16, %55) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %57 = "ttnn.point_to_point"(%17, %56) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %58 = "ttnn.point_to_point"(%18, %57) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %59 = "ttnn.point_to_point"(%19, %58) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %60 = "ttnn.point_to_point"(%20, %59) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %61 = "ttnn.point_to_point"(%21, %60) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %62 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %63 = "ttnn.point_to_point"(%15, %62) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %64 = "ttnn.point_to_point"(%16, %63) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %65 = "ttnn.point_to_point"(%17, %64) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %66 = "ttnn.point_to_point"(%18, %65) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %67 = "ttnn.point_to_point"(%19, %66) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %68 = "ttnn.point_to_point"(%20, %67) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %69 = "ttnn.point_to_point"(%21, %68) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %70 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        %71 = "ttnn.point_to_point"(%15, %70) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %72 = "ttnn.point_to_point"(%16, %71) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %73 = "ttnn.point_to_point"(%17, %72) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %74 = "ttnn.point_to_point"(%18, %73) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %75 = "ttnn.point_to_point"(%19, %74) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %76 = "ttnn.point_to_point"(%20, %75) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %77 = "ttnn.point_to_point"(%21, %76) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %78 = "ttnn.point_to_point"(%14) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %79 = "ttnn.point_to_point"(%15, %78) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %80 = "ttnn.point_to_point"(%16, %79) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %81 = "ttnn.point_to_point"(%17, %80) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %82 = "ttnn.point_to_point"(%18, %81) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %83 = "ttnn.point_to_point"(%19, %82) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %84 = "ttnn.point_to_point"(%20, %83) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %85 = "ttnn.point_to_point"(%21, %84) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %86 = "ttnn.concat"(%29, %37, %45, %53, %61, %69, %77, %85) <{dim = 1 : si32}> : (tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>, tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x8x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %87 = "ttnn.slice_static"(%86) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [2048 : i32, 1 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> tensor<2048x1x256xbf16, #ttnn_layout11> loc(#loc9)
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<2048x8x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %88 = "ttnn.reshape"(%87) <{shape = [2048 : i32, 256 : i32]}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc9)
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<2048x1x256xbf16, #ttnn_layout11>) -> () loc(#loc9)
        %89 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 256 : i32]}> : (tensor<2048xbf16, #ttnn_layout2>) -> tensor<8x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<2048xbf16, #ttnn_layout2>) -> () loc(#loc1)
        %90 = "ttnn.slice_static"(%89) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %91 = "ttnn.slice_static"(%89) <{begins = [1 : i32, 0 : i32], ends = [2 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %92 = "ttnn.slice_static"(%89) <{begins = [2 : i32, 0 : i32], ends = [3 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %93 = "ttnn.slice_static"(%89) <{begins = [3 : i32, 0 : i32], ends = [4 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %94 = "ttnn.slice_static"(%89) <{begins = [4 : i32, 0 : i32], ends = [5 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %95 = "ttnn.slice_static"(%89) <{begins = [5 : i32, 0 : i32], ends = [6 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %96 = "ttnn.slice_static"(%89) <{begins = [6 : i32, 0 : i32], ends = [7 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %97 = "ttnn.slice_static"(%89) <{begins = [7 : i32, 0 : i32], ends = [8 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<8x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %98 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %99 = "ttnn.point_to_point"(%91, %98) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %100 = "ttnn.point_to_point"(%92, %99) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %101 = "ttnn.point_to_point"(%93, %100) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %102 = "ttnn.point_to_point"(%94, %101) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %103 = "ttnn.point_to_point"(%95, %102) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %104 = "ttnn.point_to_point"(%96, %103) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %105 = "ttnn.point_to_point"(%97, %104) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 0>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %106 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %107 = "ttnn.point_to_point"(%91, %106) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %108 = "ttnn.point_to_point"(%92, %107) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %109 = "ttnn.point_to_point"(%93, %108) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %110 = "ttnn.point_to_point"(%94, %109) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %111 = "ttnn.point_to_point"(%95, %110) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %112 = "ttnn.point_to_point"(%96, %111) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %113 = "ttnn.point_to_point"(%97, %112) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 1>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %114 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %115 = "ttnn.point_to_point"(%91, %114) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %116 = "ttnn.point_to_point"(%92, %115) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %117 = "ttnn.point_to_point"(%93, %116) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %118 = "ttnn.point_to_point"(%94, %117) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %119 = "ttnn.point_to_point"(%95, %118) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %120 = "ttnn.point_to_point"(%96, %119) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %121 = "ttnn.point_to_point"(%97, %120) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 2>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %122 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %123 = "ttnn.point_to_point"(%91, %122) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %124 = "ttnn.point_to_point"(%92, %123) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %125 = "ttnn.point_to_point"(%93, %124) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %126 = "ttnn.point_to_point"(%94, %125) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %127 = "ttnn.point_to_point"(%95, %126) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %128 = "ttnn.point_to_point"(%96, %127) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %129 = "ttnn.point_to_point"(%97, %128) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 3>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %130 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %131 = "ttnn.point_to_point"(%91, %130) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %132 = "ttnn.point_to_point"(%92, %131) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %133 = "ttnn.point_to_point"(%93, %132) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %134 = "ttnn.point_to_point"(%94, %133) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %135 = "ttnn.point_to_point"(%95, %134) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %136 = "ttnn.point_to_point"(%96, %135) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %137 = "ttnn.point_to_point"(%97, %136) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 4>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %138 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %139 = "ttnn.point_to_point"(%91, %138) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %140 = "ttnn.point_to_point"(%92, %139) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %141 = "ttnn.point_to_point"(%93, %140) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %142 = "ttnn.point_to_point"(%94, %141) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %143 = "ttnn.point_to_point"(%95, %142) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %144 = "ttnn.point_to_point"(%96, %143) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %145 = "ttnn.point_to_point"(%97, %144) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 5>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %146 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        %147 = "ttnn.point_to_point"(%91, %146) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %148 = "ttnn.point_to_point"(%92, %147) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %149 = "ttnn.point_to_point"(%93, %148) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %150 = "ttnn.point_to_point"(%94, %149) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %151 = "ttnn.point_to_point"(%95, %150) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %152 = "ttnn.point_to_point"(%96, %151) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %153 = "ttnn.point_to_point"(%97, %152) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 6>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %154 = "ttnn.point_to_point"(%90) <{receive_coord = array<i64: 0, 0>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %155 = "ttnn.point_to_point"(%91, %154) <{receive_coord = array<i64: 0, 1>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %156 = "ttnn.point_to_point"(%92, %155) <{receive_coord = array<i64: 0, 2>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %157 = "ttnn.point_to_point"(%93, %156) <{receive_coord = array<i64: 0, 3>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %158 = "ttnn.point_to_point"(%94, %157) <{receive_coord = array<i64: 0, 4>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %159 = "ttnn.point_to_point"(%95, %158) <{receive_coord = array<i64: 0, 5>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %160 = "ttnn.point_to_point"(%96, %159) <{receive_coord = array<i64: 0, 6>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %161 = "ttnn.point_to_point"(%97, %160) <{receive_coord = array<i64: 0, 7>, send_coord = array<i64: 0, 7>}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %162 = "ttnn.concat"(%105, %113, %121, %129, %137, %145, %153, %161) <{dim = 0 : si32}> : (tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<8x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %163 = "ttnn.slice_static"(%162) <{begins = [0 : i32, 0 : i32], ends = [1 : i32, 256 : i32], step = [1 : i32, 1 : i32]}> : (tensor<8x256xbf16, #ttnn_layout8>) -> tensor<1x256xbf16, #ttnn_layout8> loc(#loc1)
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<8x256xbf16, #ttnn_layout8>) -> () loc(#loc1)
        %164 = "ttnn.add"(%88, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x256xbf16, #ttnn_layout8>) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc10)
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x256xbf16, #ttnn_layout8>) -> () loc(#loc10)
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc10)
        %165 = "ttnn.maximum"(%164, %0) : (tensor<2048x256xbf16, #ttnn_layout5>, tensor<1x1xbf16, #ttnn_layout>) -> tensor<2048x256xbf16, #ttnn_layout5> loc(#loc11)
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc11)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1xbf16, #ttnn_layout>) -> () loc(#loc11)
        %166 = "ttnn.to_layout"(%165) <{layout = #ttnn.layout<row_major>}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> tensor<2048x256xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout5>) -> () loc(#loc)
        %167 = "ttnn.from_device"(%166) : (tensor<2048x256xbf16, #ttnn_layout12>) -> tensor<2048x256xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout12>) -> () loc(#loc)
        %168 = "ttnn.mesh_shard"(%167, %1) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<2048x256xbf16, #ttnn_layout13>, !ttnn.device) -> tensor<2048x2048xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<2048x256xbf16, #ttnn_layout13>) -> () loc(#loc)
        return %168 : tensor<2048x2048xbf16, #ttnn_layout4> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc6 = loc("broadcast.12")
#loc7 = loc("add.13")
#loc8 = loc("maximum.16")
#loc9 = loc("dot.17")
#loc10 = loc("add.22")
#loc11 = loc("maximum.25")
#loc12 = loc("dot.17_reduceScatter"(#loc9))
#loc13 = loc("dot.17_all_gather_4d"(#loc9))
#loc14 = loc("dot.17_reduceScatter_reshape_to_4d"(#loc12))
#loc15 = loc("dot.17_reduceScatter_reduce_scatter_4d"(#loc12))
------------------ END OF MLIR MODULE ------------------
2025-10-23 04:30:27.969 (  14.421s) [        2146C480]loaded_executable_insta:69       1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-10-23 04:30:27.969 (  14.421s) [        2146C480]loaded_executable_insta:88       1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-10-23 04:30:27.969 (  14.421s) [        2146C480]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-10-23 04:30:27.969 (  14.421s) [        2146C480]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-10-23 04:30:27.969 (  14.421s) [        2146C480]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-10-23 04:30:27.969 (  14.421s) [        2146C480]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-10-23 04:30:27.969 (  14.421s) [        2146C480] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-23 04:30:27.969 (  14.421s) [        2146C480] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-23 04:30:27.973 (  14.425s) [        2146C480] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-23 04:30:27.973 (  14.425s) [        2146C480] executable_instance.cc:107      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-10-23 04:30:27.977 (  14.429s) [        B6FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.429s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     buffer_instance.cc:535      1| BufferInstance::PJRT_Buffer_Device
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640] executable_instance.cc:139      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]loaded_executable_insta:125      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]flatbuffer_loaded_execu:412      1| FlatbufferLoadedExecutableInstance::Execute
2025-10-23 04:30:27.978 (  14.430s) [        B6FFD640]     client_instance.cc:373      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 8]
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:440      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.787 (  45.239s) [        B6FFD640]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:30:58.787 (  45.239s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.788 (  45.239s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.788 (  45.239s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.788 (  45.239s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.788 (  45.239s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.788 (  45.240s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.788 (  45.240s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.788 (  45.240s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.788 (  45.240s) [        2146C480]     buffer_instance.cc:428      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-10-23 04:30:58.788 (  45.240s) [        2146C480]     buffer_instance.cc:409      1| BufferInstance::PJRT_Buffer_ElementType
2025-10-23 04:30:58.788 (  45.240s) [        2146C480]     buffer_instance.cc:417      1| BufferInstance::PJRT_Buffer_Dimensions
2025-10-23 04:30:58.788 (  45.240s) [        2146C480]     buffer_instance.cc:450      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-10-23 04:30:58.788 (  45.240s) [        2146C480]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-10-23 04:30:58.789 (  45.241s) [        F67FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-10-23 04:30:58.819 (  45.271s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.819 (  45.271s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.819 (  45.271s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.819 (  45.271s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.819 (  45.271s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.819 (  45.271s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.819 (  45.271s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.819 (  45.271s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.820 (  45.272s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.820 (  45.272s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.820 (  45.272s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.272s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.272s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.272s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.272s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.821 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.822 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.822 (  45.273s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.822 (  45.274s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.822 (  45.274s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.822 (  45.274s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.822 (  45.274s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.822 (  45.274s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.822 (  45.274s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-23 04:30:58.822 (  45.274s) [        2146C480]     buffer_instance.cc:401      1| BufferInstance::PJRT_Buffer_Destroy
PASSED

============================== 1 passed in 45.55s ==============================
