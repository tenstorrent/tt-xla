diff --git a/vllm_tt/__init__.py b/vllm_tt/__init__.py
index c7855eb..3cbcf4c 100644
--- a/vllm_tt/__init__.py
+++ b/vllm_tt/__init__.py
@@ -1,3 +1,3 @@

 def register():
-    return "vllm_tt.platform.TPUPlatform"
\ No newline at end of file
+    return "vllm_tt.platform.TTPlatform"
\ No newline at end of file
diff --git a/vllm_tt/attention.py b/vllm_tt/attention.py
index 26f9abf..72bb4ab 100644
--- a/vllm_tt/attention.py
+++ b/vllm_tt/attention.py
@@ -5,6 +5,12 @@ from dataclasses import dataclass
 from typing import Optional

 import torch
+import torch_xla.core.xla_builder as xb
+import torch_xla.experimental.custom_kernel  # noqa: F401
+# Required to register custom ops.
+from torch.library import impl
+# from torch_xla._internal.jax_workarounds import requires_jax
+from torch_xla.experimental.custom_kernel import XLA_LIB

 from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
                                               AttentionLayer, AttentionType)
@@ -15,8 +21,8 @@ from vllm.utils import cdiv, next_power_of_2

 logger = init_logger(__name__)

-# TPU requires the head size to be a multiple of 128.
-TPU_HEAD_SIZE_ALIGNMENT = 128
+# TT requires the head size to be a multiple of 32.
+TT_HEAD_SIZE_ALIGNMENT = 32

 # Note: TPU can fp8 as storage dtype but doesn't support converting from uint8
 # from to fp32 directly. That's why it has a dtype mapping different from GPU
@@ -31,71 +37,22 @@ TPU_STR_DTYPE_TO_TORCH_DTYPE = {
     "uint8": torch.uint8,
 }

-try:
-    import tpu_commons  # noqa: F401
-except ImportError:
-    # Lazy import torch_xla
-    import torch_xla.core.xla_builder as xb
-    import torch_xla.experimental.custom_kernel  # noqa: F401
-    from torch.library import impl
-    from torch_xla._internal.jax_workarounds import requires_jax
-    from torch_xla.experimental.custom_kernel import XLA_LIB
-
-    @requires_jax
-    def kv_cache_update_op_impl(kv: torch.Tensor, slot_mapping: torch.Tensor,
-                                kv_cache: torch.Tensor,
-                                num_kv_update_slices: torch.Tensor,
-                                page_size: int, num_slices_per_block: int):
-        from vllm.attention.ops.pallas_kv_cache_update import kv_cache_update
-        new_kv_cache = xb.call_jax(
-            kv_cache_update,
-            (kv, slot_mapping, kv_cache, num_kv_update_slices), {
-                "page_size": page_size,
-                "num_slices_per_block": num_slices_per_block
-            })
-        return new_kv_cache
-
-
-    XLA_LIB.define(
-        "kv_cache_update_op(Tensor kv, Tensor slot_mapping," \
-        "Tensor kv_cache, Tensor num_kv_update_slices, int page_size," \
-        "int num_slices_per_block)" \
-        "-> Tensor", )
-
-    @impl(XLA_LIB, "kv_cache_update_op", "XLA")
-    def kv_cache_update_op_xla(kv: torch.Tensor, slot_mapping: torch.Tensor,
-                               kv_cache: torch.Tensor,
-                               num_kv_update_slices: torch.Tensor,
-                               page_size: int,
-                               num_slices_per_block: int) -> torch.Tensor:
-        new_kv_cache = kv_cache_update_op_impl(kv, slot_mapping, kv_cache,
-                                               num_kv_update_slices, page_size,
-                                               num_slices_per_block)
-        return new_kv_cache
-
-    @impl(XLA_LIB, "kv_cache_update_op", "CompositeExplicitAutograd")
-    def kv_cache_update_op_non_xla(kv: torch.Tensor,
-                                   slot_mapping: torch.Tensor,
-                                   kv_cache: torch.Tensor,
-                                   num_kv_update_slices: torch.Tensor,
-                                   page_size: int,
-                                   num_slices_per_block: int) -> torch.Tensor:
-        return kv_cache
-
-
-class PallasAttentionBackend(AttentionBackend):
+torch._dynamo.config.reorderable_logging_functions.add(print)
+
+
+class TTAttentionBackend(AttentionBackend):

     @staticmethod
     def get_name() -> str:
         return "PALLAS_VLLM_V1"

     @staticmethod
-    def get_impl_cls() -> type["PallasAttentionBackendImpl"]:
-        return PallasAttentionBackendImpl
+    def get_impl_cls() -> type["TTAttentionBackendImpl"]:
+        return TTAttentionBackendImpl

     @staticmethod
-    def get_metadata_cls() -> type["PallasMetadata"]:
-        return PallasMetadata
+    def get_metadata_cls() -> type["TTMetadata"]:
+        return TTMetadata

     @staticmethod
     def get_state_cls() -> type["CommonAttentionState"]:
@@ -103,14 +60,16 @@ class PallasAttentionBackend(AttentionBackend):

     @staticmethod
     def get_kv_cache_shape(
-        num_blocks: int,
-        block_size: int,
-        num_kv_heads: int,
-        head_size: int,
+        batch_size: int,
+        num_heads: int,
+        max_seq_len: int,
+        head_size: int
+
     ) -> tuple[int, ...]:
+    #[2, batch_size, num_heads, max_cache_len), head_dim]
         padded_head_size = cdiv(
-            head_size, TPU_HEAD_SIZE_ALIGNMENT) * TPU_HEAD_SIZE_ALIGNMENT
-        return (num_blocks, block_size, num_kv_heads * 2, padded_head_size)
+            head_size, TT_HEAD_SIZE_ALIGNMENT) * TT_HEAD_SIZE_ALIGNMENT
+        return (2, batch_size, num_heads, max_seq_len, head_size)

     @staticmethod
     def swap_blocks(
@@ -121,7 +80,7 @@ class PallasAttentionBackend(AttentionBackend):
         raise RuntimeError("swap_blocks is not used for the TPU backend.")

     # In recent TPU generations, up to v6e, the SMEM size is 1MB. The
-    # block_tables within the PallasMetadata constitute almost the entire SMEM
+    # block_tables within the TTMetadata constitute almost the entire SMEM
     # requirement. Its size is max_num_seqs * num_page_per_seq * 4 (Int). Here
     # we simply make sure that the size is smaller than half of SMEM capacity.
     @staticmethod
@@ -160,7 +119,7 @@ class PallasAttentionBackend(AttentionBackend):


 @dataclass
-class PallasMetadata:
+class TTMetadata:
     # NOTE(sang): Definition of context_len, query_len, and seq_len.
     # |---------- N-1 iteration --------|
     # |---------------- N iteration ---------------------|
@@ -169,17 +128,16 @@ class PallasMetadata:
     # |-------------------- seq_len ---------------------|
     #                                   |-- query_len ---|

-    # Used in the PallasAttentionBackendImpl
-    slot_mapping: torch.Tensor
-    block_tables: torch.Tensor
+    # Used in the TTAttentionBackendImpl
     context_lens: torch.Tensor
     query_start_loc: torch.Tensor
     num_seqs: torch.Tensor
-    num_kv_update_slices: torch.Tensor
-    num_slices_per_kv_cache_update_block: int
+    # padding_in_inputs: torch.Tensor
+    attn_mask: torch.Tensor


-class PallasAttentionBackendImpl(AttentionImpl):
+
+class TTAttentionBackendImpl(AttentionImpl):

     def __init__(
         self,
@@ -210,13 +168,16 @@ class PallasAttentionBackendImpl(AttentionImpl):
             raise NotImplementedError("Encoder self-attention and "
                                       "encoder/decoder cross-attention "
                                       "are not implemented for "
-                                      "PallasAttentionBackendImpl")
+                                      "TTAttentionBackendImpl")
+
+        self.kv_cache_stored = None

         self.kv_cache_quantized_dtype = None
         if kv_cache_dtype != "auto":
             self.kv_cache_quantized_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE.get(
                 kv_cache_dtype.lower().strip())

+    # @torch.compiler.disable
     def forward(
         self,
         layer: AttentionLayer,
@@ -224,141 +185,46 @@ class PallasAttentionBackendImpl(AttentionImpl):
         key: torch.Tensor,
         value: torch.Tensor,
         kv_cache: torch.Tensor,
-        attn_metadata: PallasMetadata,
+        attn_metadata: TTMetadata,
         output: Optional[torch.Tensor] = None,
         output_scale: Optional[torch.Tensor] = None,
-        output_block_scale: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
-        """Forward pass with Pallas attention.
+        """Forward pass with TT attention.

         Args:
             query: shape = [num_tokens, num_heads * head_size]
             key: shape = [num_tokens, num_kv_heads * head_size]
             value: shape = [num_tokens, num_kv_heads * head_size]
-            kv_cache: shape =
-                [num_blocks, block_size, num_kv_heads * 2, head_size]
+            kv_cache = [num_blocks, block_size, num_kv_heads * 2, head_size]
+                    - now [2, batch_size, max_seq_len, num_kv_heads, head_size]
             attn_metadata: Metadata for attention.
         Returns:
             shape = [num_tokens, num_heads * head_size]
         """
-        if output_scale is not None or output_block_scale is not None:
-            raise NotImplementedError(
-                "fused output quantization is not yet supported"
-                " for PallasAttentionBackendImpl")
-
-        # For determine_available_memory case.
-        if kv_cache.numel() == 0:
-            if output is None:
-                output = torch.ones_like(query)
-            return output
-
         num_tokens, hidden_size = query.shape
-        query = query.view(num_tokens, self.num_heads, self.head_size)
-        key = key.view(-1, self.num_kv_heads, self.head_size)
-        value = value.view(-1, self.num_kv_heads, self.head_size)
-        if self.head_size % TPU_HEAD_SIZE_ALIGNMENT != 0:
-            padded_head_size = cdiv(
-                self.head_size,
-                TPU_HEAD_SIZE_ALIGNMENT) * TPU_HEAD_SIZE_ALIGNMENT
-            query = torch.nn.functional.pad(
-                query, (0, padded_head_size - self.head_size), value=0.0)
-            key = torch.nn.functional.pad(
-                key, (0, padded_head_size - self.head_size), value=0.0)
-            value = torch.nn.functional.pad(
-                value, (0, padded_head_size - self.head_size), value=0.0)
-
-        if self.kv_sharing_target_layer_name is None and kv_cache.numel() > 0:
-            # Write input keys and values to the KV cache.
-            # Skip this if sharing KV cache with an earlier attention layer.
-            slot_mapping = attn_metadata.slot_mapping
-            write_to_kv_cache(
-                key,
-                value,
-                kv_cache,
-                slot_mapping,
-                attn_metadata.num_slices_per_kv_cache_update_block,
-                attn_metadata.num_kv_update_slices,
-                self.kv_cache_quantized_dtype,
-                layer._k_scale_float,
-                layer._v_scale_float,
-            )
-
-        if self.kv_cache_quantized_dtype is not None and (
-                layer._k_scale_float == 0.0 or layer._v_scale_float == 0.0):
-            raise ValueError(
-                "k_scale_float and v_scale_float must be non-zero")
-        output = torch.ops.xla.ragged_paged_attention(
-            query,
-            kv_cache,
-            attn_metadata.context_lens,
-            attn_metadata.block_tables,
-            attn_metadata.query_start_loc,
-            attn_metadata.num_seqs,
-            # By default, the system utilizes optimized block size and
-            # vmem_limit_bytes parameters from the kernel repository. However,
-            # these can be manually adjusted for debugging if necessary.
-            num_kv_pages_per_block=None,
-            num_queries_per_block=None,
-            vmem_limit_bytes=None,
-            use_kernel=True,
-            sm_scale=self.scale,
-            sliding_window=self.sliding_window,
-            soft_cap=self.logits_soft_cap,
-            k_scale=layer._k_scale_float,
-            v_scale=layer._v_scale_float,
-        )
-
-        if self.head_size % TPU_HEAD_SIZE_ALIGNMENT != 0:
-            output = output[:, :, :self.head_size]
-
-        return output.reshape(num_tokens, hidden_size)
-
-
-def write_to_kv_cache(
-    key: torch.Tensor,
-    value: torch.Tensor,
-    kv_cache: torch.Tensor,
-    slot_mapping: torch.Tensor,
-    num_slices_per_kv_cache_update_block: int,
-    num_kv_update_slices: torch.Tensor,
-    kv_cache_quantized_dtype: Optional[torch.dtype] = None,
-    k_scale: float = 1.0,
-    v_scale: float = 1.0,
-) -> None:
-    """ Write the key and values to the KV cache.
-
-    Args:
-        key: shape = [num_tokens, num_kv_heads, head_size]
-        value: shape = [num_tokens, num_kv_heads, head_size]
-        kv_cache: shape = [num_blocks, block_size, num_kv_heads * 2, head_size]
-        num_slices_per_kv_cache_update_block: int
-    """
-    _, page_size, num_combined_kv_heads, head_size = kv_cache.shape
-    head_size = cdiv(head_size,
-                     TPU_HEAD_SIZE_ALIGNMENT) * TPU_HEAD_SIZE_ALIGNMENT
-
-    if kv_cache_quantized_dtype is not None:
-        dtype_info = torch.finfo(kv_cache_quantized_dtype)
-        key = key.to(torch.float32) / k_scale
-        # NOTE: clamp is added here to avoid out of range of quantized dtype
-        key = torch.clamp(key, dtype_info.min, dtype_info.max)
-        key = key.to(kv_cache_quantized_dtype)
-        value = value.to(torch.float32) / v_scale
-        value = torch.clamp(value, dtype_info.min, dtype_info.max)
-        value = value.to(kv_cache_quantized_dtype)
-
-    kv = torch.cat([key, value], axis=-1).reshape(-1, num_combined_kv_heads,
-                                                  head_size)
-
-    torch.ops.xla.dynamo_set_buffer_donor_(kv_cache, True)
-
-    kv_cache = kv_cache.flatten(0, 1)
-    new_kv_cache = torch.ops.xla.kv_cache_update_op(
-        kv, slot_mapping, kv_cache, num_kv_update_slices, page_size,
-        num_slices_per_kv_cache_update_block)
-    # NOTE: the in-place copy will be optimized away by XLA compiler.
-    kv_cache.copy_(new_kv_cache)
-
+        query = query.reshape(1, query.shape[0], query.shape[1] // self.head_size, self.head_size).transpose(-3, -2) # [1, num_tokens, num_heads, head_size]
+        key = key.reshape(1, key.shape[0], key.shape[1] // self.head_size, self.head_size).transpose(-3, -2) # [1, num_tokens, num_kv_heads, head_size]
+        value = value.reshape(1, value.shape[0], value.shape[1] // self.head_size, self.head_size).transpose(-3, -2) # [1, num_tokens, num_kv_heads, head_size]
+
+
+        if kv_cache.numel() > 1:
+            cache_position = (attn_metadata.context_lens[:1]-1).to(query.device)
+
+            k_cache = kv_cache[0]
+            v_cache = kv_cache[1]
+
+            if query.shape[-2] == 1:
+                k_cache = torch.ops.tt.update_cache(k_cache, key, cache_position)
+                v_cache = torch.ops.tt.update_cache(v_cache, value, cache_position)
+            else:
+                k_cache = torch.ops.tt.fill_cache(k_cache, key)
+                v_cache = torch.ops.tt.fill_cache(v_cache, value)
+            new_kv_cache = torch.stack([k_cache, v_cache], dim=0)
+            key = k_cache
+            value = v_cache
+            kv_cache.copy_(new_kv_cache)
+
+        return torch.nn.functional.scaled_dot_product_attention(query, key, value, dropout_p=0.0, is_causal=False, attn_mask=attn_metadata.attn_mask, scale=output_scale, enable_gqa=True).transpose(-3, -2).reshape(num_tokens, hidden_size)

 # We can move this function to a common utils file if it's also useful for other
 # hardware.
@@ -401,7 +267,7 @@ def get_page_size_bytes(block_size: int, num_kv_heads: int, head_size: int,
                         kv_cache_dtype: torch.dtype) -> int:
     """Returns the size in bytes of one page of the KV cache."""
     padded_head_size = cdiv(head_size,
-                            TPU_HEAD_SIZE_ALIGNMENT) * TPU_HEAD_SIZE_ALIGNMENT
+                            TT_HEAD_SIZE_ALIGNMENT) * TT_HEAD_SIZE_ALIGNMENT
     num_combined_kv_heads = num_kv_heads * 2

     # NOTE: for the implicit padding in XLA
diff --git a/vllm_tt/model_runner.py b/vllm_tt/model_runner.py
index 43f1291..984b1cb 100644
--- a/vllm_tt/model_runner.py
+++ b/vllm_tt/model_runner.py
@@ -3,7 +3,7 @@
 import bisect
 import gc
 import time
-from typing import TYPE_CHECKING, Any, Optional, cast
+from typing import TYPE_CHECKING, Any, Literal, Optional, Union, cast
 from unittest.mock import patch

 import numpy as np
@@ -23,8 +23,7 @@ from vllm.config import (ParallelConfig, VllmConfig,
                          get_layers_from_vllm_config, update_config)
 from vllm.distributed.kv_transfer import (get_kv_transfer_group,
                                           has_kv_transfer_group)
-from vllm.distributed.kv_transfer.kv_connector.utils import copy_kv_blocks
-from vllm.forward_context import set_forward_context
+from vllm.forward_context import set_forward_context, get_forward_context
 from vllm.logger import init_logger
 from vllm.lora.layers import BaseLayerWithLoRA
 from vllm.model_executor.model_loader import get_model_loader
@@ -40,9 +39,9 @@ from vllm.sequence import IntermediateTensors
 from vllm.tasks import GenerationTask, PoolingTask, SupportedTask
 from vllm.utils import (LayerBlockType, cdiv, is_pin_memory_available,
                         prev_power_of_2)
-from vllm.v1.attention.backends.pallas import (TPU_STR_DTYPE_TO_TORCH_DTYPE,
-                                               PallasAttentionBackend,
-                                               PallasMetadata,
+from .attention import (TPU_STR_DTYPE_TO_TORCH_DTYPE,
+                                               TTAttentionBackend,
+                                               TTMetadata,
                                                get_page_size_bytes)
 from vllm.v1.kv_cache_interface import (AttentionSpec, FullAttentionSpec,
                                         KVCacheConfig, KVCacheSpec,
@@ -56,8 +55,39 @@ from vllm.v1.worker.kv_connector_model_runner_mixin import (
 from vllm.v1.worker.lora_model_runner_mixin import LoRAModelRunnerMixin
 from vllm.v1.worker.tpu_input_batch import CachedRequestState, InputBatch

-from .utils import (MultiModalBudget, add_kv_sharing_layers_to_kv_cache_groups,
+from vllm.v1.worker.utils import (MultiModalBudget,
                     bind_kv_cache, sanity_check_mm_encoder_outputs)
+from vllm.v1.kv_cache_interface import KVCacheGroupSpec
+
+def add_kv_sharing_layers_to_kv_cache_groups(
+    shared_kv_cache_layers: dict[str, str],
+    kv_cache_groups: list[KVCacheGroupSpec],
+    runner_only_attn_layers: Optional[set[str]] = None,
+) -> None:
+    """
+    Sets up KV cache sharing by reusing the allocated KV caches in `kv_caches`
+    for layers that do not allocate its own KV cache, based on the mapping in
+    `shared_kv_cache_layers`. Adds these layers to the corresponding KV cache
+    group, which is needed to ensure that attention metadata is assigned later.
+
+    Args:
+        shared_kv_cache_layers: Layer pairings for cross-layer KV sharing.
+            If an Attention layer `layer_name` is in the keys of this dict, it
+            means this layer will perform attention using the keys and values
+            from the KV cache of `shared_kv_cache_layers[layer_name]`.
+        kv_cache_groups: The KV cache groups of the model.
+    """
+    layer_to_kv_cache_group: dict[str, KVCacheGroupSpec] = {}
+    for kv_cache_group in kv_cache_groups:
+        for layer_name in kv_cache_group.layer_names:
+            layer_to_kv_cache_group[layer_name] = kv_cache_group
+
+    for layer_name, target_layer_name in shared_kv_cache_layers.items():
+        tgt_kv_cache_group = layer_to_kv_cache_group[target_layer_name]
+        tgt_kv_cache_group.layer_names.append(layer_name)
+
+        if runner_only_attn_layers is not None:
+            runner_only_attn_layers.add(layer_name)

 if TYPE_CHECKING:
     from vllm.v1.core.sched.output import SchedulerOutput
@@ -66,7 +96,22 @@ logger = init_logger(__name__)

 INVALID_TOKEN_ID = -1
 # Smallest output size
-MIN_NUM_SEQS = 8
+MIN_NUM_SEQS = 1
+
+
+def generate_attn_mask(context_lens: torch.Tensor, num_query_tokens: int, max_model_len: int, dtype, device) -> torch.Tensor:
+    L, S = num_query_tokens, max_model_len
+    attn_mask = torch.zeros(L, S, dtype=dtype)
+
+    length = context_lens[0].item()
+    if L != 1:
+        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)
+        attn_mask = attn_mask.masked_fill(temp_mask.logical_not(), torch.ones(())*float("-inf"))
+
+    # attn_mask = attn_mask.index_copy(-1, torch.arange(length, attn_mask.shape[-1]), torch.ones(1)*float("-inf"))
+    attn_mask[:, length:] = float("-inf")
+    return attn_mask.detach().to(device)
+


 #########################################################
@@ -104,7 +149,7 @@ MIN_NUM_SEQS = 8
 # The dummy_run should be comprehensive, ensuring all potential input shapes and
 # branch predictions are included as subgraph inputs to facilitate
 # pre-compilation.
-class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
+class TTModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):

     def __init__(
         self,
@@ -160,6 +205,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         self.sliding_window = model_config.get_sliding_window()
         self.block_size = cache_config.block_size
         self.max_model_len = model_config.max_model_len
+        assert self.max_model_len == scheduler_config.max_num_batched_tokens, f"The TT plugin only supports max_model_len == max_num_batched_tokens currently, recieved: max_model_len: {self.max_model_len}, max_num_batched_tokens: {scheduler_config.max_num_batched_tokens}"
         self.most_model_len = envs.VLLM_TPU_MOST_MODEL_LEN
         self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)
         self.num_blocks_per_most_len_req = cdiv(
@@ -167,11 +213,13 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             self.block_size) if self.most_model_len is not None else None
         # InputBatch needs to work with sampling tensors greater than padding
         # to avoid dynamic shapes. Also, avoid suboptimal alignment.
+        assert scheduler_config.max_num_seqs == 1, f"The TT plugin only supports max_num_seqs == 1 currently, recieved: max_num_seqs: {scheduler_config.max_num_seqs}"
         self.max_num_reqs = max(scheduler_config.max_num_seqs, MIN_NUM_SEQS)
         self.num_tokens_paddings = _get_token_paddings(
-            min_token_size=16,
+            min_token_size=1,
             max_token_size=scheduler_config.max_num_batched_tokens,
             padding_gap=envs.VLLM_TPU_BUCKET_PADDING_GAP)
+
         # In case `max_num_tokens < max(num_tokens_paddings)` use the actual
         # padded max value to pre-allocate data structures and pre-compile.
         self.max_num_tokens = self.num_tokens_paddings[-1]
@@ -192,8 +240,9 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         # Multi-modal data support
         self.mm_registry = MULTIMODAL_REGISTRY
         self.uses_mrope = model_config.uses_mrope
-        self.supports_mm_inputs = self.mm_registry.supports_multimodal_inputs(
-            model_config)
+        # self.supports_mm_inputs = self.mm_registry.supports_multimodal_inputs(
+        #     model_config)
+        self.supports_mm_inputs = False
         # TODO: Support M-RoPE (e.g, Qwen2-VL)
         assert not self.uses_mrope, "TPU does not support M-RoPE yet."

@@ -242,11 +291,11 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             device="cpu")
         # adjust num_reqs to avoid SMEM OOM.
         self.num_reqs_most_model_len = min(
-            PallasAttentionBackend.get_max_num_seqs(self.most_model_len,
+            TTAttentionBackend.get_max_num_seqs(self.most_model_len,
                                                     self.block_size),
             self.max_num_reqs) if self.most_model_len is not None else None
         self.num_reqs_max_model_len = min(
-            PallasAttentionBackend.get_max_num_seqs(self.max_model_len,
+            TTAttentionBackend.get_max_num_seqs(self.max_model_len,
                                                     self.block_size),
             self.max_num_reqs)
         self.query_start_loc_cpu = torch.zeros(self.max_num_tokens + 1,
@@ -294,14 +343,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             self.mm_registry,
         ) if self.supports_mm_inputs else None)

-        if not self.use_spmd:
-            self.sample_from_logits_func = torch.compile(
-                self.sample_from_logits,
-                backend="openxla",
-                fullgraph=True,
-                dynamic=False)
-        else:
-            self.sample_from_logits_func = self.sample_from_logits
+        self.sample_from_logits_func = self.sample_from_logits

     def _update_num_xla_graphs(self, case_str):
         check_comp = self.check_recompilation and not self.enforce_eager
@@ -356,8 +398,9 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                 removed_req_indices.append(req_index)

         # Free the cached encoder outputs.
-        for mm_hash in scheduler_output.free_encoder_mm_hashes:
-            self.encoder_cache.pop(mm_hash, None)
+        if hasattr(scheduler_output, "free_encoder_mm_hashes"):
+            for mm_hash in scheduler_output.free_encoder_mm_hashes:
+                self.encoder_cache.pop(mm_hash, None)

         # Remove the unscheduled requests from the persistent batch.
         # NOTE(woosuk): The unscheduled requests are either preempted requests
@@ -387,7 +430,9 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             self.requests[req_id] = CachedRequestState(
                 req_id=req_id,
                 prompt_token_ids=new_req_data.prompt_token_ids,
-                mm_features=new_req_data.mm_features,
+                mm_kwargs=new_req_data.mm_kwargs,
+                mm_positions=new_req_data.mm_positions,
+                # mm_hashes=new_req_data.mm_hashes,
                 sampling_params=sampling_params,
                 pooling_params=None,
                 generator=None,
@@ -735,21 +780,6 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                 self.device)
         block_tables = block_tables.to(self.device)

-        # Calculate the slot mapping
-        slot_mapping_metadata = self._get_slot_mapping_metadata(
-            num_reqs, num_scheduled_tokens_per_req)
-        num_kv_update_slices = slot_mapping_metadata.shape[0]
-        padded_num_slices = _get_padded_num_kv_cache_update_slices(
-            padded_total_num_scheduled_tokens, self.max_num_reqs,
-            self.block_size)
-        slot_mapping_metadata = np.pad(
-            slot_mapping_metadata,
-            [[0, padded_num_slices - len(slot_mapping_metadata)], [0, 0]],
-            constant_values=0)
-        slot_mapping_metadata = np.transpose(slot_mapping_metadata)
-        slot_mapping_metadata = torch.tensor(slot_mapping_metadata,
-                                             device=self.device)
-
         if self.lora_config is not None:
             # We need to respect padding when activating LoRA adapters
             padded_num_scheduled_tokens_per_req = np.copy(
@@ -761,19 +791,13 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             self.set_active_loras(self.input_batch,
                                   padded_num_scheduled_tokens_per_req)

-        attn_metadata = PallasMetadata(
-            slot_mapping=slot_mapping_metadata,
-            block_tables=block_tables,
+        attn_metadata = TTMetadata(
             context_lens=seq_lens,
             query_start_loc=query_start_loc,
             num_seqs=torch.tensor([num_reqs],
                                   dtype=torch.int32,
                                   device=self.device),
-            num_kv_update_slices=torch.tensor([num_kv_update_slices],
-                                              dtype=torch.int32,
-                                              device=self.device),
-            num_slices_per_kv_cache_update_block=self.
-            _num_slices_per_kv_cache_update_block,
+            attn_mask=generate_attn_mask(seq_lens, self.input_ids.shape[-1], self.max_model_len, self.dtype, self.device),
         )
         # NOTE(woosuk): Due to chunked prefills, there can be at most 1 partial
         # request in the batch. While we should not sample any token from this
@@ -820,10 +844,10 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             req_state = self.requests[req_id]

             for mm_input_id in encoder_input_ids:
-                mm_feature = req_state.mm_features[mm_input_id]
-                mm_hash = mm_feature.identifier
-                mm_kwargs.append(mm_feature.data)
-                mm_hashes_pos.append((mm_hash, mm_feature.mm_position))
+                mm_hash = req_state.mm_hashes[mm_input_id]
+                mm_kwargs.append(req_state.mm_kwargs[mm_input_id])
+                mm_hashes_pos.append(
+                    (mm_hash, req_state.mm_positions[mm_input_id]))

         # Batch mm inputs as much as we can: if a request in the batch has
         # multiple modalities or a different modality than the previous one,
@@ -881,12 +905,13 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                 req_id]
             req_state = self.requests[req_id]
             num_computed_tokens = req_state.num_computed_tokens
+            mm_positions = req_state.mm_positions
+            mm_hashes = req_state.mm_hashes
             # TODO unroll loop and assume/enforce --disable_chunked_mm_input
             # NOTE (NickLucche) here we diverge from logic in other runners, as
             # we assume to only have whole mm items to process. Hence we avoid
             # the intrinsic dynamism that `gather_mm_placeholders` introduces.
-            for mm_feature in req_state.mm_features:
-                pos_info = mm_feature.mm_position
+            for i, pos_info in enumerate(mm_positions):
                 start_pos = pos_info.offset
                 num_encoder_tokens = pos_info.length

@@ -901,7 +926,8 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                     # The encoder output is already processed and stored
                     # in the decoder's KV cache.
                     continue
-                mm_hash = mm_feature.identifier
+
+                mm_hash = mm_hashes[i]
                 encoder_output = self.encoder_cache.get(mm_hash, None)
                 assert encoder_output is not None,\
                       f"Encoder cache miss for {mm_hash}."
@@ -935,6 +961,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         scheduler_output: "SchedulerOutput",
         intermediate_tensors: Optional[IntermediateTensors] = None,
     ) -> ModelRunnerOutput:
+        torch._dynamo.config.dynamic_shapes = False
         # Update cached state
         self._update_states(scheduler_output)
         if not scheduler_output.total_num_scheduled_tokens:
@@ -971,14 +998,17 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             xm.mark_step()
             # Run the decoder
             with set_forward_context(
-                    attn_metadata,
-                    self.vllm_config,
-                    num_tokens=scheduler_output.total_num_scheduled_tokens):
+                        attn_metadata,
+                        self.vllm_config,
+                        num_tokens=scheduler_output.total_num_scheduled_tokens):
+
+
                 hidden_states = self.model(
                     input_ids=input_ids,
                     positions=self.position_ids,
                     inputs_embeds=inputs_embeds,
                 )
+                xm.mark_step()
             hidden_states = self.select_hidden_states(hidden_states,
                                                       logits_indices)
             logits = self.compute_logits(hidden_states)
@@ -1118,6 +1148,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             prompt_logprobs_dict=prompt_logprobs_dict,
             pooler_output=[],
             kv_connector_output=kv_connector_output,
+            spec_token_ids=None
         )

         # Check there are no new graphs compiled - all the graphs should be
@@ -1139,6 +1170,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             setattr(self, config_name, new_config)

     def load_model(self) -> None:
+        logger.info("CALLING LOAD MODEL")
         self.device = self.device_config.device

         # NOTE(woosuk): While the executor assigns the TP ranks to the worker
@@ -1157,18 +1189,19 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                 return_value=xm_tp_rank):
             try:
                 if self.use_spmd:
+                    logger.info("Loading model with TPUModelLoader...")
                     tpu_loader = TPUModelLoader(
                         load_config=self.vllm_config.load_config)
                     model = tpu_loader.load_model(
                         vllm_config=self.vllm_config,
                         model_config=self.vllm_config.model_config,
-                        mesh=self.mesh)
+                        mesh=self.mesh).eval()
                 else:
                     model_loader = get_model_loader(self.load_config)
                     logger.info("Loading model from scratch...")
                     model = model_loader.load_model(
                         vllm_config=self.vllm_config,
-                        model_config=self.model_config)
+                        model_config=self.model_config).eval()
             except RuntimeError as e:
                 raise RuntimeError(
                     f"Unable to load model, a likely reason is the model is "
@@ -1177,10 +1210,11 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                     "or sharding the weights on more chips. "
                     f"See the detailed error: {e}") from e
         if self.lora_config is not None:
+            logger.info("Loading lora model...")
             model = self.load_lora_model(model, self.model_config,
                                          self.scheduler_config,
-                                         self.lora_config, self.device)
-            replace_set_lora(model)
+                                         self.lora_config, self.device).eval()
+            replace_set_lora(model.eval())

         # Sync all pending XLA execution during model initialization and weight
         # loading.
@@ -1188,6 +1222,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         xm.wait_device_ops()
         if not hasattr(self, "model"):
             self.model = model
+        self.model.compile(backend="tt", dynamic=False)
         self.sampler = TPUSampler()

     def reload_weights(self) -> None:
@@ -1200,6 +1235,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
     @torch.no_grad()
     def _dummy_run(self, num_tokens: int, num_reqs: int,
                    num_blocks: int) -> None:
+        torch._dynamo.config.dynamic_shapes = False
         if self.supports_mm_inputs:
             input_ids = None
             inputs_embeds = torch.zeros((num_tokens, self.hidden_size),
@@ -1212,14 +1248,6 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         actual_num_reqs = min(num_tokens, num_reqs)
         position_ids = torch.zeros(num_tokens,
                                    dtype=torch.int32).to(self.device)
-        padded_num_slices = _get_padded_num_kv_cache_update_slices(
-            num_tokens, self.max_num_reqs, self.block_size)
-        num_kv_update_slices = torch.tensor([padded_num_slices],
-                                            dtype=torch.int32).to(self.device)
-        slot_mapping = torch.zeros((3, padded_num_slices),
-                                   dtype=torch.int32).to(self.device)
-        block_tables = torch.zeros((num_reqs, num_blocks),
-                                   dtype=torch.int32).to(self.device)
         query_lens = [1] * num_reqs
         query_start_loc = torch.cumsum(torch.tensor([0] + query_lens,
                                                     dtype=torch.int32),
@@ -1229,27 +1257,13 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                                   dtype=torch.int32).to(self.device)
         num_seqs = torch.tensor([actual_num_reqs],
                                 dtype=torch.int32).to(self.device)
-        attn_metadata = PallasMetadata(
-            slot_mapping=slot_mapping,
-            block_tables=block_tables,
+        attn_metadata = TTMetadata(
             context_lens=context_lens,
             query_start_loc=query_start_loc,
             num_seqs=num_seqs,
-            num_kv_update_slices=num_kv_update_slices,
-            num_slices_per_kv_cache_update_block=self.
-            _num_slices_per_kv_cache_update_block,
+            attn_mask=generate_attn_mask(context_lens, input_ids.shape[-1], self.max_model_len, self.dtype, self.device),
         )

-        if self.supports_mm_inputs:
-            torch._dynamo.mark_dynamic(inputs_embeds, 0)
-        else:
-            torch._dynamo.mark_dynamic(input_ids, 0)
-        torch._dynamo.mark_dynamic(position_ids, 0)
-        torch._dynamo.mark_dynamic(attn_metadata.slot_mapping, 0)
-        torch._dynamo.mark_dynamic(attn_metadata.block_tables, (0, 1))
-        torch._dynamo.mark_dynamic(attn_metadata.context_lens, 0)
-        torch._dynamo.mark_dynamic(attn_metadata.query_start_loc, 0)
-
         layer_names = get_layers_from_vllm_config(self.vllm_config,
                                                   Attention).keys()
         per_layer_attn_metadata = {
@@ -1264,6 +1278,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             out = self.model(input_ids=input_ids,
                              positions=position_ids,
                              inputs_embeds=inputs_embeds)
+            xm.mark_step()
         self._hidden_states_dtype = out.dtype

     def _set_active_loras(self, prompt_lora_mapping, token_lora_mapping,
@@ -1274,6 +1289,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         xm.mark_step()  # Captures metadata updates

     def _precompile_mm_encoder(self) -> None:
+        torch._dynamo.config.dynamic_shapes = False
         if not self.supports_mm_inputs:
             return

@@ -1346,6 +1362,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                 "[secs].", mode, end - start)

     def _precompile_backbone(self) -> None:
+        torch._dynamo.config.dynamic_shapes = False
         logger.info("Compiling the model with different input shapes.")
         start = time.perf_counter()
         for num_tokens in self.num_tokens_paddings:
@@ -1361,6 +1378,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         self._update_num_xla_graphs("model backbone")

     def _precompile_select_hidden_states(self) -> None:
+        torch._dynamo.config.dynamic_shapes = False
         # Compile hidden state selection function for bucketed
         # n_tokens x max_num_reqs. Graph is really small so this is fine.
         logger.info(
@@ -1371,12 +1389,12 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             dummy_hidden = torch.zeros((num_tokens, hsize),
                                        device=self.device,
                                        dtype=self._hidden_states_dtype)
-            torch._dynamo.mark_dynamic(dummy_hidden, 0)
+            # torch._dynamo.mark_dynamic(dummy_hidden, 0)
             for num_reqs in self.num_reqs_paddings:
                 indices = torch.zeros(num_reqs,
                                       dtype=torch.int32,
                                       device=self.device)
-                torch._dynamo.mark_dynamic(indices, 0)
+                # torch._dynamo.mark_dynamic(indices, 0)
                 self.select_hidden_states(dummy_hidden, indices)
                 logger.info("  -- num_tokens: %d, num_seqs: %d", num_tokens,
                             num_reqs)
@@ -1390,6 +1408,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         self._update_num_xla_graphs("select_hidden_states")

     def _precompile_compute_logits(self) -> None:
+        torch._dynamo.config.dynamic_shapes = False
         logger.info("Compiling compute_logits with different input shapes.")
         start = time.perf_counter()
         hsize = self.model_config.get_hidden_size()
@@ -1397,7 +1416,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             dummy_hidden = torch.zeros((num_reqs, hsize),
                                        device=self.device,
                                        dtype=self._hidden_states_dtype)
-            torch._dynamo.mark_dynamic(dummy_hidden, 0)
+            # torch._dynamo.mark_dynamic(dummy_hidden, 0)
             self.compute_logits(dummy_hidden)
             logger.info("  -- num_seqs: %d", num_reqs)
         xm.wait_device_ops()
@@ -1406,6 +1425,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         self._update_num_xla_graphs("compute_logits")

     def _precompile_structured_decoding(self) -> None:
+        torch._dynamo.config.dynamic_shapes = False
         logger.info(
             "Compiling structured_decoding with different input shapes.")
         start = time.perf_counter()
@@ -1430,6 +1450,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         self._update_num_xla_graphs("structured_decoding")

     def _precompile_sample_from_logits(self) -> None:
+        torch._dynamo.config.dynamic_shapes = False
         logger.info(
             "Compiling sample_from_logits with different input shapes.")
         start = time.perf_counter()
@@ -1461,6 +1482,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         self._update_num_xla_graphs("sample_from_logits")

     def _precompile_gather_logprobs(self) -> None:
+        torch._dynamo.config.dynamic_shapes = False
         logger.info("Compiling gather_logprobs with different input shapes.")
         start = time.perf_counter()
         for num_reqs in self.num_reqs_paddings:
@@ -1482,6 +1504,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         """
         Precompile all the subgraphs with possible input shapes.
         """
+        torch._dynamo.config.dynamic_shapes = False
         with self.maybe_setup_dummy_loras(self.lora_config):
             self._precompile_mm_encoder()
             self._precompile_backbone()
@@ -1495,6 +1518,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         self,
         num_tokens: int,
     ) -> None:
+        torch._dynamo.config.dynamic_shapes = False
         # Profile with multimodal encoder & encoder cache.
         if self.supports_mm_inputs:
             if self.model_config.multimodal_config.skip_mm_profiling:
@@ -1554,6 +1578,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                         enumerate(dummy_encoder_outputs))

         # Trigger compilation for general shape.
+        torch._dynamo.config.dynamic_shapes = False
         self._dummy_run(num_tokens, self.num_reqs_max_model_len,
                         self.max_num_blocks_per_req)
         if self.most_model_len is not None:
@@ -1642,9 +1667,9 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                         assert num_kv_heads % tp_size == 0, (
                             f"num_kv_heads {num_kv_heads} must be divisible by "
                             f"tp_size {tp_size} under SPMD mode")
-                    kv_cache_shape = PallasAttentionBackend.get_kv_cache_shape(
-                        num_blocks, kv_cache_spec.block_size,
-                        kv_cache_spec.num_kv_heads, kv_cache_spec.head_size)
+                    kv_cache_shape = TTAttentionBackend.get_kv_cache_shape(
+                        1, self.num_kv_heads, self.max_model_len,
+                        kv_cache_spec.head_size)
                     dtype = kv_cache_spec.dtype

                     tpu_kv_cache = torch.zeros(kv_cache_shape,
@@ -1672,7 +1697,7 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
             get_kv_transfer_group().set_host_xfer_buffer_ops(copy_kv_blocks)

     def reset_dynamo_cache(self):
-
+        return
         # NOTE: We check `is_multimodal_model` instead of `supports_mm_inputs`
         # since the compiled model object of the language backbone of a
         # multimodal model needs to be extracted via `get_language_model`.
@@ -1686,18 +1711,21 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
                 compiled_model.original_code_object)
             compiled_model.compiled_codes.clear()

-    @torch.compile(backend="openxla", fullgraph=True, dynamic=False)
+    # @torch.compile(backend="tt", fullgraph=True, dynamic=False)
     def select_hidden_states(self, hidden_states, indices_do_sample):
-        return hidden_states[indices_do_sample]
+        device = hidden_states.device
+        indices_do_sample = indices_do_sample.to('cpu')
+        hidden_states = hidden_states.to('cpu')
+        return hidden_states[indices_do_sample].to(device)

-    @torch.compile(backend="openxla", fullgraph=True, dynamic=False)
+    # @torch.compile(backend="tt", fullgraph=True, dynamic=False)
     def compute_logits(self,
                        sample_hidden_states: torch.Tensor) -> torch.Tensor:
         return self.model.compute_logits(sample_hidden_states, None)

     # TODO: Under SPMD mode, sample_from_logits has correctness issue.
     #       Re-enable the torch.compile once the issue is fixed in torchxla.
-    # @torch.compile(backend="openxla", fullgraph=True, dynamic=False)
+    # @torch.compile(backend="tt", fullgraph=True, dynamic=False)
     def sample_from_logits(
             self, logits: torch.Tensor,
             sampling_metadata: TPUSupportedSamplingMetadata) -> torch.Tensor:
@@ -1705,14 +1733,10 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         Sample with xla-friendly function. This function is to be traced
         separately from `forward` for lighter compilation overhead.
         """
-        if sampling_metadata.all_greedy:
-            out_tokens = torch.argmax(logits, dim=-1, keepdim=True)
-        else:
-            out_tokens = self.sampler(logits,
-                                      sampling_metadata).sampled_token_ids
+        out_tokens = torch.argmax(logits.to('cpu'), dim=-1, keepdim=True).to('xla')
         return out_tokens

-    @torch.compile(backend="openxla", fullgraph=True, dynamic=False)
+    # @torch.compile(backend="tt", fullgraph=True, dynamic=False)
     def gather_logprobs(self, logits: torch.Tensor,
                         sampled_tokens: torch.Tensor) -> LogprobsTensors:
         """
@@ -1720,13 +1744,24 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         of logprobs as an alternative to having multiple pre-compiled graphs.
         Select the number of logprobs actually demanded by each request on CPU.
         """
-        logprobs = self.sampler.compute_logprobs(logits)
-        return self.sampler.gather_logprobs(
+        torch._dynamo.config.dynamic_shapes = False
+        device = logits.device
+        logprobs = self.sampler.compute_logprobs(logits.to('cpu'))
+        logprobTensors = self.sampler.gather_logprobs(
             logprobs,
             self.model_config.max_logprobs,
-            token_ids=sampled_tokens.squeeze(-1))
-
-    @torch.compile(backend="openxla", fullgraph=True, dynamic=False)
+            token_ids=sampled_tokens.to('cpu').squeeze(-1))
+
+    #     robs + 1]
+    # logprob_token_ids: torch.Tensor
+    # # [num_reqs, max_num_logprobs + 1]
+    # logprobs: torch.Tensor
+    # # [num_reqs]
+    # selected_token_ranks: torch.Tensor
+
+        return LogprobsTensors(logprob_token_ids=logprobTensors.logprob_token_ids.to(device), logprobs=logprobTensors.logprobs.to(device), selected_token_ranks=logprobTensors.selected_token_ranks.to(device))
+
+    # @torch.compile(backend="tt", fullgraph=True, dynamic=False)
     def structured_decode(self, require_struct_decoding: torch.Tensor,
                           grammar_bitmask: torch.Tensor, logits: torch.Tensor,
                           arange: torch.Tensor) -> torch.Tensor:
@@ -1765,22 +1800,28 @@ class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
         self.grammar_bitmask_cpu.zero_()
         self.require_structured_out_cpu.zero_()

-        sorted_struct_requests = sorted(
-            scheduler_output.structured_output_request_ids.items(),
-            key=lambda item: item[1])
-        cumulative_mask_idx = 0
-        for req_id, _ in sorted_struct_requests:
-            if req_id not in self.input_batch.req_id_to_index:
+        # We receive the structured output bitmask from the scheduler, but the
+        # indices of the requests in the batch may not match the indices of
+        # the bitmask since the scheduler doesn't know how the tpu runner is
+        # ordering the requests in the batch. We need to match the order of
+        # bitmask with the order of requests
+        struct_out_indices: list[int] = []
+        mask_indices: list[int] = []
+        for req_id in self.input_batch.req_ids:
+            mask_index = scheduler_output.structured_output_request_ids.get(
+                req_id)
+            if mask_index is None:
                 continue
             batch_index = self.input_batch.req_id_to_index[req_id]
-            self.grammar_bitmask_cpu[batch_index] = torch.from_numpy(
-                grammar_bitmask[cumulative_mask_idx])
-            # It's not guaranteed that all requests in this batch require
-            # structured output, so create a bool tensor to represent
-            # the requests that need structured output.
-            self.require_structured_out_cpu[batch_index] = True
-            cumulative_mask_idx += 1
-
+            struct_out_indices.append(batch_index)
+            mask_indices.append(mask_index)
+        self.grammar_bitmask_cpu[struct_out_indices] = torch.from_numpy(
+            grammar_bitmask[mask_indices])
+        # It's not guaranteed that all requests in this batch require
+        # structured output, so create a bool tensor to represent
+        # the requests that need structured output.
+        struct_out_indices = torch.tensor(struct_out_indices, dtype=torch.long)
+        self.require_structured_out_cpu[struct_out_indices] = True
         return self.require_structured_out_cpu[:num_reqs].to(logits.device), \
             self.grammar_bitmask_cpu[:num_reqs].to(logits.device), \
             self.structured_decode_arange.to(logits.device)
@@ -1878,6 +1919,75 @@ def _get_padded_token_len(paddings: list[int], x: int) -> int:
     return paddings[index]


+def _make_src_and_dst_indices(
+    src_block_ids: list[int],
+    dst_block_ids: list[int],
+    src_device: Union[torch.device, str],
+    dst_device: Union[torch.device, str],
+) -> tuple[torch.Tensor, torch.Tensor]:
+    src_indices = torch.tensor(src_block_ids,
+                               device=src_device,
+                               dtype=torch.int64)
+    dst_indices = torch.tensor(dst_block_ids,
+                               device=dst_device,
+                               dtype=torch.int64)
+    return src_indices, dst_indices
+
+
+@torch.compile(backend="tt")
+def _insert_blocks_to_tpu(
+    cpu_cache: torch.Tensor,
+    tpu_cache: torch.Tensor,
+    cpu_block_indices: torch.Tensor,
+    tpu_block_indices: torch.Tensor,
+) -> None:
+    torch.ops.xla.dynamo_set_buffer_donor_(tpu_cache, True)
+    tpu_cache[tpu_block_indices] = cpu_cache[cpu_block_indices].to(
+        tpu_cache.device)
+
+
+@torch.compile(backend="tt")
+def _swap_out_tpu_blocks(
+    tpu_cache: torch.Tensor,
+    cpu_cache: torch.Tensor,
+    tpu_block_indices: torch.Tensor,
+    cpu_block_indices: torch.Tensor,
+) -> None:
+    """ tpu blocks to cpu blocks"""
+    torch.ops.xla.dynamo_set_buffer_donor_(tpu_cache, True)
+    cpu_cache[cpu_block_indices] = tpu_cache[tpu_block_indices].cpu()
+
+
+def copy_kv_blocks(
+    src_kv_caches: dict[str, torch.Tensor],
+    dst_kv_caches: dict[str, torch.Tensor],
+    src_block_ids: list[int],
+    dst_block_ids: list[int],
+    direction: Literal["h2d", "d2h"],
+) -> None:
+    """Copy kv blocks between different buffers."""
+    if not src_kv_caches or not dst_kv_caches or \
+       not src_block_ids or not dst_block_ids or \
+       len(src_block_ids) != len(dst_block_ids):
+        return
+
+    src_device = next(iter(src_kv_caches.values())).device
+    dst_device = next(iter(dst_kv_caches.values())).device
+
+    src_indices, dst_indices = _make_src_and_dst_indices(
+        src_block_ids=src_block_ids,
+        dst_block_ids=dst_block_ids,
+        src_device=src_device,
+        dst_device=dst_device)
+
+    _copy_fn = _insert_blocks_to_tpu if direction == "h2d" else \
+               _swap_out_tpu_blocks
+    for layer_name in src_kv_caches:
+        src_tensor = src_kv_caches[layer_name]
+        dst_tensor = dst_kv_caches[layer_name]
+        _copy_fn(src_tensor, dst_tensor, src_indices, dst_indices)
+
+
 def _get_padded_num_kv_cache_update_slices(num_tokens: int, max_num_reqs: int,
                                            page_size: int) -> int:
     """Calculates the padded number of KV cache update slices to avoid
diff --git a/vllm_tt/platform.py b/vllm_tt/platform.py
index 6a06195..3828470 100644
--- a/vllm_tt/platform.py
+++ b/vllm_tt/platform.py
@@ -4,14 +4,13 @@
 from typing import TYPE_CHECKING, Optional, Union, cast

 import torch
-from tpu_info import device

 from vllm.inputs import ProcessorInputs, PromptType
 from vllm.logger import init_logger
 from vllm.sampling_params import SamplingParams, SamplingType
 from vllm.utils import DEFAULT_MAX_NUM_BATCHED_TOKENS

-from .interface import Platform, PlatformEnum, _Backend
+from vllm.platforms.interface import Platform, PlatformEnum, _Backend

 if TYPE_CHECKING:
     from vllm.config import BlockSize, ModelConfig, VllmConfig
@@ -24,52 +23,50 @@ else:

 logger = init_logger(__name__)

-USE_TPU_COMMONS = False
+from torch_xla import runtime as xrt


-class TpuPlatform(Platform):
-    _enum = PlatformEnum.TPU
-    device_name: str = "tpu"
-    device_type: str = "tpu"
+class TTPlatform(Platform):
+    _enum = PlatformEnum.OOT
+    device_name: str = "xla"
+    device_type: str = "xla"
     dispatch_key: str = "XLA"
-    ray_device_key: str = "TPU"
+    ray_device_key: str = "TT"
     dist_backend: str = "gloo"
-    device_control_env_var: str = "TPU_VISIBLE_CHIPS"
+    # device_control_env_var: str = "TPU_VISIBLE_CHIPS"
     simple_compile_backend: str = "openxla"

     supported_quantization: list[str] = [
-        "fp8", "tpu_int8", "compressed-tensors"
+        # "fp8", "tpu_int8", "compressed-tensors"
     ]

     additional_env_vars: list[str] = [
-        "TPU_CHIPS_PER_HOST_BOUNDS", "TPU_HOST_BOUNDS"
+        # "TPU_CHIPS_PER_HOST_BOUNDS", "TPU_HOST_BOUNDS"
     ]

+    def __post_init__(self):
+        torch._dynamo.config.ignore_logging_methods(logger.info)
+
     @classmethod
     def get_attn_backend_cls(cls, selected_backend: _Backend, head_size: int,
                              dtype: torch.dtype, kv_cache_dtype: Optional[str],
                              block_size: int, use_v1: bool, use_mla: bool,
                              has_sink) -> str:
-        if (selected_backend != _Backend.PALLAS
-                and selected_backend != _Backend.PALLAS_VLLM_V1):
-            logger.info("Cannot use %s backend on TPU.", selected_backend)
-
         if not use_v1:
-            raise ValueError("TPU backend only supports V1.")
+            raise ValueError("TT backend only supports V1.")
         logger.info("Using Pallas V1 backend.")
-        return "vllm.v1.attention.backends.pallas.PallasAttentionBackend"
+        return "vllm_tt.attention.TTAttentionBackend"

     @classmethod
     def set_device(cls, device: torch.device) -> None:
         """
         Set the device for the current platform.
         """
-        torch.tpu.set_device(device)
+        cls.device = device

     @classmethod
     def get_device_name(cls, device_id: int = 0) -> str:
-        chip_type, _ = device.get_local_chips()
-        return f"TPU {chip_type.name}"
+        return f"xla:{device_id}"

     @classmethod
     def get_device_total_memory(cls, device_id: int = 0) -> int:
@@ -81,7 +78,8 @@ class TpuPlatform(Platform):

     @classmethod
     def get_punica_wrapper(cls) -> str:
-        return "vllm.lora.punica_wrapper.punica_tpu.PunicaWrapperTPU"
+        return NotImplementedError
+        # return "vllm.lora.punica_wrapper.punica_tpu.PunicaWrapperTPU"

     @classmethod
     def get_infinity_values(cls, dtype: torch.dtype) -> tuple[float, float]:
@@ -102,6 +100,7 @@ class TpuPlatform(Platform):
     @classmethod
     def check_and_update_config(cls, vllm_config: VllmConfig) -> None:
         from vllm.config import CompilationLevel, CUDAGraphMode
+

         cache_config = vllm_config.cache_config
         # For v0, the default block size is 16.
@@ -109,48 +108,48 @@ class TpuPlatform(Platform):
             cache_config.block_size = cast(BlockSize, 16)
         compilation_config = vllm_config.compilation_config

-        # TPU only supports DYNAMO_ONCE compilation level
+        # TT only supports DYNAMO_ONCE compilation level
         if compilation_config.level != CompilationLevel.DYNAMO_ONCE:
-            logger.info("[TPU] Forcing DYNAMO_ONCE compilation level, and "
+            logger.info("[TT] Forcing DYNAMO_ONCE compilation level, and "
                         "disabling cudagraph.")
             compilation_config.level = CompilationLevel.DYNAMO_ONCE

         if compilation_config.cudagraph_mode is None or \
                 compilation_config.cudagraph_mode.max_cudagraph_mode() \
                     != CUDAGraphMode.NONE:
-            logger.info("[TPU] CUDA graph is not supported on TPU, "
+            logger.info("[TT] CUDA graph is not supported on TT, "
                         "disabling cudagraphs.")
             compilation_config.cudagraph_mode = CUDAGraphMode.NONE

         if compilation_config.backend == "":
-            compilation_config.backend = "openxla"
+            compilation_config.backend = "tt"

         assert vllm_config.speculative_config is None, \
-            "TPU does not support speculative decoding"
+            "TT does not support speculative decoding"

         model_config = vllm_config.model_config
         if model_config is not None and model_config.dtype in (torch.float16,
                                                                torch.float32):
             logger.warning(
-                "The TPU backend currently does not support %s. "
+                "The TT backend currently does not support %s. "
                 "Using bfloat16 instead.", model_config.dtype)
             model_config.dtype = torch.bfloat16

-        from vllm.v1.attention.backends.pallas import PallasAttentionBackend
-        cache_config.block_size = PallasAttentionBackend.get_page_size(
+        from .attention import TTAttentionBackend
+        cache_config.block_size = TTAttentionBackend.get_page_size(
             vllm_config)  # type: ignore[assignment]

         parallel_config = vllm_config.parallel_config
         scheduler_config = vllm_config.scheduler_config
         if parallel_config.worker_cls == "auto":
-            parallel_config.worker_cls = "vllm.v1.worker.tpu_worker.TPUWorker"
+            parallel_config.worker_cls = "vllm_tt.worker.TTWorker"

         assert not vllm_config.speculative_config, (
-            "Speculative decoding is not yet supported for TPU backend")
+            "Speculative decoding is not yet supported for TT backend")

         if scheduler_config.is_multimodal_model and not \
                 scheduler_config.disable_chunked_mm_input:
-            logger.warning("TPU does not support running Multimodal models"\
+            logger.warning("TT does not support running Multimodal models"\
             " without setting `--disable_chunked_mm_input`. " \
             "Forcing --disable_chunked_mm_input.")
             scheduler_config.disable_chunked_mm_input = True
@@ -167,7 +166,7 @@ class TpuPlatform(Platform):

     @classmethod
     def is_pin_memory_available(cls):
-        logger.warning("Pin memory is not supported on TPU.")
+        logger.warning("Pin memory is not supported on TT.")
         return False

     @classmethod
@@ -200,37 +199,11 @@ class TpuPlatform(Platform):
                                     model_config: "ModelConfig") -> bool:
         return True

-    @classmethod
-    @torch.compile(backend="openxla")
-    def insert_blocks_to_device(
-        cls,
-        src_cache: torch.Tensor,
-        dst_cache: torch.Tensor,
-        src_block_indices: torch.Tensor,
-        dst_block_indices: torch.Tensor,
-    ) -> None:
-        torch.ops.xla.dynamo_set_buffer_donor_(dst_cache, True)
-        dst_cache[dst_block_indices] = src_cache[src_block_indices].to(
-            dst_cache.device)

-    @classmethod
-    @torch.compile(backend="openxla")
-    def swap_out_blocks_to_host(
-        cls,
-        src_cache: torch.Tensor,
-        dst_cache: torch.Tensor,
-        src_block_indices: torch.Tensor,
-        dst_block_indices: torch.Tensor,
-    ) -> None:
-        """ tpu blocks to cpu blocks"""
-        torch.ops.xla.dynamo_set_buffer_donor_(src_cache, True)
-        dst_cache[dst_block_indices] = src_cache[src_block_indices].cpu()
-
-
-try:
-    from tpu_commons.platforms import TpuPlatform as TpuCommonsPlatform
-    TpuPlatform = TpuCommonsPlatform  # type: ignore
-    USE_TPU_COMMONS = True
-except ImportError:
-    logger.info("tpu_commons not found, using vLLM's TpuPlatform")
-    pass
+# try:
+#     from tpu_commons.platforms import TpuPlatform as TpuCommonsPlatform
+#     TpuPlatform = TpuCommonsPlatform  # type: ignore
+#     USE_TPU_COMMONS = True
+# except ImportError:
+#     logger.info("tpu_commons not found, using vLLM's TpuPlatform")
+#     pass
diff --git a/vllm_tt/worker.py b/vllm_tt/worker.py
index fc72b95..26d483e 100644
--- a/vllm_tt/worker.py
+++ b/vllm_tt/worker.py
@@ -19,7 +19,6 @@ from vllm.logger import init_logger
 from vllm.lora.request import LoRARequest
 from vllm.model_executor import set_random_seed
 from vllm.platforms import current_platform
-from vllm.platforms.tpu import USE_TPU_COMMONS
 from vllm.tasks import SupportedTask
 from vllm.utils import STR_DTYPE_TO_TORCH_DTYPE, cdiv
 from vllm.v1.core.sched.output import SchedulerOutput
@@ -29,19 +28,17 @@ from vllm.v1.outputs import ModelRunnerOutput
 from vllm.v1.utils import report_usage_stats
 from vllm.v1.worker.utils import bind_kv_cache

-logger = init_logger(__name__)
+import torch_xla.core.xla_model as xm
+import torch_xla.debug.profiler as xp
+import torch_xla.runtime as xr
+from .model_runner import TTModelRunner
+from .attention import TT_HEAD_SIZE_ALIGNMENT

-if not USE_TPU_COMMONS:
-    logger.info("tpu_commons not found, using vLLM's TPUWorker.")
-    import torch_xla.core.xla_model as xm
-    import torch_xla.debug.profiler as xp
-    import torch_xla.runtime as xr

-    from vllm.v1.attention.backends.pallas import TPU_HEAD_SIZE_ALIGNMENT
-    from vllm.v1.worker.tpu_model_runner import TPUModelRunner
+logger = init_logger(__name__)


-class TPUWorker:
+class TTWorker:

     def __init__(
         self,
@@ -90,12 +87,12 @@ class TPUWorker:

         # Delay profiler initialization to the start of the profiling.
         # This is because in vLLM V1, MP runtime is initialized before the
-        # TPU Worker is initialized. The profiler server needs to start after
+        # TT Worker is initialized. The profiler server needs to start after
         # MP runtime is initialized.
         self.profiler = None
         self.profile_dir = None
         if envs.VLLM_TORCH_PROFILER_DIR and self.rank < 1:
-            # For TPU, we can only have 1 active profiler session for 1 profiler
+            # For TT, we can only have 1 active profiler session for 1 profiler
             # server. So we only profile on rank0.
             self.profile_dir = envs.VLLM_TORCH_PROFILER_DIR
             logger.info("Profiling enabled. Traces will be saved to: %s",
@@ -110,17 +107,8 @@ class TPUWorker:
         self.cache_config.num_cpu_blocks = num_cpu_blocks

     def init_device(self):
-        os.environ["PJRT_DEVICE"] = "TPU"
-        # Note: Currently the XLA compiler wrongly uses 2D ring strategy on 1D
-        # ring, the xla tpu compiler flag
-        # `xla_tpu_force_1d_allreduce_at_chunk_count` is a temporary solution to
-        # fix this. It will be removed after the bug in XLA compiler is fixed.
-        os.environ["LIBTPU_INIT_ARGS"] = (
-            os.environ.get("LIBTPU_INIT_ARGS", "") +
-            " --xla_tpu_force_1d_allreduce_at_chunk_count=1"
-            " --xla_jf_conv_input_fusion=False")
-        # --xla_jf_conv_input_fusion=False is used to improve the perf of
-        # quantized matmul.
+        # os.environ["PJRT_DEVICE"] = "TT"
+        xr.set_device_type("TT")
         torch.set_grad_enabled(False)
         torch.set_default_dtype(self.model_config.dtype)

@@ -162,7 +150,7 @@ class TPUWorker:

         # Init ModelRunner here, so that we have access to self.device.
         self.model_runner = \
-            TPUModelRunner(self.vllm_config, self.device,
+            TTModelRunner(self.vllm_config, self.device,
                            self.original_parallel_config)

         if rank == 0:
@@ -178,7 +166,7 @@ class TPUWorker:

                 # Use an empty tensor instead of `None`` to force Dynamo to pass
                 # it by reference, rather by specializing on the value ``None``.
-                tpu_kv_cache = torch.tensor([], dtype=dtype).to(self.device)
+                tpu_kv_cache = torch.tensor([0], dtype=dtype).to(self.device)
                 kv_caches[layer_name] = tpu_kv_cache
             else:
                 raise NotImplementedError(
@@ -218,9 +206,9 @@ class TPUWorker:
             total_memory_size = device_usage[0].total_memory
             current_mem = device_usage[0].memory_usage
         else:
-            m = xm.get_memory_info(self.device)
-            total_memory_size = m["bytes_limit"]
-            current_mem = m["bytes_used"]
+            # m = xm.get_memory_info(self.device)
+            total_memory_size = 12*1024**3#m["bytes_limit"]
+            current_mem = 0#m["bytes_used"]
         # Ideally we would use profiled = m["peak_bytes_used"] to
         # get weights + activations. But there is memory used during
         # compilation / weight loading that impacts the peak and
@@ -235,7 +223,7 @@ class TPUWorker:
         head_size = self.model_config.get_head_size()
         if head_size > 0:
             padded_head_size = cdiv(
-                head_size, TPU_HEAD_SIZE_ALIGNMENT) * TPU_HEAD_SIZE_ALIGNMENT
+                head_size, TT_HEAD_SIZE_ALIGNMENT) * TT_HEAD_SIZE_ALIGNMENT
             if padded_head_size != head_size:
                 logger.warning_once("head size is padded to %d",
                                     padded_head_size)
@@ -329,12 +317,3 @@ class TPUWorker:
             parallel_config.pipeline_parallel_size)

         ensure_kv_transfer_initialized(vllm_config)
-
-    def shutdown(self) -> None:
-        self.model_runner.ensure_kv_transfer_shutdown()
-
-
-if USE_TPU_COMMONS:
-    from tpu_commons.worker import TPUWorker as TPUCommonsWorker
-
-    TPUWorker = TPUCommonsWorker  # type: ignore
