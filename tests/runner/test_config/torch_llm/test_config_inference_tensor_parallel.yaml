# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0


test_config:
  falcon/causal_lm/pytorch-tiiuae/Falcon3-7B-Base-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: EXPECTED_PASSING

  falcon/causal_lm/pytorch-tiiuae/Falcon3-10B-Base-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: EXPECTED_PASSING
    required_pcc: 0.977 # https://github.com/tenstorrent/tt-xla/issues/3000

  qwen_2_5/causal_lm/pytorch-7b_instruct-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    bringup_status: FAILED_RUNTIME
    reason: "DRAM OOM - shard spec issues - https://github.com/tenstorrent/tt-xla/issues/2150"

  qwen_2_5/causal_lm/pytorch-14b_instruct-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: EXPECTED_PASSING

  qwen_2_5/causal_lm/pytorch-32b_instruct-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    bringup_status: FAILED_RUNTIME
    reason: "trisc2 compile failure - https://github.com/tenstorrent/tt-xla/issues/2958"

  qwen_2_5/causal_lm/pytorch-72b_instruct-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    reason: "Needs bringup on galaxy"
    bringup_status: FAILED_RUNTIME

  gemma/pytorch-google/gemma-1.1-7b-it-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: EXPECTED_PASSING
    assert_pcc: false # ComputeConfig math_fidelity/fp32_dest_acc_en - https://github.com/tenstorrent/tt-xla/issues/2861

  gemma/pytorch-google/gemma-2-9b-it-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: EXPECTED_PASSING
    assert_pcc: false # https://github.com/tenstorrent/tt-xla/issues/2845

  gemma/pytorch-google/gemma-2-27b-it-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: EXPECTED_PASSING

  llama/causal_lm/pytorch-llama_3_1_8b_instruct-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: EXPECTED_PASSING

  llama/causal_lm/pytorch-llama_3_1_70b-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    reason: "Needs bringup on galaxy"
    bringup_status: FAILED_RUNTIME

  llama/causal_lm/pytorch-llama_3_1_70b_instruct-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    reason: "Needs bringup on galaxy"
    bringup_status: FAILED_RUNTIME

  llama/causal_lm/pytorch-llama_3_1_405b-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    reason: "Needs bringup on galaxy"
    bringup_status: FAILED_RUNTIME

  llama/causal_lm/pytorch-llama_3_3_70b_instruct-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    reason: "Needs bringup on galaxy"
    bringup_status: FAILED_RUNTIME

  qwen_3/causal_lm/pytorch-8b-llm_decode-tensor_parallel-inference:
    required_pcc: 0.985 # https://github.com/tenstorrent/tt-xla/issues/2942
    supported_archs: ["n300-llmbox"]
    status: EXPECTED_PASSING

  qwen_3/causal_lm/pytorch-14b-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    bringup_status: FAILED_RUNTIME
    reason: "trisc2 compile failure - https://github.com/tenstorrent/tt-xla/issues/2958"

  qwen_3/causal_lm/pytorch-32b-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    bringup_status: FAILED_RUNTIME
    reason: "trisc2 compile failure - https://github.com/tenstorrent/tt-xla/issues/2958"

  qwen_3/causal_lm/pytorch-30b_a3b-llm_decode-tensor_parallel-inference:
    supported_archs: ["n300-llmbox"]
    status: NOT_SUPPORTED_SKIP
    reason: "Needs bringup on n300-llmbox"
    bringup_status: FAILED_RUNTIME
