# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

# Inference single-device test configuration
# Use Python enum names or values for fields below (names preferred for readability):
#   - status: ModelTestStatus names (e.g., EXPECTED_PASSING) or values (e.g., expected_passing)
#   - bringup_status (if present): BringupStatus names (e.g., FAILED_TTMLIR_COMPILATION) or values (e.g., failed_ttmlir_compilation)

test_config:

  qwen_3/causal_lm/pytorch-4b-single_device-full-inference:
    status: EXPECTED_PASSING

  qwen_3/causal_lm/pytorch-1_7b-single_device-full-inference:
    status: EXPECTED_PASSING

  qwen_3/causal_lm/pytorch-0_6b-single_device-full-inference:
    status: EXPECTED_PASSING

  qwen_3/causal_lm/pytorch-14b-single_device-full-inference:
    arch_overrides:
      p150:
        status: EXPECTED_PASSING
      n150:
        status: NOT_SUPPORTED_SKIP
        reason: "Too large for single chip"
        bringup_status: FAILED_RUNTIME

  qwen_3/causal_lm/pytorch-8b-single_device-full-inference:
    arch_overrides:
      p150:
        status: EXPECTED_PASSING
      n150:
        status: NOT_SUPPORTED_SKIP
        reason: "Too large for single chip"
        bringup_status: FAILED_RUNTIME
