# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0


test_config:
  falcon/pytorch-tiiuae/Falcon3-7B-Base-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    assert_pcc: false
    status: EXPECTED_PASSING
    bringup_status: INCORRECT_RESULT
    reason: "PCC comparison failed. Calculated: pcc=0.970244288444519. Required: pcc=0.99. - https://github.com/tenstorrent/tt-xla/issues/1789"

  falcon/pytorch-tiiuae/Falcon3-10B-Base-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  falcon/pytorch-tiiuae/Falcon3-Mamba-7B-Base-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: KNOWN_FAILURE_XFAIL
    reason: " Error: loc('compare.1209'): error: Compare operation is not supported in stablehlo-pipeline for meshes not 1x1 - https://github.com/tenstorrent/tt-mlir/issues/3497"
    bringup_status: FAILED_TTMLIR_COMPILATION

  gemma/pytorch-google/gemma-1.1-7b-it-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    assert_pcc: false
    status: EXPECTED_PASSING
    bringup_status: INCORRECT_RESULT
    reason: "AssertionError: PCC comparison failed. Calculated: pcc=0.9626210927963257. Required: pcc=0.97"

  gemma/pytorch-google/gemma-2-9b-it-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  gemma/pytorch-google/gemma-2-27b-it-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: NOT_SUPPORTED_SKIP
    reason: "Too large for single chip or even n300-llmbox either, needs debug - https://github.com/tenstorrent/tt-xla/issues/1494"
    bringup_status: FAILED_RUNTIME

  falcon/pytorch-tiiuae/falcon-7b-instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  llama/causal_lm/pytorch-llama_3_1_8b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  llama/causal_lm/pytorch-llama_3_1_8b_instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  llama/causal_lm/pytorch-llama_3_8b_instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  mistral/pixtral/pytorch-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1473

  mistral/pytorch-7b_instruct_v03-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  qwen_3/causal_lm/pytorch-0_6b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/causal_lm/pytorch-14b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/causal_lm/pytorch-1_7b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/causal_lm/pytorch-32b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    markers: ["large_device_dram"]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/causal_lm/pytorch-8b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/embedding/pytorch-embedding_8b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  mistral/pytorch-ministral_8b_instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1473

  mistral/pytorch-mistral_small_24b_instruct_2501-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    markers: ["large_device_dram"]
    status: EXPECTED_PASSING

  mistral/pytorch-mistral_nemo_instruct_2407-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    markers: ["large_device_dram"]
    status: EXPECTED_PASSING

  mistral/pytorch-mistral_large_instruct_2411-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: NOT_SUPPORTED_SKIP
    reason: "xr.global_runtime_device_count() - Expected 2 eth links from physical chip 4 to physical chip 0 - https://github.com/tenstorrent/tt-xla/issues/1975"

  qwen_2_5/causal_lm/pytorch-7b_instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
  # TODO (kmabee): Add back when DRAM leak between tests resolved - https://github.com/tenstorrent/tt-xla/issues/1940
  #   status: EXPECTED_PASSING
  #   bringup_status: INCORRECT_RESULT
  #   assert_pcc: false # https://github.com/tenstorrent/tt-xla/issues/1474
    status: NOT_SUPPORTED_SKIP
    bringup_status: FAILED_RUNTIME
    reason: "DRAM leak between tests prevents running this test - https://github.com/tenstorrent/tt-xla/issues/1940"

  qwen_2_5/causal_lm/pytorch-14b_instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    bringup_status: INCORRECT_RESULT
    assert_pcc: false # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_2_5/causal_lm/pytorch-32b_instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    markers: ["large_device_dram"]
    status: EXPECTED_PASSING
    bringup_status: INCORRECT_RESULT
    assert_pcc: false # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_2_5_coder/pytorch-32b_instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    markers: ["large_device_dram"]
    status: EXPECTED_PASSING
    bringup_status: INCORRECT_RESULT
    required_pcc: 0.98 # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_2_5/causal_lm/pytorch-math_7b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    # TODO (kmabee): Add back when DRAM leak between tests resolved - https://github.com/tenstorrent/tt-xla/issues/1940
    # status: EXPECTED_PASSING
    # bringup_status: INCORRECT_RESULT
    # assert_pcc: false # https://github.com/tenstorrent/tt-xla/issues/1474
    status: NOT_SUPPORTED_SKIP
    bringup_status: FAILED_RUNTIME
    reason: "DRAM leak between tests prevents running this test - https://github.com/tenstorrent/tt-xla/issues/1940"
