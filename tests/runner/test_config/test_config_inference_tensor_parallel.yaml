# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0


test_config:
  falcon/pytorch-tiiuae/Falcon3-7B-Base-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    assert_pcc: false
    status: EXPECTED_PASSING
    bringup_status: INCORRECT_RESULT
    reason: "PCC comparison failed. Calculated: pcc=0.970244288444519. Required: pcc=0.99. - https://github.com/tenstorrent/tt-xla/issues/1789"

  falcon/pytorch-tiiuae/Falcon3-10B-Base-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  falcon/pytorch-tiiuae/Falcon3-Mamba-7B-Base-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: KNOWN_FAILURE_XFAIL
    reason: " Error: loc('compare.1209'): error: Compare operation is not supported in stablehlo-pipeline for meshes not 1x1 - https://github.com/tenstorrent/tt-mlir/issues/3497"
    bringup_status: FAILED_TTMLIR_COMPILATION

  gemma/pytorch-google/gemma-1.1-7b-it-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    assert_pcc: false
    status: EXPECTED_PASSING
    bringup_status: INCORRECT_RESULT
    reason: "AssertionError: PCC comparison failed. Calculated: pcc=0.9626210927963257. Required: pcc=0.97"

  gemma/pytorch-google/gemma-2-9b-it-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  gemma/pytorch-google/gemma-2-27b-it-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: NOT_SUPPORTED_SKIP
    reason: "Too large for single chip or even n300-llmbox either, needs debug - https://github.com/tenstorrent/tt-xla/issues/1494"
    bringup_status: FAILED_RUNTIME

  falcon/pytorch-tiiuae/falcon-7b-instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: KNOWN_FAILURE_XFAIL
    reason: "Not enough space to allocate 41295872 B DRAM buffer across 12 banks, where each bank needs to store 3442688 B - https://github.com/tenstorrent/tt-xla/issues/1755"
    bringup_status: FAILED_RUNTIME

  llama/causal_lm/pytorch-llama_3_1_8b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  llama/causal_lm/pytorch-llama_3_1_8b_instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  llama/causal_lm/pytorch-llama_3_8b_instruct-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  mistral/pixtral/pytorch-tensor_parallel-full-inference:
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1473

  mistral/pytorch-7b_instruct_v03-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING

  qwen_3/causal_lm/pytorch-0_6b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/causal_lm/pytorch-14b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/causal_lm/pytorch-1_7b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/causal_lm/pytorch-32b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/causal_lm/pytorch-8b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
    assert_pcc: false
    bringup_status: INCORRECT_RESULT # https://github.com/tenstorrent/tt-xla/issues/1474

  qwen_3/embedding/pytorch-embedding_8b-tensor_parallel-full-inference:
    supported_archs: [n300-llmbox]
    status: EXPECTED_PASSING
