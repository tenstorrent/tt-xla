Using PJRT plugin directory: /localdev/vkovinic/tt-xla/python_package/pjrt_plugin_tt
Using TT-Metal from the source tree: /localdev/vkovinic/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.
Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.
2026-01-12 10:24:04.436 (   0.000s) [        E2DEE000]   plugin_attributes.cc:60       1| PluginAttributes::PJRT_Plugin_Initialize
2026-01-12 10:24:04.436 (   0.000s) [        E2DEE000]     client_instance.cc:546      1| ClientInstance::PJRT_Client_Create
2026-01-12 10:24:04.438 (   0.001s) [        E2DEE000]     client_instance.cc:177      1| ClientInstance::ClientInstance
2026-01-12 10:24:04.438 (   0.001s) [        E2DEE000]     client_instance.cc:198      1| ClientInstance::Initialize
                 Always |    DEBUG | Device grid size = { 8, 8 }
                 Always |    DEBUG | Device grid size = { 8, 8 }
2026-01-12 10:24:04.946 (   0.510s) [        E2DEE000]              stubs.inc:103   WARN| STUB: PJRT_Client_TopologyDescription
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]     client_instance.cc:600      1| ClientInstance::PJRT_Client_PlatformVersion
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]     client_instance.cc:581      1| ClientInstance::PJRT_Client_PlatformName
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]     client_instance.cc:611      1| ClientInstance::PJRT_Client_Devices
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]     client_instance.cc:624      1| ClientInstance::PJRT_Client_AddressableDevices
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]     client_instance.cc:674      1| ClientInstance::PJRT_Client_AddressableMemories
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]   plugin_attributes.cc:66       1| PluginAttributes::PJRT_Plugin_Attributes
2026-01-12 10:24:04.947114: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:24:04.947 (   0.510s) [        E2DEE000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id

======================================================================
TEST: load_conv1_then_norm2
======================================================================
Loading Mochi VAE conv1 → norm2 chain...
Patching conv1 to use constant padding...
Patched : padding 'replicate' → constant (via fake_context_parallel_forward)

Successfully patched 1 CogVideoXCausalConv3d layer(s)
✓ Created conv1 → norm2 chain
  Conv1 output will create problematic tile layout
  Norm2 reshape should fail on that layout
2026-01-12 10:24:05.596 (   1.159s) [        E2DEE000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:24:05.596 (   1.159s) [        E2DEE000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:24:05.596 (   1.159s) [        E2DEE000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:24:05.596 (   1.160s) [        E2DEE000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:24:05.596 (   1.160s) [        E2DEE000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:24:05.601 (   1.165s) [        E2DEE000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:24:05.601 (   1.165s) [        E2DEE000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:24:05.601 (   1.165s) [        E2DEE000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:24:05.602 (   1.165s) [        E2DEE000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:24:05.909 (   1.472s) [        E2DEE000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:24:05.909 (   1.472s) [        E2DEE000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:24:05.909 (   1.472s) [        E2DEE000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:24:05.909 (   1.473s) [        E2DEE000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:24:05.909 (   1.473s) [        E2DEE000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy

Input shape: [1, 768, 4, 60, 106]
Memory estimate: 37.27 MB

Running forward pass...
2026-01-12 10:24:06.256 (   1.820s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.256 (   1.820s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.256 (   1.820s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.256 (   1.820s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.256 (   1.820s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.256 (   1.820s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.256 (   1.820s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.256 (   1.820s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.256 (   1.820s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.259 (   1.823s) [        E2DEE000]     client_instance.cc:687      1| ClientInstance::PJRT_Client_Compile
2026-01-12 10:24:06.260 (   1.823s) [        E2DEE000]      module_builder.cc:211      1| ModuleBuilder::buildModule
2026-01-12 10:24:06.260 (   1.824s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module vhlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.16 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: !vhlo.tensor_v1<768x768x3x3x3x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data")) -> (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.custom_call_v1"(%arg2) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc(#loc2)
    %2 = "vhlo.pad_v1"(%1, %0) <{edge_padding_high = #vhlo.tensor_v1<dense<[0, 0, 0, 1, 1]> : tensor<5xi64>>, edge_padding_low = #vhlo.tensor_v1<dense<[0, 0, 2, 1, 1]> : tensor<5xi64>>, interior_padding = #vhlo.tensor_v1<dense<0> : tensor<5xi64>>}> : (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x6x62x108x!vhlo.bf16_v1> loc(#loc3)
    %3 = "vhlo.custom_call_v1"(%arg1) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___conv1_conv_weight">}>} : (!vhlo.tensor_v1<768x768x3x3x3x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x3x3x3x!vhlo.bf16_v1> loc(#loc2)
    %4 = "vhlo.convolution_v1"(%2, %3) <{batch_group_count = #vhlo.integer_v1<1 : i64>, feature_group_count = #vhlo.integer_v1<1 : i64>, input_batch_dimension = #vhlo.integer_v1<0 : i64>, input_feature_dimension = #vhlo.integer_v1<1 : i64>, input_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3, 4]> : tensor<3xi64>>, kernel_input_feature_dimension = #vhlo.integer_v1<1 : i64>, kernel_output_feature_dimension = #vhlo.integer_v1<0 : i64>, kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3, 4]> : tensor<3xi64>>, lhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<3xi64>>, output_batch_dimension = #vhlo.integer_v1<0 : i64>, output_feature_dimension = #vhlo.integer_v1<1 : i64>, output_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3, 4]> : tensor<3xi64>>, padding = #vhlo.tensor_v1<dense<0> : tensor<3x2xi64>>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<3xi64>>, window_reversal = #vhlo.tensor_v1<dense<false> : tensor<3xi1>>, window_strides = #vhlo.tensor_v1<dense<1> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x768x6x62x108x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x3x3x3x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc(#loc4)
    %5 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc5)
    %6 = "vhlo.custom_call_v1"(%5) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___conv1_conv_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %7 = "vhlo.reshape_v1"(%6) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc5)
    %8 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc(#loc4)
    %9 = "vhlo.add_v1"(%4, %8) : (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc(#loc4)
    "vhlo.return_v1"(%9) : (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|xla__custom_call")
#loc3 = loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd")
#loc4 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
#loc5 = loc("-1|unknown|unknown|-1|aten__view")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:06.264 (   1.827s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module shlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.16 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<768xbf16> loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768x768x3x3x3xbf16> loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<1x768x4x60x106xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.custom_call @tt.mark_argument(%arg2) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x768x4x60x106xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc2)
    %1 = stablehlo.pad %0, %cst, low = [0, 0, 2, 1, 1], high = [0, 0, 0, 1, 1], interior = [0, 0, 0, 0, 0] : (tensor<1x768x4x60x106xbf16>, tensor<bf16>) -> tensor<1x768x6x62x108xbf16> loc(#loc3)
    %2 = stablehlo.custom_call @tt.mark_argument(%arg1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___conv1_conv_weight"}} : (tensor<768x768x3x3x3xbf16>) -> tensor<768x768x3x3x3xbf16> loc(#loc2)
    %3 = stablehlo.convolution(%1, %2) dim_numbers = [b, f, 0, 1, 2]x[o, i, 0, 1, 2]->[b, f, 0, 1, 2], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x768x6x62x108xbf16>, tensor<768x768x3x3x3xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc4)
    %4 = stablehlo.reshape %arg0 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc5)
    %5 = stablehlo.custom_call @tt.mark_argument(%4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___conv1_conv_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %6 = stablehlo.reshape %5 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc5)
    %7 = stablehlo.broadcast_in_dim %6, dims = [1] : (tensor<768xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc4)
    %8 = stablehlo.add %3, %7 : tensor<1x768x4x60x106xbf16> loc(#loc4)
    return %8 : tensor<1x768x4x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|xla__custom_call")
#loc3 = loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd")
#loc4 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
#loc5 = loc("-1|unknown|unknown|-1|aten__view")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:06.266 (   1.829s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module shlo_frontend:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.16 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___conv1_conv_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768x768x3x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___conv1_conv_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<1x768x4x60x106xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.pad %arg2, %cst, low = [0, 0, 2, 1, 1], high = [0, 0, 0, 1, 1], interior = [0, 0, 0, 0, 0] : (tensor<1x768x4x60x106xbf16>, tensor<bf16>) -> tensor<1x768x6x62x108xbf16> loc(#loc2)
    %1 = stablehlo.convolution(%0, %arg1) dim_numbers = [b, f, 0, 1, 2]x[o, i, 0, 1, 2]->[b, f, 0, 1, 2], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x768x6x62x108xbf16>, tensor<768x768x3x3x3xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
    %2 = stablehlo.reshape %arg0 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc4)
    %3 = stablehlo.reshape %2 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc4)
    %4 = stablehlo.broadcast_in_dim %3, dims = [1] : (tensor<768xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
    %5 = stablehlo.add %1, %4 : tensor<1x768x4x60x106xbf16> loc(#loc3)
    return %5 : tensor<1x768x4x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd")
#loc3 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
#loc4 = loc("-1|unknown|unknown|-1|aten__view")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:06.269 (   1.832s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module shlo_compiler:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.16 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_conv_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768x768x3x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_conv_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<1x768x4x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.pad %arg2, %cst, low = [0, 0, 2, 1, 1], high = [0, 0, 0, 1, 1], interior = [0, 0, 0, 0, 0] : (tensor<1x768x4x60x106xbf16>, tensor<bf16>) -> tensor<1x768x6x62x108xbf16> loc(#loc2)
    %1 = stablehlo.convolution(%0, %arg1) dim_numbers = [b, f, 0, 1, 2]x[o, i, 0, 1, 2]->[b, f, 0, 1, 2], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x768x6x62x108xbf16>, tensor<768x768x3x3x3xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
    %2 = stablehlo.reshape %arg0 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc4)
    %3 = stablehlo.reshape %2 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc4)
    %4 = stablehlo.broadcast_in_dim %3, dims = [1] : (tensor<768xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
    %5 = stablehlo.add %1, %4 : tensor<1x768x4x60x106xbf16> loc(#loc3)
    return %5 : tensor<1x768x4x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd")
#loc3 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
#loc4 = loc("-1|unknown|unknown|-1|aten__view")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:06.271 (   1.834s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module ttir:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.16 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.16 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
      func.func @main(%arg0: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_conv_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768x768x3x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_conv_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<1x768x4x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.pad"(%arg2) <{padding = array<i32: 0, 0, 0, 0, 2, 0, 1, 1, 1, 1>, value = 0.000000e+00 : f32}> : (tensor<1x768x4x60x106xbf16>) -> tensor<1x768x6x62x108xbf16> loc(#loc2)
        %1 = "ttir.convolution"(%0, %arg1) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3x4, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3x4, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3x4>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1, 1>, window_reversal = array<i1: false, false, false>, window_strides = array<i64: 1, 1, 1>}> : (tensor<1x768x6x62x108xbf16>, tensor<768x768x3x3x3xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
        %2 = "ttir.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc4)
        %3 = "ttir.reshape"(%2) <{shape = [768 : i32]}> : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc4)
        %4 = "ttir.reshape"(%3) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16>) -> tensor<1x768x1x1x1xbf16> loc(#loc3)
        %5 = "ttir.broadcast"(%4) <{broadcast_dimensions = array<i64: 1, 1, 4, 60, 106>}> : (tensor<1x768x1x1x1xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
        %6 = "ttir.add"(%1, %5) : (tensor<1x768x4x60x106xbf16>, tensor<1x768x4x60x106xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
        return %6 : tensor<1x768x4x60x106xbf16> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd")
#loc3 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
#loc4 = loc("-1|unknown|unknown|-1|aten__view")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:06.273 (   1.836s) [        E2DEE000]      module_builder.cc:766   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2026-01-12 10:24:06.273 (   1.836s) [        E2DEE000]      module_builder.cc:780   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2026-01-12 10:24:06.273 (   1.836s) [        E2DEE000]      module_builder.cc:790   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2026-01-12 10:24:06.280 (   1.843s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module ttnn:
#dram = #ttnn.buffer_type<dram>
#loc = loc(unknown)
#loc3 = loc("-1|unknown|unknown|-1|xla__device_data")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102656, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073125888, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 24576 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 32 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 6912 + d1 * 9 + d2 * 3 + d3, d4), <1x1>, memref<5308416x3xbf16, #system_memory>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<648x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 6912 + d1 * 9 + d2 * 3 + d3, d4), <1x1>, memref<5308416x3xbf16, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 73728 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<1769472x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 184320 + d1 * 240 + d2 * 60 + d3, d4), <1x1>, memref<184320x106xbf16, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 285696 + d1 * 372 + d2 * 62 + d3, d4), <1x1>, memref<285696x108xbf16, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 294912 + d1 * 384 + d2 * 64 + d3, d4), <1x1>, memref<9216x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 47616 + d1 * 7936 + d2 * 128 + d3, d4), <1x1>, memref<1488x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 40176 + d1 * 6696 + d2 * 108 + d3, d4), <1x1>, memref<40176x768xbf16, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 25440 + d1 * 6360 + d2 * 106 + d3, d4), <1x1>, memref<25440x768xbf16, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 30720 + d1 * 7680 + d2 * 128 + d3, d4), <1x1>, memref<960x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.16 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.16 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func private @main_const_eval_0(%arg0: tensor<768xbf16, #ttnn_layout> loc(unknown)) -> tensor<1x768xbf16, #ttnn_layout1> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<768xbf16, #ttnn_layout>, !ttnn.device) -> tensor<768xbf16, #ttnn_layout2> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn_layout2>) -> tensor<768xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn_layout2>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn_layout3>) -> tensor<1x768x1x1x1xbf16, #ttnn_layout4> loc(#loc1)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn_layout3>) -> () loc(#loc1)
        %4 = "ttnn.permute"(%3) <{permutation = array<i64: 0, 2, 3, 4, 1>}> : (tensor<1x768x1x1x1xbf16, #ttnn_layout4>) -> tensor<1x1x1x1x768xbf16, #ttnn_layout5> loc(#loc7)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1x1xbf16, #ttnn_layout4>) -> () loc(#loc7)
        %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 768 : i32]}> : (tensor<1x1x1x1x768xbf16, #ttnn_layout5>) -> tensor<1x768xbf16, #ttnn_layout1> loc(#loc1)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1x768xbf16, #ttnn_layout5>) -> () loc(#loc1)
        return %5 : tensor<1x768xbf16, #ttnn_layout1> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_1(%arg0: tensor<768x768x3x3x3xbf16, #ttnn_layout6> {ttir.conv2d_weight} loc(unknown)) -> tensor<20736x768xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc2)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<768x768x3x3x3xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<768x768x3x3x3xbf16, #ttnn_layout8> loc(#loc2)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768x768x3x3x3xbf16, #ttnn_layout8>) -> tensor<768x768x3x3x3xbf16, #ttnn_layout9> loc(#loc2)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<768x768x3x3x3xbf16, #ttnn_layout8>) -> () loc(#loc2)
        %3 = "ttnn.reshape"(%2) <{shape = [20736 : i32, 768 : i32]}> : (tensor<768x768x3x3x3xbf16, #ttnn_layout9>) -> tensor<20736x768xbf16, #ttnn_layout7> loc(#loc1)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<768x768x3x3x3xbf16, #ttnn_layout9>) -> () loc(#loc1)
        return %3 : tensor<20736x768xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<768xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_conv_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768x768x3x3x3xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___conv1_conv_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<1x768x4x60x106xbf16, #ttnn_layout11> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<768xbf16, #ttnn_layout>) -> tensor<1x768xbf16, #ttnn_layout1> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<768xbf16, #ttnn_layout>) -> () loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<768x768x3x3x3xbf16, #ttnn_layout6>) -> tensor<20736x768xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<768x768x3x3x3xbf16, #ttnn_layout6>) -> () loc(#loc)
        %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc4)
        %3 = "ttnn.pad"(%arg2) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>, padding = array<i32: 0, 0, 0, 0, 2, 0, 1, 1, 1, 1>, use_multicore = true, value = 0.000000e+00 : f32}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout10>) -> tensor<1x768x6x62x108xbf16, #ttnn_layout12> loc(#loc5)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout10>) -> () loc(#loc5)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1x768x6x62x108xbf16, #ttnn_layout12>) -> tensor<1x768x6x62x108xbf16, #ttnn_layout13> loc(#loc8)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x6x62x108xbf16, #ttnn_layout12>) -> () loc(#loc8)
        %5 = "ttnn.permute"(%4) <{permutation = array<i64: 0, 2, 3, 4, 1>}> : (tensor<1x768x6x62x108xbf16, #ttnn_layout13>) -> tensor<1x6x62x108x768xbf16, #ttnn_layout14> loc(#loc9)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x768x6x62x108xbf16, #ttnn_layout13>) -> () loc(#loc9)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<row_major>}> : (tensor<1x6x62x108x768xbf16, #ttnn_layout14>) -> tensor<1x6x62x108x768xbf16, #ttnn_layout15> loc(#loc9)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x6x62x108x768xbf16, #ttnn_layout14>) -> () loc(#loc9)
        %7 = "ttnn.conv3d"(%6, %1, %0, %2) <{batch_size = 1 : i32, conv3d_config = #ttnn.conv3d_config<weights_dtype = bf16, t_out_block = 1, w_out_block = 16, h_out_block = 2, c_out_block = 96, c_in_block = 128>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 768 : i32, input_depth = 6 : i32, input_height = 62 : i32, input_width = 108 : i32, kernel_size = array<i32: 3, 3, 3>, out_channels = 768 : i32, padding = array<i32: 0, 0, 0>, padding_mode = "zeros", stride = array<i32: 1, 1, 1>}> : (tensor<1x6x62x108x768xbf16, #ttnn_layout15>, tensor<20736x768xbf16, #ttnn_layout7>, tensor<1x768xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1x4x60x106x768xbf16, #ttnn_layout16> loc(#loc1)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x6x62x108x768xbf16, #ttnn_layout15>) -> () loc(#loc1)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<20736x768xbf16, #ttnn_layout7>) -> () loc(#loc1)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x768xbf16, #ttnn_layout1>) -> () loc(#loc1)
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<tile>}> : (tensor<1x4x60x106x768xbf16, #ttnn_layout16>) -> tensor<1x4x60x106x768xbf16, #ttnn_layout17> loc(#loc6)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x4x60x106x768xbf16, #ttnn_layout16>) -> () loc(#loc6)
        %9 = "ttnn.permute"(%8) <{permutation = array<i64: 0, 4, 1, 2, 3>}> : (tensor<1x4x60x106x768xbf16, #ttnn_layout17>) -> tensor<1x768x4x60x106xbf16, #ttnn_layout11> loc(#loc1)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x4x60x106x768xbf16, #ttnn_layout17>) -> () loc(#loc1)
        return %9 : tensor<1x768x4x60x106xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
#loc2 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_in_1_layout")
#loc4 = loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd_in_0_layout")
#loc5 = loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd")
#loc6 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_in_0_layout")
#loc7 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_bias_permute"(#loc1))
#loc8 = loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd_workaround"(#loc5))
#loc9 = loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_input"(#loc1))
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:06.286 (   1.849s) [        E2DEE000]loaded_executable_insta:290      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2026-01-12 10:24:06.286 (   1.849s) [        E2DEE000]loaded_executable_insta:309      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2026-01-12 10:24:06.286 (   1.850s) [        E2DEE000]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2026-01-12 10:24:06.286 (   1.850s) [        E2DEE000]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2026-01-12 10:24:06.286 (   1.850s) [        E2DEE000]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2026-01-12 10:24:06.286 (   1.850s) [        E2DEE000]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2026-01-12 10:24:06.286 (   1.850s) [        E2DEE000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:24:06.286 (   1.850s) [        E2DEE000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:24:06.289 (   1.852s) [        E2DEE000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:24:06.289 (   1.852s) [        E2DEE000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:24:06.290 (   1.854s) [        4A7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2026-01-12 10:24:06.290 (   1.854s) [        4A7FC640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:24:06.290 (   1.854s) [        4A7FC640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:24:06.290 (   1.854s) [        4A7FC640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:24:06.290 (   1.854s) [        4A7FC640] executable_instance.cc:140      1| ExecutableInstance::PJRT_Executable_NumOutputs
2026-01-12 10:24:06.290 (   1.854s) [        4A7FC640]loaded_executable_insta:345      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2026-01-12 10:24:06.290 (   1.854s) [        4A7FC640]flatbuffer_loaded_execu:260      1| FlatbufferLoadedExecutableInstance::Execute
2026-01-12 10:24:06.290 (   1.854s) [        4A7FC640]     client_instance.cc:416      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 1]
                 Always |     INFO | Device memory state before submit
                 Always |     INFO | Device DRAM memory state: MemoryView{numBanks: 12, totalBytesPerBank: 1024.000 MB, totalBytesAllocatedPerBank: 3.281 MB, totalBytesFreePerBank: 1020.719 MB, largestContiguousBytesFreePerBank: 1020.719 MB}
                 Always |     INFO | Device L1 memory state: MemoryView{numBanks: 64, totalBytesPerBank: 1.269 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 1.269 MB, largestContiguousBytesFreePerBank: 1.269 MB}
                 Always |     INFO | Device L1_SMALL memory state: MemoryView{numBanks: 64, totalBytesPerBank: 0.062 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.062 MB, largestContiguousBytesFreePerBank: 0.062 MB}
                 Always |     INFO | Device TRACE memory state: MemoryView{numBanks: 12, totalBytesPerBank: 0.000 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.000 MB, largestContiguousBytesFreePerBank: 0.000 MB}
            RuntimeTTNN |    DEBUG | Starting execution of program: main
            RuntimeTTNN |    DEBUG | Executing operation: %0 = ttcore.load_cached(@main_const_eval_0, [%arg0]) : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x768xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
                 Always |    DEBUG | Running LoadCachedOp for function main_const_eval_0 with hash: e4d772c6b796cf9bb3269c90c2d5df0e9159fe3adcad119ec3ce9fd1c20b312c
                 Always |    DEBUG | Cache miss or invalid cache for function: main_const_eval_0
            RuntimeTTNN |    DEBUG | Starting execution of program: main_const_eval_0
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 24576 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Executing operation: %4 = "ttnn.permute"(%3) <{permutation = array<i64: 0, 2, 3, 4, 1>}> : (tensor<1x768x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 24576 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 32 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_bias_permute"("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable"))
2026-01-12 10:24:06.502 (   2.066s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.502 (   2.066s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.502 (   2.066s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:06.502 (   2.066s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 24576 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_bias_permute"("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable"))
            RuntimeTTNN |    DEBUG | Executing operation: %5 = "ttnn.reshape"(%4) <{shape = [1 : i32, 768 : i32]}> : (tensor<1x1x1x1x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 32 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x1x1x1x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 32 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Finished execution of program: main_const_eval_0
                 Always |    DEBUG | executed sub-func: main_const_eval_0
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<768x768x3x3x3xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 6912 + d1 * 9 + d2 * 3 + d3, d4), <1x1>, memref<5308416x3xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<20736x768xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<648x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
                 Always |    DEBUG | Running LoadCachedOp for function main_const_eval_1 with hash: 139a4775333eefbab936197c139903d987653a61f83802a16ac143569411a41b
                 Always |    DEBUG | Cache miss or invalid cache for function: main_const_eval_1
            RuntimeTTNN |    DEBUG | Starting execution of program: main_const_eval_1
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_in_1_layout")
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<768x768x3x3x3xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 6912 + d1 * 9 + d2 * 3 + d3, d4), <1x1>, memref<5308416x3xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<768x768x3x3x3xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 6912 + d1 * 9 + d2 * 3 + d3, d4), <1x1>, memref<5308416x3xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_in_1_layout")
            RuntimeTTNN |    DEBUG | Executing operation: %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768x768x3x3x3xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 6912 + d1 * 9 + d2 * 3 + d3, d4), <1x1>, memref<5308416x3xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<768x768x3x3x3xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 73728 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<1769472x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_in_1_layout")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<768x768x3x3x3xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 6912 + d1 * 9 + d2 * 3 + d3, d4), <1x1>, memref<5308416x3xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_in_1_layout")
            RuntimeTTNN |    DEBUG | Executing operation: %3 = "ttnn.reshape"(%2) <{shape = [20736 : i32, 768 : i32]}> : (tensor<768x768x3x3x3xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 73728 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<1769472x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<20736x768xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<648x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<768x768x3x3x3xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 73728 + d1 * 96 + d2 * 32 + d3, d4), <1x1>, memref<1769472x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Finished execution of program: main_const_eval_1
                 Always |    DEBUG | executed sub-func: main_const_eval_1
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<768x768x3x3x3xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 6912 + d1 * 9 + d2 * 3 + d3, d4), <1x1>, memref<5308416x3xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %2 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd_in_0_layout")
            RuntimeTTNN |    DEBUG | Executing operation: %3 = "ttnn.pad"(%arg2) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>, padding = array<i32: 0, 0, 0, 0, 2, 0, 1, 1, 1, 1>, use_multicore = true, value = 0.000000e+00 : f32}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 184320 + d1 * 240 + d2 * 60 + d3, d4), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x6x62x108xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 285696 + d1 * 372 + d2 * 62 + d3, d4), <1x1>, memref<285696x108xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 184320 + d1 * 240 + d2 * 60 + d3, d4), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1x768x6x62x108xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 285696 + d1 * 372 + d2 * 62 + d3, d4), <1x1>, memref<285696x108xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x6x62x108xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 294912 + d1 * 384 + d2 * 64 + d3, d4), <1x1>, memref<9216x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd_workaround"("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd"))
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x6x62x108xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 285696 + d1 * 372 + d2 * 62 + d3, d4), <1x1>, memref<285696x108xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd_workaround"("0|CogVideoXCausalConv3d[conv1]|/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5128|pad|5209|aten__constant_pad_nd"))
            RuntimeTTNN |    DEBUG | Executing operation: %5 = "ttnn.permute"(%4) <{permutation = array<i64: 0, 2, 3, 4, 1>}> : (tensor<1x768x6x62x108xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 294912 + d1 * 384 + d2 * 64 + d3, d4), <1x1>, memref<9216x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x6x62x108x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 47616 + d1 * 7936 + d2 * 128 + d3, d4), <1x1>, memref<1488x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_input"("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable"))
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x768x6x62x108xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 294912 + d1 * 384 + d2 * 64 + d3, d4), <1x1>, memref<9216x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_input"("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable"))
            RuntimeTTNN |    DEBUG | Executing operation: %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<row_major>}> : (tensor<1x6x62x108x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 47616 + d1 * 7936 + d2 * 128 + d3, d4), <1x1>, memref<1488x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x6x62x108x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 40176 + d1 * 6696 + d2 * 108 + d3, d4), <1x1>, memref<40176x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_input"("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable"))
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x6x62x108x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 47616 + d1 * 7936 + d2 * 128 + d3, d4), <1x1>, memref<1488x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_input"("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable"))
            RuntimeTTNN |    DEBUG | Executing operation: %7 = "ttnn.conv3d"(%6, %1, %0, %2) <{batch_size = 1 : i32, conv3d_config = #ttnn.conv3d_config<weights_dtype = bf16, t_out_block = 1, w_out_block = 16, h_out_block = 2, c_out_block = 96, c_in_block = 128>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 768 : i32, input_depth = 6 : i32, input_height = 62 : i32, input_width = 108 : i32, kernel_size = array<i32: 3, 3, 3>, out_channels = 768 : i32, padding = array<i32: 0, 0, 0>, padding_mode = "zeros", stride = array<i32: 1, 1, 1>}> : (tensor<1x6x62x108x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 40176 + d1 * 6696 + d2 * 108 + d3, d4), <1x1>, memref<40176x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<20736x768xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<648x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x768xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x4x60x106x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 25440 + d1 * 6360 + d2 * 106 + d3, d4), <1x1>, memref<25440x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x6x62x108x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 40176 + d1 * 6696 + d2 * 108 + d3, d4), <1x1>, memref<40176x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<20736x768xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<648x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x768xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<tile>}> : (tensor<1x4x60x106x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 25440 + d1 * 6360 + d2 * 106 + d3, d4), <1x1>, memref<25440x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x60x106x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 30720 + d1 * 7680 + d2 * 128 + d3, d4), <1x1>, memref<960x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_in_0_layout")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x4x60x106x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 25440 + d1 * 6360 + d2 * 106 + d3, d4), <1x1>, memref<25440x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable_in_0_layout")
            RuntimeTTNN |    DEBUG | Executing operation: %9 = "ttnn.permute"(%8) <{permutation = array<i64: 0, 4, 1, 2, 3>}> : (tensor<1x4x60x106x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 30720 + d1 * 7680 + d2 * 128 + d3, d4), <1x1>, memref<960x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x4x60x106x768xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 30720 + d1 * 7680 + d2 * 128 + d3, d4), <1x1>, memref<960x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("1|CogVideoXCausalConv3d[conv1]|CogVideoXSafeConv3d[conv1.conv]|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__convolution_overrideable")
            RuntimeTTNN |    DEBUG | Finished execution of program: main
                 Always |     INFO | Device memory state after submit
                 Always |     INFO | Device DRAM memory state: MemoryView{numBanks: 12, totalBytesPerBank: 1024.000 MB, totalBytesAllocatedPerBank: 19.074 MB, totalBytesFreePerBank: 1004.926 MB, largestContiguousBytesFreePerBank: 707.831 MB}
                 Always |     INFO | Device L1 memory state: MemoryView{numBanks: 64, totalBytesPerBank: 1.269 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 1.269 MB, largestContiguousBytesFreePerBank: 1.269 MB}
                 Always |     INFO | Device L1_SMALL memory state: MemoryView{numBanks: 64, totalBytesPerBank: 0.062 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.062 MB, largestContiguousBytesFreePerBank: 0.062 MB}
                 Always |     INFO | Device TRACE memory state: MemoryView{numBanks: 12, totalBytesPerBank: 0.000 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.000 MB, largestContiguousBytesFreePerBank: 0.000 MB}
2026-01-12 10:24:08.899 (   4.462s) [        4A7FC640]flatbuffer_loaded_execu:195      1| Filled output at output_index 0 device_index 0 with shape [1, 768, 4, 60, 106] and UID 5
2026-01-12 10:24:08.899 (   4.463s) [        4A7FC640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:24:08.899 (   4.463s) [        4A7FC640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:24:08.899 (   4.463s) [        4A7FC640]     buffer_instance.cc:552      1| BufferInstance::PJRT_Buffer_Dimensions
2026-01-12 10:24:08.899 (   4.463s) [        4A7FC640]     buffer_instance.cc:575      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2026-01-12 10:24:08.899 (   4.463s) [        4A7FC640]     buffer_instance.cc:552      1| BufferInstance::PJRT_Buffer_Dimensions
2026-01-12 10:24:08.899 (   4.463s) [        4A7FC640]     buffer_instance.cc:544      1| BufferInstance::PJRT_Buffer_ElementType
2026-01-12 10:24:08.899 (   4.463s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:08.899 (   4.463s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:08.899 (   4.463s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:08.899 (   4.463s) [        E2DEE000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:24:08.904 (   4.467s) [        E2DEE000]     client_instance.cc:687      1| ClientInstance::PJRT_Client_Compile
2026-01-12 10:24:08.904 (   4.467s) [        E2DEE000]      module_builder.cc:211      1| ModuleBuilder::buildModule
2026-01-12 10:24:08.904 (   4.467s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module vhlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
#loc5 = loc("-1|unknown|unknown|-1|aten__var_mean")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data")) -> (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.001360e-05> : tensor<4x32x1x1xbf16>>}> : () -> !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<6.556510e-06> : tensor<4x32xbf16>>}> : () -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.custom_call_v1"(%arg2) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc(#loc2)
    %4 = "vhlo.transpose_v1"(%3) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3, 4]> : tensor<5xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[4, 3, 1, 2, 0]> : tensor<5xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,4,768,60,106]{4,3,1,2,0}">} : (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1> loc(#loc3)
    %5 = "vhlo.reshape_v1"(%4) : (!vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc4)
    %6 = "vhlo.reduce_v1"(%5, %2) <{dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|aten__var_mean")):
      %30 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc6)
      "vhlo.return_v1"(%30) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc5)
    %7 = "vhlo.multiply_v1"(%6, %1) : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc5)
    %8 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc7)
    %9 = "vhlo.subtract_v1"(%5, %8) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc7)
    %10 = "vhlo.multiply_v1"(%9, %9) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc5)
    %11 = "vhlo.reduce_v1"(%10, %2) <{dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|aten__var_mean")):
      %30 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc8)
      "vhlo.return_v1"(%30) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc5)
    %12 = "vhlo.multiply_v1"(%11, %1) : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc5)
    %13 = "vhlo.reshape_v1"(%12) : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1> loc(#loc5)
    %14 = "vhlo.add_v1"(%13, %0) : (!vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1> loc(#loc9)
    %15 = "vhlo.rsqrt_v2"(%14) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1> loc(#loc10)
    %16 = "vhlo.reshape_v1"(%15) : (!vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc11)
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc11)
    %18 = "vhlo.multiply_v1"(%9, %17) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc11)
    %19 = "vhlo.reshape_v1"(%18) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc4)
    %20 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc4)
    %21 = "vhlo.custom_call_v1"(%20) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___norm_layer_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %22 = "vhlo.reshape_v1"(%21) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc4)
    %23 = "vhlo.broadcast_in_dim_v1"(%22) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc11)
    %24 = "vhlo.multiply_v1"(%19, %23) : (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc11)
    %25 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc4)
    %26 = "vhlo.custom_call_v1"(%25) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___norm_layer_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %27 = "vhlo.reshape_v1"(%26) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc4)
    %28 = "vhlo.broadcast_in_dim_v1"(%27) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc9)
    %29 = "vhlo.add_v1"(%24, %28) : (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc9)
    "vhlo.return_v1"(%29) : (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|xla__custom_call")
#loc3 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc4 = loc("-1|unknown|unknown|-1|aten__view")
#loc6 = loc("add.68")
#loc7 = loc("-1|unknown|unknown|-1|aten__sub")
#loc8 = loc("add.51")
#loc9 = loc("-1|unknown|unknown|-1|aten__add")
#loc10 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc11 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:08.908 (   4.472s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module shlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<768xbf16> loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16> loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<4x768x60x106xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<4x32x1x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<6.556510e-06> : tensor<4x32xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.custom_call @tt.mark_argument(%arg2) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x768x4x60x106xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc2)
    %1 = stablehlo.transpose %0, dims = [0, 2, 1, 3, 4] {result_layout = dense<[4, 3, 1, 2, 0]> : tensor<5xindex>, xla_shape = "bf16[1,4,768,60,106]{4,3,1,2,0}"} : (tensor<1x768x4x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc3)
    %2 = stablehlo.reshape %1 : (tensor<1x4x768x60x106xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc4)
    %3 = stablehlo.reduce(%2 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc5)
    %4 = stablehlo.multiply %3, %cst_0 : tensor<4x32xbf16> loc(#loc5)
    %5 = stablehlo.broadcast_in_dim %4, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc6)
    %6 = stablehlo.subtract %2, %5 : tensor<4x32x24x6360xbf16> loc(#loc6)
    %7 = stablehlo.multiply %6, %6 : tensor<4x32x24x6360xbf16> loc(#loc5)
    %8 = stablehlo.reduce(%7 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc5)
    %9 = stablehlo.multiply %8, %cst_0 : tensor<4x32xbf16> loc(#loc5)
    %10 = stablehlo.reshape %9 : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc5)
    %11 = stablehlo.add %10, %cst : tensor<4x32x1x1xbf16> loc(#loc7)
    %12 = stablehlo.rsqrt %11 : tensor<4x32x1x1xbf16> loc(#loc8)
    %13 = stablehlo.reshape %12 : (tensor<4x32x1x1xbf16>) -> tensor<4x32xbf16> loc(#loc9)
    %14 = stablehlo.broadcast_in_dim %13, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc9)
    %15 = stablehlo.multiply %6, %14 : tensor<4x32x24x6360xbf16> loc(#loc9)
    %16 = stablehlo.reshape %15 : (tensor<4x32x24x6360xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc4)
    %17 = stablehlo.reshape %arg1 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc4)
    %18 = stablehlo.custom_call @tt.mark_argument(%17) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___norm_layer_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %19 = stablehlo.reshape %18 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc4)
    %20 = stablehlo.broadcast_in_dim %19, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc9)
    %21 = stablehlo.multiply %16, %20 : tensor<4x768x60x106xbf16> loc(#loc9)
    %22 = stablehlo.reshape %arg0 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc4)
    %23 = stablehlo.custom_call @tt.mark_argument(%22) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___norm_layer_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %24 = stablehlo.reshape %23 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc4)
    %25 = stablehlo.broadcast_in_dim %24, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc7)
    %26 = stablehlo.add %21, %25 : tensor<4x768x60x106xbf16> loc(#loc7)
    return %26 : tensor<4x768x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|xla__custom_call")
#loc3 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc4 = loc("-1|unknown|unknown|-1|aten__view")
#loc5 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc6 = loc("-1|unknown|unknown|-1|aten__sub")
#loc7 = loc("-1|unknown|unknown|-1|aten__add")
#loc8 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc9 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:08.911 (   4.474s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module shlo_frontend:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___norm_layer_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___norm_layer_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<4x768x60x106xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<4x32x1x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<6.556510e-06> : tensor<4x32xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.transpose %arg2, dims = [0, 2, 1, 3, 4] {result_layout = dense<[4, 3, 1, 2, 0]> : tensor<5xindex>, xla_shape = "bf16[1,4,768,60,106]{4,3,1,2,0}"} : (tensor<1x768x4x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    %1 = stablehlo.reshape %0 : (tensor<1x4x768x60x106xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc3)
    %2 = stablehlo.reduce(%1 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc4)
    %3 = stablehlo.multiply %2, %cst_0 : tensor<4x32xbf16> loc(#loc4)
    %4 = stablehlo.broadcast_in_dim %3, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc5)
    %5 = stablehlo.subtract %1, %4 : tensor<4x32x24x6360xbf16> loc(#loc5)
    %6 = stablehlo.multiply %5, %5 : tensor<4x32x24x6360xbf16> loc(#loc4)
    %7 = stablehlo.reduce(%6 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc4)
    %8 = stablehlo.multiply %7, %cst_0 : tensor<4x32xbf16> loc(#loc4)
    %9 = stablehlo.reshape %8 : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc4)
    %10 = stablehlo.add %9, %cst : tensor<4x32x1x1xbf16> loc(#loc6)
    %11 = stablehlo.rsqrt %10 : tensor<4x32x1x1xbf16> loc(#loc7)
    %12 = stablehlo.reshape %11 : (tensor<4x32x1x1xbf16>) -> tensor<4x32xbf16> loc(#loc8)
    %13 = stablehlo.broadcast_in_dim %12, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc8)
    %14 = stablehlo.multiply %5, %13 : tensor<4x32x24x6360xbf16> loc(#loc8)
    %15 = stablehlo.reshape %14 : (tensor<4x32x24x6360xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc3)
    %16 = stablehlo.reshape %arg1 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %17 = stablehlo.reshape %16 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
    %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc8)
    %19 = stablehlo.multiply %15, %18 : tensor<4x768x60x106xbf16> loc(#loc8)
    %20 = stablehlo.reshape %arg0 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %21 = stablehlo.reshape %20 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
    %22 = stablehlo.broadcast_in_dim %21, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc6)
    %23 = stablehlo.add %19, %22 : tensor<4x768x60x106xbf16> loc(#loc6)
    return %23 : tensor<4x768x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc3 = loc("-1|unknown|unknown|-1|aten__view")
#loc4 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc5 = loc("-1|unknown|unknown|-1|aten__sub")
#loc6 = loc("-1|unknown|unknown|-1|aten__add")
#loc7 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc8 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:08.916 (   4.480s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module shlo_compiler:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<4x768x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<4x32x1x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<6.556510e-06> : tensor<4x32xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.transpose %arg2, dims = [0, 2, 1, 3, 4] {result_layout = dense<[4, 3, 1, 2, 0]> : tensor<5xindex>, xla_shape = "bf16[1,4,768,60,106]{4,3,1,2,0}"} : (tensor<1x768x4x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    %1 = stablehlo.reshape %0 : (tensor<1x4x768x60x106xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc3)
    %2 = stablehlo.reduce(%1 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc4)
    %3 = stablehlo.multiply %2, %cst_0 : tensor<4x32xbf16> loc(#loc4)
    %4 = stablehlo.broadcast_in_dim %3, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc5)
    %5 = stablehlo.subtract %1, %4 : tensor<4x32x24x6360xbf16> loc(#loc5)
    %6 = stablehlo.multiply %5, %5 : tensor<4x32x24x6360xbf16> loc(#loc4)
    %7 = stablehlo.reduce(%6 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc4)
    %8 = stablehlo.multiply %7, %cst_0 : tensor<4x32xbf16> loc(#loc4)
    %9 = stablehlo.reshape %8 : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc4)
    %10 = stablehlo.add %9, %cst : tensor<4x32x1x1xbf16> loc(#loc6)
    %11 = stablehlo.rsqrt %10 : tensor<4x32x1x1xbf16> loc(#loc7)
    %12 = stablehlo.reshape %11 : (tensor<4x32x1x1xbf16>) -> tensor<4x32xbf16> loc(#loc8)
    %13 = stablehlo.broadcast_in_dim %12, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc8)
    %14 = stablehlo.multiply %5, %13 : tensor<4x32x24x6360xbf16> loc(#loc8)
    %15 = stablehlo.reshape %14 : (tensor<4x32x24x6360xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc3)
    %16 = stablehlo.reshape %arg1 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %17 = stablehlo.reshape %16 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
    %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc8)
    %19 = stablehlo.multiply %15, %18 : tensor<4x768x60x106xbf16> loc(#loc8)
    %20 = stablehlo.reshape %arg0 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %21 = stablehlo.reshape %20 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
    %22 = stablehlo.broadcast_in_dim %21, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc6)
    %23 = stablehlo.add %19, %22 : tensor<4x768x60x106xbf16> loc(#loc6)
    return %23 : tensor<4x768x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc3 = loc("-1|unknown|unknown|-1|aten__view")
#loc4 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc5 = loc("-1|unknown|unknown|-1|aten__sub")
#loc6 = loc("-1|unknown|unknown|-1|aten__add")
#loc7 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc8 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:08.920 (   4.483s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module ttir:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
      func.func @main(%arg0: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<4x768x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<1.001360e-05> : tensor<4x32x1x1xbf16>}> : () -> tensor<4x32x1x1xbf16> loc(#loc)
        %1 = "ttir.constant"() <{value = dense<6.556510e-06> : tensor<4x32xbf16>}> : () -> tensor<4x32xbf16> loc(#loc)
        %2 = "ttir.permute"(%arg2) <{permutation = array<i64: 0, 2, 1, 3, 4>}> : (tensor<1x768x4x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
        %3 = "ttir.reshape"(%2) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<1x4x768x60x106xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc3)
        %4 = "ttir.sum"(%3) <{dim_arg = [2 : i32, 3 : i32], keep_dim = false}> : (tensor<4x32x24x6360xbf16>) -> tensor<4x32xbf16> loc(#loc4)
        %5 = "ttir.multiply"(%4, %1) : (tensor<4x32xbf16>, tensor<4x32xbf16>) -> tensor<4x32xbf16> loc(#loc4)
        %6 = "ttir.reshape"(%5) <{shape = [4 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc5)
        %7 = "ttir.broadcast"(%6) <{broadcast_dimensions = array<i64: 1, 1, 24, 6360>}> : (tensor<4x32x1x1xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc5)
        %8 = "ttir.subtract"(%3, %7) : (tensor<4x32x24x6360xbf16>, tensor<4x32x24x6360xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc5)
        %9 = "ttir.multiply"(%8, %8) : (tensor<4x32x24x6360xbf16>, tensor<4x32x24x6360xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc4)
        %10 = "ttir.sum"(%9) <{dim_arg = [2 : i32, 3 : i32], keep_dim = false}> : (tensor<4x32x24x6360xbf16>) -> tensor<4x32xbf16> loc(#loc4)
        %11 = "ttir.multiply"(%10, %1) : (tensor<4x32xbf16>, tensor<4x32xbf16>) -> tensor<4x32xbf16> loc(#loc4)
        %12 = "ttir.reshape"(%11) <{shape = [4 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc4)
        %13 = "ttir.add"(%12, %0) : (tensor<4x32x1x1xbf16>, tensor<4x32x1x1xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc6)
        %14 = "ttir.rsqrt"(%13) : (tensor<4x32x1x1xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc7)
        %15 = "ttir.reshape"(%14) <{shape = [4 : i32, 32 : i32]}> : (tensor<4x32x1x1xbf16>) -> tensor<4x32xbf16> loc(#loc8)
        %16 = "ttir.reshape"(%15) <{shape = [4 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc8)
        %17 = "ttir.broadcast"(%16) <{broadcast_dimensions = array<i64: 1, 1, 24, 6360>}> : (tensor<4x32x1x1xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc8)
        %18 = "ttir.multiply"(%8, %17) : (tensor<4x32x24x6360xbf16>, tensor<4x32x24x6360xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc8)
        %19 = "ttir.reshape"(%18) <{shape = [4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x32x24x6360xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc3)
        %20 = "ttir.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
        %21 = "ttir.reshape"(%20) <{shape = [768 : i32]}> : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
        %22 = "ttir.reshape"(%21) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16>) -> tensor<1x768x1x1xbf16> loc(#loc8)
        %23 = "ttir.broadcast"(%22) <{broadcast_dimensions = array<i64: 4, 1, 60, 106>}> : (tensor<1x768x1x1xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc8)
        %24 = "ttir.multiply"(%19, %23) : (tensor<4x768x60x106xbf16>, tensor<4x768x60x106xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc8)
        %25 = "ttir.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
        %26 = "ttir.reshape"(%25) <{shape = [768 : i32]}> : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
        %27 = "ttir.reshape"(%26) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16>) -> tensor<1x768x1x1xbf16> loc(#loc6)
        %28 = "ttir.broadcast"(%27) <{broadcast_dimensions = array<i64: 4, 1, 60, 106>}> : (tensor<1x768x1x1xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc6)
        %29 = "ttir.add"(%24, %28) : (tensor<4x768x60x106xbf16>, tensor<4x768x60x106xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc6)
        return %29 : tensor<4x768x60x106xbf16> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc3 = loc("-1|unknown|unknown|-1|aten__view")
#loc4 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc5 = loc("-1|unknown|unknown|-1|aten__sub")
#loc6 = loc("-1|unknown|unknown|-1|aten__add")
#loc7 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc8 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:08.924 (   4.487s) [        E2DEE000]      module_builder.cc:766   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2026-01-12 10:24:08.924 (   4.487s) [        E2DEE000]      module_builder.cc:780   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2026-01-12 10:24:08.924 (   4.487s) [        E2DEE000]      module_builder.cc:790   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2026-01-12 10:24:08.929 (   4.493s) [        E2DEE000]      module_builder.cc:1006     1| MLIR Module ttnn:
#dram = #ttnn.buffer_type<dram>
#loc = loc(unknown)
#loc3 = loc("-1|unknown|unknown|-1|xla__device_data")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102656, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073125888, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 184320 + d1 * 240 + d2 * 60 + d3, d4), <1x1>, memref<184320x106xbf16, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func private @main_const_eval_0() -> tensor<4x32x1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 1.00135803E-5 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<4x32x1x1>}> : (!ttnn.device) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc)
        return %1 : tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_1(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<768xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<768xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn_layout3>) -> tensor<768xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn_layout4>) -> tensor<1x768x1x1xbf16, #ttnn_layout5> loc(#loc1)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn_layout4>) -> () loc(#loc1)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<4x1x60x106>}> : (tensor<1x768x1x1xbf16, #ttnn_layout5>) -> tensor<4x768x60x106xbf16, #ttnn_layout6> loc(#loc1)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1)
        %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn_layout6>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc1)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn_layout6>) -> () loc(#loc1)
        return %5 : tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_2(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<768xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<768xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn_layout3>) -> tensor<768xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn_layout4>) -> tensor<1x768x1x1xbf16, #ttnn_layout5> loc(#loc2)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn_layout4>) -> () loc(#loc2)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<4x1x60x106>}> : (tensor<1x768x1x1xbf16, #ttnn_layout5>) -> tensor<4x768x60x106xbf16, #ttnn_layout6> loc(#loc2)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1xbf16, #ttnn_layout5>) -> () loc(#loc2)
        %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn_layout6>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc1)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn_layout6>) -> () loc(#loc1)
        return %5 : tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<4x768x60x106xbf16, #ttnn_layout6> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg0]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %3 = "ttnn.to_layout"(%arg2) <{layout = #ttnn.layout<tile>}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout7>) -> tensor<1x768x4x60x106xbf16, #ttnn_layout8> loc(#loc4)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout7>) -> () loc(#loc4)
        %4 = "ttnn.permute"(%3) <{permutation = array<i64: 0, 2, 1, 3, 4>}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout8>) -> tensor<1x4x768x60x106xbf16, #ttnn_layout9> loc(#loc5)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout8>) -> () loc(#loc5)
        %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<1x4x768x60x106xbf16, #ttnn_layout9>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc6)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x4x768x60x106xbf16, #ttnn_layout9>) -> () loc(#loc6)
        %6 = "ttnn.mean"(%5) <{dim_arg = [2 : i32, 3 : i32], keep_dim = true}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc10)
        %7 = "ttnn.neg"(%6) : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc11)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc11)
        %8 = "ttnn.add"(%5, %7) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc8)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc8)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc8)
        %9 = "ttnn.multiply"(%8, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc7)
        %10 = "ttnn.mean"(%9) <{dim_arg = [2 : i32, 3 : i32], keep_dim = true}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc10)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc10)
        %11 = "ttnn.add"(%10, %0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x1x1xbf16, #ttnn_layout>, tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc2)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc2)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc2)
        %12 = "ttnn.rsqrt"(%11) : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc9)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc9)
        %13 = "ttnn.multiply"(%8, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc1)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc1)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc1)
        %14 = "ttnn.multiply"(%13, %1) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc1)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc1)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc1)
        %15 = "ttnn.add"(%14, %2) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc2)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc2)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc2)
        %16 = "ttnn.reshape"(%15) <{shape = [4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x768x60x106xbf16, #ttnn_layout6> loc(#loc2)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc2)
        return %16 : tensor<4x768x60x106xbf16, #ttnn_layout6> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("-1|unknown|unknown|-1|aten__mul")
#loc2 = loc("-1|unknown|unknown|-1|aten__add")
#loc4 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute_in_0_layout")
#loc5 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc6 = loc("-1|unknown|unknown|-1|aten__view")
#loc7 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc8 = loc("-1|unknown|unknown|-1|aten__sub")
#loc9 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc10 = loc("-1|unknown|unknown|-1|aten__var_mean_mean"(#loc7))
#loc11 = loc("-1|unknown|unknown|-1|aten__sub_neg"(#loc8))
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:24:08.936 (   4.499s) [        E2DEE000]loaded_executable_insta:290      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2026-01-12 10:24:08.936 (   4.499s) [        E2DEE000]loaded_executable_insta:309      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2026-01-12 10:24:08.936 (   4.500s) [        E2DEE000]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2026-01-12 10:24:08.936 (   4.500s) [        E2DEE000]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2026-01-12 10:24:08.936 (   4.500s) [        E2DEE000]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2026-01-12 10:24:08.936 (   4.500s) [        E2DEE000]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2026-01-12 10:24:08.936 (   4.500s) [        E2DEE000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:24:08.936 (   4.500s) [        E2DEE000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:24:08.939 (   4.502s) [        E2DEE000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:24:08.939 (   4.502s) [        E2DEE000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:24:08.940 (   4.504s) [        4A7FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2026-01-12 10:24:08.940 (   4.504s) [        4A7FC640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:24:08.940 (   4.504s) [        4A7FC640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:24:08.940 (   4.504s) [        4A7FC640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:24:08.940 (   4.504s) [        4A7FC640] executable_instance.cc:140      1| ExecutableInstance::PJRT_Executable_NumOutputs
2026-01-12 10:24:08.941 (   4.504s) [        4A7FC640]loaded_executable_insta:345      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2026-01-12 10:24:08.941 (   4.504s) [        4A7FC640]flatbuffer_loaded_execu:260      1| FlatbufferLoadedExecutableInstance::Execute
2026-01-12 10:24:08.941 (   4.504s) [        4A7FC640]     client_instance.cc:416      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 1]
2026-01-12 10:24:08.941 (   4.504s) [        4A7FC640]flatbuffer_loaded_execu:108      1| Re-laying out already prepared input tensor for argument index 2 with shape [1, 768, 4, 60, 106].
                 Always |     INFO | Device memory state before submit
                 Always |     INFO | Device DRAM memory state: MemoryView{numBanks: 12, totalBytesPerBank: 1024.000 MB, totalBytesAllocatedPerBank: 18.717 MB, totalBytesFreePerBank: 1005.283 MB, largestContiguousBytesFreePerBank: 707.829 MB}
                 Always |     INFO | Device L1 memory state: MemoryView{numBanks: 64, totalBytesPerBank: 1.269 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 1.269 MB, largestContiguousBytesFreePerBank: 1.269 MB}
                 Always |     INFO | Device L1_SMALL memory state: MemoryView{numBanks: 64, totalBytesPerBank: 0.062 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.062 MB, largestContiguousBytesFreePerBank: 0.062 MB}
                 Always |     INFO | Device TRACE memory state: MemoryView{numBanks: 12, totalBytesPerBank: 0.000 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.000 MB, largestContiguousBytesFreePerBank: 0.000 MB}
            RuntimeTTNN |    DEBUG | Starting execution of program: main
            RuntimeTTNN |    DEBUG | Executing operation: %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
                 Always |    DEBUG | Running LoadCachedOp for function main_const_eval_0 with hash: 581d0f944c67f11eafc3d6069dccaf6118fb1aa46a3fb7a0d488a3aa11fca2ab
                 Always |    DEBUG | Cache miss or invalid cache for function: main_const_eval_0
            RuntimeTTNN |    DEBUG | Starting execution of program: main_const_eval_0
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 1.00135803E-5 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<4x32x1x1>}> : (!ttnn.device) -> tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Finished execution of program: main_const_eval_0
                 Always |    DEBUG | executed sub-func: main_const_eval_0
            RuntimeTTNN |    DEBUG | Executing operation: %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
                 Always |    DEBUG | Running LoadCachedOp for function main_const_eval_1 with hash: dff445c572a562bb4de69ea6c0b14387f37ec626ee5c73782f490ca8242181f1
                 Always |    DEBUG | Cache miss or invalid cache for function: main_const_eval_1
            RuntimeTTNN |    DEBUG | Starting execution of program: main_const_eval_1
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<4x1x60x106>}> : (tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Finished execution of program: main_const_eval_1
                 Always |    DEBUG | executed sub-func: main_const_eval_1
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %2 = ttcore.load_cached(@main_const_eval_2, [%arg0]) : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
                 Always |    DEBUG | Running LoadCachedOp for function main_const_eval_2 with hash: dff445c572a562bb4de69ea6c0b14387f37ec626ee5c73782f490ca8242181f1
                 Always |    DEBUG | Cache miss or invalid cache for function: main_const_eval_2
            RuntimeTTNN |    DEBUG | Starting execution of program: main_const_eval_2
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<4x1x60x106>}> : (tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Finished execution of program: main_const_eval_2
                 Always |    DEBUG | executed sub-func: main_const_eval_2
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %3 = "ttnn.to_layout"(%arg2) <{layout = #ttnn.layout<tile>}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 184320 + d1 * 240 + d2 * 60 + d3, d4), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute_in_0_layout")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 184320 + d1 * 240 + d2 * 60 + d3, d4), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute_in_0_layout")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %4 = "ttnn.permute"(%3) <{permutation = array<i64: 0, 2, 1, 3, 4>}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
            RuntimeTTNN |    DEBUG | Executing operation: %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<1x4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__view")
2026-01-12 10:24:10.404 | critical |          Always | TT_FATAL: Invalid arguments to reshape (assert.hpp:103)
2026-01-12 10:24:10.431 (   5.994s) [        4A7FC640]                utils.h:62     ERR| Exception:
{TT_FATAL @ /localdev/vkovinic/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal/ttnn/core/tensor/tensor_utils.cpp:54: new_volume == old_volume
info:
Invalid arguments to reshape
backtrace:
 --- /localdev/vkovinic/tt-xla/third_party/tt-mlir/install/lib/libTTMLIRCompiler.so(+0xd2bacc8) [0x7ff3249d0cc8]
 --- tt::tt_metal::infer_dims_for_reshape(tt::tt_metal::Tensor const&, std::span<int const, 18446744073709551615ul>)
 --- ttnn::operations::data_movement::ReshapeViewOperation::invoke(tt::tt_metal::Tensor const&, std::span<int const, 18446744073709551615ul>, std::optional<tt::tt_metal::MemoryConfig> const&, std::optional<std::variant<unsigned int, float> > const&, ttnn::TileReshapeMapMode, std::optional<tt::tt_metal::CoreRangeSet> const&)
 --- /localdev/vkovinic/tt-xla/third_party/tt-mlir/install/lib/libTTMLIRRuntime.so(+0x2a3512) [0x7ff35cb8e512]
 --- tt::runtime::ttnn::operations::data_movement::run(tt::target::ttnn::ReshapeOp const*, tt::runtime::ttnn::ProgramContext&)
 --- tt::runtime::ttnn::ProgramExecutor::runOperation(tt::target::ttnn::Operation const*)
 --- tt::runtime::ttnn::ProgramExecutor::execute()
 --- tt::runtime::ttnn::submit(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&)
 --- tt::runtime::submit(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&)
 --- std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > std::__invoke_impl<std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&>(std::__invoke_other, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&)
 --- std::__invoke_result<std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&>::type std::__invoke<std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&>(std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&)
 --- std::invoke_result<std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&>::type std::invoke<std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&>(std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&)
 --- std::optional<std::conditional<std::is_same_v<std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >, void>, std::monostate, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > >::type> tt::pjrt::utils::invoke_noexcept<std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > >(std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> > (&)(tt::runtime::Device, tt::runtime::Binary, unsigned int, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&), tt::runtime::Device&, tt::runtime::Binary const&, unsigned int&, std::vector<tt::runtime::Tensor, std::allocator<tt::runtime::Tensor> >&)
 --- tt::pjrt::FlatbufferLoadedExecutableInstance::execute(PJRT_LoadedExecutable_Execute_Args*)
 --- tt::pjrt::internal::onLoadedExecutableExecute(PJRT_LoadedExecutable_Execute_Args*)
 --- xla::PjRtCApiLoadedExecutable::ExecuteWithSingleDevice(absl::lts_20230802::Span<xla::PjRtBuffer* const>, xla::PjRtDevice*, xla::ExecuteOptions const&, std::optional<xla::PjRtFuture<void> >&, bool)
 --- xla::PjRtCApiLoadedExecutable::ExecutePortable(absl::lts_20230802::Span<xla::PjRtBuffer* const>, xla::PjRtDevice*, xla::ExecuteOptions const&, std::optional<xla::PjRtFuture<void> >&, bool)
 --- torch_xla::runtime::PjRtComputationClient::ExecuteComputation(torch_xla::runtime::ComputationClient::Computation const&, absl::lts_20230802::Span<std::shared_ptr<torch_xla::runtime::ComputationClient::Data> const>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch_xla::runtime::ComputationClient::ExecuteComputationOptions const&)
 --- /usr/local/lib/python3.11/dist-packages/_XLAC.cpython-311-x86_64-linux-gnu.so(+0x6ddf7f6) [0x7ff3aaabb7f6]
 --- torch::lazy::MultiWait::Complete(std::function<void ()> const&)
 --- Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)
 --- void absl::lts_20230802::internal_any_invocable::RemoteInvoker<false, void, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&>(absl::lts_20230802::internal_any_invocable::TypeErasedState*)
 --- /usr/local/lib/python3.11/dist-packages/_XLAC.cpython-311-x86_64-linux-gnu.so(+0x11678942) [0x7ff3b5354942]
 --- /lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7ff3e2e83ac3]
 --- /lib/x86_64-linux-gnu/libc.so.6(+0x1268c0) [0x7ff3e2f158c0]
}

2026-01-12 10:24:10.438 (   6.001s) [        4A7FC640]     client_instance.cc:499      1| Closing parent mesh.
2026-01-12 10:24:10.439 (   6.002s) [        4A7FC640]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2026-01-12 10:24:10.439 (   6.002s) [        4A7FC640]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2026-01-12 10:24:10.439 (   6.002s) [        4A7FC640]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
✗ FAILED: Bad StatusOr access: INTERNAL: Error code: 13
Traceback (most recent call last):
  File "/localdev/vkovinic/tt-xla/tests/torch/models/mochi/single_ops.py", line 251, in <module>
    test_module(load_conv1_then_norm2) # Minimal repro of reshape error
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/vkovinic/tt-xla/tests/torch/models/mochi/single_ops.py", line 233, in test_module
    output = mod(x)
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/localdev/vkovinic/tt-xla/tests/torch/models/mochi/single_ops.py", line 201, in forward
    x = self.norm2(x)  # Should fail at reshape
        ^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_mochi.py", line 65, in forward
    output = output.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
  File "/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_mochi.py", line 65, in torch_dynamo_resume_in_forward_at_65
    output = output.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
  File "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/localdev/vkovinic/tt-xla/python_package/tt_torch/backend/backend.py", line 174, in __call__
    torch_xla._XLAC._xla_sync_multi(list(output), self.devices, wait=False)
RuntimeError: Bad StatusOr access: INTERNAL: Error code: 13
2026-01-12 10:24:10.441 (   6.004s) [        E2DEE000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:24:10.441 (   6.004s) [        E2DEE000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:24:10.555 (   6.119s) [        E2DEE000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:24:10.557 (   6.120s) [        E2DEE000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:24:10.689 (   6.252s) [        E2DEE000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:24:10.689 (   6.252s) [        E2DEE000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:24:10.689 (   6.252s) [        E2DEE000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:24:10.689 (   6.253s) [        E2DEE000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:24:10.960 (   6.523s) [        E2DEE000]     client_instance.cc:192      1| ClientInstance::~ClientInstance