Using PJRT plugin directory: /localdev/vkovinic/tt-xla/python_package/pjrt_plugin_tt
Using TT-Metal from the source tree: /localdev/vkovinic/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.
Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.
2026-01-12 10:22:08.014 (   0.000s) [        17B1B000]   plugin_attributes.cc:60       1| PluginAttributes::PJRT_Plugin_Initialize
2026-01-12 10:22:08.014 (   0.000s) [        17B1B000]     client_instance.cc:546      1| ClientInstance::PJRT_Client_Create
2026-01-12 10:22:08.015 (   0.001s) [        17B1B000]     client_instance.cc:177      1| ClientInstance::ClientInstance
2026-01-12 10:22:08.015 (   0.001s) [        17B1B000]     client_instance.cc:198      1| ClientInstance::Initialize
                 Always |    DEBUG | Device grid size = { 8, 8 }
                 Always |    DEBUG | Device grid size = { 8, 8 }
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]              stubs.inc:103   WARN| STUB: PJRT_Client_TopologyDescription
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]     client_instance.cc:600      1| ClientInstance::PJRT_Client_PlatformVersion
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]     client_instance.cc:581      1| ClientInstance::PJRT_Client_PlatformName
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]     client_instance.cc:611      1| ClientInstance::PJRT_Client_Devices
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]     client_instance.cc:624      1| ClientInstance::PJRT_Client_AddressableDevices
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]     client_instance.cc:674      1| ClientInstance::PJRT_Client_AddressableMemories
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]   plugin_attributes.cc:66       1| PluginAttributes::PJRT_Plugin_Attributes
2026-01-12 10:22:08.200684: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:22:08.200 (   0.186s) [        17B1B000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id

======================================================================
TEST: norm2
======================================================================
Loading Mochi VAE norm2

2026-01-12 10:22:08.887 (   0.873s) [        17B1B000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:22:08.887 (   0.873s) [        17B1B000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:22:08.887 (   0.873s) [        17B1B000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:22:08.887 (   0.873s) [        17B1B000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:22:08.887 (   0.873s) [        17B1B000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:22:08.892 (   0.878s) [        17B1B000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:22:08.893 (   0.878s) [        17B1B000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:22:08.893 (   0.878s) [        17B1B000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:22:09.203 (   1.189s) [        17B1B000]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2026-01-12 10:22:09.203 (   1.189s) [        17B1B000]     client_instance.cc:735      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2026-01-12 10:22:09.203 (   1.189s) [        17B1B000]     memory_instance.cc:57       1| MemoryInstance::getDevice
2026-01-12 10:22:09.203 (   1.189s) [        17B1B000]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:22:09.203 (   1.189s) [        17B1B000]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy

Input shape: [1, 768, 4, 60, 106]
Memory estimate: 37.27 MB

Running forward pass...
2026-01-12 10:22:09.577 (   1.563s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:09.577 (   1.563s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:09.577 (   1.563s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:09.577 (   1.563s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:09.577 (   1.563s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:09.577 (   1.563s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:09.577 (   1.563s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:09.577 (   1.563s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:09.577 (   1.563s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:09.581 (   1.567s) [        17B1B000]     client_instance.cc:687      1| ClientInstance::PJRT_Client_Compile
2026-01-12 10:22:09.582 (   1.567s) [        17B1B000]      module_builder.cc:211      1| ModuleBuilder::buildModule
2026-01-12 10:22:09.583 (   1.568s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module vhlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
#loc5 = loc("-1|unknown|unknown|-1|aten__var_mean")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data")) -> (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.001360e-05> : tensor<4x32x1x1xbf16>>}> : () -> !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<6.556510e-06> : tensor<4x32xbf16>>}> : () -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.custom_call_v1"(%arg2) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc(#loc2)
    %4 = "vhlo.transpose_v1"(%3) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3, 4]> : tensor<5xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[4, 3, 1, 2, 0]> : tensor<5xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,4,768,60,106]{4,3,1,2,0}">} : (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1> loc(#loc3)
    %5 = "vhlo.reshape_v1"(%4) : (!vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc4)
    %6 = "vhlo.reduce_v1"(%5, %2) <{dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|aten__var_mean")):
      %30 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc6)
      "vhlo.return_v1"(%30) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc5)
    %7 = "vhlo.multiply_v1"(%6, %1) : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc5)
    %8 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc7)
    %9 = "vhlo.subtract_v1"(%5, %8) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc7)
    %10 = "vhlo.multiply_v1"(%9, %9) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc5)
    %11 = "vhlo.reduce_v1"(%10, %2) <{dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|aten__var_mean")):
      %30 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc8)
      "vhlo.return_v1"(%30) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc5)
    %12 = "vhlo.multiply_v1"(%11, %1) : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc5)
    %13 = "vhlo.reshape_v1"(%12) : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1> loc(#loc5)
    %14 = "vhlo.add_v1"(%13, %0) : (!vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1> loc(#loc9)
    %15 = "vhlo.rsqrt_v2"(%14) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1> loc(#loc10)
    %16 = "vhlo.reshape_v1"(%15) : (!vhlo.tensor_v1<4x32x1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x!vhlo.bf16_v1> loc(#loc11)
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<4x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc11)
    %18 = "vhlo.multiply_v1"(%9, %17) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1> loc(#loc11)
    %19 = "vhlo.reshape_v1"(%18) : (!vhlo.tensor_v1<4x32x24x6360x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc4)
    %20 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc4)
    %21 = "vhlo.custom_call_v1"(%20) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___norm_layer_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %22 = "vhlo.reshape_v1"(%21) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc4)
    %23 = "vhlo.broadcast_in_dim_v1"(%22) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc11)
    %24 = "vhlo.multiply_v1"(%19, %23) : (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc11)
    %25 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc4)
    %26 = "vhlo.custom_call_v1"(%25) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___norm_layer_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc2)
    %27 = "vhlo.reshape_v1"(%26) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc4)
    %28 = "vhlo.broadcast_in_dim_v1"(%27) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc9)
    %29 = "vhlo.add_v1"(%24, %28) : (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>, !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc(#loc9)
    "vhlo.return_v1"(%29) : (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|xla__custom_call")
#loc3 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc4 = loc("-1|unknown|unknown|-1|aten__view")
#loc6 = loc("add.68")
#loc7 = loc("-1|unknown|unknown|-1|aten__sub")
#loc8 = loc("add.51")
#loc9 = loc("-1|unknown|unknown|-1|aten__add")
#loc10 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc11 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:09.587 (   1.573s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module shlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<768xbf16> loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16> loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<4x768x60x106xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<4x32x1x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<6.556510e-06> : tensor<4x32xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.custom_call @tt.mark_argument(%arg2) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x768x4x60x106xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc2)
    %1 = stablehlo.transpose %0, dims = [0, 2, 1, 3, 4] {result_layout = dense<[4, 3, 1, 2, 0]> : tensor<5xindex>, xla_shape = "bf16[1,4,768,60,106]{4,3,1,2,0}"} : (tensor<1x768x4x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc3)
    %2 = stablehlo.reshape %1 : (tensor<1x4x768x60x106xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc4)
    %3 = stablehlo.reduce(%2 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc5)
    %4 = stablehlo.multiply %3, %cst_0 : tensor<4x32xbf16> loc(#loc5)
    %5 = stablehlo.broadcast_in_dim %4, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc6)
    %6 = stablehlo.subtract %2, %5 : tensor<4x32x24x6360xbf16> loc(#loc6)
    %7 = stablehlo.multiply %6, %6 : tensor<4x32x24x6360xbf16> loc(#loc5)
    %8 = stablehlo.reduce(%7 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc5)
    %9 = stablehlo.multiply %8, %cst_0 : tensor<4x32xbf16> loc(#loc5)
    %10 = stablehlo.reshape %9 : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc5)
    %11 = stablehlo.add %10, %cst : tensor<4x32x1x1xbf16> loc(#loc7)
    %12 = stablehlo.rsqrt %11 : tensor<4x32x1x1xbf16> loc(#loc8)
    %13 = stablehlo.reshape %12 : (tensor<4x32x1x1xbf16>) -> tensor<4x32xbf16> loc(#loc9)
    %14 = stablehlo.broadcast_in_dim %13, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc9)
    %15 = stablehlo.multiply %6, %14 : tensor<4x32x24x6360xbf16> loc(#loc9)
    %16 = stablehlo.reshape %15 : (tensor<4x32x24x6360xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc4)
    %17 = stablehlo.reshape %arg1 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc4)
    %18 = stablehlo.custom_call @tt.mark_argument(%17) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___norm_layer_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %19 = stablehlo.reshape %18 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc4)
    %20 = stablehlo.broadcast_in_dim %19, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc9)
    %21 = stablehlo.multiply %16, %20 : tensor<4x768x60x106xbf16> loc(#loc9)
    %22 = stablehlo.reshape %arg0 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc4)
    %23 = stablehlo.custom_call @tt.mark_argument(%22) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___norm_layer_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc2)
    %24 = stablehlo.reshape %23 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc4)
    %25 = stablehlo.broadcast_in_dim %24, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc7)
    %26 = stablehlo.add %21, %25 : tensor<4x768x60x106xbf16> loc(#loc7)
    return %26 : tensor<4x768x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|xla__custom_call")
#loc3 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc4 = loc("-1|unknown|unknown|-1|aten__view")
#loc5 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc6 = loc("-1|unknown|unknown|-1|aten__sub")
#loc7 = loc("-1|unknown|unknown|-1|aten__add")
#loc8 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc9 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:09.590 (   1.576s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module shlo_frontend:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___norm_layer_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___norm_layer_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<4x768x60x106xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<4x32x1x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<6.556510e-06> : tensor<4x32xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.transpose %arg2, dims = [0, 2, 1, 3, 4] {result_layout = dense<[4, 3, 1, 2, 0]> : tensor<5xindex>, xla_shape = "bf16[1,4,768,60,106]{4,3,1,2,0}"} : (tensor<1x768x4x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    %1 = stablehlo.reshape %0 : (tensor<1x4x768x60x106xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc3)
    %2 = stablehlo.reduce(%1 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc4)
    %3 = stablehlo.multiply %2, %cst_0 : tensor<4x32xbf16> loc(#loc4)
    %4 = stablehlo.broadcast_in_dim %3, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc5)
    %5 = stablehlo.subtract %1, %4 : tensor<4x32x24x6360xbf16> loc(#loc5)
    %6 = stablehlo.multiply %5, %5 : tensor<4x32x24x6360xbf16> loc(#loc4)
    %7 = stablehlo.reduce(%6 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc4)
    %8 = stablehlo.multiply %7, %cst_0 : tensor<4x32xbf16> loc(#loc4)
    %9 = stablehlo.reshape %8 : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc4)
    %10 = stablehlo.add %9, %cst : tensor<4x32x1x1xbf16> loc(#loc6)
    %11 = stablehlo.rsqrt %10 : tensor<4x32x1x1xbf16> loc(#loc7)
    %12 = stablehlo.reshape %11 : (tensor<4x32x1x1xbf16>) -> tensor<4x32xbf16> loc(#loc8)
    %13 = stablehlo.broadcast_in_dim %12, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc8)
    %14 = stablehlo.multiply %5, %13 : tensor<4x32x24x6360xbf16> loc(#loc8)
    %15 = stablehlo.reshape %14 : (tensor<4x32x24x6360xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc3)
    %16 = stablehlo.reshape %arg1 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %17 = stablehlo.reshape %16 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
    %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc8)
    %19 = stablehlo.multiply %15, %18 : tensor<4x768x60x106xbf16> loc(#loc8)
    %20 = stablehlo.reshape %arg0 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %21 = stablehlo.reshape %20 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
    %22 = stablehlo.broadcast_in_dim %21, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc6)
    %23 = stablehlo.add %19, %22 : tensor<4x768x60x106xbf16> loc(#loc6)
    return %23 : tensor<4x768x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc3 = loc("-1|unknown|unknown|-1|aten__view")
#loc4 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc5 = loc("-1|unknown|unknown|-1|aten__sub")
#loc6 = loc("-1|unknown|unknown|-1|aten__add")
#loc7 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc8 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:09.595 (   1.581s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module shlo_compiler:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<4x768x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<4x32x1x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<6.556510e-06> : tensor<4x32xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.transpose %arg2, dims = [0, 2, 1, 3, 4] {result_layout = dense<[4, 3, 1, 2, 0]> : tensor<5xindex>, xla_shape = "bf16[1,4,768,60,106]{4,3,1,2,0}"} : (tensor<1x768x4x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    %1 = stablehlo.reshape %0 : (tensor<1x4x768x60x106xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc3)
    %2 = stablehlo.reduce(%1 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc4)
    %3 = stablehlo.multiply %2, %cst_0 : tensor<4x32xbf16> loc(#loc4)
    %4 = stablehlo.broadcast_in_dim %3, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc5)
    %5 = stablehlo.subtract %1, %4 : tensor<4x32x24x6360xbf16> loc(#loc5)
    %6 = stablehlo.multiply %5, %5 : tensor<4x32x24x6360xbf16> loc(#loc4)
    %7 = stablehlo.reduce(%6 init: %cst_1) applies stablehlo.add across dimensions = [2, 3] : (tensor<4x32x24x6360xbf16>, tensor<bf16>) -> tensor<4x32xbf16> loc(#loc4)
    %8 = stablehlo.multiply %7, %cst_0 : tensor<4x32xbf16> loc(#loc4)
    %9 = stablehlo.reshape %8 : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc4)
    %10 = stablehlo.add %9, %cst : tensor<4x32x1x1xbf16> loc(#loc6)
    %11 = stablehlo.rsqrt %10 : tensor<4x32x1x1xbf16> loc(#loc7)
    %12 = stablehlo.reshape %11 : (tensor<4x32x1x1xbf16>) -> tensor<4x32xbf16> loc(#loc8)
    %13 = stablehlo.broadcast_in_dim %12, dims = [0, 1] : (tensor<4x32xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc8)
    %14 = stablehlo.multiply %5, %13 : tensor<4x32x24x6360xbf16> loc(#loc8)
    %15 = stablehlo.reshape %14 : (tensor<4x32x24x6360xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc3)
    %16 = stablehlo.reshape %arg1 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %17 = stablehlo.reshape %16 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
    %18 = stablehlo.broadcast_in_dim %17, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc8)
    %19 = stablehlo.multiply %15, %18 : tensor<4x768x60x106xbf16> loc(#loc8)
    %20 = stablehlo.reshape %arg0 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
    %21 = stablehlo.reshape %20 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
    %22 = stablehlo.broadcast_in_dim %21, dims = [1] : (tensor<768xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc6)
    %23 = stablehlo.add %19, %22 : tensor<4x768x60x106xbf16> loc(#loc6)
    return %23 : tensor<4x768x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc3 = loc("-1|unknown|unknown|-1|aten__view")
#loc4 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc5 = loc("-1|unknown|unknown|-1|aten__sub")
#loc6 = loc("-1|unknown|unknown|-1|aten__add")
#loc7 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc8 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:09.598 (   1.584s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module ttir:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
      func.func @main(%arg0: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<4x768x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<1.001360e-05> : tensor<4x32x1x1xbf16>}> : () -> tensor<4x32x1x1xbf16> loc(#loc)
        %1 = "ttir.constant"() <{value = dense<6.556510e-06> : tensor<4x32xbf16>}> : () -> tensor<4x32xbf16> loc(#loc)
        %2 = "ttir.permute"(%arg2) <{permutation = array<i64: 0, 2, 1, 3, 4>}> : (tensor<1x768x4x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
        %3 = "ttir.reshape"(%2) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<1x4x768x60x106xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc3)
        %4 = "ttir.sum"(%3) <{dim_arg = [2 : i32, 3 : i32], keep_dim = false}> : (tensor<4x32x24x6360xbf16>) -> tensor<4x32xbf16> loc(#loc4)
        %5 = "ttir.multiply"(%4, %1) : (tensor<4x32xbf16>, tensor<4x32xbf16>) -> tensor<4x32xbf16> loc(#loc4)
        %6 = "ttir.reshape"(%5) <{shape = [4 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc5)
        %7 = "ttir.broadcast"(%6) <{broadcast_dimensions = array<i64: 1, 1, 24, 6360>}> : (tensor<4x32x1x1xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc5)
        %8 = "ttir.subtract"(%3, %7) : (tensor<4x32x24x6360xbf16>, tensor<4x32x24x6360xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc5)
        %9 = "ttir.multiply"(%8, %8) : (tensor<4x32x24x6360xbf16>, tensor<4x32x24x6360xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc4)
        %10 = "ttir.sum"(%9) <{dim_arg = [2 : i32, 3 : i32], keep_dim = false}> : (tensor<4x32x24x6360xbf16>) -> tensor<4x32xbf16> loc(#loc4)
        %11 = "ttir.multiply"(%10, %1) : (tensor<4x32xbf16>, tensor<4x32xbf16>) -> tensor<4x32xbf16> loc(#loc4)
        %12 = "ttir.reshape"(%11) <{shape = [4 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc4)
        %13 = "ttir.add"(%12, %0) : (tensor<4x32x1x1xbf16>, tensor<4x32x1x1xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc6)
        %14 = "ttir.rsqrt"(%13) : (tensor<4x32x1x1xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc7)
        %15 = "ttir.reshape"(%14) <{shape = [4 : i32, 32 : i32]}> : (tensor<4x32x1x1xbf16>) -> tensor<4x32xbf16> loc(#loc8)
        %16 = "ttir.reshape"(%15) <{shape = [4 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<4x32xbf16>) -> tensor<4x32x1x1xbf16> loc(#loc8)
        %17 = "ttir.broadcast"(%16) <{broadcast_dimensions = array<i64: 1, 1, 24, 6360>}> : (tensor<4x32x1x1xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc8)
        %18 = "ttir.multiply"(%8, %17) : (tensor<4x32x24x6360xbf16>, tensor<4x32x24x6360xbf16>) -> tensor<4x32x24x6360xbf16> loc(#loc8)
        %19 = "ttir.reshape"(%18) <{shape = [4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x32x24x6360xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc3)
        %20 = "ttir.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
        %21 = "ttir.reshape"(%20) <{shape = [768 : i32]}> : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
        %22 = "ttir.reshape"(%21) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16>) -> tensor<1x768x1x1xbf16> loc(#loc8)
        %23 = "ttir.broadcast"(%22) <{broadcast_dimensions = array<i64: 4, 1, 60, 106>}> : (tensor<1x768x1x1xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc8)
        %24 = "ttir.multiply"(%19, %23) : (tensor<4x768x60x106xbf16>, tensor<4x768x60x106xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc8)
        %25 = "ttir.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 768 : i32]}> : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc3)
        %26 = "ttir.reshape"(%25) <{shape = [768 : i32]}> : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc3)
        %27 = "ttir.reshape"(%26) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16>) -> tensor<1x768x1x1xbf16> loc(#loc6)
        %28 = "ttir.broadcast"(%27) <{broadcast_dimensions = array<i64: 4, 1, 60, 106>}> : (tensor<1x768x1x1xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc6)
        %29 = "ttir.add"(%24, %28) : (tensor<4x768x60x106xbf16>, tensor<4x768x60x106xbf16>) -> tensor<4x768x60x106xbf16> loc(#loc6)
        return %29 : tensor<4x768x60x106xbf16> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc3 = loc("-1|unknown|unknown|-1|aten__view")
#loc4 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc5 = loc("-1|unknown|unknown|-1|aten__sub")
#loc6 = loc("-1|unknown|unknown|-1|aten__add")
#loc7 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc8 = loc("-1|unknown|unknown|-1|aten__mul")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:09.600 (   1.586s) [        17B1B000]      module_builder.cc:766   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2026-01-12 10:22:09.600 (   1.586s) [        17B1B000]      module_builder.cc:780   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2026-01-12 10:22:09.600 (   1.586s) [        17B1B000]      module_builder.cc:790   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2026-01-12 10:22:09.608 (   1.594s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module ttnn:
#dram = #ttnn.buffer_type<dram>
#loc = loc(unknown)
#loc3 = loc("-1|unknown|unknown|-1|xla__device_data")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102656, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073125888, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 184320 + d1 * 240 + d2 * 60 + d3, d4), <1x1>, memref<184320x106xbf16, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.104 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func private @main_const_eval_0() -> tensor<4x32x1x1xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 1.00135803E-5 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<4x32x1x1>}> : (!ttnn.device) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc)
        return %1 : tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_1(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<768xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<768xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn_layout3>) -> tensor<768xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn_layout4>) -> tensor<1x768x1x1xbf16, #ttnn_layout5> loc(#loc1)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn_layout4>) -> () loc(#loc1)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<4x1x60x106>}> : (tensor<1x768x1x1xbf16, #ttnn_layout5>) -> tensor<4x768x60x106xbf16, #ttnn_layout6> loc(#loc1)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1xbf16, #ttnn_layout5>) -> () loc(#loc1)
        %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn_layout6>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc1)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn_layout6>) -> () loc(#loc1)
        return %5 : tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_2(%arg0: tensor<768xbf16, #ttnn_layout1> loc(unknown)) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<768xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<768xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn_layout3>) -> tensor<768xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn_layout4>) -> tensor<1x768x1x1xbf16, #ttnn_layout5> loc(#loc2)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn_layout4>) -> () loc(#loc2)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<4x1x60x106>}> : (tensor<1x768x1x1xbf16, #ttnn_layout5>) -> tensor<4x768x60x106xbf16, #ttnn_layout6> loc(#loc2)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1xbf16, #ttnn_layout5>) -> () loc(#loc2)
        %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn_layout6>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc1)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn_layout6>) -> () loc(#loc1)
        return %5 : tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_bias"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg1: tensor<768xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___norm_layer_weight"} loc("-1|unknown|unknown|-1|xla__device_data"), %arg2: tensor<1x768x4x60x106xbf16, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<4x768x60x106xbf16, #ttnn_layout6> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg0]) : (tensor<768xbf16, #ttnn_layout1>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<768xbf16, #ttnn_layout1>) -> () loc(#loc)
        %3 = "ttnn.to_layout"(%arg2) <{layout = #ttnn.layout<tile>}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout7>) -> tensor<1x768x4x60x106xbf16, #ttnn_layout8> loc(#loc4)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout7>) -> () loc(#loc4)
        %4 = "ttnn.permute"(%3) <{permutation = array<i64: 0, 2, 1, 3, 4>}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout8>) -> tensor<1x4x768x60x106xbf16, #ttnn_layout9> loc(#loc5)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn_layout8>) -> () loc(#loc5)
        %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<1x4x768x60x106xbf16, #ttnn_layout9>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc6)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x4x768x60x106xbf16, #ttnn_layout9>) -> () loc(#loc6)
        %6 = "ttnn.mean"(%5) <{dim_arg = [2 : i32, 3 : i32], keep_dim = true}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc10)
        %7 = "ttnn.neg"(%6) : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc11)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc11)
        %8 = "ttnn.add"(%5, %7) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc8)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc8)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc8)
        %9 = "ttnn.multiply"(%8, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc7)
        %10 = "ttnn.mean"(%9) <{dim_arg = [2 : i32, 3 : i32], keep_dim = true}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc10)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc10)
        %11 = "ttnn.add"(%10, %0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x1x1xbf16, #ttnn_layout>, tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc2)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc2)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc2)
        %12 = "ttnn.rsqrt"(%11) : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x1x1xbf16, #ttnn_layout> loc(#loc9)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc9)
        %13 = "ttnn.multiply"(%8, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x1x1xbf16, #ttnn_layout>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc1)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn_layout>) -> () loc(#loc1)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc1)
        %14 = "ttnn.multiply"(%13, %1) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc1)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc1)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc1)
        %15 = "ttnn.add"(%14, %2) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>, tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x32x24x6360xbf16, #ttnn_layout2> loc(#loc2)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc2)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc2)
        %16 = "ttnn.reshape"(%15) <{shape = [4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> tensor<4x768x60x106xbf16, #ttnn_layout6> loc(#loc2)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn_layout2>) -> () loc(#loc2)
        return %16 : tensor<4x768x60x106xbf16, #ttnn_layout6> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("-1|unknown|unknown|-1|aten__mul")
#loc2 = loc("-1|unknown|unknown|-1|aten__add")
#loc4 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute_in_0_layout")
#loc5 = loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
#loc6 = loc("-1|unknown|unknown|-1|aten__view")
#loc7 = loc("-1|unknown|unknown|-1|aten__var_mean")
#loc8 = loc("-1|unknown|unknown|-1|aten__sub")
#loc9 = loc("-1|unknown|unknown|-1|aten__rsqrt")
#loc10 = loc("-1|unknown|unknown|-1|aten__var_mean_mean"(#loc7))
#loc11 = loc("-1|unknown|unknown|-1|aten__sub_neg"(#loc8))
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:09.615 (   1.601s) [        17B1B000]loaded_executable_insta:290      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2026-01-12 10:22:09.615 (   1.601s) [        17B1B000]loaded_executable_insta:309      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2026-01-12 10:22:09.615 (   1.601s) [        17B1B000]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2026-01-12 10:22:09.615 (   1.601s) [        17B1B000]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2026-01-12 10:22:09.615 (   1.601s) [        17B1B000]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2026-01-12 10:22:09.615 (   1.601s) [        17B1B000]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2026-01-12 10:22:09.615 (   1.601s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:09.615 (   1.601s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:09.618 (   1.604s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:09.618 (   1.604s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:09.620 (   1.606s) [        9F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2026-01-12 10:22:09.620 (   1.606s) [        9F7FE640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:22:09.620 (   1.606s) [        9F7FE640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:22:09.620 (   1.606s) [        9F7FE640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:22:09.620 (   1.606s) [        9F7FE640] executable_instance.cc:140      1| ExecutableInstance::PJRT_Executable_NumOutputs
2026-01-12 10:22:09.620 (   1.606s) [        9F7FE640]loaded_executable_insta:345      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2026-01-12 10:22:09.620 (   1.606s) [        9F7FE640]flatbuffer_loaded_execu:260      1| FlatbufferLoadedExecutableInstance::Execute
2026-01-12 10:22:09.620 (   1.606s) [        9F7FE640]     client_instance.cc:416      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 1]
                 Always |     INFO | Device memory state before submit
                 Always |     INFO | Device DRAM memory state: MemoryView{numBanks: 12, totalBytesPerBank: 1024.000 MB, totalBytesAllocatedPerBank: 3.281 MB, totalBytesFreePerBank: 1020.719 MB, largestContiguousBytesFreePerBank: 1020.719 MB}
                 Always |     INFO | Device L1 memory state: MemoryView{numBanks: 64, totalBytesPerBank: 1.269 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 1.269 MB, largestContiguousBytesFreePerBank: 1.269 MB}
                 Always |     INFO | Device L1_SMALL memory state: MemoryView{numBanks: 64, totalBytesPerBank: 0.062 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.062 MB, largestContiguousBytesFreePerBank: 0.062 MB}
                 Always |     INFO | Device TRACE memory state: MemoryView{numBanks: 12, totalBytesPerBank: 0.000 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.000 MB, largestContiguousBytesFreePerBank: 0.000 MB}
            RuntimeTTNN |    DEBUG | Starting execution of program: main
            RuntimeTTNN |    DEBUG | Executing operation: %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
                 Always |    DEBUG | Running LoadCachedOp for function main_const_eval_0 with hash: 581d0f944c67f11eafc3d6069dccaf6118fb1aa46a3fb7a0d488a3aa11fca2ab
                 Always |    DEBUG | Cache miss or invalid cache for function: main_const_eval_0
            RuntimeTTNN |    DEBUG | Starting execution of program: main_const_eval_0
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 1.00135803E-5 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<4x32x1x1>}> : (!ttnn.device) -> tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Finished execution of program: main_const_eval_0
                 Always |    DEBUG | executed sub-func: main_const_eval_0
            RuntimeTTNN |    DEBUG | Executing operation: %1 = ttcore.load_cached(@main_const_eval_1, [%arg1]) : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
                 Always |    DEBUG | Running LoadCachedOp for function main_const_eval_1 with hash: dff445c572a562bb4de69ea6c0b14387f37ec626ee5c73782f490ca8242181f1
                 Always |    DEBUG | Cache miss or invalid cache for function: main_const_eval_1
            RuntimeTTNN |    DEBUG | Starting execution of program: main_const_eval_1
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<4x1x60x106>}> : (tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Finished execution of program: main_const_eval_1
                 Always |    DEBUG | executed sub-func: main_const_eval_1
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %2 = ttcore.load_cached(@main_const_eval_2, [%arg0]) : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
                 Always |    DEBUG | Running LoadCachedOp for function main_const_eval_2 with hash: dff445c572a562bb4de69ea6c0b14387f37ec626ee5c73782f490ca8242181f1
                 Always |    DEBUG | Cache miss or invalid cache for function: main_const_eval_2
            RuntimeTTNN |    DEBUG | Starting execution of program: main_const_eval_2
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
            RuntimeTTNN |    DEBUG | Executing operation: %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 768 : i32, 1 : i32, 1 : i32]}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x24x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<4x1x60x106>}> : (tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 32 + d2, d3), <1x1>, memref<768x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Finished execution of program: main_const_eval_2
                 Always |    DEBUG | executed sub-func: main_const_eval_2
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<768xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x768xbf16, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %3 = "ttnn.to_layout"(%arg2) <{layout = #ttnn.layout<tile>}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 184320 + d1 * 240 + d2 * 60 + d3, d4), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute_in_0_layout")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 184320 + d1 * 240 + d2 * 60 + d3, d4), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute_in_0_layout")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %4 = "ttnn.permute"(%3) <{permutation = array<i64: 0, 2, 1, 3, 4>}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("3|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
            RuntimeTTNN |    DEBUG | Executing operation: %5 = "ttnn.reshape"(%4) <{shape = [4 : i32, 32 : i32, 24 : i32, 6360 : i32]}> : (tensor<1x4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__view")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__view")
            RuntimeTTNN |    DEBUG | Executing operation: %6 = "ttnn.mean"(%5) <{dim_arg = [2 : i32, 3 : i32], keep_dim = true}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__var_mean_mean"("-1|unknown|unknown|-1|aten__var_mean"))
            RuntimeTTNN |    DEBUG | Executing operation: %7 = "ttnn.neg"(%6) : (tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__sub_neg"("-1|unknown|unknown|-1|aten__sub"))
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%6) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__sub_neg"("-1|unknown|unknown|-1|aten__sub"))
            RuntimeTTNN |    DEBUG | Executing operation: %8 = "ttnn.add"(%5, %7) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__sub")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__sub")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__sub")
            RuntimeTTNN |    DEBUG | Executing operation: %9 = "ttnn.multiply"(%8, %8) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__var_mean")
            RuntimeTTNN |    DEBUG | Executing operation: %10 = "ttnn.mean"(%9) <{dim_arg = [2 : i32, 3 : i32], keep_dim = true}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__var_mean_mean"("-1|unknown|unknown|-1|aten__var_mean"))
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%9) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__var_mean_mean"("-1|unknown|unknown|-1|aten__var_mean"))
            RuntimeTTNN |    DEBUG | Executing operation: %11 = "ttnn.add"(%10, %0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%10) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%0) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__add")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %12 = "ttnn.rsqrt"(%11) : (tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__rsqrt")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%11) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__rsqrt")
            RuntimeTTNN |    DEBUG | Executing operation: %13 = "ttnn.multiply"(%8, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%12) <{force = false}> : (tensor<4x32x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: %14 = "ttnn.multiply"(%13, %1) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%13) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__mul")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %15 = "ttnn.add"(%14, %2) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%14) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__add")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %16 = "ttnn.reshape"(%15) <{shape = [4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%15) <{force = false}> : (tensor<4x32x24x6360xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<128x199x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__add")
            RuntimeTTNN |    DEBUG | Finished execution of program: main
                 Always |     INFO | Device memory state after submit
                 Always |     INFO | Device DRAM memory state: MemoryView{numBanks: 12, totalBytesPerBank: 1024.000 MB, totalBytesAllocatedPerBank: 29.018 MB, totalBytesFreePerBank: 994.982 MB, largestContiguousBytesFreePerBank: 986.590 MB}
                 Always |     INFO | Device L1 memory state: MemoryView{numBanks: 64, totalBytesPerBank: 1.269 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 1.269 MB, largestContiguousBytesFreePerBank: 1.269 MB}
                 Always |     INFO | Device L1_SMALL memory state: MemoryView{numBanks: 64, totalBytesPerBank: 0.062 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.062 MB, largestContiguousBytesFreePerBank: 0.062 MB}
                 Always |     INFO | Device TRACE memory state: MemoryView{numBanks: 12, totalBytesPerBank: 0.000 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.000 MB, largestContiguousBytesFreePerBank: 0.000 MB}
2026-01-12 10:22:12.772 (   4.758s) [        9F7FE640]flatbuffer_loaded_execu:195      1| Filled output at output_index 0 device_index 0 with shape [4, 768, 60, 106] and UID 5
2026-01-12 10:22:12.772 (   4.758s) [        9F7FE640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:22:12.772 (   4.758s) [        9F7FE640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:22:12.772 (   4.758s) [        9F7FE640]     buffer_instance.cc:552      1| BufferInstance::PJRT_Buffer_Dimensions
2026-01-12 10:22:12.772 (   4.758s) [        9F7FE640]     buffer_instance.cc:575      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2026-01-12 10:22:12.772 (   4.758s) [        9F7FE640]     buffer_instance.cc:552      1| BufferInstance::PJRT_Buffer_Dimensions
2026-01-12 10:22:12.772 (   4.758s) [        9F7FE640]     buffer_instance.cc:544      1| BufferInstance::PJRT_Buffer_ElementType
2026-01-12 10:22:12.772 (   4.758s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:12.772 (   4.758s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:12.774 (   4.760s) [        17B1B000]     client_instance.cc:687      1| ClientInstance::PJRT_Client_Compile
2026-01-12 10:22:12.774 (   4.760s) [        17B1B000]      module_builder.cc:211      1| ModuleBuilder::buildModule
2026-01-12 10:22:12.774 (   4.760s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module vhlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data")) -> (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) {
    %0 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1> loc(#loc2)
    %1 = "vhlo.custom_call_v1"(%0) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1> loc(#loc3)
    %2 = "vhlo.transpose_v1"(%1) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3, 4]> : tensor<5xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[4, 3, 1, 2, 0]> : tensor<5xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,768,4,60,106]{4,3,1,2,0}">} : (!vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1> loc(#loc4)
    "vhlo.return_v1"(%2) : (!vhlo.tensor_v1<1x768x4x60x106x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
#loc3 = loc("-1|unknown|unknown|-1|xla__custom_call")
#loc4 = loc("0|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.775 (   4.761s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module shlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<4x768x60x106xbf16> loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<1x768x4x60x106xbf16> {
    %0 = stablehlo.reshape %arg0 : (tensor<4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    %1 = stablehlo.custom_call @tt.mark_argument(%0) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc3)
    %2 = stablehlo.transpose %1, dims = [0, 2, 1, 3, 4] {result_layout = dense<[4, 3, 1, 2, 0]> : tensor<5xindex>, xla_shape = "bf16[1,768,4,60,106]{4,3,1,2,0}"} : (tensor<1x4x768x60x106xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc4)
    return %2 : tensor<1x768x4x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
#loc3 = loc("-1|unknown|unknown|-1|xla__custom_call")
#loc4 = loc("0|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.776 (   4.762s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module shlo_frontend:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<4x768x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<1x768x4x60x106xbf16> {
    %0 = stablehlo.reshape %arg0 : (tensor<4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    %1 = stablehlo.transpose %0, dims = [0, 2, 1, 3, 4] {result_layout = dense<[4, 3, 1, 2, 0]> : tensor<5xindex>, xla_shape = "bf16[1,768,4,60,106]{4,3,1,2,0}"} : (tensor<1x4x768x60x106xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
    return %1 : tensor<1x768x4x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
#loc3 = loc("0|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.778 (   4.764s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module shlo_compiler:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<4x768x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<1x768x4x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = stablehlo.reshape %arg0 : (tensor<4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    %1 = stablehlo.transpose %0, dims = [0, 2, 1, 3, 4] {result_layout = dense<[4, 3, 1, 2, 0]> : tensor<5xindex>, xla_shape = "bf16[1,768,4,60,106]{4,3,1,2,0}"} : (tensor<1x4x768x60x106xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
    return %1 : tensor<1x768x4x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
#loc3 = loc("0|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.779 (   4.765s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module ttir:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
      func.func @main(%arg0: tensor<4x768x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<1x768x4x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.reshape"(%arg0) <{shape = [1 : i32, 4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
        %1 = "ttir.permute"(%0) <{permutation = array<i64: 0, 2, 1, 3, 4>}> : (tensor<1x4x768x60x106xbf16>) -> tensor<1x768x4x60x106xbf16> loc(#loc3)
        return %1 : tensor<1x768x4x60x106xbf16> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
#loc3 = loc("0|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.780 (   4.765s) [        17B1B000]      module_builder.cc:766   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2026-01-12 10:22:12.780 (   4.765s) [        17B1B000]      module_builder.cc:780   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2026-01-12 10:22:12.780 (   4.765s) [        17B1B000]      module_builder.cc:790   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2026-01-12 10:22:12.783 (   4.769s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module ttnn:
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102656, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073125888, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 46080 + d1 * 60 + d2, d3), <1x1>, memref<184320x106xbf16, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main(%arg0: tensor<4x768x60x106xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<1x768x4x60x106xbf16, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.to_layout"(%arg0) <{layout = #ttnn.layout<tile>}> : (tensor<4x768x60x106xbf16, #ttnn_layout>) -> tensor<4x768x60x106xbf16, #ttnn_layout2> loc(#loc2)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn_layout>) -> () loc(#loc2)
        %1 = "ttnn.reshape"(%0) <{shape = [1 : i32, 4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn_layout2>) -> tensor<1x4x768x60x106xbf16, #ttnn_layout3> loc(#loc3)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn_layout2>) -> () loc(#loc3)
        %2 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 1, 3, 4>}> : (tensor<1x4x768x60x106xbf16, #ttnn_layout3>) -> tensor<1x768x4x60x106xbf16, #ttnn_layout1> loc(#loc4)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x768x60x106xbf16, #ttnn_layout3>) -> () loc(#loc4)
        return %2 : tensor<1x768x4x60x106xbf16, #ttnn_layout1> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view_in_0_layout")
#loc3 = loc("-1|unknown|unknown|-1|aten__view")
#loc4 = loc("0|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.785 (   4.771s) [        17B1B000]loaded_executable_insta:290      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2026-01-12 10:22:12.785 (   4.771s) [        17B1B000]loaded_executable_insta:309      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2026-01-12 10:22:12.785 (   4.771s) [        17B1B000]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2026-01-12 10:22:12.785 (   4.771s) [        17B1B000]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2026-01-12 10:22:12.785 (   4.771s) [        17B1B000]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2026-01-12 10:22:12.785 (   4.771s) [        17B1B000]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2026-01-12 10:22:12.785 (   4.771s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:12.785 (   4.771s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:12.787 (   4.772s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:12.787 (   4.772s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:12.788 (   4.774s) [        9F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2026-01-12 10:22:12.788 (   4.774s) [        9F7FE640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:22:12.788 (   4.774s) [        9F7FE640] executable_instance.cc:140      1| ExecutableInstance::PJRT_Executable_NumOutputs
2026-01-12 10:22:12.788 (   4.774s) [        9F7FE640]loaded_executable_insta:345      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2026-01-12 10:22:12.788 (   4.774s) [        9F7FE640]flatbuffer_loaded_execu:260      1| FlatbufferLoadedExecutableInstance::Execute
2026-01-12 10:22:12.788 (   4.774s) [        9F7FE640]     client_instance.cc:416      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 1]
2026-01-12 10:22:12.788 (   4.774s) [        9F7FE640]flatbuffer_loaded_execu:108      1| Re-laying out already prepared input tensor for argument index 0 with shape [4, 768, 60, 106].
2026-01-12 10:22:12.792 (   4.778s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:12.792 (   4.778s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:12.792 (   4.778s) [        17B1B000]     buffer_instance.cc:637      1| BufferInstance::PJRT_Buffer_IsDeleted
2026-01-12 10:22:12.794 (   4.780s) [        17B1B000]     client_instance.cc:687      1| ClientInstance::PJRT_Client_Compile
2026-01-12 10:22:12.794 (   4.780s) [        17B1B000]      module_builder.cc:211      1| ModuleBuilder::buildModule
2026-01-12 10:22:12.794 (   4.780s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module vhlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.5 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|xla__device_data")) -> (!vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1>) {
    %0 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1> loc(#loc2)
    %1 = "vhlo.custom_call_v1"(%0) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1> loc(#loc3)
    "vhlo.return_v1"(%1) : (!vhlo.tensor_v1<1x4x768x60x106x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
#loc3 = loc("-1|unknown|unknown|-1|xla__custom_call")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.795 (   4.780s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module shlo:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.5 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<4x768x60x106xbf16> loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<1x4x768x60x106xbf16> {
    %0 = stablehlo.reshape %arg0 : (tensor<4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    %1 = stablehlo.custom_call @tt.mark_argument(%0) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc3)
    return %1 : tensor<1x4x768x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
#loc3 = loc("-1|unknown|unknown|-1|xla__custom_call")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.795 (   4.781s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module shlo_frontend:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.5 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<4x768x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> tensor<1x4x768x60x106xbf16> {
    %0 = stablehlo.reshape %arg0 : (tensor<4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    return %0 : tensor<1x4x768x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.796 (   4.782s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module shlo_compiler:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.5 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<4x768x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<1x4x768x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = stablehlo.reshape %arg0 : (tensor<4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
    return %0 : tensor<1x4x768x60x106xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.797 (   4.783s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module ttir:
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
module @SyncTensorsGraph.5 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.5 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
      func.func @main(%arg0: tensor<4x768x60x106xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<1x4x768x60x106xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.reshape"(%arg0) <{shape = [1 : i32, 4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x768x60x106xbf16>) -> tensor<1x4x768x60x106xbf16> loc(#loc2)
        return %0 : tensor<1x4x768x60x106xbf16> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.798 (   4.784s) [        17B1B000]      module_builder.cc:766   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2026-01-12 10:22:12.798 (   4.784s) [        17B1B000]      module_builder.cc:780   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2026-01-12 10:22:12.798 (   4.784s) [        17B1B000]      module_builder.cc:790   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
2026-01-12 10:22:12.801 (   4.787s) [        17B1B000]      module_builder.cc:1006     1| MLIR Module ttnn:
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("-1|unknown|unknown|-1|xla__device_data")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102656, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073125888, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 46080 + d1 * 60 + d2, d3), <1x1>, memref<184320x106xbf16, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.5 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.5 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main(%arg0: tensor<4x768x60x106xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|xla__device_data")) -> (tensor<1x4x768x60x106xbf16, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.to_layout"(%arg0) <{layout = #ttnn.layout<tile>}> : (tensor<4x768x60x106xbf16, #ttnn_layout>) -> tensor<4x768x60x106xbf16, #ttnn_layout2> loc(#loc2)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn_layout>) -> () loc(#loc2)
        %1 = "ttnn.reshape"(%0) <{shape = [1 : i32, 4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn_layout2>) -> tensor<1x4x768x60x106xbf16, #ttnn_layout1> loc(#loc3)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn_layout2>) -> () loc(#loc3)
        return %1 : tensor<1x4x768x60x106xbf16, #ttnn_layout1> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|aten__view_in_0_layout")
#loc3 = loc("-1|unknown|unknown|-1|aten__view")
------------------ END OF MLIR MODULE ------------------
2026-01-12 10:22:12.803 (   4.789s) [        17B1B000]loaded_executable_insta:290      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2026-01-12 10:22:12.803 (   4.789s) [        17B1B000]loaded_executable_insta:309      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2026-01-12 10:22:12.803 (   4.789s) [        17B1B000]              stubs.inc:70    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2026-01-12 10:22:12.803 (   4.789s) [        17B1B000]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2026-01-12 10:22:12.803 (   4.789s) [        17B1B000]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2026-01-12 10:22:12.803 (   4.789s) [        17B1B000]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2026-01-12 10:22:12.803 (   4.789s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:12.803 (   4.789s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:12.804 (   4.790s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2026-01-12 10:22:12.804 (   4.790s) [        17B1B000] executable_instance.cc:108      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
                 Always |     INFO | Device memory state before submit
                 Always |     INFO | Device DRAM memory state: MemoryView{numBanks: 12, totalBytesPerBank: 1024.000 MB, totalBytesAllocatedPerBank: 28.301 MB, totalBytesFreePerBank: 995.699 MB, largestContiguousBytesFreePerBank: 986.588 MB}
                 Always |     INFO | Device L1 memory state: MemoryView{numBanks: 64, totalBytesPerBank: 1.269 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 1.269 MB, largestContiguousBytesFreePerBank: 1.269 MB}
                 Always |     INFO | Device L1_SMALL memory state: MemoryView{numBanks: 64, totalBytesPerBank: 0.062 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.062 MB, largestContiguousBytesFreePerBank: 0.062 MB}
                 Always |     INFO | Device TRACE memory state: MemoryView{numBanks: 12, totalBytesPerBank: 0.000 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.000 MB, largestContiguousBytesFreePerBank: 0.000 MB}
            RuntimeTTNN |    DEBUG | Starting execution of program: main
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.to_layout"(%arg0) <{layout = #ttnn.layout<tile>}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 46080 + d1 * 60 + d2, d3), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__view_in_0_layout")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 46080 + d1 * 60 + d2, d3), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__view_in_0_layout")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.reshape"(%0) <{shape = [1 : i32, 4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__view")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%0) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__view")
            RuntimeTTNN |    DEBUG | Executing operation: %2 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 1, 3, 4>}> : (tensor<1x4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x768x4x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 256 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("0|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("0|/localdev/vkovinic/tt-xla/python_package/tt_torch/torch_overrides.py:9|__torch_function__|22|aten__permute")
            RuntimeTTNN |    DEBUG | Finished execution of program: main
                 Always |     INFO | Device memory state after submit
                 Always |     INFO | Device DRAM memory state: MemoryView{numBanks: 12, totalBytesPerBank: 1024.000 MB, totalBytesAllocatedPerBank: 32.303 MB, totalBytesFreePerBank: 991.697 MB, largestContiguousBytesFreePerBank: 986.586 MB}
                 Always |     INFO | Device L1 memory state: MemoryView{numBanks: 64, totalBytesPerBank: 1.269 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 1.269 MB, largestContiguousBytesFreePerBank: 1.269 MB}
                 Always |     INFO | Device L1_SMALL memory state: MemoryView{numBanks: 64, totalBytesPerBank: 0.062 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.062 MB, largestContiguousBytesFreePerBank: 0.062 MB}
                 Always |     INFO | Device TRACE memory state: MemoryView{numBanks: 12, totalBytesPerBank: 0.000 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.000 MB, largestContiguousBytesFreePerBank: 0.000 MB}
2026-01-12 10:22:12.843 (   4.828s) [        9F7FE640]flatbuffer_loaded_execu:195      1| Filled output at output_index 0 device_index 0 with shape [1, 768, 4, 60, 106] and UID 6
2026-01-12 10:22:12.843 (   4.828s) [        9F7FE640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:22:12.843 (   4.828s) [        9F7FE640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:22:12.843 (   4.828s) [        9F7FE640]     buffer_instance.cc:552      1| BufferInstance::PJRT_Buffer_Dimensions
2026-01-12 10:22:12.843 (   4.828s) [        9F7FE640]     buffer_instance.cc:575      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2026-01-12 10:22:12.843 (   4.828s) [        9F7FE640]     buffer_instance.cc:552      1| BufferInstance::PJRT_Buffer_Dimensions
2026-01-12 10:22:12.843 (   4.828s) [        9F7FE640]     buffer_instance.cc:544      1| BufferInstance::PJRT_Buffer_ElementType
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]     buffer_instance.cc:689      1| BufferInstance::PJRT_Buffer_Device
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640] executable_instance.cc:140      1| ExecutableInstance::PJRT_Executable_NumOutputs
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]loaded_executable_insta:345      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]flatbuffer_loaded_execu:260      1| FlatbufferLoadedExecutableInstance::Execute
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]     client_instance.cc:416      1| ClientInstance::getOrCreateMeshDevice - reusing already opened mesh device [1, 1]
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]flatbuffer_loaded_execu:92       1| Reusing already prepared input tensor for argument index 0 with shape [4, 768, 60, 106]
                 Always |     INFO | Device memory state before submit
                 Always |     INFO | Device DRAM memory state: MemoryView{numBanks: 12, totalBytesPerBank: 1024.000 MB, totalBytesAllocatedPerBank: 32.303 MB, totalBytesFreePerBank: 991.697 MB, largestContiguousBytesFreePerBank: 986.586 MB}
                 Always |     INFO | Device L1 memory state: MemoryView{numBanks: 64, totalBytesPerBank: 1.269 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 1.269 MB, largestContiguousBytesFreePerBank: 1.269 MB}
                 Always |     INFO | Device L1_SMALL memory state: MemoryView{numBanks: 64, totalBytesPerBank: 0.062 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.062 MB, largestContiguousBytesFreePerBank: 0.062 MB}
                 Always |     INFO | Device TRACE memory state: MemoryView{numBanks: 12, totalBytesPerBank: 0.000 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.000 MB, largestContiguousBytesFreePerBank: 0.000 MB}
 SUCCESS! Output shape: torch.Size([1, 768, 4, 60, 106])
            RuntimeTTNN |    DEBUG | Starting execution of program: main
            RuntimeTTNN |    DEBUG | Executing operation: %0 = "ttnn.to_layout"(%arg0) <{layout = #ttnn.layout<tile>}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 46080 + d1 * 60 + d2, d3), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__view_in_0_layout")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 46080 + d1 * 60 + d2, d3), <1x1>, memref<184320x106xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__view_in_0_layout")
                 Always |    DEBUG | Tensor is retained thus not deallocating. To deallocate, set retain to false first
            RuntimeTTNN |    DEBUG | Executing operation: %1 = "ttnn.reshape"(%0) <{shape = [1 : i32, 4 : i32, 768 : i32, 60 : i32, 106 : i32]}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 196608 + d1 * 49152 + d2 * 64 + d3, d4), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("-1|unknown|unknown|-1|aten__view")
            RuntimeTTNN |    DEBUG | Executing operation: "ttnn.deallocate"(%0) <{force = false}> : (tensor<4x768x60x106xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49152 + d1 * 64 + d2, d3), <1x1>, memref<6144x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("-1|unknown|unknown|-1|aten__view")
            RuntimeTTNN |    DEBUG | Finished execution of program: main
                 Always |     INFO | Device memory state after submit
                 Always |     INFO | Device DRAM memory state: MemoryView{numBanks: 12, totalBytesPerBank: 1024.000 MB, totalBytesAllocatedPerBank: 36.303 MB, totalBytesFreePerBank: 987.697 MB, largestContiguousBytesFreePerBank: 986.586 MB}
                 Always |     INFO | Device L1 memory state: MemoryView{numBanks: 64, totalBytesPerBank: 1.269 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 1.269 MB, largestContiguousBytesFreePerBank: 1.269 MB}
                 Always |     INFO | Device L1_SMALL memory state: MemoryView{numBanks: 64, totalBytesPerBank: 0.062 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.062 MB, largestContiguousBytesFreePerBank: 0.062 MB}
                 Always |     INFO | Device TRACE memory state: MemoryView{numBanks: 12, totalBytesPerBank: 0.000 MB, totalBytesAllocatedPerBank: 0.000 MB, totalBytesFreePerBank: 0.000 MB, largestContiguousBytesFreePerBank: 0.000 MB}
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]flatbuffer_loaded_execu:195      1| Filled output at output_index 0 device_index 0 with shape [1, 4, 768, 60, 106] and UID 7
2026-01-12 10:22:12.843 (   4.829s) [        17B1B000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]     buffer_instance.cc:552      1| BufferInstance::PJRT_Buffer_Dimensions
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]     buffer_instance.cc:575      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]     buffer_instance.cc:552      1| BufferInstance::PJRT_Buffer_Dimensions
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]     buffer_instance.cc:544      1| BufferInstance::PJRT_Buffer_ElementType
2026-01-12 10:22:12.843 (   4.829s) [        9F7FE640]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:22:12.843 (   4.829s) [        17B1B000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:22:12.843 (   4.829s) [        17B1B000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:22:12.844 (   4.829s) [        17B1B000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:22:12.844 (   4.829s) [        17B1B000]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2026-01-12 10:22:12.956 (   4.942s) [        17B1B000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:22:12.956 (   4.942s) [        17B1B000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:22:12.956 (   4.942s) [        17B1B000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:22:12.956 (   4.942s) [        17B1B000]     buffer_instance.cc:536      1| BufferInstance::PJRT_Buffer_Destroy
2026-01-12 10:22:13.343 (   5.329s) [        17B1B000]     client_instance.cc:192      1| ClientInstance::~ClientInstance
2026-01-12 10:22:13.343 (   5.329s) [        17B1B000]     client_instance.cc:499      1| Closing parent mesh.