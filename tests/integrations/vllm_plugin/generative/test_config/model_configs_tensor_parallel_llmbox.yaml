# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

# vLLM generative models that OOM on single-device; run in tensor parallel on llmbox.
# Defaults are applied in test_config/__init__.py (DEFAULT_TENSOR_PARALLEL_LLMBOX_CONFIG).
# Only "model" is required. "marks" default to [tensor_parallel]; "status" defaults to "unspecified".

model_configs:
  mistral-7b:
    model: mistralai/Mistral-7B-v0.1
    prompts:
    - "The future of AI is"
    status: "expected_passing"
  falcon-7b:
    model: tiiuae/falcon-7b
    prompts:
    - "The best way to learn"
    status: "known_failure_xfail"
    reason: "'FalconForCausalLM' object has no attribute 'model'"
  phi-4:
    model: microsoft/phi-4
    status: "known_failure_xfail"
    reason: "loc(reshape.175): error: number of output elements (40960) doesn't match expected number of elements (5120)"
  llama-3.1-8b:
    model: meta-llama/Llama-3.1-8B
    additional_config:
      enable_const_eval: true
      min_context_len: 32
      enable_tensor_parallel: true
      experimental_enable_weight_bfp8_conversion: true
    prompts:
    - "I like taking walks in the"
    status: "expected_passing"
  llama-3.1-8b-instruct:
    model: meta-llama/Llama-3.1-8B-Instruct
    additional_config:
      enable_const_eval: true
      min_context_len: 32
      enable_tensor_parallel: true
      experimental_enable_weight_bfp8_conversion: true
    prompts:
    - "I like taking walks in the"
    status: "expected_passing"
  llama-3-8b:
    model: meta-llama/Meta-Llama-3-8B
    additional_config:
      enable_const_eval: true
      min_context_len: 32
      enable_tensor_parallel: true
      experimental_enable_weight_bfp8_conversion: true
    prompts:
    - "I like taking walks in the"
    status: "expected_passing"
  llama-3-8b-instruct:
    model: meta-llama/Meta-Llama-3-8B-Instruct
    additional_config:
      enable_const_eval: true
      min_context_len: 32
      enable_tensor_parallel: true
      experimental_enable_weight_bfp8_conversion: true
    prompts:
    - "I like taking walks in the"
    status: "expected_passing"
  qwen2.5-14b-instruct:
    model: Qwen/Qwen2.5-14B-Instruct
    prompts:
    - "The capital of China is"
    status: "known_failure_xfail"
    reason: 'TT_FATAL: Input rank 3 and begins 2 must have the same size'
  qwen3-14b:
    model: Qwen/Qwen3-14B
    status: "expected_passing"
  qwen3-8b:
    model: Qwen/Qwen3-8B
    status: "expected_passing"
  falcon-7b-instruct:
    model: tiiuae/falcon-7b-instruct
    prompts:
    - "The best way to learn"
    status: "known_failure_xfail"
    reason: "'FalconForCausalLM' object has no attribute 'model'"
  mistral-nemo-instruct-2407:
    model: mistralai/Mistral-Nemo-Instruct-2407
    prompts:
    - The future of AI is
    status: "expected_passing"
  allam-7b-instruct:
    model: ALLaM-AI/ALLaM-7B-Instruct-preview
    status: "known_failure_xfail"
    reason: "Model hangs"
  falcon3-7b-base:
    model: tiiuae/Falcon3-7B-Base
    status: "known_failure_xfail"
    reason: "loc('reshape.151'): error: Could not apply propagated tensor shardings to tensor dimensions"
  falcon3-10b-base:
    model: tiiuae/Falcon3-10B-Base
    status: "known_failure_xfail"
    reason: "loc('reshape.175'): error: Could not apply propagated tensor shardings to tensor dimensions"
  huggyllama-llama-7b:
    model: huggyllama/llama-7b
    status: "known_failure_xfail"
    reason: "Model hangs"
  mistral-7b-instruct-v0.3:
    model: mistralai/Mistral-7B-Instruct-v0.3
    status: "expected_passing"
  vllm_sweep_aquila:
    model: BAAI/Aquila-7B
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_aquila_chat:
    model: BAAI/AquilaChat-7B
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_baichuan:
    model: baichuan-inc/Baichuan-7B
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_deepseek:
    model: deepseek-ai/deepseek-llm-7b-chat
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_chat_glm2:
    model: THUDM/chatglm2-6b
    status: "known_failure_xfail"
    reason: "'ChatGLMForCausalLM' object has no attribute 'model'"
  vllm_sweep_chat_glm3:
    model: THUDM/chatglm3-6b
    status: "known_failure_xfail"
    reason: "'ChatGLMForCausalLM' object has no attribute 'model'"
  vllm_sweep_glm:
    model: THUDM/glm-4-9b-chat-hf
    status: "known_failure_xfail"
    reason: "loc(`reshape.202`): error: Could not apply propagated tensor shardings to tensor dimensions"
  vllm_sweep_internlm:
    model: internlm/internlm-7b
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_internlm2:
    model: internlm/internlm2-7b
    status: "known_failure_xfail"
    reason: "InternLM2ForCausalLM.forward() missing 1 required positional argument: 'intermediate_tensors'"
  vllm_sweep_internlm3:
    model: internlm/internlm3-8b-instruct
    status: "known_failure_xfail"
    reason: "loc('reshape.191'): error: Could not apply propagated tensor shardings to tensor dimensions"
  vllm_sweep_olmo2:
    model: allenai/OLMo2-7B-1124
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_qwen:
    model: Qwen/Qwen-7B
    status: "known_failure_xfail"
    reason: "'QWenLMHeadModel' object has no attribute 'model'"
  vllm_sweep_solar:
    model: upstage/solar-pro-preview-instruct
    status: "known_failure_xfail"
    reason: "loc('reshape.235'): error: number of output elements (40960) doesn't match expected number of elements (5120)"
