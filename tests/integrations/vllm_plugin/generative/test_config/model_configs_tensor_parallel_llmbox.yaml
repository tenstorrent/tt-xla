# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

# vLLM generative models that OOM on single-device; run in tensor parallel on llmbox.
# Defaults are applied in test_config/__init__.py (DEFAULT_TENSOR_PARALLEL_LLMBOX_CONFIG).
# Only "model" is required. "marks" default to [tensor_parallel]; "status" defaults to "unspecified".

model_configs:
  mistral-7b:
    model: mistralai/Mistral-7B-v0.1
    prompts:
    - "The future of AI is"
    status: "expected_passing"
  falcon-7b:
    model: tiiuae/falcon-7b
    prompts:
    - "The best way to learn"
    status: "known_failure_xfail"
    reason: "'FalconForCausalLM' object has no attribute 'model'"
  phi-4:
    model: microsoft/phi-4
    status: "known_failure_xfail"
    reason: "loc(reshape.175): error: number of output elements (40960) doesn't match expected number of elements (5120)"
  llama-3.1-8b:
    model: meta-llama/Llama-3.1-8B
    additional_config:
      enable_const_eval: true
      min_context_len: 32
      enable_tensor_parallel: true
      experimental_enable_weight_bfp8_conversion: true
    prompts:
    - "I like taking walks in the"
    status: "expected_passing"
  llama-3.1-8b-instruct:
    model: meta-llama/Llama-3.1-8B-Instruct
    additional_config:
      enable_const_eval: true
      min_context_len: 32
      enable_tensor_parallel: true
      experimental_enable_weight_bfp8_conversion: true
    prompts:
    - "I like taking walks in the"
    status: "expected_passing"
  llama-3-8b:
    model: meta-llama/Meta-Llama-3-8B
    additional_config:
      enable_const_eval: true
      min_context_len: 32
      enable_tensor_parallel: true
      experimental_enable_weight_bfp8_conversion: true
    prompts:
    - "I like taking walks in the"
    status: "expected_passing"
  llama-3-8b-instruct:
    model: meta-llama/Meta-Llama-3-8B-Instruct
    additional_config:
      enable_const_eval: true
      min_context_len: 32
      enable_tensor_parallel: true
      experimental_enable_weight_bfp8_conversion: true
    prompts:
    - "I like taking walks in the"
    status: "expected_passing"
  qwen2.5-14b-instruct:
    model: Qwen/Qwen2.5-14B-Instruct
    prompts:
    - "The capital of China is"
    status: "known_failure_xfail"
    reason: 'TT_FATAL: Input rank 3 and begins 2 must have the same size'
  qwen3-14b:
    model: Qwen/Qwen3-14B
    status: "expected_passing"
  qwen3-8b:
    model: Qwen/Qwen3-8B
    status: "expected_passing"
  falcon-7b-instruct:
    model: tiiuae/falcon-7b-instruct
    prompts:
    - "The best way to learn"
    status: "known_failure_xfail"
    reason: "'FalconForCausalLM' object has no attribute 'model'"
  mistral-nemo-instruct-2407:
    model: mistralai/Mistral-Nemo-Instruct-2407
    prompts:
    - The future of AI is
    status: "expected_passing"
  allam-7b-instruct:
    model: ALLaM-AI/ALLaM-7B-Instruct-preview
    status: "known_failure_xfail"
    reason: "Model hangs"
  falcon3-7b-base:
    model: tiiuae/Falcon3-7B-Base
    status: "known_failure_xfail"
    reason: "loc('reshape.151'): error: Could not apply propagated tensor shardings to tensor dimensions"
  falcon3-10b-base:
    model: tiiuae/Falcon3-10B-Base
    status: "known_failure_xfail"
    reason: "loc('reshape.175'): error: Could not apply propagated tensor shardings to tensor dimensions"
  huggyllama-llama-7b:
    model: huggyllama/llama-7b
    status: "known_failure_xfail"
    reason: "Model hangs"
  mistral-7b-instruct-v0.3:
    model: mistralai/Mistral-7B-Instruct-v0.3
    status: "expected_passing"
  vllm_sweep_aquila:
    model: BAAI/Aquila-7B
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_aquila_chat:
    model: BAAI/AquilaChat-7B
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_baichuan:
    model: baichuan-inc/Baichuan-7B
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_deepseek:
    model: deepseek-ai/deepseek-llm-7b-chat
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_chat_glm2:
    model: THUDM/chatglm2-6b
    status: "known_failure_xfail"
    reason: "'ChatGLMForCausalLM' object has no attribute 'model'"
  vllm_sweep_chat_glm3:
    model: THUDM/chatglm3-6b
    status: "known_failure_xfail"
    reason: "'ChatGLMForCausalLM' object has no attribute 'model'"
  vllm_sweep_glm:
    model: THUDM/glm-4-9b-chat-hf
    status: "known_failure_xfail"
    reason: "loc(`reshape.202`): error: Could not apply propagated tensor shardings to tensor dimensions"
  vllm_sweep_internlm:
    model: internlm/internlm-7b
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_internlm2:
    model: internlm/internlm2-7b
    status: "known_failure_xfail"
    reason: "InternLM2ForCausalLM.forward() missing 1 required positional argument: 'intermediate_tensors'"
  vllm_sweep_internlm3:
    model: internlm/internlm3-8b-instruct
    status: "known_failure_xfail"
    reason: "loc('reshape.191'): error: Could not apply propagated tensor shardings to tensor dimensions"
  vllm_sweep_olmo2:
    model: allenai/OLMo2-7B-1124
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_qwen:
    model: Qwen/Qwen-7B
    status: "known_failure_xfail"
    reason: "'QWenLMHeadModel' object has no attribute 'model'"
  vllm_sweep_solar:
    model: upstage/solar-pro-preview-instruct
    status: "known_failure_xfail"
    reason: "loc('reshape.235'): error: number of output elements (40960) doesn't match expected number of elements (5120)"
  vllm_sweep_apertus:
    model: swiss-ai/Apertus-8B-2509
  vllm_sweep_bamba:
    model: ibm-ai-platform/Bamba-9B
  vllm_sweep_ernie4_5_moe:
    model: baidu/ERNIE-4.5-21B-A3B-PT
  vllm_sweep_exaone4:
    model: LGAI-EXAONE/EXAONE-4.0-32B
  vllm_sweep_falcon_h1:
    model: tiiuae/Falcon-H1-34B-Base
  vllm_sweep_flex_olmo:
    model: allenai/FlexOlmo-7x7B-1T
  vllm_sweep_glm4:
    model: zai-org/GLM-4-32B-0414
  vllm_sweep_gpt_oss:
    model: openai/gpt-oss-20b
  vllm_sweep_hunyuan_moe:
    model: tencent/Hunyuan-A13B-Instruct
  vllm_sweep_jais:
    model: inceptionai/jais-13b
    max_model_len: 64
    max_num_batched_tokens: 64
  vllm_sweep_jais2:
    model: inceptionai/Jais-2-8B-Chat
  vllm_sweep_kimi_linear:
    model: moonshotai/Kimi-Linear-48B-A3B-Base
  vllm_sweep_lfm2_moe:
    model: LiquidAI/LFM2-8B-A1B-preview
  vllm_sweep_nemotron:
    model: nvidia/Minitron-8B-Base
  vllm_sweep_nemotron_h:
    model: nvidia/Nemotron-H-8B-Base-8K
  vllm_sweep_qwen3:
    model: Qwen/Qwen3-8B
  vllm_sweep_qwen3_moe:
    model: Qwen/Qwen3-30B-A3B
  vllm_sweep_qwen3_next:
    model: Qwen/Qwen3-Next-80B-A3B-Instruct
  vllm_sweep_seed_oss:
    model: ByteDance-Seed/Seed-OSS-36B-Instruct
  vllm_sweep_grok1:
    model: hpcai-tech/grok-1
  vllm_sweep_mimo:
    model: XiaomiMiMo/MiMo-7B-RL
  vllm_sweep_olmo3:
    model: allenai/Olmo-3-7B-Instruct
  vllm_sweep_glm4_moe:
    model: zai-org/GLM-4.5
  vllm_sweep_pangu_embedded:
    model: FreedomIntelligence/openPangu-Embedded-7B-V1.1
  # vllm_sweep_longcat_flash: # 560B parameter model
  #   model: meituan-longcat/LongCat-Flash-Chat
  # vllm_sweep_dots1: # 142B parameter model
  #   model: rednote-hilab/dots.llm1.base
  # vllm_sweep_minimax_m1:
  #   model: MiniMaxAI/MiniMax-M1-40k
  # vllm_sweep_minimax_m2:
  #   model: MiniMaxAI/MiniMax-M2
  # vllm_sweep_minimax_text01:
  #   model: MiniMaxAI/MiniMax-Text-01
  # vllm_sweep_hcx_vision:
  #   model: naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B
