# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

# vLLM generative model test configurations.
# marks: list of pytest marker names (e.g. push, single_device, nightly).

model_configs:
  opt-125m:  # PASSING
    model: "facebook/opt-125m"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.001
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "Hello, my name is"
    marks: [push, single_device]

  opt-125m-multibatch:  # PASSING
    model: "facebook/opt-125m"
    max_num_batched_tokens: 256
    max_num_seqs: 2
    max_model_len: 128
    gpu_memory_utilization: 0.001
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "Hello, my name is"
    - "Paris is the capital of"
    marks: [push, single_device]

  llama3-3b:  # PASSING
    model: "meta-llama/Llama-3.2-3B"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "I like taking walks in the"
    marks: [nightly, single_device]

  mistral-7b:  # Out of Memory
    model: "mistralai/Mistral-7B-v0.1"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "The future of AI is"
    marks: [nightly, single_device]

  qwen2-0.5b:  # PASSING
    model: "Qwen/Qwen2-0.5B"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.001
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "The capital of China is"
    marks: [push, single_device]

  gemma-2b:  # RuntimeError: Engine core initialization failed
    model: "google/gemma-2b"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.001
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "The meaning of life is"
    marks: [push, single_device]

  phi-2:  # PASSING
    model: "microsoft/phi-2"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.004
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "Write a poem about"
    marks: [push, single_device]

  falcon-7b:  # Out of Memory
    model: "tiiuae/falcon-7b"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "The best way to learn"
    marks: [nightly, single_device]

  qwen_2_5_causal_lm_pytorch_1_5b: # PASSING
    model: Qwen/Qwen2.5-1.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device

  mamba_pytorch_mamba_790m_hf: #  ZeroDivisionError: integer division or modulo by zero
    model: state-spaces/mamba-790m-hf
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device

  falcon_pytorch_tiiuae_falcon3_1b_base: # PASSING
    model: tiiuae/Falcon3-1B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  falcon_pytorch_tiiuae_falcon3_3b_base:
    model: tiiuae/Falcon3-3B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  falcon_pytorch_tiiuae_falcon3_7b_base:
    model: tiiuae/Falcon3-7B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  falcon_pytorch_tiiuae_falcon3_10b_base:
    model: tiiuae/Falcon3-10B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  falcon_pytorch_tiiuae_falcon3_mamba_7b_base: # ZeroDivisionError: integer division or modulo by zero
    model: tiiuae/Falcon3-Mamba-7B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  mamba_pytorch_mamba_1_4b_hf: # ZeroDivisionError: integer division or modulo by zero
    model: state-spaces/mamba-1.4b-hf
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  mamba_pytorch_mamba_370m_hf:
    model: state-spaces/mamba-370m-hf
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  nanogpt_pytorch_financialsupport_nanogpt: # Value error, No model architectures are specified
    model: FinancialSupport/NanoGPT
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  mamba_pytorch_mamba_2_8b_hf:
    model: state-spaces/mamba-2.8b-hf
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  mistral_pytorch_ministral_3b_instruct: # TT_THROW: Statically allocated circular buffers on core range [(x=0,y=0) - (x=3,y=0)] grow to 1529120 B which is beyond max L1 size of 1499136 B
    model: ministral/Ministral-3b-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  phi2_causal_lm_pytorch_microsoft_phi_2: # PASSING
    model: microsoft/phi-2
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.004
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  phi2_causal_lm_pytorch_microsoft_phi_2_pytdml:
    model: microsoft/phi-2-pytdml
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  phi1_5_causal_lm_pytorch_microsoft_phi_1_5:
    model: microsoft/phi-1_5
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  opt_qa_pytorch_facebook_opt_125m: # PASSING
    model: facebook/opt-125m
    max_num_batched_tokens: 32
    max_num_seqs: 1
    max_model_len: 32
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  opt_qa_pytorch_facebook_opt_350m:
    model: facebook/opt-350m
    max_num_batched_tokens: 32
    max_num_seqs: 1
    max_model_len: 32
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  opt_causal_lm_pytorch_facebook_opt_125m: # PASSING
    model: facebook/opt-125m
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  opt_causal_lm_pytorch_facebook_opt_350m:
    model: facebook/opt-350m
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  deepcogito_pytorch_v1_preview_llama_3b: # PASSING
    model: deepcogito/cogito-v1-preview-llama-3B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  fuyu_pytorch_adept_fuyu_8b:
    model: adept/fuyu-8b
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  phi1_causal_lm_pytorch_microsoft_phi_1:
    model: microsoft/phi-1
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  opt_causal_lm_pytorch_facebook_opt_1_3b:
    model: facebook/opt-1.3b
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  opt_qa_pytorch_facebook_opt_1_3b:
    model: facebook/opt-1.3b
    max_num_batched_tokens: 32
    max_num_seqs: 1
    max_model_len: 32
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_1_5_causal_lm_pytorch_0_5b:
    model: Qwen/Qwen1.5-0.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_1_5_causal_lm_pytorch_0_5b_chat:
    model: Qwen/Qwen1.5-0.5B-Chat
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 512
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_3_causal_lm_pytorch_4b:
    model: Qwen/Qwen3-4B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_3_causal_lm_pytorch_1_7b:
    model: Qwen/Qwen3-1.7B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_coder_pytorch_3b:
    model: Qwen/Qwen2.5-Coder-3B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_3_causal_lm_pytorch_0_6b:
    model: Qwen/Qwen3-0.6B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_causal_lm_pytorch_3b:
    model: Qwen/Qwen2.5-3B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_causal_lm_pytorch_3b_instruct:
    model: Qwen/Qwen2.5-3B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_coder_pytorch_3b_instruct:
    model: Qwen/Qwen2.5-Coder-3B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_coder_pytorch_1_5b:
    model: Qwen/Qwen2.5-Coder-1.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_causal_lm_pytorch_1_5b_instruct:
    model: Qwen/Qwen2.5-1.5B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_coder_pytorch_1_5b_instruct:
    model: Qwen/Qwen2.5-Coder-1.5B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_coder_pytorch_0_5b:
    model: Qwen/Qwen2.5-Coder-0.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_causal_lm_pytorch_0_5b:
    model: Qwen/Qwen2.5-0.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_llama_3_2_1b:
    model: meta-llama/Llama-3.2-1B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_llama_3_2_3b:
    model: meta-llama/Llama-3.2-3B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_llama_3_2_1b_instruct:
    model: meta-llama/Llama-3.2-1B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_causal_lm_pytorch_0_5b_instruct:
    model: Qwen/Qwen2.5-0.5B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_llama_3_2_3b_instruct:
    model: meta-llama/Llama-3.2-3B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  unet_for_conditional_generation_pytorch_base:
    model: stabilityai/stable-diffusion-xl-base-1.0
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  deepseek_deepseek_coder_pytorch_1_3b_instruct:
    model: deepseek-ai/deepseek-coder-1.3b-instruct
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 2048
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gemma_pytorch_google_gemma_1_1_2b_it:
    model: google/gemma-1.1-2b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gpt2_pytorch_gpt2:
    model: gpt2
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gemma_pytorch_google_gemma_2_2b_it:
    model: google/gemma-2-2b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gemma_pytorch_google_gemma_2b:
    model: google/gemma-2b
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  phi3_phi_3_5_pytorch_mini_instruct:
    model: microsoft/Phi-3.5-mini-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gemma_pytorch_google_gemma_1_1_7b_it:
    model: google/gemma-1.1-7b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  phi4_causal_lm_pytorch_microsoft_phi_4:
    model: microsoft/phi-4
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  phi3_causal_lm_pytorch_microsoft_phi_3_mini_128k_instruct:
    model: microsoft/Phi-3-mini-128k-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  phi3_causal_lm_pytorch_microsoft_phi_3_mini_4k_instruct:
    model: microsoft/Phi-3-mini-4k-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  huggyllama_pytorch_llama_7b:
    model: huggyllama/llama-7b
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_huggyllama_7b:
    model: huggyllama/llama-7b
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_llama_3_1_8b:
    model: meta-llama/Llama-3.1-8B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_llama_3_1_8b_instruct:
    model: meta-llama/Llama-3.1-8B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_llama_3_8b:
    model: meta-llama/Meta-Llama-3-8B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_llama_3_8b_instruct:
    model: meta-llama/Meta-Llama-3-8B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  mistral_pytorch_7b_instruct_v03:
    model: mistralai/Mistral-7B-Instruct-v0.3
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_2_5_causal_lm_pytorch_14b_instruct:
    model: Qwen/Qwen2.5-14B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_3_causal_lm_pytorch_14b:
    model: Qwen/Qwen3-14B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  qwen_3_causal_lm_pytorch_8b:
    model: Qwen/Qwen3-8B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gemma_pytorch_google_gemma_2_9b_it:
    model: google/gemma-2-9b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  falcon_pytorch_tiiuae_falcon_7b_instruct:
    model: tiiuae/falcon-7b-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gemma_codegemma_pytorch_google_codegemma_2b:
    model: google/codegemma-2b
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  llama_causal_lm_pytorch_tinyllama_v1_1:
    model: TinyLlama/TinyLlama_v1.1
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  mistral_pytorch_mistral_nemo_instruct_2407:
    model: mistralai/Mistral-Nemo-Instruct-2407
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gemma_text_translation_pytorch_translategemma_4b_it:
    model: google/translategemma-4b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  allam_causal_lm_pytorch_allam_7b_instruct:
    model: ALLaM-AI/ALLaM-7B-Instruct-preview
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
