# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

# vLLM generative model test configurations.
# marks: list of pytest marker names (e.g. vllm_sweep, single_device).

model_configs:
  opt-125m:  # PASSING
    model: "facebook/opt-125m"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.001
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "Hello, my name is"
    marks: [single_device]

  opt-125m-multibatch:  # PASSING
    model: "facebook/opt-125m"
    max_num_batched_tokens: 256
    max_num_seqs: 2
    max_model_len: 128
    gpu_memory_utilization: 0.001
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "Hello, my name is"
    - "Paris is the capital of"
    marks: [single_device]

  llama3-3b:  # PASSING
    model: "meta-llama/Llama-3.2-3B"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "I like taking walks in the"
    marks: [single_device]

  mistral-7b:  # Out of Memory: Not enough space to allocate 234881024 B DRAM buffer across 12 banks, where each bank needs to store 19574784 B, but bank size is 1073741792 B (allocated: 1041740800 B, free: 32000992 B, largest free block: 15384576 B)
    model: "mistralai/Mistral-7B-v0.1"
    max_num_batched_tokens: 64
    max_num_seqs: 1
    max_model_len: 64
    gpu_memory_utilization: 0.001
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "The future of AI is"
    marks: [single_device]

  qwen2-0.5b:  # PASSING
    model: "Qwen/Qwen2-0.5B"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.001
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "The capital of China is"
    marks: [single_device]

  gemma-2b:  # Failed -> no error message in CI
    model: "google/gemma-2b"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.001
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "The meaning of life is"
    marks: [single_device]

  phi-2:  # PASSING
    model: "microsoft/phi-2"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.004
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "Write a poem about"
    marks: [single_device]

  falcon-7b:  # Out of Memory: Not enough space to allocate 165183488 B DRAM buffer across 12 banks, where each bank needs to store 13777408 B, but bank size is 1073741792 B (allocated: 989598720 B, free: 84143072 B, largest free block: 11322336 B) (assert.hpp:104)
    model: "tiiuae/falcon-7b"
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - "The best way to learn"
    marks: [single_device]

  qwen_2_5_causal_lm_pytorch_1_5b: # PASSING
    model: Qwen/Qwen2.5-1.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device

  mamba_pytorch_mamba_790m_hf: #  ZeroDivisionError: integer division or modulo by zero
    model: state-spaces/mamba-790m-hf
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device

  falcon_pytorch_tiiuae_falcon3_1b_base: # PASSING
    model: tiiuae/Falcon3-1B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  falcon_pytorch_tiiuae_falcon3_3b_base: # PASSING
    model: tiiuae/Falcon3-3B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  falcon_pytorch_tiiuae_falcon3_7b_base: # TT_FATAL: Out of Memory: Not enough space to allocate 283115520 B DRAM buffer across 12 banks, where each bank needs to store 23592960 B, but bank size is 1073741792 B (allocated: 995730432 B, free: 78011360 B, largest free block: 19355616 B)
    model: tiiuae/Falcon3-7B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  falcon_pytorch_tiiuae_falcon3_10b_base: # TT_FATAL: Out of Memory: Not enough space to allocate 283115520 B DRAM buffer across 12 banks, where each bank needs to store 23592960 B, but bank size is 1073741792 B (allocated: 995730432 B, free: 78011360 B, largest free block: 19355616 B)
    model: tiiuae/Falcon3-10B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  falcon_pytorch_tiiuae_falcon3_mamba_7b_base: # ZeroDivisionError: integer division or modulo by zero
    model: tiiuae/Falcon3-Mamba-7B-Base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  mamba_pytorch_mamba_1_4b_hf: # ZeroDivisionError: integer division or modulo by zero
    model: state-spaces/mamba-1.4b-hf
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  mamba_pytorch_mamba_370m_hf: # ZeroDivisionError: integer division or modulo by zero
    model: state-spaces/mamba-370m-hf
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  mamba_pytorch_mamba_2_8b_hf: # ZeroDivisionError: integer division or modulo by zero
    model: state-spaces/mamba-2.8b-hf
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  mistral_pytorch_ministral_3b_instruct: # TT_THROW: Statically allocated circular buffers on core range [(x=0,y=0) - (x=3,y=0)] grow to 1529120 B which is beyond max L1 size of 1499136 B
    model: ministral/Ministral-3b-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  phi2_causal_lm_pytorch_microsoft_phi_2: # PASSING
    model: microsoft/phi-2
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.004
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  phi2_causal_lm_pytorch_microsoft_phi_2_pytdml: # (0.04 GiB KV cache is needed, which is larger than the available KV cache memory (0.02 GiB).
    model: microsoft/phi-2-pytdml
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.004
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - vllm_sweep
    - single_device
  phi1_5_causal_lm_pytorch_microsoft_phi_1_5: # (0.05 GiB KV cache is needed, which is larger than the available KV cache memory (0.02 GiB).
    model: microsoft/phi-1_5
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.005
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - vllm_sweep
    - single_device
  opt_qa_pytorch_facebook_opt_125m: # PASSING
    model: facebook/opt-125m
    max_num_batched_tokens: 32
    max_num_seqs: 1
    max_model_len: 32
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  opt_qa_pytorch_facebook_opt_350m: # RuntimeError: a and b must have same reduction dim, but got [1, 1024] X [512, 50304].
    model: facebook/opt-350m
    max_num_batched_tokens: 32
    max_num_seqs: 1
    max_model_len: 32
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  opt_causal_lm_pytorch_facebook_opt_125m: # PASSING
    model: facebook/opt-125m
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  opt_causal_lm_pytorch_facebook_opt_350m: # RuntimeError: a and b must have same reduction dim, but got [1, 1024] X [512, 50304].
    model: facebook/opt-350m
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  deepcogito_pytorch_v1_preview_llama_3b: # PASSING
    model: deepcogito/cogito-v1-preview-llama-3B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  fuyu_pytorch_adept_fuyu_8b: # torch._dynamo.exc.TorchRuntimeError: Dynamo failed to run FX node with fake tensors: call_method view(*(FakeTensor(..., device='xla:0', size=(1, 128, 4096)), 1, 64, 64), **{}): got RuntimeError("shape '[1, 64, 64]' is invalid for input of size 524288")
    model: adept/fuyu-8b
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  phi1_causal_lm_pytorch_microsoft_phi_1: # (0.05 GiB KV cache is needed, which is larger than the available KV cache memory (0.02 GiB)
    model: microsoft/phi-1
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.005
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - vllm_sweep
    - single_device
  opt_causal_lm_pytorch_facebook_opt_1_3b: # (0.05 GiB KV cache is needed, which is larger than the available KV cache memory (0.02 GiB).
    model: facebook/opt-1.3b
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.005
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - vllm_sweep
    - single_device
  opt_qa_pytorch_facebook_opt_1_3b: # PASSING
    model: facebook/opt-1.3b
    max_num_batched_tokens: 32
    max_num_seqs: 1
    max_model_len: 32
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_1_5_causal_lm_pytorch_0_5b: # PASSING
    model: Qwen/Qwen1.5-0.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_1_5_causal_lm_pytorch_0_5b_chat: # AssertionError: The max_num_batched_tokens 256 must be larger than or equal to max_model_len (512) * max_num_seqs (1)
    model: Qwen/Qwen1.5-0.5B-Chat
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 512
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_3_causal_lm_pytorch_4b: # PASSING
    model: Qwen/Qwen3-4B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_3_causal_lm_pytorch_1_7b: # PASSING
    model: Qwen/Qwen3-1.7B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_coder_pytorch_3b: # PASSING
    model: Qwen/Qwen2.5-Coder-3B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_3_causal_lm_pytorch_0_6b: # PASSING
    model: Qwen/Qwen3-0.6B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_causal_lm_pytorch_3b: # PASSING
    model: Qwen/Qwen2.5-3B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_causal_lm_pytorch_3b_instruct: # PASSING
    model: Qwen/Qwen2.5-3B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_coder_pytorch_3b_instruct: # PASSING
    model: Qwen/Qwen2.5-Coder-3B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_coder_pytorch_1_5b: # PASSING
    model: Qwen/Qwen2.5-Coder-1.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_causal_lm_pytorch_1_5b_instruct: # PASSING
    model: Qwen/Qwen2.5-1.5B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_coder_pytorch_1_5b_instruct: # PASSING
    model: Qwen/Qwen2.5-Coder-1.5B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_coder_pytorch_0_5b: # PASSING
    model: Qwen/Qwen2.5-Coder-0.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_causal_lm_pytorch_0_5b: # PASSING
    model: Qwen/Qwen2.5-0.5B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_llama_3_2_1b: # PASSING
    model: meta-llama/Llama-3.2-1B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_llama_3_2_3b: # PASSING
    model: meta-llama/Llama-3.2-3B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_llama_3_2_1b_instruct: # PASSING
    model: meta-llama/Llama-3.2-1B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_causal_lm_pytorch_0_5b_instruct: # PASSING
    model: Qwen/Qwen2.5-0.5B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_llama_3_2_3b_instruct: # PASSING
    model: meta-llama/Llama-3.2-3B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  unet_for_conditional_generation_pytorch_base: # FAILED -> no error message in CI
    model: stabilityai/stable-diffusion-xl-base-1.0
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  deepseek_deepseek_coder_pytorch_1_3b_instruct: # AssertionError: The max_num_batched_tokens 256 must be larger than or equal to max_model_len (2048) * max_num_seqs (1)
    model: deepseek-ai/deepseek-coder-1.3b-instruct
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 2048
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  gemma_pytorch_google_gemma_1_1_2b_it: # Failed -> no error message in CI
    model: google/gemma-1.1-2b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  gpt2_pytorch_gpt2: # AttributeError: 'GPT2LMHeadModel' object has no attribute 'model'
    model: gpt2
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  gemma_pytorch_google_gemma_2_2b_it: # Failed -> no error message in CI
    model: google/gemma-2-2b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  gemma_pytorch_google_gemma_2b: # Failed -> no error message in CI
    model: google/gemma-2b
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  phi3_phi_3_5_pytorch_mini_instruct: # RuntimeError: Check failed: casted->xla_shape().dimensions_size() <= 1 (2 vs. 1)
    model: microsoft/Phi-3.5-mini-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  gemma_pytorch_google_gemma_1_1_7b_it: # Failed -> no error message in CI
    model: google/gemma-1.1-7b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  phi4_causal_lm_pytorch_microsoft_phi_4: # Out of Memory: Not enough space to allocate 367001600 B DRAM buffer across 12 banks, where each bank needs to store 30586880 B, but bank size is 1073741792 B (allocated: 1022012416 B, free: 51729376 B, largest free block: 24033280 B) (assert.hpp:104)
    model: microsoft/phi-4
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  phi3_causal_lm_pytorch_microsoft_phi_3_mini_128k_instruct: # RuntimeError: Check failed: casted->xla_shape().dimensions_size() <= 1 (2 vs. 1)
    model: microsoft/Phi-3-mini-128k-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  phi3_causal_lm_pytorch_microsoft_phi_3_mini_4k_instruct: # (0.06 GiB KV cache is needed, which is larger than the available KV cache memory (0.02 GiB)
    model: microsoft/Phi-3-mini-4k-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.006
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - vllm_sweep
    - single_device
  huggyllama_pytorch_llama_7b: # Out of Memory: Not enough space to allocate 100663296 B DRAM buffer across 12 banks, where each bank needs to store 8388608 B, but bank size is 1073741792 B (allocated: 1046447104 B, free: 27294688 B, largest free block: 7729120 B)
    model: huggyllama/llama-7b
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_huggyllama_7b: # TT_FATAL: Out of Memory: Not enough space to allocate 100663296 B DRAM buffer across 12 banks, where each bank needs to store 8388608 B, but bank size is 1073741792 B (allocated: 1046447104 B, free: 27294688 B, largest free block: 7729120 B)
    model: huggyllama/llama-7b
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_llama_3_1_8b: # TT_FATAL: Out of Memory: Not enough space to allocate 234881024 B DRAM buffer across 12 banks, where each bank needs to store 19574784 B, but bank size is 1073741792 B (allocated: 1036749824 B, free: 36991968 B, largest free block: 17670112 B) (assert.hpp:104)
    model: meta-llama/Llama-3.1-8B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_llama_3_1_8b_instruct: # Out of Memory: Not enough space to allocate 234881024 B DRAM buffer across 12 banks, where each bank needs to store 19574784 B, but bank size is 1073741792 B (allocated: 1036749824 B, free: 36991968 B, largest free block: 17670112 B) (assert.hpp:104)
    model: meta-llama/Llama-3.1-8B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_llama_3_8b: # TT_FATAL: Out of Memory: Not enough space to allocate 234881024 B DRAM buffer across 12 banks, where each bank needs to store 19574784 B, but bank size is 1073741792 B (allocated: 1034128384 B, free: 39613408 B, largest free block: 17674208 B) (assert.hpp:104)
    model: meta-llama/Meta-Llama-3-8B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_llama_3_8b_instruct: # Out of Memory: Not enough space to allocate 234881024 B DRAM buffer across 12 banks, where each bank needs to store 19574784 B, but bank size is 1073741792 B (allocated: 1034128384 B, free: 39613408 B, largest free block: 17674208 B) (assert.hpp:104)
    model: meta-llama/Meta-Llama-3-8B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  mistral_pytorch_7b_instruct_v03: # TT_FATAL: Out of Memory: Not enough space to allocate 234881024 B DRAM buffer across 12 banks, where each bank needs to store 19574784 B, but bank size is 1073741792 B (allocated: 1042265088 B, free: 31476704 B, largest free block: 15384576 B)
    model: mistralai/Mistral-7B-Instruct-v0.3
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_2_5_causal_lm_pytorch_14b_instruct: # Out of Memory: Not enough space to allocate 141557760 B DRAM buffer across 12 banks, where each bank needs to store 11796480 B, but bank size is 1073741792 B (allocated: 1026367488 B, free: 47374304 B, largest free block: 11787264 B) (assert.hpp:104)
    model: Qwen/Qwen2.5-14B-Instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_3_causal_lm_pytorch_14b: # TT_FATAL: Out of Memory: Not enough space to allocate 178257920 B DRAM buffer across 12 banks, where each bank needs to store 14856192 B, but bank size is 1073741792 B (allocated: 1037814784 B, free: 35927008 B, largest free block: 12144608 B) (assert.hpp:104)
    model: Qwen/Qwen3-14B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  qwen_3_causal_lm_pytorch_8b: #  TT_FATAL: Out of Memory: Not enough space to allocate 201326592 B DRAM buffer across 12 banks, where each bank needs to store 16777216 B, but bank size is 1073741792 B (allocated: 1021836288 B, free: 51905504 B, largest free block: 9785344 B) (assert.hpp:104)
    model: Qwen/Qwen3-8B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  gemma_pytorch_google_gemma_2_9b_it: # Failed -> no error message in CI
    model: google/gemma-2-9b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  falcon_pytorch_tiiuae_falcon_7b_instruct: # TT_FATAL: Out of Memory: Not enough space to allocate 165183488 B DRAM buffer across 12 banks, where each bank needs to store 13777408 B, but bank size is 1073741792 B (allocated: 989598720 B, free: 84143072 B, largest free block: 11322336 B) (assert.hpp:104)
    model: tiiuae/falcon-7b-instruct
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  gemma_codegemma_pytorch_google_codegemma_2b: # Failed -> no error message in CI
    model: google/codegemma-2b
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  llama_causal_lm_pytorch_tinyllama_v1_1: # PASSING
    model: TinyLlama/TinyLlama_v1.1
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  mistral_pytorch_mistral_nemo_instruct_2407: # TT_FATAL: Out of Memory: Not enough space to allocate 293601280 B DRAM buffer across 12 banks, where each bank needs to store 24473600 B, but bank size is 1073741792 B (allocated: 1000166400 B, free: 73575392 B, largest free block: 18016224 B) (assert.hpp:104)
    model: mistralai/Mistral-Nemo-Instruct-2407
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  gemma_text_translation_pytorch_translategemma_4b_it: # Failed -> no error message in CI
    model: google/translategemma-4b-it
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
  allam_causal_lm_pytorch_allam_7b_instruct: # TT_FATAL: Out of Memory: Not enough space to allocate 90177536 B DRAM buffer across 12 banks, where each bank needs to store 7516160 B, but bank size is 1073741792 B (allocated: 1042110464 B, free: 31631328 B, largest free block: 7502848 B) (assert.hpp:104)
    model: ALLaM-AI/ALLaM-7B-Instruct-preview
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - single_device
