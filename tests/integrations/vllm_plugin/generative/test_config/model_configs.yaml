# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

# vLLM generative model test configurations.
# Defaults are applied in test_config/__init__.py (DEFAULT_GENERATIVE_CONFIG).
# Only "model" is required. "marks" default to [single_device]; add marks only when overriding.
# "status" defaults to "unspecified" when omitted.

model_configs:
  allam_causal_lm_pytorch_allam_7b_instruct:
    model: ALLaM-AI/ALLaM-7B-Instruct-preview
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  deepcogito_pytorch_v1_preview_llama_3b:
    model: deepcogito/cogito-v1-preview-llama-3B
    status: "expected_passing"
  deepseek_deepseek_coder_pytorch_1_3b_instruct:
    model: deepseek-ai/deepseek-coder-1.3b-instruct
    status: "expected_passing"
  falcon-7b:
    model: tiiuae/falcon-7b
    prompts:
    - The best way to learn
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  falcon_pytorch_tiiuae_falcon3_10b_base:
    model: tiiuae/Falcon3-10B-Base
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  falcon_pytorch_tiiuae_falcon3_1b_base:
    model: tiiuae/Falcon3-1B-Base
    status: "expected_passing"
  falcon_pytorch_tiiuae_falcon3_3b_base:
    model: tiiuae/Falcon3-3B-Base
    status: "expected_passing"
  falcon_pytorch_tiiuae_falcon3_7b_base:
    model: tiiuae/Falcon3-7B-Base
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  falcon_pytorch_tiiuae_falcon3_mamba_7b_base:
    model: tiiuae/Falcon3-Mamba-7B-Base
    status: "known_failure_xfail"
    reason: "ZeroDivisionError: integer division or modulo by zero"
  falcon_pytorch_tiiuae_falcon_7b_instruct:
    model: tiiuae/falcon-7b-instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  fuyu_pytorch_adept_fuyu_8b:
    model: adept/fuyu-8b
    status: "known_failure_xfail"
    reason: "Dynamo failed to run FX node with fake tensors"
  gemma_codegemma_pytorch_google_codegemma_2b:
    model: google/codegemma-2b
    max_model_len: 256
    max_num_batched_tokens: 256
    status: "known_failure_xfail"
    reason: "Unknown failure"
  gemma_pytorch_google_gemma_1_1_2b_it:
    model: google/gemma-1.1-2b-it
    status: "known_failure_xfail"
    reason: "Unknown failure"
  gemma_pytorch_google_gemma_1_1_7b_it:
    model: google/gemma-1.1-7b-it
    status: "known_failure_xfail"
    reason: "Unknown failure"
  gemma_pytorch_google_gemma_2_2b_it:
    model: google/gemma-2-2b-it
    status: "known_failure_xfail"
    reason: "Unknown failure"
  gemma_pytorch_google_gemma_2_9b_it:
    model: google/gemma-2-9b-it
    status: "known_failure_xfail"
    reason: "Unknown failure"
  gemma_pytorch_google_gemma_2b:
    model: google/gemma-2b
    max_model_len: 256
    max_num_batched_tokens: 256
    status: "known_failure_xfail"
    reason: "Unknown failure"
  gemma_text_translation_pytorch_translategemma_4b_it:
    model: google/translategemma-4b-it
    status: "known_failure_xfail"
    reason: "Unknown failure"
  gpt2_pytorch_gpt2:
    model: gpt2
    max_model_len: 256
    max_num_batched_tokens: 256
    status: "known_failure_xfail"
    reason: "'GPT2LMHeadModel' object has no attribute 'model'"
  huggyllama_pytorch_llama_7b:
    model: huggyllama/llama-7b
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  llama_causal_lm_pytorch_huggyllama_7b:
    model: huggyllama/llama-7b
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  llama_causal_lm_pytorch_llama_3_1_8b:
    model: meta-llama/Llama-3.1-8B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  llama_causal_lm_pytorch_llama_3_1_8b_instruct:
    model: meta-llama/Llama-3.1-8B-Instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  llama_causal_lm_pytorch_llama_3_2_1b:
    model: meta-llama/Llama-3.2-1B
    status: "expected_passing"
  llama_causal_lm_pytorch_llama_3_2_1b_instruct:
    model: meta-llama/Llama-3.2-1B-Instruct
    status: "expected_passing"
  llama_causal_lm_pytorch_llama_3_2_3b:
    model: meta-llama/Llama-3.2-3B
    status: "expected_passing"
  llama_causal_lm_pytorch_llama_3_2_3b_instruct:
    model: meta-llama/Llama-3.2-3B-Instruct
    status: "expected_passing"
  llama_causal_lm_pytorch_llama_3_8b:
    model: meta-llama/Meta-Llama-3-8B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  llama_causal_lm_pytorch_llama_3_8b_instruct:
    model: meta-llama/Meta-Llama-3-8B-Instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  llama_causal_lm_pytorch_tinyllama_v1_1:
    model: TinyLlama/TinyLlama_v1.1
    status: "expected_passing"
  mamba_pytorch_mamba_1_4b_hf:
    model: state-spaces/mamba-1.4b-hf
    status: "known_failure_xfail"
    reason: "ZeroDivisionError: integer division or modulo by zero"
  mamba_pytorch_mamba_2_8b_hf:
    model: state-spaces/mamba-2.8b-hf
    status: "known_failure_xfail"
    reason: "ZeroDivisionError: integer division or modulo by zero"
  mamba_pytorch_mamba_370m_hf:
    model: state-spaces/mamba-370m-hf
    status: "known_failure_xfail"
    reason: "ZeroDivisionError: integer division or modulo by zero"
  mamba_pytorch_mamba_790m_hf:
    model: state-spaces/mamba-790m-hf
    status: "known_failure_xfail"
    reason: "ZeroDivisionError: integer division or modulo by zero"
  mistral-7b:
    model: mistralai/Mistral-7B-v0.1
    gpu_memory_utilization: 0.001
    max_model_len: 64
    max_num_batched_tokens: 64
    prompts:
    - The future of AI is
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  mistral_pytorch_7b_instruct_v03:
    model: mistralai/Mistral-7B-Instruct-v0.3
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  mistral_pytorch_ministral_3b_instruct:
    model: ministral/Ministral-3b-instruct
    status: "known_failure_xfail"
    reason: "L1 Out of Memory"
  mistral_pytorch_mistral_nemo_instruct_2407:
    model: mistralai/Mistral-Nemo-Instruct-2407
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  opt-125m:
    model: facebook/opt-125m
    gpu_memory_utilization: 0.001
    status: "expected_passing"
  # opt-125m-multibatch:
  #   model: facebook/opt-125m
  #   gpu_memory_utilization: 0.001
  #   max_num_batched_tokens: 256
  #   max_num_seqs: 2
  #   prompts:
  #   - Hello, my name is
  #   - Paris is the capital of
  #   status: "expected_passing"
  opt_causal_lm_pytorch_facebook_opt_1_3b:
    model: facebook/opt-1.3b
    gpu_memory_utilization: 0.005
    max_model_len: 256
    max_num_batched_tokens: 256
    status: "expected_passing"
  opt_causal_lm_pytorch_facebook_opt_350m:
    model: facebook/opt-350m
    max_model_len: 256
    max_num_batched_tokens: 256
    status: "known_failure_xfail"
    reason: "RuntimeError: a and b must have same reduction dim, but got [1, 1024] X [512, 50304]."
  phi1_5_causal_lm_pytorch_microsoft_phi_1_5:
    model: microsoft/phi-1_5
    gpu_memory_utilization: 0.005
    max_model_len: 256
    max_num_batched_tokens: 256
    status: "expected_passing"
  phi1_causal_lm_pytorch_microsoft_phi_1:
    model: microsoft/phi-1
    gpu_memory_utilization: 0.005
    max_model_len: 256
    max_num_batched_tokens: 256
    status: "expected_passing"
  phi2_causal_lm_pytorch_microsoft_phi_2:
    model: microsoft/phi-2
    gpu_memory_utilization: 0.004
    status: "expected_passing"
  phi2_causal_lm_pytorch_microsoft_phi_2_pytdml:
    model: microsoft/phi-2-pytdml
    gpu_memory_utilization: 0.004
    status: "expected_passing"
  phi3_causal_lm_pytorch_microsoft_phi_3_mini_128k_instruct:
    model: microsoft/Phi-3-mini-128k-instruct
    status: "known_failure_xfail"
    reason: "RuntimeError: Check failed: casted->xla_shape().dimensions_size() <= 1 (2 vs. 1)"
  phi3_causal_lm_pytorch_microsoft_phi_3_mini_4k_instruct:
    model: microsoft/Phi-3-mini-4k-instruct
    gpu_memory_utilization: 0.006
    status: "expected_passing"
  phi3_phi_3_5_pytorch_mini_instruct:
    model: microsoft/Phi-3.5-mini-instruct
    status: "known_failure_xfail"
    reason: "RuntimeError: Check failed: casted->xla_shape().dimensions_size() <= 1 (2 vs. 1)"
  phi4_causal_lm_pytorch_microsoft_phi_4:
    model: microsoft/phi-4
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  qwen2-0.5b:
    model: Qwen/Qwen2-0.5B
    gpu_memory_utilization: 0.001
    prompts:
    - The capital of China is
    status: "expected_passing"
  qwen_1_5_causal_lm_pytorch_0_5b:
    model: Qwen/Qwen1.5-0.5B
    status: "expected_passing"
  qwen_1_5_causal_lm_pytorch_0_5b_chat:
    model: Qwen/Qwen1.5-0.5B-Chat
    max_model_len: 512
    max_num_batched_tokens: 256
    status: "known_failure_xfail"
    reason: "The max_num_batched_tokens 256 must be larger than or equal to max_model_len (512) * max_num_seqs (1)"
  qwen_2_5_causal_lm_pytorch_0_5b:
    model: Qwen/Qwen2.5-0.5B
    status: "expected_passing"
  qwen_2_5_causal_lm_pytorch_0_5b_instruct:
    model: Qwen/Qwen2.5-0.5B-Instruct
    status: "expected_passing"
  qwen_2_5_causal_lm_pytorch_14b_instruct:
    model: Qwen/Qwen2.5-14B-Instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  qwen_2_5_causal_lm_pytorch_1_5b:
    model: Qwen/Qwen2.5-1.5B
    status: "expected_passing"
  qwen_2_5_causal_lm_pytorch_1_5b_instruct:
    model: Qwen/Qwen2.5-1.5B-Instruct
    status: "expected_passing"
  qwen_2_5_causal_lm_pytorch_3b:
    model: Qwen/Qwen2.5-3B
    status: "expected_passing"
  qwen_2_5_causal_lm_pytorch_3b_instruct:
    model: Qwen/Qwen2.5-3B-Instruct
    status: "expected_passing"
  qwen_2_5_coder_pytorch_0_5b:
    model: Qwen/Qwen2.5-Coder-0.5B
    status: "expected_passing"
  qwen_2_5_coder_pytorch_1_5b:
    model: Qwen/Qwen2.5-Coder-1.5B
    status: "expected_passing"
  qwen_2_5_coder_pytorch_1_5b_instruct:
    model: Qwen/Qwen2.5-Coder-1.5B-Instruct
    status: "expected_passing"
  qwen_2_5_coder_pytorch_3b:
    model: Qwen/Qwen2.5-Coder-3B
    status: "expected_passing"
  qwen_2_5_coder_pytorch_3b_instruct:
    model: Qwen/Qwen2.5-Coder-3B-Instruct
    status: "expected_passing"
  qwen_3_causal_lm_pytorch_0_6b:
    model: Qwen/Qwen3-0.6B
    status: "expected_passing"
  qwen_3_causal_lm_pytorch_14b:
    model: Qwen/Qwen3-14B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  qwen_3_causal_lm_pytorch_1_7b:
    model: Qwen/Qwen3-1.7B
    status: "expected_passing"
  qwen_3_causal_lm_pytorch_4b:
    model: Qwen/Qwen3-4B
    status: "expected_passing"
  qwen_3_causal_lm_pytorch_8b:
    model: Qwen/Qwen3-8B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  unet_for_conditional_generation_pytorch_base:
    model: stabilityai/stable-diffusion-xl-base-1.0
    status: "known_failure_xfail"
    reason: "Unknown failure"
  vllm_sweep_arctic:
    model: Snowflake/snowflake-arctic-base
    status: "known_failure_xfail"
    reason: "ImportError: cannot import name 'fused_experts' from 'vllm.model_executor.layers.fused_moe'"
  vllm_sweep_aquila:
    model: BAAI/Aquila-7B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_apertus:
    model: swiss-ai/Apertus-8B-2509
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_aquila_chat:
    model: BAAI/AquilaChat-7B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_arcee: # Test this locally to see if it also hangs or if it's just a CI issue
    model: arcee-ai/AFM-4.5B-Base
    status: "known_failure_xfail"
    reason: "hangs? "
  vllm_sweep_baichuan:
    model: baichuan-inc/Baichuan-7B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_bailing_moe:
    model: inclusionAI/Ling-lite-1.5
    status: "known_failure_xfail"
    reason: "too many values to unpack (expected 2) - hidden_states.shape"
  vllm_sweep_bailing_moe_v2:
    model: inclusionAI/Ling-mini-2.0
    status: "known_failure_xfail"
    reason: "self.instruction_pointer is None"
  vllm_sweep_bamba:
    model: ibm-ai-platform/Bamba-9B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_bloom:
    model: bigscience/bloom-560m
    status: "known_failure_xfail"
    reason: "Alibi slopes is not supported."
  vllm_sweep_chat_glm2:
    model: THUDM/chatglm2-6b
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_chat_glm3:
    model: THUDM/chatglm3-6b
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_cohere:
    model: CohereForAI/c4ai-command-r-v01
    status: "known_failure_xfail"
    reason: "Unknown failure"
  vllm_sweep_cohere2:
    model: CohereLabs/c4ai-command-r7b-12-2024
    status: "known_failure_xfail"
    reason: "Model is too large for single chip - Gated huggingface model"
  vllm_sweep_dbrx:
    model: Undi95/dbrx-base
    status: "known_failure_xfail"
    reason: "Unknown failure"
  vllm_sweep_decilm:
    model: Deci/DeciLM-7B
    max_model_len: 64
    max_num_batched_tokens: 64
    status: "known_failure_xfail"
    reason: "AttributeError: 'DeciLMConfig' object has no attribute 'block_configs'"
  vllm_sweep_deepseek:
    model: deepseek-ai/deepseek-llm-7b-chat
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_deepseekv2:
    model: deepseek-ai/DeepSeek-V2
    max_model_len: 64
    max_num_batched_tokens: 64
    status: "known_failure_xfail"
    reason: "TypeError: TTAttentionBackendImpl.__init__() got an unexpected keyword argument 'q_lora_rank'"
  vllm_sweep_deepseekv3:
    model: deepseek-ai/DeepSeek-V3
    max_model_len: 64
    max_num_batched_tokens: 64
    status: "known_failure_xfail"
    reason: "TypeError: TTAttentionBackendImpl.__init__() got an unexpected keyword argument 'q_lora_rank'"
  # vllm_sweep_dots1: # 142B parameter model
  #   model: rednote-hilab/dots.llm1.base
  #   status: "known_failure_xfail"
  #   reason: "Model is too large for single chip"
  vllm_sweep_dots_ocr: 
    model: rednote-hilab/dots.ocr
    status: "known_failure_xfail"
    reason: "Chunked MM input disabled but max_tokens_per_mm_item (14400) is larger than max_num_batched_tokens (128). Please increase max_num_batched_tokens."
  vllm_sweep_ernie4_5:
    model: baidu/ERNIE-4.5-0.3B-PT
    status: "known_failure_xfail"
    reason: "You marked L['positions'].size()[1] as dynamic but your code specialized it to be a constant (128)"
  vllm_sweep_ernie4_5_moe:
    model: baidu/ERNIE-4.5-21B-A3B-PT
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_exaone3:
    model: LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip - Gated huggingface model"
  vllm_sweep_exaone4:
    model: LGAI-EXAONE/EXAONE-4.0-32B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_fairseq2_llama:
    model: mgleize/fairseq2-dummy-Llama-3.2-1B
    status: "expected_passing"
  vllm_sweep_falcon_h1:
    model: tiiuae/Falcon-H1-34B-Base
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_flex_olmo:
    model: allenai/FlexOlmo-7x7B-1T
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  # vllm_sweep_falcon:
  #   model: tiiuae/falcon-7b
  #   marks: [vllm_sweep]
  # vllm_sweep_falconmamba:
  #   model: tiiuae/falcon-mamba-7b
  #   marks: [vllm_sweep]
  # vllm_sweep_gemma:
  #   model: google/gemma-2b
  #   gpu_memory_utilization: 0.001
  #   marks: [vllm_sweep]
  # vllm_sweep_gemma2:
  #   model: google/gemma-2-2b
  #   marks: [vllm_sweep]
  vllm_sweep_gemma3:
    model: google/gemma-3-1b-it
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_gemma3n:
    model: google/gemma-3n-E2B-it
    status: "known_failure_xfail"
    reason: "'NoneType' object has no attribute 'shape' - input_embeds"
  vllm_sweep_glm:
    model: THUDM/glm-4-9b-chat-hf
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_glm4:
    model: zai-org/GLM-4-32B-0414
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_glm4_moe:
    model: zai-org/GLM-4.5
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  # vllm_sweep_gpt2:
  #   model: gpt2
  #   status: "known_failure_xfail"
  #   reason: "'GPT2LMHeadModel' object has no attribute 'model'"
  vllm_sweep_gpt_oss:
    model: openai/gpt-oss-20b
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_gptbigcode: # 16B
    model: bigcode/starcoder
    status: "known_failure_xfail"
    reason: "Model is too large for single chip - Gated huggingface model"
  vllm_sweep_gptj:
    model: EleutherAI/gpt-j-6b
    status: "known_failure_xfail"
    reason: "'GPTJForCausalLM' object has no attribute 'model'"
  vllm_sweep_gptneox:
    model: EleutherAI/pythia-70m
    status: "known_failure_xfail"
    reason: "AttributeError: 'GPTNeoXForCausalLM' object has no attribute 'model'"
  vllm_sweep_granite:
    model: ibm-granite/granite-3.0-2b-base
    status: "expected_passing"
  vllm_sweep_granitemoe:
    model: ibm-granite/granite-3.0-1b-a400m-base
    status: "known_failure_xfail"
    reason: "'_OpNamespace' '_moe_C' object has no attribute 'topk_softmax'"
  vllm_sweep_granite_moe_hybrid:
    model: ibm-granite/granite-4.0-tiny-preview
    status: "known_failure_xfail"
    reason: "RuntimeError: model.layers.0.mixer - While executing %auto_functionalized"
  vllm_sweep_granite_moe_shared:
    model: ibm-research/moe-7b-1b-active-shared-experts
    status: "known_failure_xfail"
    reason: "'_OpNamespace' '_moe_C' object has no attribute 'topk_softmax'"
  vllm_sweep_gritlm:
    model: parasail-ai/GritLM-7B-vllm
    status: "known_failure_xfail"
    reason: "ZeroDivisionError: integer division or modulo by zero"
  vllm_sweep_grok1:
    model: hpcai-tech/grok-1
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_hunyuan_dense: 
    model: tencent/Hunyuan-7B-Instruct
    status: "known_failure_xfail"
    reason: "AssertionError: Attempt to trace forbidden callable <function mark_dynamic at 0x7fecb642f600>"
  vllm_sweep_hunyuan_moe:
    model: tencent/Hunyuan-A13B-Instruct 
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  # vllm_sweep_hcx_vision:
  #   model: naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B
  #   status: "known_failure_xfail"
  #   reason: "Model is too large for single chip"
  vllm_sweep_internlm:
    model: internlm/internlm-7b
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_internlm2:
    model: internlm/internlm2-7b
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_internlm3:
    model: internlm/internlm3-8b-instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_jais:
    model: inceptionai/jais-13b 
    max_model_len: 64
    max_num_batched_tokens: 64
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_jais2:
    model: inceptionai/Jais-2-8B-Chat
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_jamba: # need to agree to terms to access - 12B parameter model√ü
    model: ai21labs/AI21-Jamba-Mini-1.5
    status: "known_failure_xfail"
    reason: "Model is too large for single chip - Gated huggingface model"
  vllm_sweep_kimi_linear:
    model: moonshotai/Kimi-Linear-48B-A3B-Base
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_lfm2:
    model: LiquidAI/LFM2-350M
    status: "known_failure_xfail"
    reason: "KeyError: 'model.layers.0.conv - While executing %auto_functionalized"
  vllm_sweep_lfm2_moe:
    model: LiquidAI/LFM2-8B-A1B-preview
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  # vllm_sweep_llama:
  #   model: meta-llama/Llama-3.2-1B
  #   marks: [vllm_sweep]
  # vllm_sweep_longcat_flash: # 560B parameter model
  #   model: meituan-longcat/LongCat-Flash-Chat
  #   status: "known_failure_xfail"
  #   reason: "Model is too large for single chip"
  vllm_sweep_mamba:
    model: state-spaces/mamba-130m-hf
    status: "known_failure_xfail"
    reason: "ZeroDivisionError: integer division or modulo by zero"
  vllm_sweep_mamba2:
    model: mistralai/Mamba-Codestral-7B-v0.1
    status: "known_failure_xfail"
    reason: "ZeroDivisionError: integer division or modulo by zero"
  vllm_sweep_minicpm:
    model: openbmb/MiniCPM-2B-sft-bf16
    gpu_memory_utilization: 0.004
    status: "known_failure_xfail"
    reason: "hangs?"
  vllm_sweep_minicpm3:
    model: openbmb/MiniCPM3-4B
    status: "known_failure_xfail"
    reason: "torch._dynamo.exc.TorchRuntimeError: Dynamo failed to run FX node with fake tensors: call_method view(*(FakeTensor(..., device='xla:0', size=(0, 32)), 0, -1, 32), **{}): got RuntimeError('cannot reshape tensor of 0 elements into shape [0, -1, 32] because the unspecified dimension size -1 can be any value and is ambiguous')"
  vllm_sweep_mimo:
    model: XiaomiMiMo/MiMo-7B-RL
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  # vllm_sweep_minimax_m1:
  #   model: MiniMaxAI/MiniMax-M1-40k
  #   status: "known_failure_xfail"
  #   reason: "Model is too large for single chip"
  # vllm_sweep_minimax_m2:
  #   model: MiniMaxAI/MiniMax-M2
  #   status: "known_failure_xfail"
  #   reason: "Model is too large for single chip"
  # vllm_sweep_minimax_text01:
  #   model: MiniMaxAI/MiniMax-Text-01
  #   status: "known_failure_xfail"
  #   reason: "Model is too large for single chip"
  # vllm_sweep_mistral:
  #   model: mistralai/Mistral-7B-v0.1
  #   gpu_memory_utilization: 0.001
  #   max_model_len: 64
  #   max_num_batched_tokens: 64
  #   prompts:
  #   - The future of AI is
  #   marks: [vllm_sweep]
  # vllm_sweep_mistral_large3:
  #   model: mistralai/Mistral-Large-3-675B-Base-2512
  #   marks: [vllm_sweep]
  # vllm_sweep_mixtral:
  #   model: mistralai/Mixtral-8x7B-v0.1
  #   max_model_len: 64
  #   max_num_batched_tokens: 64
  #   marks: [vllm_sweep]
  vllm_sweep_mpt:
    model: mosaicml/mpt-7b
    status: "known_failure_xfail"
    reason: "not a valid model identifier listed on 'https://huggingface.co/models'"
  vllm_sweep_nemotron:
    model: nvidia/Minitron-8B-Base
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_nemotron_h:
    model: nvidia/Nemotron-H-8B-Base-8K
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_olmo:
    model: allenai/OLMo-1B-hf
    status: "expected_passing"
  vllm_sweep_olmo2:
    model: allenai/OLMo2-7B-1124
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_olmo3:
    model: allenai/Olmo-3-7B-Instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_olmoe:
    model: allenai/OLMoE-1B-7B-0924
    status: "known_failure_xfail"
    reason: "'_OpNamespace' '_moe_C' object has no attribute 'topk_softmax'"
  vllm_sweep_ouro:
    model: ByteDance/Ouro-1.4B
    gpu_memory_utilization: 0.009
    status: "expected_passing"
  # vllm_sweep_opt:
  #   model: facebook/opt-125m
  #   marks: [vllm_sweep]
  # vllm_sweep_orion:
  #   model: OrionStarAI/Orion-14B-Base
  #   max_model_len: 64
  #   max_num_batched_tokens: 64
  #   marks: [vllm_sweep]
  vllm_sweep_pangu_embedded:
    model: FreedomIntelligence/openPangu-Embedded-7B-V1.1
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  # vllm_sweep_pangu_ultra_moe:
  #   model: FreedomIntelligence/openPangu-Ultra-MoE-718B-V1.1
  #   marks: [vllm_sweep]
  # vllm_sweep_phi:
  #   model: microsoft/phi-2
  #   gpu_memory_utilization: 0.004
  #   marks: [vllm_sweep]
  vllm_sweep_phi3:
    model: microsoft/Phi-3-mini-4k-instruct
    gpu_memory_utilization: 0.006
    status: "known_failure_xfail"
    reason: "TT_FATAL: Size of cb: 80 must be divisible by the size of block used by the reader and compute kernel."
  # vllm_sweep_phi4:
  #   model: microsoft/phi-4
  #   marks: [vllm_sweep]
  vllm_sweep_phi3_small:
    model: microsoft/Phi-3-small-8k-instruct
    status: "known_failure_xfail"
    reason: "Model architecture Phi3SmallForCausalLM was supported in vLLM until v0.9.2, and is not supported anymore."
  # vllm_sweep_phimoe:
  #   model: microsoft/Phi-3.5-MoE-instruct
  #   marks: [vllm_sweep]
  vllm_sweep_plamo2:
    model: pfnet/plamo-2-1b
    status: "known_failure_xfail"
    reason: "AssertionError: Attempt to trace forbidden callable <function mark_dynamic at 0x7fbd198ff880>"
  vllm_sweep_plamo3:
    model: pfnet/plamo-3-nict-2b-base
    status: "known_failure_xfail"
    reason: "Gated huggingface model"
  vllm_sweep_persimmon:
    model: adept/persimmon-8b-base
    status: "known_failure_xfail"
    reason: "Dynamo failed to run FX node with fake tensors: call_method view(*(FakeTensor(..., device='xla:0', size=(1, 128, 4096)), 1, 64, 64), **{}): got RuntimeError('shape '[1, 64, 64]' is invalid for input of size 524288')"
  vllm_sweep_qwen:
    model: Qwen/Qwen-7B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  # vllm_sweep_qwen2:
  #   model: Qwen/Qwen2-0.5B
  #   gpu_memory_utilization: 0.001
  #   marks: [vllm_sweep]
  vllm_sweep_qwen2moe:
    model: Qwen/Qwen1.5-MoE-A2.7B
    status: "known_failure_xfail"
    reason: "RuntimeError: !at::functionalization::impl::isFunctionalTensor(t) INTERNAL ASSERT FAILED at '/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp':850, please report a bug to PyTorch. The composite op functionalization fallback expects its inputs all not to be functional tensors"
  # vllm_sweep_qwen3:
  #   model: Qwen/Qwen3-8B
  #   status: "known_failure_xfail"
  #   reason: "Model is too large for single chip"
  vllm_sweep_qwen3_moe:
    model: Qwen/Qwen3-30B-A3B
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_qwen3_next:
    model: Qwen/Qwen3-Next-80B-A3B-Instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_stablelm:
    model: stabilityai/stablelm-3b-4e1t
    gpu_memory_utilization: 0.004
    status: "expected_passing"
  vllm_sweep_seed_oss:
    model: ByteDance-Seed/Seed-OSS-36B-Instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_starcoder2:
    model: bigcode/starcoder2-3b
    status: "expected_passing"
  vllm_sweep_solar:
    model: upstage/solar-pro-preview-instruct
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_telechat2:
    model: Tele-AI/TeleChat2-3B
    gpu_memory_utilization: 0.004
    status: "expected_passing"
  # vllm_sweep_tele_flm:
  #   model: CofeAI/FLM-2-52B-Instruct-2407
  #   marks: [vllm_sweep]
  vllm_sweep_xverse:
    model: xverse/XVERSE-7B-Chat
    status: "known_failure_xfail"
    reason: "Exception: add_prefix_space does not match declared prepend_scheme at line 78 column 3"
  vllm_sweep_zamba2:
    model: Zyphra/Zamba2-1.2B-instruct
    status: "known_failure_xfail"
    reason: "RuntimeError: 6.mixer - While executing %auto_functionalized"
