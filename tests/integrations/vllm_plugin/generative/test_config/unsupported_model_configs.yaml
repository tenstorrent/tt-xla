# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

# vLLM generative models that are not supported by vLLM (architecture not in vLLM supported list).
# Moved by scripts/move_unsupported_vllm_models.py. Do not run these in test_generative_models.

model_configs:
  gpt_neo_causal_lm_pytorch_gpt_neo_125m:
    model: EleutherAI/gpt-neo-125M
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gpt_neo_causal_lm_pytorch_gpt_neo_1_3b:
    model: EleutherAI/gpt-neo-1.3B
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  gpt_neo_causal_lm_pytorch_gpt_neo_2_7b:
    model: EleutherAI/gpt-neo-2.7B
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  xglm_pytorch_xglm_564m:
    model: facebook/xglm-564M
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  xglm_pytorch_xglm_1_7b:
    model: facebook/xglm-1.7B
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  t5_pytorch_google_flan_t5_small:
    model: google/flan-t5-small
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 512
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  t5_pytorch_google_flan_t5_base:
    model: google/flan-t5-base
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 512
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  t5_pytorch_google_flan_t5_large:
    model: google/flan-t5-large
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 512
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  bert_question_answering_pytorch_phiyodr_bert_large_finetuned_squad2:
    model: phiyodr/bert-large-finetuned-squad2
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 384
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  bert_question_answering_pytorch_bert_large_cased_whole_word_masking_finetuned_squad:
    model: bert-large-cased-whole-word-masking-finetuned-squad
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 384
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  codegen_pytorch_salesforce_codegen_350m_mono:
    model: Salesforce/codegen-350M-mono
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  codegen_pytorch_salesforce_codegen_350m_multi:
    model: Salesforce/codegen-350M-multi
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  codegen_pytorch_salesforce_codegen_350m_nl:
    model: Salesforce/codegen-350M-nl
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 256
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  distilbert_question_answering_pytorch_distilbert_base_cased_distilled_squad:
    model: distilbert-base-cased-distilled-squad
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 384
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  dpr_question_encoder_pytorch_facebook_dpr_question_encoder_single_nq_base:
    model: facebook/dpr-question_encoder-single-nq-base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  dpr_question_encoder_pytorch_facebook_dpr_question_encoder_multiset_base:
    model: facebook/dpr-question_encoder-multiset-base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  dpr_context_encoder_pytorch_facebook_dpr_ctx_encoder_single_nq_base:
    model: facebook/dpr-ctx_encoder-single-nq-base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  dpr_context_encoder_pytorch_facebook_dpr_ctx_encoder_multiset_base:
    model: facebook/dpr-ctx_encoder-multiset-base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  dpr_reader_pytorch_facebook_dpr_reader_single_nq_base:
    model: facebook/dpr-reader-single-nq-base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  dpr_reader_pytorch_facebook_dpr_reader_multiset_base:
    model: facebook/dpr-reader-multiset-base
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  albert_question_answering_pytorch_squad2:
    model: twmkn9/albert-base-v2-squad2
    max_num_batched_tokens: 128
    max_num_seqs: 1
    max_model_len: 128
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  t5_pytorch_t5_small:
    model: t5-small
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 512
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  t5_pytorch_t5_base:
    model: t5-base
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 512
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
  t5_pytorch_t5_large:
    model: t5-large
    max_num_batched_tokens: 256
    max_num_seqs: 1
    max_model_len: 512
    gpu_memory_utilization: 0.002
    additional_config:
      enable_const_eval: false
      min_context_len: 32
    prompts:
    - Hello, my name is
    marks:
    - push
    - single_device
