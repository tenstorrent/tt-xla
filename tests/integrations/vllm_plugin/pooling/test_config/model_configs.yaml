# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

# vLLM pooling (embedding) model test configurations.
# Defaults are applied in test_config/__init__.py (DEFAULT_POOLING_CONFIG).
# Only "model" is required. "marks" default to [single_device]; add marks only when overriding.
# "status" defaults to "unspecified" when omitted.

model_configs:
  vllm_sweep_bge_m3:
    model: "BAAI/bge-m3"
    max_model_len: 512
    max_num_batched_tokens: 512
    max_num_seqs: 2
    prompts:
      - "Hello, my name is"
      - "The capital of France is Paris"
    status: "expected_passing"
  vllm_sweep_bge_base:
    model: "BAAI/bge-base-en"
    max_model_len: 64
    max_num_seqs: 2
    prompts:
      - "Hello, my name is"
      - "The capital of France is Paris"
    status: "expected_passing"
  vllm_sweep_bge_small:
    model: "BAAI/bge-small-en"
    max_model_len: 64
    max_num_seqs: 2
    status: "expected_passing"
  vllm_sweep_e5_small:
    model: "intfloat/e5-small-v2"
    max_model_len: 64
    max_num_seqs: 2
    prompts:
      - "Hello, my name is"
      - "The capital of France is Paris"
    status: "expected_passing"
  vllm_sweep_e5_base:
    model: "intfloat/e5-base-v2"
    max_model_len: 64
    max_num_seqs: 2
    status: "expected_passing"
  vllm_sweep_qwen3_embedding_0_6b:
    model: "Qwen/Qwen3-Embedding-0.6B"
    status: "expected_passing"
  vllm_sweep_qwen3_embedding_4b:
    model: "Qwen/Qwen3-Embedding-4B"
    status: "expected_passing"
  vllm_sweep_all_minilm:
    model: "sentence-transformers/all-MiniLM-L6-v2"
    max_model_len: 64
    max_num_seqs: 2
    status: "expected_passing"
  vllm_sweep_multi_qa_minilm:
    model: "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"
    max_model_len: 64
    max_num_seqs: 2
    status: "expected_passing"
  vllm_sweep_e5_mistral_7b_instruct:
    model: "intfloat/e5-mistral-7b-instruct"
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_gritlm:
    model: "parasail-ai/GritLM-7B-vllm"
    status: "known_failure_xfail"
    reason: "Model is too large for single chip"
  vllm_sweep_roberta:
    model: "sentence-transformers/nli-roberta-base-v2"
    max_model_len: 64
    max_num_seqs: 2
    status: "expected_passing"
  vllm_sweep_xlm_roberta:
    model: "sentence-transformers/stsb-xlm-r-multilingual"
    max_model_len: 64
    max_num_seqs: 2
    status: "expected_passing"
  vllm_sweep_splade:
    model: "naver/splade-v3"
    status: "known_failure_xfail"
    reason: "Gated huggingface model"
  vllm_sweep_bge_multilingual_gemma2:
    model: "BAAI/bge-multilingual-gemma2"
    status: "known_failure_xfail"
    reason: "Unknown failure"
  vllm_sweep_embedding_gemma3:
    model: "google/embeddinggemma-300m"
    status: "known_failure_xfail"
    reason: "missing a required argument: 'intermediate_tensors'"
  vllm_sweep_arctic_embed_m_v2:
    model: "Snowflake/snowflake-arctic-embed-m-v2.0"
    status: "expected_passing"
  vllm_sweep_gte_multilingual:
    model: "Alibaba-NLP/gte-multilingual-base"
    hf_overrides: {"architectures": ["GteNewModel"]}
    status: "expected_passing"
  vllm_sweep_modernbert:
    model: "Alibaba-NLP/gte-modernbert-base"
    status: "expected_passing"
  vllm_sweep_nomic_embed:
    model: "nomic-ai/nomic-embed-text-v1"
    status: "expected_passing"
  vllm_sweep_llama_nemotron_embed:
    model: "nvidia/llama-nemotron-embed-1b-v2"
    status: "known_failure_xfail"
    reason: "ImportError: Transformers modeling backend requires transformers>=5.0.0.dev0 for encoder models support, but got 4.57.1"
  vllm_sweep_qwen2_embed:
    model: "ssmits/Qwen2-7B-Instruct-embed-base"
    pooler_config: {"pooling_type": "MEAN"}
    status: "known_failure_xfail"
    reason: "ValueError: Following weights were not initialized from checkpoint: {'lm_head.weight'}"
