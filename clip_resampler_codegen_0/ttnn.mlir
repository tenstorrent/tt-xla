#dram = #ttnn.buffer_type<dram>
#loc = loc(unknown)
#loc5 = loc("p22.211")
#loc7 = loc("p118.755")
#loc8 = loc("p334.1979")
#loc9 = loc("p274.1639")
#loc14 = loc("p202.1231")
#loc25 = loc("p46.347")
#loc26 = loc("p106.687")
#loc30 = loc("p298.1775")
#loc38 = loc("p286.1707")
#loc55 = loc("p58.415")
#loc68 = loc("p94.619")
#loc72 = loc("p154.959")
#loc86 = loc("p130.823")
#loc104 = loc("p34.279")
#loc109 = loc("p166.1027")
#loc125 = loc("p358.2115")
#loc129 = loc("p370.2183")
#loc134 = loc("p70.483")
#loc135 = loc("p322.1911")
#loc136 = loc("p226.1367")
#loc143 = loc("p178.1095")
#loc144 = loc("p262.1571")
#loc145 = loc("p142.891")
#loc151 = loc("p190.1163")
#loc157 = loc("p346.2047")
#loc168 = loc("p82.551")
#loc169 = loc("p310.1843")
#loc174 = loc("p382.2251")
#loc208 = loc("p214.1299")
#loc228 = loc("p250.1503")
#loc231 = loc("p238.1435")
#loc233 = loc("p0.2")
#loc234 = loc("p1.7")
#loc235 = loc("p2.15")
#loc236 = loc("p3.19")
#loc237 = loc("p4.32")
#loc238 = loc("p5.38")
#loc239 = loc("p6.43")
#loc240 = loc("p7.49")
#loc241 = loc("p8.54")
#loc242 = loc("p9.139")
#loc243 = loc("p10.144")
#loc244 = loc("p11.152")
#loc245 = loc("p12.156")
#loc246 = loc("p13.163")
#loc247 = loc("p14.167")
#loc248 = loc("p15.173")
#loc249 = loc("p16.177")
#loc250 = loc("p17.183")
#loc251 = loc("p18.188")
#loc252 = loc("p19.197")
#loc253 = loc("p20.201")
#loc254 = loc("p21.207")
#loc255 = loc("p23.217")
#loc256 = loc("p24.222")
#loc257 = loc("p25.231")
#loc258 = loc("p26.235")
#loc259 = loc("p27.241")
#loc260 = loc("p28.245")
#loc261 = loc("p29.251")
#loc262 = loc("p30.256")
#loc263 = loc("p31.265")
#loc264 = loc("p32.269")
#loc265 = loc("p33.275")
#loc266 = loc("p35.285")
#loc267 = loc("p36.290")
#loc268 = loc("p37.299")
#loc269 = loc("p38.303")
#loc270 = loc("p39.309")
#loc271 = loc("p40.313")
#loc272 = loc("p41.319")
#loc273 = loc("p42.324")
#loc274 = loc("p43.333")
#loc275 = loc("p44.337")
#loc276 = loc("p45.343")
#loc277 = loc("p47.353")
#loc278 = loc("p48.358")
#loc279 = loc("p49.367")
#loc280 = loc("p50.371")
#loc281 = loc("p51.377")
#loc282 = loc("p52.381")
#loc283 = loc("p53.387")
#loc284 = loc("p54.392")
#loc285 = loc("p55.401")
#loc286 = loc("p56.405")
#loc287 = loc("p57.411")
#loc288 = loc("p59.421")
#loc289 = loc("p60.426")
#loc290 = loc("p61.435")
#loc291 = loc("p62.439")
#loc292 = loc("p63.445")
#loc293 = loc("p64.449")
#loc294 = loc("p65.455")
#loc295 = loc("p66.460")
#loc296 = loc("p67.469")
#loc297 = loc("p68.473")
#loc298 = loc("p69.479")
#loc299 = loc("p71.489")
#loc300 = loc("p72.494")
#loc301 = loc("p73.503")
#loc302 = loc("p74.507")
#loc303 = loc("p75.513")
#loc304 = loc("p76.517")
#loc305 = loc("p77.523")
#loc306 = loc("p78.528")
#loc307 = loc("p79.537")
#loc308 = loc("p80.541")
#loc309 = loc("p81.547")
#loc310 = loc("p83.557")
#loc311 = loc("p84.562")
#loc312 = loc("p85.571")
#loc313 = loc("p86.575")
#loc314 = loc("p87.581")
#loc315 = loc("p88.585")
#loc316 = loc("p89.591")
#loc317 = loc("p90.596")
#loc318 = loc("p91.605")
#loc319 = loc("p92.609")
#loc320 = loc("p93.615")
#loc321 = loc("p95.625")
#loc322 = loc("p96.630")
#loc323 = loc("p97.639")
#loc324 = loc("p98.643")
#loc325 = loc("p99.649")
#loc326 = loc("p100.653")
#loc327 = loc("p101.659")
#loc328 = loc("p102.664")
#loc329 = loc("p103.673")
#loc330 = loc("p104.677")
#loc331 = loc("p105.683")
#loc332 = loc("p107.693")
#loc333 = loc("p108.698")
#loc334 = loc("p109.707")
#loc335 = loc("p110.711")
#loc336 = loc("p111.717")
#loc337 = loc("p112.721")
#loc338 = loc("p113.727")
#loc339 = loc("p114.732")
#loc340 = loc("p115.741")
#loc341 = loc("p116.745")
#loc342 = loc("p117.751")
#loc343 = loc("p119.761")
#loc344 = loc("p120.766")
#loc345 = loc("p121.775")
#loc346 = loc("p122.779")
#loc347 = loc("p123.785")
#loc348 = loc("p124.789")
#loc349 = loc("p125.795")
#loc350 = loc("p126.800")
#loc351 = loc("p127.809")
#loc352 = loc("p128.813")
#loc353 = loc("p129.819")
#loc354 = loc("p131.829")
#loc355 = loc("p132.834")
#loc356 = loc("p133.843")
#loc357 = loc("p134.847")
#loc358 = loc("p135.853")
#loc359 = loc("p136.857")
#loc360 = loc("p137.863")
#loc361 = loc("p138.868")
#loc362 = loc("p139.877")
#loc363 = loc("p140.881")
#loc364 = loc("p141.887")
#loc365 = loc("p143.897")
#loc366 = loc("p144.902")
#loc367 = loc("p145.911")
#loc368 = loc("p146.915")
#loc369 = loc("p147.921")
#loc370 = loc("p148.925")
#loc371 = loc("p149.931")
#loc372 = loc("p150.936")
#loc373 = loc("p151.945")
#loc374 = loc("p152.949")
#loc375 = loc("p153.955")
#loc376 = loc("p155.965")
#loc377 = loc("p156.970")
#loc378 = loc("p157.979")
#loc379 = loc("p158.983")
#loc380 = loc("p159.989")
#loc381 = loc("p160.993")
#loc382 = loc("p161.999")
#loc383 = loc("p162.1004")
#loc384 = loc("p163.1013")
#loc385 = loc("p164.1017")
#loc386 = loc("p165.1023")
#loc387 = loc("p167.1033")
#loc388 = loc("p168.1038")
#loc389 = loc("p169.1047")
#loc390 = loc("p170.1051")
#loc391 = loc("p171.1057")
#loc392 = loc("p172.1061")
#loc393 = loc("p173.1067")
#loc394 = loc("p174.1072")
#loc395 = loc("p175.1081")
#loc396 = loc("p176.1085")
#loc397 = loc("p177.1091")
#loc398 = loc("p179.1101")
#loc399 = loc("p180.1106")
#loc400 = loc("p181.1115")
#loc401 = loc("p182.1119")
#loc402 = loc("p183.1125")
#loc403 = loc("p184.1129")
#loc404 = loc("p185.1135")
#loc405 = loc("p186.1140")
#loc406 = loc("p187.1149")
#loc407 = loc("p188.1153")
#loc408 = loc("p189.1159")
#loc409 = loc("p191.1169")
#loc410 = loc("p192.1174")
#loc411 = loc("p193.1183")
#loc412 = loc("p194.1187")
#loc413 = loc("p195.1193")
#loc414 = loc("p196.1197")
#loc415 = loc("p197.1203")
#loc416 = loc("p198.1208")
#loc417 = loc("p199.1217")
#loc418 = loc("p200.1221")
#loc419 = loc("p201.1227")
#loc420 = loc("p203.1237")
#loc421 = loc("p204.1242")
#loc422 = loc("p205.1251")
#loc423 = loc("p206.1255")
#loc424 = loc("p207.1261")
#loc425 = loc("p208.1265")
#loc426 = loc("p209.1271")
#loc427 = loc("p210.1276")
#loc428 = loc("p211.1285")
#loc429 = loc("p212.1289")
#loc430 = loc("p213.1295")
#loc431 = loc("p215.1305")
#loc432 = loc("p216.1310")
#loc433 = loc("p217.1319")
#loc434 = loc("p218.1323")
#loc435 = loc("p219.1329")
#loc436 = loc("p220.1333")
#loc437 = loc("p221.1339")
#loc438 = loc("p222.1344")
#loc439 = loc("p223.1353")
#loc440 = loc("p224.1357")
#loc441 = loc("p225.1363")
#loc442 = loc("p227.1373")
#loc443 = loc("p228.1378")
#loc444 = loc("p229.1387")
#loc445 = loc("p230.1391")
#loc446 = loc("p231.1397")
#loc447 = loc("p232.1401")
#loc448 = loc("p233.1407")
#loc449 = loc("p234.1412")
#loc450 = loc("p235.1421")
#loc451 = loc("p236.1425")
#loc452 = loc("p237.1431")
#loc453 = loc("p239.1441")
#loc454 = loc("p240.1446")
#loc455 = loc("p241.1455")
#loc456 = loc("p242.1459")
#loc457 = loc("p243.1465")
#loc458 = loc("p244.1469")
#loc459 = loc("p245.1475")
#loc460 = loc("p246.1480")
#loc461 = loc("p247.1489")
#loc462 = loc("p248.1493")
#loc463 = loc("p249.1499")
#loc464 = loc("p251.1509")
#loc465 = loc("p252.1514")
#loc466 = loc("p253.1523")
#loc467 = loc("p254.1527")
#loc468 = loc("p255.1533")
#loc469 = loc("p256.1537")
#loc470 = loc("p257.1543")
#loc471 = loc("p258.1548")
#loc472 = loc("p259.1557")
#loc473 = loc("p260.1561")
#loc474 = loc("p261.1567")
#loc475 = loc("p263.1577")
#loc476 = loc("p264.1582")
#loc477 = loc("p265.1591")
#loc478 = loc("p266.1595")
#loc479 = loc("p267.1601")
#loc480 = loc("p268.1605")
#loc481 = loc("p269.1611")
#loc482 = loc("p270.1616")
#loc483 = loc("p271.1625")
#loc484 = loc("p272.1629")
#loc485 = loc("p273.1635")
#loc486 = loc("p275.1645")
#loc487 = loc("p276.1650")
#loc488 = loc("p277.1659")
#loc489 = loc("p278.1663")
#loc490 = loc("p279.1669")
#loc491 = loc("p280.1673")
#loc492 = loc("p281.1679")
#loc493 = loc("p282.1684")
#loc494 = loc("p283.1693")
#loc495 = loc("p284.1697")
#loc496 = loc("p285.1703")
#loc497 = loc("p287.1713")
#loc498 = loc("p288.1718")
#loc499 = loc("p289.1727")
#loc500 = loc("p290.1731")
#loc501 = loc("p291.1737")
#loc502 = loc("p292.1741")
#loc503 = loc("p293.1747")
#loc504 = loc("p294.1752")
#loc505 = loc("p295.1761")
#loc506 = loc("p296.1765")
#loc507 = loc("p297.1771")
#loc508 = loc("p299.1781")
#loc509 = loc("p300.1786")
#loc510 = loc("p301.1795")
#loc511 = loc("p302.1799")
#loc512 = loc("p303.1805")
#loc513 = loc("p304.1809")
#loc514 = loc("p305.1815")
#loc515 = loc("p306.1820")
#loc516 = loc("p307.1829")
#loc517 = loc("p308.1833")
#loc518 = loc("p309.1839")
#loc519 = loc("p311.1849")
#loc520 = loc("p312.1854")
#loc521 = loc("p313.1863")
#loc522 = loc("p314.1867")
#loc523 = loc("p315.1873")
#loc524 = loc("p316.1877")
#loc525 = loc("p317.1883")
#loc526 = loc("p318.1888")
#loc527 = loc("p319.1897")
#loc528 = loc("p320.1901")
#loc529 = loc("p321.1907")
#loc530 = loc("p323.1917")
#loc531 = loc("p324.1922")
#loc532 = loc("p325.1931")
#loc533 = loc("p326.1935")
#loc534 = loc("p327.1941")
#loc535 = loc("p328.1945")
#loc536 = loc("p329.1951")
#loc537 = loc("p330.1956")
#loc538 = loc("p331.1965")
#loc539 = loc("p332.1969")
#loc540 = loc("p333.1975")
#loc541 = loc("p335.1985")
#loc542 = loc("p336.1990")
#loc543 = loc("p337.1999")
#loc544 = loc("p338.2003")
#loc545 = loc("p339.2009")
#loc546 = loc("p340.2013")
#loc547 = loc("p341.2019")
#loc548 = loc("p342.2024")
#loc549 = loc("p343.2033")
#loc550 = loc("p344.2037")
#loc551 = loc("p345.2043")
#loc552 = loc("p347.2053")
#loc553 = loc("p348.2058")
#loc554 = loc("p349.2067")
#loc555 = loc("p350.2071")
#loc556 = loc("p351.2077")
#loc557 = loc("p352.2081")
#loc558 = loc("p353.2087")
#loc559 = loc("p354.2092")
#loc560 = loc("p355.2101")
#loc561 = loc("p356.2105")
#loc562 = loc("p357.2111")
#loc563 = loc("p359.2121")
#loc564 = loc("p360.2126")
#loc565 = loc("p361.2135")
#loc566 = loc("p362.2139")
#loc567 = loc("p363.2145")
#loc568 = loc("p364.2149")
#loc569 = loc("p365.2155")
#loc570 = loc("p366.2160")
#loc571 = loc("p367.2169")
#loc572 = loc("p368.2173")
#loc573 = loc("p369.2179")
#loc574 = loc("p371.2189")
#loc575 = loc("p372.2194")
#loc576 = loc("p373.2203")
#loc577 = loc("p374.2207")
#loc578 = loc("p375.2213")
#loc579 = loc("p376.2217")
#loc580 = loc("p377.2223")
#loc581 = loc("p378.2228")
#loc582 = loc("p379.2237")
#loc583 = loc("p380.2241")
#loc584 = loc("p381.2247")
#loc585 = loc("p383.2257")
#loc586 = loc("p384.2262")
#loc587 = loc("p385.2270")
#loc588 = loc("p386.2275")
#loc589 = loc("p387.2283")
#loc590 = loc("p388.2288")
#loc591 = loc("p389.2295")
#loc592 = loc("p390.2297")
#loc593 = loc("p391.2302")
#loc594 = loc("p392.2478")
#loc595 = loc("p393.2482")
#loc596 = loc("p394.2502")
#loc597 = loc("p395.2506")
#loc598 = loc("p396.2788")
#loc599 = loc("p397.2792")
#loc600 = loc("p398.2812")
#loc601 = loc("p399.2816")
#loc602 = loc("p400.3098")
#loc603 = loc("p401.3102")
#loc604 = loc("p402.3122")
#loc605 = loc("p403.3126")
#loc606 = loc("p404.3408")
#loc607 = loc("p405.3412")
#loc608 = loc("p406.3432")
#loc609 = loc("p407.3436")
#loc610 = loc("p408.3718")
#loc611 = loc("p409.3722")
#loc612 = loc("p410.3742")
#loc613 = loc("p411.3746")
#loc614 = loc("p412.4028")
#loc615 = loc("p413.4032")
#loc616 = loc("p414.4052")
#loc617 = loc("p415.4056")
#loc618 = loc("p416.4338")
#loc619 = loc("p417.4342")
#loc620 = loc("p418.4362")
#loc621 = loc("p419.4366")
#loc622 = loc("p420.4648")
#loc623 = loc("p421.4652")
#loc624 = loc("p422.4672")
#loc625 = loc("p423.4676")
#loc626 = loc("p424.4958")
#loc627 = loc("p425.4962")
#loc628 = loc("p426.4982")
#loc629 = loc("p427.4986")
#loc630 = loc("p428.5268")
#loc631 = loc("p429.5272")
#loc632 = loc("p430.5292")
#loc633 = loc("p431.5296")
#loc634 = loc("p432.5578")
#loc635 = loc("p433.5582")
#loc636 = loc("p434.5602")
#loc637 = loc("p435.5606")
#loc638 = loc("p436.5888")
#loc639 = loc("p437.5892")
#loc640 = loc("p438.5912")
#loc641 = loc("p439.5916")
#loc642 = loc("p440.6198")
#loc643 = loc("p441.6202")
#loc644 = loc("p442.6222")
#loc645 = loc("p443.6226")
#loc646 = loc("p444.6508")
#loc647 = loc("p445.6512")
#loc648 = loc("p446.6532")
#loc649 = loc("p447.6536")
#loc650 = loc("p448.6818")
#loc651 = loc("p449.6822")
#loc652 = loc("p450.6842")
#loc653 = loc("p451.6846")
#loc654 = loc("p452.7128")
#loc655 = loc("p453.7132")
#loc656 = loc("p454.7152")
#loc657 = loc("p455.7156")
#loc658 = loc("p456.7438")
#loc659 = loc("p457.7442")
#loc660 = loc("p458.7462")
#loc661 = loc("p459.7466")
#loc662 = loc("p460.7748")
#loc663 = loc("p461.7752")
#loc664 = loc("p462.7772")
#loc665 = loc("p463.7776")
#loc666 = loc("p464.8058")
#loc667 = loc("p465.8062")
#loc668 = loc("p466.8082")
#loc669 = loc("p467.8086")
#loc670 = loc("p468.8368")
#loc671 = loc("p469.8372")
#loc672 = loc("p470.8392")
#loc673 = loc("p471.8396")
#loc674 = loc("p472.8678")
#loc675 = loc("p473.8682")
#loc676 = loc("p474.8702")
#loc677 = loc("p475.8706")
#loc678 = loc("p476.8988")
#loc679 = loc("p477.8992")
#loc680 = loc("p478.9012")
#loc681 = loc("p479.9016")
#loc682 = loc("p480.9298")
#loc683 = loc("p481.9302")
#loc684 = loc("p482.9322")
#loc685 = loc("p483.9326")
#loc686 = loc("p484.9608")
#loc687 = loc("p485.9612")
#loc688 = loc("p486.9632")
#loc689 = loc("p487.9636")
#loc690 = loc("p488.9918")
#loc691 = loc("p489.9922")
#loc692 = loc("p490.9942")
#loc693 = loc("p491.9946")
#loc694 = loc("p492.10228")
#loc695 = loc("p493.10232")
#loc696 = loc("p494.10252")
#loc697 = loc("p495.10256")
#loc698 = loc("p496.10538")
#loc699 = loc("p497.10542")
#loc700 = loc("p498.10562")
#loc701 = loc("p499.10566")
#loc702 = loc("p500.10848")
#loc703 = loc("p501.10852")
#loc704 = loc("p502.10872")
#loc705 = loc("p503.10876")
#loc706 = loc("p504.11158")
#loc707 = loc("p505.11162")
#loc708 = loc("p506.11182")
#loc709 = loc("p507.11186")
#loc710 = loc("p508.11468")
#loc711 = loc("p509.11472")
#loc712 = loc("p510.11492")
#loc713 = loc("p511.11496")
#loc714 = loc("p512.11778")
#loc715 = loc("p513.11782")
#loc716 = loc("p514.11802")
#loc717 = loc("p515.11806")
#loc718 = loc("p516.12091")
#loc719 = loc("p517.12106")
#loc720 = loc("p518.12186")
#loc721 = loc("p519.12191")
#loc722 = loc("p520.12197")
#loc723 = loc("p521.12202")
#loc724 = loc("p522.12309")
#loc725 = loc("p523.12314")
#loc726 = loc("p524.12320")
#loc727 = loc("p525.12325")
#loc728 = loc("p526.12410")
#loc729 = loc("p527.12415")
#loc730 = loc("p528.12507")
#loc731 = loc("p529.12522")
#loc732 = loc("p530.12602")
#loc733 = loc("p531.12607")
#loc734 = loc("p532.12613")
#loc735 = loc("p533.12618")
#loc736 = loc("p534.12725")
#loc737 = loc("p535.12730")
#loc738 = loc("p536.12736")
#loc739 = loc("p537.12741")
#loc740 = loc("p538.12826")
#loc741 = loc("p539.12831")
#loc742 = loc("p540.12923")
#loc743 = loc("p541.12938")
#loc744 = loc("p542.13018")
#loc745 = loc("p543.13023")
#loc746 = loc("p544.13029")
#loc747 = loc("p545.13034")
#loc748 = loc("p546.13141")
#loc749 = loc("p547.13146")
#loc750 = loc("p548.13152")
#loc751 = loc("p549.13157")
#loc752 = loc("p550.13242")
#loc753 = loc("p551.13247")
#loc754 = loc("p552.13339")
#loc755 = loc("p553.13354")
#loc756 = loc("p554.13434")
#loc757 = loc("p555.13439")
#loc758 = loc("p556.13445")
#loc759 = loc("p557.13450")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 9x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 103552, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073051456, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1280xbf16, #system_memory>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 288 + d1, d2), <1x1>, memref<9x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1280xbf16, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x5120xbf16, #system_memory>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 288 + d1, d2), <1x1>, memref<9x160x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x5120xbf16, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x160x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x160x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1280x1280xbf16, #system_memory>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<120x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1280x1280xbf16, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<40x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 288 + d1, d2), <1x1>, memref<9x120x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 96 + d2, d3), <1x1>, memref<48x9x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 42 + d1 * 14 + d2, d3), <1x1>, memref<53760x14xbf16, #system_memory>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 608 + d1 * 608 + d2, d3), <1x1>, memref<19x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2048xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2048xbf16, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 640 + d1 * 32 + d2, d3), <1x1>, memref<20x9x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x257xsi32, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<257x1280xbf16, #system_memory>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1280 + d1, d2), <1x1>, memref<40x9x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x257xsi32, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x9x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x9x!ttcore.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x257xui32, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<257x1280xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4608 + d1 * 288 + d2, d3), <1x1>, memref<144x9x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1280 + d1, d2), <1x1>, memref<40x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x9x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 640 + d1 * 32 + d2, d3), <1x1>, memref<20x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 32 + d2, d3), <1x1>, memref<16x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 640 + d1 * 32 + d2, d3), <1x1>, memref<20x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4608 + d1 * 288 + d2, d3), <1x1>, memref<144x3x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1280 + d1 * 64 + d2, d3), <1x1>, memref<40x9x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<64x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<40x160x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<160x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 672 + d1 * 224 + d2, d3), <1x1>, memref<672x224xbf16, #dram>, <interleaved>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 672 + d1 * 224 + d2, d3), <1x1>, memref<21x7x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 224 + d2, d3), <1x1>, memref<1568x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 50176 + d2, d3), <1x1>, memref<1568x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 256 + d2, d3), <1x1>, memref<8x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 32 + d2, d3), <1x1>, memref<16x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 40960 + d1 * 32 + d2, d3), <1x1>, memref<1280x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 1280 + d1, d2), <1x1>, memref<40x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<9x40x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<9x120x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8224 + d1 * 32 + d2, d3), <1x1>, memref<257x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4608 + d1 * 288 + d2, d3), <1x1>, memref<144x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4608 + d1 * 288 + d2, d3), <1x1>, memref<144x9x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4608 + d1 * 288 + d2, d3), <1x1>, memref<144x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout58 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<9x160x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout59 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<9x40x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout60 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8736 + d1 * 32 + d2, d3), <1x1>, memref<273x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout61 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 640 + d1 * 32 + d2, d3), <1x1>, memref<20x9x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout62 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 640 + d1 * 32 + d2, d3), <1x1>, memref<20x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout63 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 5760 + d1 * 288 + d2, d3), <1x1>, memref<180x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout64 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x160x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout65 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x40x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout66 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 32 + d2, d3), <1x1>, memref<16x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout67 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.13641 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.13641 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<9x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func private @main_const_eval_0() -> tensor<1x20x16xbf16, #ttnn_layout> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x20x16>}> : (!ttnn.device) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc)
        return %1 : tensor<1x20x16xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_1(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc2)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc2)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc2)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_2(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc3)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc3)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc3)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc3)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_3(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc4)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc4)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc4)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc4)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_4(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc5)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc5)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc5)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc5)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_5(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc6)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc6)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc6)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc6)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_6(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc7)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc7)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc7)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc7)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_7(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc8)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc8)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc8)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc8)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_8(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc9)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc9)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc9)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc9)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_9(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc10)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc10)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc10)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc10)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_10(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc11)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc11)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc11)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc11)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc12)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc12)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc12)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc12)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc13)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc13)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc13)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc13)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc13)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc13)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc13)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc13)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_11(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc14)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc14)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc14)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc14)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_12(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc15)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc15)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc15)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc15)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_13(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc16)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc16)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc16)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc16)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc17)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc17)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc17)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc17)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc18)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc18)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc18)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc18)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc18)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc18)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc18)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc18)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_14(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc19)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc19)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc19)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc19)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc20)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc20)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc20)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc20)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc21)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc21)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc21)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc21)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc21)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc21)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc21)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc21)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_15(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc22)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc22)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc22)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc22)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc23)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc23)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc23)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc23)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc24)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc24)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc24)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc24)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc24)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc24)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc24)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc24)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_16(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc25)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc25)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc25)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc25)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_17(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc26)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc26)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc26)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc26)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_18(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc27)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc27)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc27)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc27)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc28)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc28)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc28)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc28)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc29)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc29)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc29)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc29)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc29)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc29)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc29)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc29)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_19(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc30)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc30)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc30)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc30)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_20(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc31)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc31)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc31)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc31)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_21(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc32)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc32)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc32)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc32)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_22() -> tensor<1x16x1280xbf16, #ttnn_layout5> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 1.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x16x1280>}> : (!ttnn.device) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc)
        return %1 : tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_23(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc33)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc33)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc33)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc33)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc34)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc34)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc34)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc34)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc35)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc35)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc35)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc35)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc35)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc35)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc35)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc35)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_24(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc36)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc36)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc36)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc36)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_25(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc37)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc37)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc37)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc37)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_26(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc38)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc38)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc38)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc38)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_27(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc39)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc39)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc39)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc39)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_28(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc40)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc40)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc40)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc40)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc41)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc41)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc41)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc41)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc42)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc42)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc42)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc42)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc42)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc42)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc42)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc42)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_29(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc43)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc43)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc43)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc43)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_30(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc44)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc44)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc44)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc44)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc45)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc45)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc45)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc45)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc46)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc46)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc46)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc46)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc46)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc46)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc46)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc46)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_31(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc47)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc47)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc47)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc47)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_32() -> tensor<1x16x80x257xf32, #ttnn_layout16> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 0.334370166 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x16x80x257>}> : (!ttnn.device) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc)
        return %1 : tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_33(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc48)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc48)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc48)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc48)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc49)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc49)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc49)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc49)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc50)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc50)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc50)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc50)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc50)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc50)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc50)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc50)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_34(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc51)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc51)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc51)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc51)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_35(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc52)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc52)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc52)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc52)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc53)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc53)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc53)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc53)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc54)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc54)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc54)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc54)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc54)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc54)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc54)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc54)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_36(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc55)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc55)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc55)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc55)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_37(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc56)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc56)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc56)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc56)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_38(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc57)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc57)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc57)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc57)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_39(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc58)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc58)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc58)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc58)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_40(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc59)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc59)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc59)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc59)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_41(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc60)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc60)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc60)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc60)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_42(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc61)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc61)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc61)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc61)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_43(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc62)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc62)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc62)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc62)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc63)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc63)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc63)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc63)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc64)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc64)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc64)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc64)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc64)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc64)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc64)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc64)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_44(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc65)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc65)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc65)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc65)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc66)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc66)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc66)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc66)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc67)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc67)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc67)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc67)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc67)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc67)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc67)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc67)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_45(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc68)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc68)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc68)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc68)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_46(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc69)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc69)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc69)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc69)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_47(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc70)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc70)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc70)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc70)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_48(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc71)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc71)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc71)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc71)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_49(%arg0: tensor<1280x3x14x14xbf16, #ttnn_layout17> {ttir.conv2d_weight} loc(unknown)) -> tensor<1x1x588x1280xbf16, #ttnn_layout18> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.prepare_conv2d_weights"(%arg0, %0) <{batch_size = 1 : i32, compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = true, act_block_h_override = 0, enable_kernel_stride_folding = false, config_tensors_in_dram = true>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, groups = 1 : i32, has_bias = false, in_channels = 3 : i32, input_dtype = #ttcore.supportedDataTypes<bf16>, input_height = 224 : i32, input_memory_config = #ttnn.memory_config<#dram, <interleaved>>, input_tensor_layout = #ttnn.layout<tile>, input_width = 224 : i32, kernel_size = array<i32: 14, 14>, out_channels = 1280 : i32, output_dtype = #ttcore.supportedDataTypes<bf16>, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 14, 14>, weights_format = "OIHW"}> : (tensor<1280x3x14x14xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x588x1280xbf16, #ttnn_layout18> loc(#loc1694)
        return %1 : tensor<1x1x588x1280xbf16, #ttnn_layout18> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_50(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc72)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc72)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc72)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc72)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_51(%arg0: tensor<2048xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x16x2048xbf16, #ttnn_layout20> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2048xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<2048xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<2048xbf16, #ttnn_layout21>) -> tensor<2048xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2048xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16, #ttnn_layout22>) -> tensor<1x1x2048xbf16, #ttnn_layout20> loc(#loc73)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2048xbf16, #ttnn_layout22>) -> () loc(#loc73)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x16x1>}> : (tensor<1x1x2048xbf16, #ttnn_layout20>) -> tensor<1x16x2048xbf16, #ttnn_layout20> loc(#loc73)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x2048xbf16, #ttnn_layout20>) -> () loc(#loc73)
        return %4 : tensor<1x16x2048xbf16, #ttnn_layout20> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_52(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc74)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc74)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc74)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc74)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_53() -> tensor<1x20x16x273xf32, #ttnn_layout23> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 0xFF800000 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x20x16x273>}> : (!ttnn.device) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc)
        return %1 : tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_54(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc75)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc75)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc75)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc75)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc76)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc76)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc76)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc76)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc77)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc77)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc77)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc77)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc77)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc77)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc77)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc77)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_55(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc78)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc78)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc78)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc78)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_56(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc79)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc79)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc79)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc79)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_57(%arg0: tensor<1x257xsi32, #ttnn_layout24> loc(unknown), %arg1: tensor<257x1280xbf16, #ttnn_layout25> loc(unknown)) -> tensor<1x1280x257xbf16, #ttnn_layout26> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x257xsi32, #ttnn_layout24>, !ttnn.device) -> tensor<1x257xsi32, #ttnn_layout27> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1x257xsi32, #ttnn_layout27>) -> tensor<1x257xsi32, #ttnn_layout28> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x257xsi32, #ttnn_layout27>) -> () loc(#loc)
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x257xsi32, #ttnn_layout28>) -> tensor<1x257xui32, #ttnn_layout29> loc(#loc80)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x257xsi32, #ttnn_layout28>) -> () loc(#loc80)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<row_major>}> : (tensor<1x257xui32, #ttnn_layout29>) -> tensor<1x257xui32, #ttnn_layout30> loc(#loc1695)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x257xui32, #ttnn_layout29>) -> () loc(#loc1695)
        %5 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<257x1280xbf16, #ttnn_layout25>, !ttnn.device) -> tensor<257x1280xbf16, #ttnn_layout31> loc(#loc1695)
        %6 = "ttnn.embedding"(%4, %5) : (tensor<1x257xui32, #ttnn_layout30>, tensor<257x1280xbf16, #ttnn_layout31>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc81)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout31>) -> () loc(#loc81)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x257xui32, #ttnn_layout30>) -> () loc(#loc81)
        %7 = "ttnn.permute"(%6) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x1280x257xbf16, #ttnn_layout26> loc(#loc82)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc82)
        return %7 : tensor<1x1280x257xbf16, #ttnn_layout26> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_58(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc83)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc83)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc83)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc83)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc84)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc84)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc84)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc84)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc85)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc85)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc85)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc85)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc85)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc85)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc85)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc85)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_59() -> tensor<1x16x257x257xf32, #ttnn_layout32> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x16x257x257>}> : (!ttnn.device) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc)
        return %1 : tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_60(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc86)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc86)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc86)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc86)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_61(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc87)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc87)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc87)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc87)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_62(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc88)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc88)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc88)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc88)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_63(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc89)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc89)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc89)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc89)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_64(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc90)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc90)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc90)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc90)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_65(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc91)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc91)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc91)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc91)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_66(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc92)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc92)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc92)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc92)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_67(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc93)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc93)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc93)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc93)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_68(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc94)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc94)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc94)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc94)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_69(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc95)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc95)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc95)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc95)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_70(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc96)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc96)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc96)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc96)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc97)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc97)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc97)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc97)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc98)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc98)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc98)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc98)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc98)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc98)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc98)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc98)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_71(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc99)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc99)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc99)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc99)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_72(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc100)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc100)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc100)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc100)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc101)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc101)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc101)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc101)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc102)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc102)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc102)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc102)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc102)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc102)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc102)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc102)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_73(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc103)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc103)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc103)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc103)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_74(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc104)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc104)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc104)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc104)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_75(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc105)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc105)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc105)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc105)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_76(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc106)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc106)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc106)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc106)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc107)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc107)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc107)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc107)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc108)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc108)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc108)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc108)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc108)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc108)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc108)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc108)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_77(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc109)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc109)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc109)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc109)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_78(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc110)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc110)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc110)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc110)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc111)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc111)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc111)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc111)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc112)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc112)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc112)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc112)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc112)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc112)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc112)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc112)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_79(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc113)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc113)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc113)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc113)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_80(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc114)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc114)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc114)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc114)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc115)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc115)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc115)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc115)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc116)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc116)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc116)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc116)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc116)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc116)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc116)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc116)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_81(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc117)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc117)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc117)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc117)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_82(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc118)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc118)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc118)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc118)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_83(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc119)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc119)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc119)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc119)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_84(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc120)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc120)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc120)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc120)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_85(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc121)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc121)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc121)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc121)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_86(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc122)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc122)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc122)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc122)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_87(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc123)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc123)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc123)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc123)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_88(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc124)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc124)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc124)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc124)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_89(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc125)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc125)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc125)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc125)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_90(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc126)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc126)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc126)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc126)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_91(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc127)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc127)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc127)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc127)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_92(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc128)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc128)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc128)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc128)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_93(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc129)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc129)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc129)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc129)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_94(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc130)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc130)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc130)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc130)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_95(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc131)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc131)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc131)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc131)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_96(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc132)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc132)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc132)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc132)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_97(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc133)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc133)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc133)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc133)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_98(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc134)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc134)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc134)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc134)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_99(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc135)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc135)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc135)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc135)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_100(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc136)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc136)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc136)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc136)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_101() -> tensor<1x16x257x257xf32, #ttnn_layout32> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 0xFF800000 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x16x257x257>}> : (!ttnn.device) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc)
        return %1 : tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_102(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc137)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc137)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc137)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc137)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc138)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc138)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc138)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc138)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc139)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc139)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc139)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc139)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc139)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc139)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc139)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc139)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_103(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc140)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc140)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc140)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc140)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_104(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc141)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc141)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc141)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc141)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_105(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc142)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc142)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc142)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc142)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_106(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc143)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc143)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc143)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc143)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_107(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc144)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc144)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc144)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc144)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_108(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc145)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc145)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc145)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc145)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_109(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc146)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc146)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc146)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc146)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc147)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc147)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc147)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc147)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc148)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc148)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc148)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc148)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc148)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc148)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc148)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc148)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_110(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc149)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc149)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc149)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc149)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_111(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc150)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc150)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc150)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc150)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_112(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc151)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc151)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc151)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc151)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_113(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc152)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc152)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc152)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc152)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc153)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc153)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc153)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc153)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc154)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc154)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc154)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc154)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc154)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc154)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc154)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc154)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_114(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc155)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc155)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc155)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc155)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_115(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc156)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc156)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc156)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc156)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_116(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc157)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc157)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc157)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc157)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_117(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc158)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc158)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc158)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc158)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc159)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc159)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc159)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc159)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc160)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc160)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc160)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc160)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc160)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc160)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc160)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc160)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_118(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc161)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc161)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc161)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc161)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_119(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc162)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc162)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc162)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc162)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_120(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc163)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc163)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc163)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc163)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_121(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc164)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc164)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc164)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc164)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_122(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x1280x1xbf16, #ttnn_layout33> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc165)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc165)
        %4 = "ttnn.permute"(%3) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x1280x1xbf16, #ttnn_layout33> loc(#loc82)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc82)
        return %4 : tensor<1x1280x1xbf16, #ttnn_layout33> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_123(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc166)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc166)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc166)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc166)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_124() -> tensor<1x20x16x273xf32, #ttnn_layout23> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x20x16x273>}> : (!ttnn.device) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc)
        return %1 : tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_125(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc167)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc167)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc167)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc167)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_126(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc168)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc168)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc168)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc168)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_127(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc169)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc169)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc169)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc169)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_128(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc170)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc170)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc170)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc170)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_129(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc171)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc171)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc171)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc171)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc172)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc172)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc172)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc172)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc173)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc173)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc173)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc173)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc173)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc173)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc173)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc173)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_130(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc174)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc174)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc174)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc174)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_131(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc175)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc175)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc175)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc175)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc176)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc176)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc176)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc176)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc177)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc177)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc177)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc177)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc177)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc177)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc177)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc177)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_132(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc178)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc178)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc178)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc178)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_133(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc179)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc179)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc179)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc179)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_134(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc180)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc180)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc180)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc180)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc181)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc181)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc181)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc181)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc182)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc182)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc182)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc182)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc182)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc182)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc182)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc182)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_135(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc183)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc183)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc183)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc183)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc184)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc184)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc184)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc184)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc185)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc185)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc185)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc185)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc185)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc185)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc185)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc185)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_136() -> tensor<1x16x257xbf16, #ttnn_layout34> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 0.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x16x257>}> : (!ttnn.device) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc)
        return %1 : tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_137(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc186)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc186)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc186)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc186)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_138(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc187)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc187)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc187)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc187)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_139(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc188)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc188)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc188)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc188)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_140(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc189)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc189)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc189)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc189)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_141(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc190)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc190)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc190)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc190)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_142(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc191)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc191)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc191)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc191)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_143(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc192)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc192)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc192)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc192)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_144(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc193)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc193)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc193)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc193)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_145(%arg0: tensor<1x16x1280xbf16, #ttnn_layout5> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg3: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<16x1280xbf16, #ttnn_layout36>) attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.to_device"(%arg3, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 0.353553385 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x20x16x64>}> : (!ttnn.device) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc)
        %8 = "ttnn.layer_norm"(%arg0, %4, %6) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc194)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc194)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc194)
        %9 = "ttnn.reshape"(%8) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc195)
        %10 = "ttnn.matmul"(%9, %2) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc196)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc196)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc196)
        %11 = "ttnn.reshape"(%10) <{shape = [1 : i32, 16 : i32, 20 : i32, 64 : i32]}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<1x16x20x64xbf16, #ttnn_layout37> loc(#loc197)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc197)
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x20x64xbf16, #ttnn_layout37>) -> tensor<1x20x16x64xbf16, #ttnn_layout38> loc(#loc198)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x16x20x64xbf16, #ttnn_layout37>) -> () loc(#loc198)
        %13 = "ttnn.typecast"(%12) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc199)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> () loc(#loc199)
        %14 = "ttnn.multiply"(%13, %7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x16x64xf32, #ttnn_layout35>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc200)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc200)
        %15 = "ttnn.reshape"(%8) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc201)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc201)
        return %7, %14, %15 : tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<16x1280xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_146(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc202)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc202)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc202)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc202)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc203)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc203)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc203)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc203)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc204)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc204)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc204)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc204)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc204)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc204)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc204)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc204)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_147(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc205)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc205)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc205)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc205)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_148(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc206)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc206)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc206)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc206)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_149(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc207)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc207)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc207)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc207)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_150(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc208)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc208)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc208)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc208)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_151() -> tensor<1x16x257x80xf32, #ttnn_layout39> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 0.334370166 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x16x257x80>}> : (!ttnn.device) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc)
        return %1 : tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_152(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc209)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc209)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc209)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc209)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_153(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc210)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc210)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc210)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc210)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc211)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc211)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc211)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc211)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc212)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc212)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc212)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc212)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc212)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc212)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc212)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc212)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_154(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc213)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc213)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc213)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc213)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_155(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc214)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc214)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc214)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc214)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_156(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc215)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc215)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc215)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc215)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_157(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc216)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc216)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc216)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc216)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_158(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc217)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc217)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc217)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc217)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_159(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc218)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc218)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc218)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc218)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_160(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc219)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc219)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc219)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc219)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_161(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc220)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc220)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc220)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc220)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_162(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc221)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc221)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc221)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc221)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc222)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc222)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc222)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc222)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc223)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc223)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc223)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc223)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc223)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc223)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc223)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc223)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_163(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg1: tensor<1280xbf16, #ttnn_layout1> loc(unknown), %arg2: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x3840xbf16, #ttnn_layout15> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %7 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc224)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc224)
        %8 = "ttnn.repeat"(%7) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc224)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc224)
        %9 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc225)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc225)
        %10 = "ttnn.repeat"(%9) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc225)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc225)
        %11 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc226)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc226)
        %12 = "ttnn.repeat"(%11) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc226)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc226)
        %13 = "ttnn.concat"(%8, %10, %12) <{dim = 2 : si32}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc226)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc226)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc226)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc226)
        return %13 : tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_164(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc227)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc227)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc227)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc227)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_165() -> tensor<1x20x64x273xf32, #ttnn_layout40> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 0.353553385 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x20x64x273>}> : (!ttnn.device) -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc)
        return %1 : tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_166(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc228)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc228)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc228)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc228)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_167(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc229)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc229)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc229)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc229)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_168(%arg0: tensor<5120xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x257x5120xbf16, #ttnn_layout7> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<5120xbf16, #ttnn_layout6>, !ttnn.device) -> tensor<5120xbf16, #ttnn_layout8> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<5120xbf16, #ttnn_layout8>) -> tensor<5120xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<5120xbf16, #ttnn_layout8>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16, #ttnn_layout9>) -> tensor<1x1x5120xbf16, #ttnn_layout10> loc(#loc230)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<5120xbf16, #ttnn_layout9>) -> () loc(#loc230)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc230)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x5120xbf16, #ttnn_layout10>) -> () loc(#loc230)
        return %4 : tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_169(%arg0: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg1: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown), %arg2: tensor<1280x1280xbf16, #ttnn_layout11> loc(unknown)) -> tensor<3840x1280xbf16, #ttnn_layout12> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg2, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %3 = "ttnn.to_device"(%arg1, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %4 = "ttnn.to_layout"(%3) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %5 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280x1280xbf16, #ttnn_layout11>, !ttnn.device) -> tensor<1280x1280xbf16, #ttnn_layout13> loc(#loc)
        %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> tensor<1280x1280xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout13>) -> () loc(#loc)
        %7 = "ttnn.concat"(%2, %4, %6) <{dim = 0 : si32}> : (tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc231)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc231)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc231)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc231)
        return %7 : tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_170(%arg0: tensor<1280xbf16, #ttnn_layout1> loc(unknown)) -> tensor<1x257x1280xbf16, #ttnn_layout2> attributes {tt.function_type = "const_eval"} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1280xbf16, #ttnn_layout1>, !ttnn.device) -> tensor<1280xbf16, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<1280xbf16, #ttnn_layout3>) -> tensor<1280xbf16, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1280xbf16, #ttnn_layout3>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x1x1280xbf16, #ttnn_layout5> loc(#loc232)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc232)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x257x1>}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc232)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x1280xbf16, #ttnn_layout5>) -> () loc(#loc232)
        return %4 : tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<2048xbf16, #ttnn_layout22> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_norm_out_bias"} loc("p0.2"), %arg1: tensor<2048xbf16, #ttnn_layout22> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_norm_out_weight"} loc("p1.7"), %arg2: tensor<2048xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_proj_out_bias"} loc("p2.15"), %arg3: tensor<2048x1280xbf16, #ttnn_layout41> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_proj_out_weight"} loc("p3.19"), %arg4: tensor<1x16x1280xbf16, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_latents"} loc("p4.32"), %arg5: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_attn_to_out_0_weight"} loc("p5.38"), %arg6: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_attn_to_v_weight"} loc("p6.43"), %arg7: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_ln1_bias"} loc("p7.49"), %arg8: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_ln1_weight"} loc("p8.54"), %arg9: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_ln0_bias"} loc("p9.139"), %arg10: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_ln0_weight"} loc("p10.144"), %arg11: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_proj_in_bias"} loc("p11.152"), %arg12: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_proj_in_weight"} loc("p12.156"), %arg13: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_bias"} loc("p13.163"), %arg14: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_weight"} loc("p14.167"), %arg15: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_bias"} loc("p15.173"), %arg16: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_weight"} loc("p16.177"), %arg17: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_bias"} loc("p17.183"), %arg18: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_weight"} loc("p18.188"), %arg19: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_bias"} loc("p19.197"), %arg20: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_weight"} loc("p20.201"), %arg21: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_bias"} loc("p21.207"), %arg22: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_weight"} loc("p22.211"), %arg23: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_bias"} loc("p23.217"), %arg24: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_weight"} loc("p24.222"), %arg25: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_bias"} loc("p25.231"), %arg26: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_weight"} loc("p26.235"), %arg27: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_bias"} loc("p27.241"), %arg28: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_weight"} loc("p28.245"), %arg29: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_bias"} loc("p29.251"), %arg30: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_weight"} loc("p30.256"), %arg31: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_bias"} loc("p31.265"), %arg32: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_weight"} loc("p32.269"), %arg33: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_bias"} loc("p33.275"), %arg34: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_weight"} loc("p34.279"), %arg35: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_bias"} loc("p35.285"), %arg36: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_weight"} loc("p36.290"), %arg37: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_bias"} loc("p37.299"), %arg38: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_weight"} loc("p38.303"), %arg39: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_bias"} loc("p39.309"), %arg40: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_weight"} loc("p40.313"), %arg41: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_bias"} loc("p41.319"), %arg42: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_weight"} loc("p42.324"), %arg43: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_bias"} loc("p43.333"), %arg44: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_weight"} loc("p44.337"), %arg45: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_bias"} loc("p45.343"), %arg46: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_weight"} loc("p46.347"), %arg47: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_bias"} loc("p47.353"), %arg48: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_weight"} loc("p48.358"), %arg49: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_bias"} loc("p49.367"), %arg50: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_weight"} loc("p50.371"), %arg51: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_bias"} loc("p51.377"), %arg52: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_weight"} loc("p52.381"), %arg53: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_bias"} loc("p53.387"), %arg54: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_weight"} loc("p54.392"), %arg55: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_bias"} loc("p55.401"), %arg56: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_weight"} loc("p56.405"), %arg57: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_bias"} loc("p57.411"), %arg58: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_weight"} loc("p58.415"), %arg59: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_bias"} loc("p59.421"), %arg60: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_weight"} loc("p60.426"), %arg61: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_bias"} loc("p61.435"), %arg62: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_weight"} loc("p62.439"), %arg63: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_bias"} loc("p63.445"), %arg64: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_weight"} loc("p64.449"), %arg65: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_bias"} loc("p65.455"), %arg66: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_weight"} loc("p66.460"), %arg67: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_bias"} loc("p67.469"), %arg68: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_weight"} loc("p68.473"), %arg69: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_bias"} loc("p69.479"), %arg70: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_weight"} loc("p70.483"), %arg71: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_bias"} loc("p71.489"), %arg72: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_weight"} loc("p72.494"), %arg73: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_bias"} loc("p73.503"), %arg74: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_weight"} loc("p74.507"), %arg75: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_bias"} loc("p75.513"), %arg76: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_weight"} loc("p76.517"), %arg77: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_bias"} loc("p77.523"), %arg78: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_weight"} loc("p78.528"), %arg79: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_bias"} loc("p79.537"), %arg80: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_weight"} loc("p80.541"), %arg81: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_bias"} loc("p81.547"), %arg82: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_weight"} loc("p82.551"), %arg83: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_bias"} loc("p83.557"), %arg84: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_weight"} loc("p84.562"), %arg85: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_bias"} loc("p85.571"), %arg86: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_weight"} loc("p86.575"), %arg87: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_bias"} loc("p87.581"), %arg88: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_weight"} loc("p88.585"), %arg89: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_bias"} loc("p89.591"), %arg90: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_weight"} loc("p90.596"), %arg91: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_bias"} loc("p91.605"), %arg92: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_weight"} loc("p92.609"), %arg93: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_bias"} loc("p93.615"), %arg94: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_weight"} loc("p94.619"), %arg95: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_bias"} loc("p95.625"), %arg96: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_weight"} loc("p96.630"), %arg97: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_bias"} loc("p97.639"), %arg98: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_weight"} loc("p98.643"), %arg99: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_bias"} loc("p99.649"), %arg100: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_weight"} loc("p100.653"), %arg101: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_bias"} loc("p101.659"), %arg102: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_weight"} loc("p102.664"), %arg103: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_bias"} loc("p103.673"), %arg104: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_weight"} loc("p104.677"), %arg105: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_bias"} loc("p105.683"), %arg106: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_weight"} loc("p106.687"), %arg107: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_bias"} loc("p107.693"), %arg108: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_weight"} loc("p108.698"), %arg109: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_bias"} loc("p109.707"), %arg110: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_weight"} loc("p110.711"), %arg111: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_bias"} loc("p111.717"), %arg112: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_weight"} loc("p112.721"), %arg113: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_bias"} loc("p113.727"), %arg114: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_weight"} loc("p114.732"), %arg115: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_bias"} loc("p115.741"), %arg116: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_weight"} loc("p116.745"), %arg117: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_bias"} loc("p117.751"), %arg118: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_weight"} loc("p118.755"), %arg119: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_bias"} loc("p119.761"), %arg120: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_weight"} loc("p120.766"), %arg121: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_bias"} loc("p121.775"), %arg122: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_weight"} loc("p122.779"), %arg123: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_bias"} loc("p123.785"), %arg124: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_weight"} loc("p124.789"), %arg125: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_bias"} loc("p125.795"), %arg126: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_weight"} loc("p126.800"), %arg127: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_bias"} loc("p127.809"), %arg128: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_weight"} loc("p128.813"), %arg129: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_bias"} loc("p129.819"), %arg130: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_weight"} loc("p130.823"), %arg131: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_bias"} loc("p131.829"), %arg132: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_weight"} loc("p132.834"), %arg133: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_bias"} loc("p133.843"), %arg134: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_weight"} loc("p134.847"), %arg135: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_bias"} loc("p135.853"), %arg136: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_weight"} loc("p136.857"), %arg137: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_bias"} loc("p137.863"), %arg138: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_weight"} loc("p138.868"), %arg139: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_bias"} loc("p139.877"), %arg140: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_weight"} loc("p140.881"), %arg141: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_bias"} loc("p141.887"), %arg142: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_weight"} loc("p142.891"), %arg143: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_bias"} loc("p143.897"), %arg144: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_weight"} loc("p144.902"), %arg145: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_bias"} loc("p145.911"), %arg146: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_weight"} loc("p146.915"), %arg147: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_bias"} loc("p147.921"), %arg148: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_weight"} loc("p148.925"), %arg149: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_bias"} loc("p149.931"), %arg150: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_weight"} loc("p150.936"), %arg151: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_bias"} loc("p151.945"), %arg152: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_weight"} loc("p152.949"), %arg153: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_bias"} loc("p153.955"), %arg154: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_weight"} loc("p154.959"), %arg155: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_bias"} loc("p155.965"), %arg156: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_weight"} loc("p156.970"), %arg157: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_bias"} loc("p157.979"), %arg158: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_weight"} loc("p158.983"), %arg159: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_bias"} loc("p159.989"), %arg160: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_weight"} loc("p160.993"), %arg161: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_bias"} loc("p161.999"), %arg162: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_weight"} loc("p162.1004"), %arg163: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_bias"} loc("p163.1013"), %arg164: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_weight"} loc("p164.1017"), %arg165: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_bias"} loc("p165.1023"), %arg166: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_weight"} loc("p166.1027"), %arg167: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_bias"} loc("p167.1033"), %arg168: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_weight"} loc("p168.1038"), %arg169: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_bias"} loc("p169.1047"), %arg170: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_weight"} loc("p170.1051"), %arg171: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_bias"} loc("p171.1057"), %arg172: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_weight"} loc("p172.1061"), %arg173: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_bias"} loc("p173.1067"), %arg174: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_weight"} loc("p174.1072"), %arg175: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_bias"} loc("p175.1081"), %arg176: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_weight"} loc("p176.1085"), %arg177: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_bias"} loc("p177.1091"), %arg178: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_weight"} loc("p178.1095"), %arg179: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_bias"} loc("p179.1101"), %arg180: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_weight"} loc("p180.1106"), %arg181: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_bias"} loc("p181.1115"), %arg182: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_weight"} loc("p182.1119"), %arg183: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_bias"} loc("p183.1125"), %arg184: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_weight"} loc("p184.1129"), %arg185: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_bias"} loc("p185.1135"), %arg186: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_weight"} loc("p186.1140"), %arg187: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_bias"} loc("p187.1149"), %arg188: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_weight"} loc("p188.1153"), %arg189: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_bias"} loc("p189.1159"), %arg190: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_weight"} loc("p190.1163"), %arg191: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_bias"} loc("p191.1169"), %arg192: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_weight"} loc("p192.1174"), %arg193: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_bias"} loc("p193.1183"), %arg194: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_weight"} loc("p194.1187"), %arg195: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_bias"} loc("p195.1193"), %arg196: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_weight"} loc("p196.1197"), %arg197: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_bias"} loc("p197.1203"), %arg198: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_weight"} loc("p198.1208"), %arg199: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_bias"} loc("p199.1217"), %arg200: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_weight"} loc("p200.1221"), %arg201: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_bias"} loc("p201.1227"), %arg202: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_weight"} loc("p202.1231"), %arg203: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_bias"} loc("p203.1237"), %arg204: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_weight"} loc("p204.1242"), %arg205: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_bias"} loc("p205.1251"), %arg206: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_weight"} loc("p206.1255"), %arg207: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_bias"} loc("p207.1261"), %arg208: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_weight"} loc("p208.1265"), %arg209: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_bias"} loc("p209.1271"), %arg210: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_weight"} loc("p210.1276"), %arg211: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_bias"} loc("p211.1285"), %arg212: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_weight"} loc("p212.1289"), %arg213: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_bias"} loc("p213.1295"), %arg214: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_weight"} loc("p214.1299"), %arg215: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_bias"} loc("p215.1305"), %arg216: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_weight"} loc("p216.1310"), %arg217: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_bias"} loc("p217.1319"), %arg218: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_weight"} loc("p218.1323"), %arg219: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_bias"} loc("p219.1329"), %arg220: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_weight"} loc("p220.1333"), %arg221: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_bias"} loc("p221.1339"), %arg222: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_weight"} loc("p222.1344"), %arg223: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_bias"} loc("p223.1353"), %arg224: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_weight"} loc("p224.1357"), %arg225: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_bias"} loc("p225.1363"), %arg226: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_weight"} loc("p226.1367"), %arg227: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_bias"} loc("p227.1373"), %arg228: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_weight"} loc("p228.1378"), %arg229: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_bias"} loc("p229.1387"), %arg230: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_weight"} loc("p230.1391"), %arg231: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_bias"} loc("p231.1397"), %arg232: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_weight"} loc("p232.1401"), %arg233: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_bias"} loc("p233.1407"), %arg234: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_weight"} loc("p234.1412"), %arg235: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_bias"} loc("p235.1421"), %arg236: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_weight"} loc("p236.1425"), %arg237: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_bias"} loc("p237.1431"), %arg238: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_weight"} loc("p238.1435"), %arg239: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_bias"} loc("p239.1441"), %arg240: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_weight"} loc("p240.1446"), %arg241: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_bias"} loc("p241.1455"), %arg242: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_weight"} loc("p242.1459"), %arg243: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_bias"} loc("p243.1465"), %arg244: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_weight"} loc("p244.1469"), %arg245: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_bias"} loc("p245.1475"), %arg246: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_weight"} loc("p246.1480"), %arg247: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_bias"} loc("p247.1489"), %arg248: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_weight"} loc("p248.1493"), %arg249: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_bias"} loc("p249.1499"), %arg250: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_weight"} loc("p250.1503"), %arg251: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_bias"} loc("p251.1509"), %arg252: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_weight"} loc("p252.1514"), %arg253: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_bias"} loc("p253.1523"), %arg254: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_weight"} loc("p254.1527"), %arg255: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_bias"} loc("p255.1533"), %arg256: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_weight"} loc("p256.1537"), %arg257: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_bias"} loc("p257.1543"), %arg258: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_weight"} loc("p258.1548"), %arg259: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_bias"} loc("p259.1557"), %arg260: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_weight"} loc("p260.1561"), %arg261: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_bias"} loc("p261.1567"), %arg262: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_weight"} loc("p262.1571"), %arg263: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_bias"} loc("p263.1577"), %arg264: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_weight"} loc("p264.1582"), %arg265: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_bias"} loc("p265.1591"), %arg266: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_weight"} loc("p266.1595"), %arg267: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_bias"} loc("p267.1601"), %arg268: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_weight"} loc("p268.1605"), %arg269: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_bias"} loc("p269.1611"), %arg270: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_weight"} loc("p270.1616"), %arg271: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_bias"} loc("p271.1625"), %arg272: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_weight"} loc("p272.1629"), %arg273: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_bias"} loc("p273.1635"), %arg274: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_weight"} loc("p274.1639"), %arg275: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_bias"} loc("p275.1645"), %arg276: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_weight"} loc("p276.1650"), %arg277: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_bias"} loc("p277.1659"), %arg278: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_weight"} loc("p278.1663"), %arg279: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_bias"} loc("p279.1669"), %arg280: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_weight"} loc("p280.1673"), %arg281: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_bias"} loc("p281.1679"), %arg282: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_weight"} loc("p282.1684"), %arg283: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_bias"} loc("p283.1693"), %arg284: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_weight"} loc("p284.1697"), %arg285: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_bias"} loc("p285.1703"), %arg286: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_weight"} loc("p286.1707"), %arg287: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_bias"} loc("p287.1713"), %arg288: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_weight"} loc("p288.1718"), %arg289: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_bias"} loc("p289.1727"), %arg290: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_weight"} loc("p290.1731"), %arg291: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_bias"} loc("p291.1737"), %arg292: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_weight"} loc("p292.1741"), %arg293: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_bias"} loc("p293.1747"), %arg294: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_weight"} loc("p294.1752"), %arg295: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_bias"} loc("p295.1761"), %arg296: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_weight"} loc("p296.1765"), %arg297: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_bias"} loc("p297.1771"), %arg298: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_weight"} loc("p298.1775"), %arg299: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_bias"} loc("p299.1781"), %arg300: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_weight"} loc("p300.1786"), %arg301: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_bias"} loc("p301.1795"), %arg302: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_weight"} loc("p302.1799"), %arg303: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_bias"} loc("p303.1805"), %arg304: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_weight"} loc("p304.1809"), %arg305: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_bias"} loc("p305.1815"), %arg306: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_weight"} loc("p306.1820"), %arg307: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_bias"} loc("p307.1829"), %arg308: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_weight"} loc("p308.1833"), %arg309: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_bias"} loc("p309.1839"), %arg310: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_weight"} loc("p310.1843"), %arg311: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_bias"} loc("p311.1849"), %arg312: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_weight"} loc("p312.1854"), %arg313: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_bias"} loc("p313.1863"), %arg314: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_weight"} loc("p314.1867"), %arg315: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_bias"} loc("p315.1873"), %arg316: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_weight"} loc("p316.1877"), %arg317: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_bias"} loc("p317.1883"), %arg318: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_weight"} loc("p318.1888"), %arg319: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_bias"} loc("p319.1897"), %arg320: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_weight"} loc("p320.1901"), %arg321: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_bias"} loc("p321.1907"), %arg322: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_weight"} loc("p322.1911"), %arg323: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_bias"} loc("p323.1917"), %arg324: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_weight"} loc("p324.1922"), %arg325: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_bias"} loc("p325.1931"), %arg326: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_weight"} loc("p326.1935"), %arg327: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_bias"} loc("p327.1941"), %arg328: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_weight"} loc("p328.1945"), %arg329: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_bias"} loc("p329.1951"), %arg330: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_weight"} loc("p330.1956"), %arg331: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_bias"} loc("p331.1965"), %arg332: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_weight"} loc("p332.1969"), %arg333: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_bias"} loc("p333.1975"), %arg334: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_weight"} loc("p334.1979"), %arg335: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_bias"} loc("p335.1985"), %arg336: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_weight"} loc("p336.1990"), %arg337: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_bias"} loc("p337.1999"), %arg338: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_weight"} loc("p338.2003"), %arg339: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_bias"} loc("p339.2009"), %arg340: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_weight"} loc("p340.2013"), %arg341: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_bias"} loc("p341.2019"), %arg342: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_weight"} loc("p342.2024"), %arg343: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_bias"} loc("p343.2033"), %arg344: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_weight"} loc("p344.2037"), %arg345: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_bias"} loc("p345.2043"), %arg346: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_weight"} loc("p346.2047"), %arg347: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_bias"} loc("p347.2053"), %arg348: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_weight"} loc("p348.2058"), %arg349: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_bias"} loc("p349.2067"), %arg350: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_weight"} loc("p350.2071"), %arg351: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_bias"} loc("p351.2077"), %arg352: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_weight"} loc("p352.2081"), %arg353: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_bias"} loc("p353.2087"), %arg354: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_weight"} loc("p354.2092"), %arg355: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_bias"} loc("p355.2101"), %arg356: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_weight"} loc("p356.2105"), %arg357: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_bias"} loc("p357.2111"), %arg358: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_weight"} loc("p358.2115"), %arg359: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_bias"} loc("p359.2121"), %arg360: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_weight"} loc("p360.2126"), %arg361: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_bias"} loc("p361.2135"), %arg362: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_weight"} loc("p362.2139"), %arg363: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_bias"} loc("p363.2145"), %arg364: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_weight"} loc("p364.2149"), %arg365: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_bias"} loc("p365.2155"), %arg366: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_weight"} loc("p366.2160"), %arg367: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_bias"} loc("p367.2169"), %arg368: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_weight"} loc("p368.2173"), %arg369: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_bias"} loc("p369.2179"), %arg370: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_weight"} loc("p370.2183"), %arg371: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_bias"} loc("p371.2189"), %arg372: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_weight"} loc("p372.2194"), %arg373: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_bias"} loc("p373.2203"), %arg374: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_weight"} loc("p374.2207"), %arg375: tensor<5120xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_bias"} loc("p375.2213"), %arg376: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_weight"} loc("p376.2217"), %arg377: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_bias"} loc("p377.2223"), %arg378: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_weight"} loc("p378.2228"), %arg379: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_bias"} loc("p379.2237"), %arg380: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_weight"} loc("p380.2241"), %arg381: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_bias"} loc("p381.2247"), %arg382: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_weight"} loc("p382.2251"), %arg383: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_bias"} loc("p383.2257"), %arg384: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_weight"} loc("p384.2262"), %arg385: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_bias"} loc("p385.2270"), %arg386: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_weight"} loc("p386.2275"), %arg387: tensor<1x257xsi32, #ttnn_layout24> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_embeddings_position_ids"} loc("p387.2283"), %arg388: tensor<257x1280xbf16, #ttnn_layout25> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_embeddings_position_embedding_weight"} loc("p388.2288"), %arg389: tensor<1280x3x14x14xbf16, #ttnn_layout17> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___image_encoder_vision_model_embeddings_patch_embedding_weight"} loc("p389.2295"), %arg390: tensor<1x3x224x224xbf16, #ttnn_layout44> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("p390.2297"), %arg391: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_embeddings_class_embedding"} loc("p391.2302"), %arg392: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_bias"} loc("p392.2478"), %arg393: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_weight"} loc("p393.2482"), %arg394: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_bias"} loc("p394.2502"), %arg395: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_weight"} loc("p395.2506"), %arg396: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_bias"} loc("p396.2788"), %arg397: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_weight"} loc("p397.2792"), %arg398: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_bias"} loc("p398.2812"), %arg399: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_weight"} loc("p399.2816"), %arg400: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_bias"} loc("p400.3098"), %arg401: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_weight"} loc("p401.3102"), %arg402: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_bias"} loc("p402.3122"), %arg403: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_weight"} loc("p403.3126"), %arg404: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_bias"} loc("p404.3408"), %arg405: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_weight"} loc("p405.3412"), %arg406: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_bias"} loc("p406.3432"), %arg407: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_weight"} loc("p407.3436"), %arg408: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_bias"} loc("p408.3718"), %arg409: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_weight"} loc("p409.3722"), %arg410: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_bias"} loc("p410.3742"), %arg411: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_weight"} loc("p411.3746"), %arg412: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_bias"} loc("p412.4028"), %arg413: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_weight"} loc("p413.4032"), %arg414: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_bias"} loc("p414.4052"), %arg415: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_weight"} loc("p415.4056"), %arg416: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_bias"} loc("p416.4338"), %arg417: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_weight"} loc("p417.4342"), %arg418: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_bias"} loc("p418.4362"), %arg419: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_weight"} loc("p419.4366"), %arg420: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_bias"} loc("p420.4648"), %arg421: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_weight"} loc("p421.4652"), %arg422: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_bias"} loc("p422.4672"), %arg423: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_weight"} loc("p423.4676"), %arg424: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_bias"} loc("p424.4958"), %arg425: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_weight"} loc("p425.4962"), %arg426: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_bias"} loc("p426.4982"), %arg427: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_weight"} loc("p427.4986"), %arg428: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_bias"} loc("p428.5268"), %arg429: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_weight"} loc("p429.5272"), %arg430: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_bias"} loc("p430.5292"), %arg431: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_weight"} loc("p431.5296"), %arg432: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_bias"} loc("p432.5578"), %arg433: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_weight"} loc("p433.5582"), %arg434: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_bias"} loc("p434.5602"), %arg435: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_weight"} loc("p435.5606"), %arg436: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_bias"} loc("p436.5888"), %arg437: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_weight"} loc("p437.5892"), %arg438: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_bias"} loc("p438.5912"), %arg439: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_weight"} loc("p439.5916"), %arg440: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_bias"} loc("p440.6198"), %arg441: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_weight"} loc("p441.6202"), %arg442: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_bias"} loc("p442.6222"), %arg443: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_weight"} loc("p443.6226"), %arg444: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_bias"} loc("p444.6508"), %arg445: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_weight"} loc("p445.6512"), %arg446: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_bias"} loc("p446.6532"), %arg447: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_weight"} loc("p447.6536"), %arg448: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_bias"} loc("p448.6818"), %arg449: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_weight"} loc("p449.6822"), %arg450: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_bias"} loc("p450.6842"), %arg451: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_weight"} loc("p451.6846"), %arg452: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_bias"} loc("p452.7128"), %arg453: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_weight"} loc("p453.7132"), %arg454: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_bias"} loc("p454.7152"), %arg455: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_weight"} loc("p455.7156"), %arg456: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_bias"} loc("p456.7438"), %arg457: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_weight"} loc("p457.7442"), %arg458: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_bias"} loc("p458.7462"), %arg459: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_weight"} loc("p459.7466"), %arg460: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_bias"} loc("p460.7748"), %arg461: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_weight"} loc("p461.7752"), %arg462: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_bias"} loc("p462.7772"), %arg463: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_weight"} loc("p463.7776"), %arg464: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_bias"} loc("p464.8058"), %arg465: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_weight"} loc("p465.8062"), %arg466: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_bias"} loc("p466.8082"), %arg467: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_weight"} loc("p467.8086"), %arg468: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_bias"} loc("p468.8368"), %arg469: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_weight"} loc("p469.8372"), %arg470: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_bias"} loc("p470.8392"), %arg471: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_weight"} loc("p471.8396"), %arg472: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_bias"} loc("p472.8678"), %arg473: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_weight"} loc("p473.8682"), %arg474: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_bias"} loc("p474.8702"), %arg475: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_weight"} loc("p475.8706"), %arg476: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_bias"} loc("p476.8988"), %arg477: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_weight"} loc("p477.8992"), %arg478: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_bias"} loc("p478.9012"), %arg479: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_weight"} loc("p479.9016"), %arg480: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_bias"} loc("p480.9298"), %arg481: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_weight"} loc("p481.9302"), %arg482: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_bias"} loc("p482.9322"), %arg483: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_weight"} loc("p483.9326"), %arg484: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_bias"} loc("p484.9608"), %arg485: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_weight"} loc("p485.9612"), %arg486: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_bias"} loc("p486.9632"), %arg487: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_weight"} loc("p487.9636"), %arg488: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_bias"} loc("p488.9918"), %arg489: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_weight"} loc("p489.9922"), %arg490: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_bias"} loc("p490.9942"), %arg491: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_weight"} loc("p491.9946"), %arg492: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_bias"} loc("p492.10228"), %arg493: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_weight"} loc("p493.10232"), %arg494: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_bias"} loc("p494.10252"), %arg495: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_weight"} loc("p495.10256"), %arg496: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_bias"} loc("p496.10538"), %arg497: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_weight"} loc("p497.10542"), %arg498: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_bias"} loc("p498.10562"), %arg499: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_weight"} loc("p499.10566"), %arg500: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_bias"} loc("p500.10848"), %arg501: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_weight"} loc("p501.10852"), %arg502: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_bias"} loc("p502.10872"), %arg503: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_weight"} loc("p503.10876"), %arg504: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_bias"} loc("p504.11158"), %arg505: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_weight"} loc("p505.11162"), %arg506: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_bias"} loc("p506.11182"), %arg507: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_weight"} loc("p507.11186"), %arg508: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_bias"} loc("p508.11468"), %arg509: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_weight"} loc("p509.11472"), %arg510: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_bias"} loc("p510.11492"), %arg511: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_weight"} loc("p511.11496"), %arg512: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_bias"} loc("p512.11778"), %arg513: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_weight"} loc("p513.11782"), %arg514: tensor<1280xbf16, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_bias"} loc("p514.11802"), %arg515: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_weight"} loc("p515.11806"), %arg516: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_attn_to_k_weight"} loc("p516.12091"), %arg517: tensor<1280x1280xbf16, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_attn_to_q_weight"} loc("p517.12106"), %arg518: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_2_weight"} loc("p518.12186"), %arg519: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_0_proj_weight"} loc("p519.12191"), %arg520: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_ff_0_bias"} loc("p520.12197"), %arg521: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_0_ff_0_weight"} loc("p521.12202"), %arg522: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_attn_to_out_0_weight"} loc("p522.12309"), %arg523: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_attn_to_v_weight"} loc("p523.12314"), %arg524: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_ln1_bias"} loc("p524.12320"), %arg525: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_ln1_weight"} loc("p525.12325"), %arg526: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_ln0_bias"} loc("p526.12410"), %arg527: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_ln0_weight"} loc("p527.12415"), %arg528: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_attn_to_k_weight"} loc("p528.12507"), %arg529: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_attn_to_q_weight"} loc("p529.12522"), %arg530: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_2_weight"} loc("p530.12602"), %arg531: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_0_proj_weight"} loc("p531.12607"), %arg532: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_ff_0_bias"} loc("p532.12613"), %arg533: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_1_ff_0_weight"} loc("p533.12618"), %arg534: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_attn_to_out_0_weight"} loc("p534.12725"), %arg535: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_attn_to_v_weight"} loc("p535.12730"), %arg536: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_ln1_bias"} loc("p536.12736"), %arg537: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_ln1_weight"} loc("p537.12741"), %arg538: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_ln0_bias"} loc("p538.12826"), %arg539: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_ln0_weight"} loc("p539.12831"), %arg540: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_attn_to_k_weight"} loc("p540.12923"), %arg541: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_attn_to_q_weight"} loc("p541.12938"), %arg542: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_2_weight"} loc("p542.13018"), %arg543: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_0_proj_weight"} loc("p543.13023"), %arg544: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_ff_0_bias"} loc("p544.13029"), %arg545: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_2_ff_0_weight"} loc("p545.13034"), %arg546: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_attn_to_out_0_weight"} loc("p546.13141"), %arg547: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_attn_to_v_weight"} loc("p547.13146"), %arg548: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_ln1_bias"} loc("p548.13152"), %arg549: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_ln1_weight"} loc("p549.13157"), %arg550: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_ln0_bias"} loc("p550.13242"), %arg551: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_ln0_weight"} loc("p551.13247"), %arg552: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_attn_to_k_weight"} loc("p552.13339"), %arg553: tensor<1280x1280xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_attn_to_q_weight"} loc("p553.13354"), %arg554: tensor<1280x5120xbf16, #ttnn_layout42> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_2_weight"} loc("p554.13434"), %arg555: tensor<5120x1280xbf16, #ttnn_layout43> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_0_proj_weight"} loc("p555.13439"), %arg556: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_ff_0_bias"} loc("p556.13445"), %arg557: tensor<1280xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resampler_layers_3_ff_0_weight"} loc("p557.13450")) -> (tensor<1x16x2048xbf16, #ttnn_layout20> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) attributes {tt.function_type = "forward_device"} {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg289]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg289) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg231]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg231) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %3 = ttcore.load_cached(@main_const_eval_3, [%arg61]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg61) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg22, %arg513, %arg515]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg515) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg513) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg22) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %5 = ttcore.load_cached(@main_const_eval_5, [%arg13]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %6 = ttcore.load_cached(@main_const_eval_6, [%arg118, %arg481, %arg483]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg483) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg481) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg118) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %7 = ttcore.load_cached(@main_const_eval_7, [%arg334, %arg409, %arg411]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg411) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg409) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg334) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %8 = ttcore.load_cached(@main_const_eval_8, [%arg274, %arg429, %arg431]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg431) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg429) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg274) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %9 = ttcore.load_cached(@main_const_eval_9, [%arg67]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg67) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %10 = ttcore.load_cached(@main_const_eval_10, [%arg249, %arg436, %arg438]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg438) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg436) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg249) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %11 = ttcore.load_cached(@main_const_eval_11, [%arg202, %arg453, %arg455]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg455) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg453) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg202) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %12 = ttcore.load_cached(@main_const_eval_12, [%arg343]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg343) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %13 = ttcore.load_cached(@main_const_eval_13, [%arg213, %arg448, %arg450]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg450) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg448) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg213) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %14 = ttcore.load_cached(@main_const_eval_14, [%arg285, %arg424, %arg426]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg426) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg424) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg285) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %15 = ttcore.load_cached(@main_const_eval_15, [%arg33, %arg508, %arg510]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg510) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg508) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg33) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %16 = ttcore.load_cached(@main_const_eval_16, [%arg46, %arg505, %arg507]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg507) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg505) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg46) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %17 = ttcore.load_cached(@main_const_eval_17, [%arg106, %arg485, %arg487]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg487) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg485) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg106) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %18 = ttcore.load_cached(@main_const_eval_18, [%arg165, %arg464, %arg466]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg466) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg464) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg165) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %19 = ttcore.load_cached(@main_const_eval_19, [%arg298, %arg421, %arg423]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg423) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg421) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg298) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %20 = ttcore.load_cached(@main_const_eval_20, [%arg211]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg211) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %21 = ttcore.load_cached(@main_const_eval_21, [%arg253]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg253) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %22 = ttcore.load_cached(@main_const_eval_22, []) : () -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc)
        %23 = ttcore.load_cached(@main_const_eval_23, [%arg189, %arg456, %arg458]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg458) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg456) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg189) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %24 = ttcore.load_cached(@main_const_eval_24, [%arg103]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg103) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %25 = ttcore.load_cached(@main_const_eval_25, [%arg267]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg267) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %26 = ttcore.load_cached(@main_const_eval_26, [%arg286, %arg425, %arg427]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg427) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg425) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg286) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %27 = ttcore.load_cached(@main_const_eval_27, [%arg361]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg361) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %28 = ttcore.load_cached(@main_const_eval_28, [%arg93, %arg488, %arg490]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg490) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg488) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg93) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %29 = ttcore.load_cached(@main_const_eval_29, [%arg217]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg217) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %30 = ttcore.load_cached(@main_const_eval_30, [%arg69, %arg496, %arg498]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg498) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg496) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg69) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %31 = ttcore.load_cached(@main_const_eval_31, [%arg55]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg55) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %32 = ttcore.load_cached(@main_const_eval_32, []) : () -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc)
        %33 = ttcore.load_cached(@main_const_eval_33, [%arg81, %arg492, %arg494]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg494) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg492) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg81) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %34 = ttcore.load_cached(@main_const_eval_34, [%arg163]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg163) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %35 = ttcore.load_cached(@main_const_eval_35, [%arg117, %arg480, %arg482]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg482) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg480) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg117) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %36 = ttcore.load_cached(@main_const_eval_36, [%arg58, %arg501, %arg503]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg503) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg501) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg58) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %37 = ttcore.load_cached(@main_const_eval_37, [%arg151]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg151) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %38 = ttcore.load_cached(@main_const_eval_38, [%arg79]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg79) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %39 = ttcore.load_cached(@main_const_eval_39, [%arg133]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg133) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %40 = ttcore.load_cached(@main_const_eval_40, [%arg207]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg207) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %41 = ttcore.load_cached(@main_const_eval_41, [%arg205]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg205) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %42 = ttcore.load_cached(@main_const_eval_42, [%arg15]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %43 = ttcore.load_cached(@main_const_eval_43, [%arg201, %arg452, %arg454]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg454) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg452) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg201) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %44 = ttcore.load_cached(@main_const_eval_44, [%arg45, %arg504, %arg506]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg506) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg504) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg45) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %45 = ttcore.load_cached(@main_const_eval_45, [%arg94, %arg489, %arg491]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg491) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg489) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg94) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %46 = ttcore.load_cached(@main_const_eval_46, [%arg63]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg63) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %47 = ttcore.load_cached(@main_const_eval_47, [%arg171]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg171) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %48 = ttcore.load_cached(@main_const_eval_48, [%arg339]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg339) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %49 = ttcore.load_cached(@main_const_eval_49, [%arg389]) : (tensor<1280x3x14x14xbf16, #ttnn_layout17>) -> tensor<1x1x588x1280xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%arg389) <{force = false}> : (tensor<1280x3x14x14xbf16, #ttnn_layout17>) -> () loc(#loc)
        %50 = ttcore.load_cached(@main_const_eval_50, [%arg154, %arg469, %arg471]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg471) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg469) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg154) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %51 = ttcore.load_cached(@main_const_eval_51, [%arg2]) : (tensor<2048xbf16, #ttnn_layout19>) -> tensor<1x16x2048xbf16, #ttnn_layout20> loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<2048xbf16, #ttnn_layout19>) -> () loc(#loc)
        %52 = ttcore.load_cached(@main_const_eval_52, [%arg51]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg51) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %53 = ttcore.load_cached(@main_const_eval_53, []) : () -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc)
        %54 = ttcore.load_cached(@main_const_eval_54, [%arg21, %arg512, %arg514]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg514) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg512) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg21) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %55 = ttcore.load_cached(@main_const_eval_55, [%arg235]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg235) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %56 = ttcore.load_cached(@main_const_eval_56, [%arg183]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg183) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %57 = ttcore.load_cached(@main_const_eval_57, [%arg387, %arg388]) : (tensor<1x257xsi32, #ttnn_layout24>, tensor<257x1280xbf16, #ttnn_layout25>) -> tensor<1x1280x257xbf16, #ttnn_layout26> loc(#loc)
        "ttnn.deallocate"(%arg388) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout25>) -> () loc(#loc)
        "ttnn.deallocate"(%arg387) <{force = false}> : (tensor<1x257xsi32, #ttnn_layout24>) -> () loc(#loc)
        %58 = ttcore.load_cached(@main_const_eval_58, [%arg153, %arg468, %arg470]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg470) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg468) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg153) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %59 = ttcore.load_cached(@main_const_eval_59, []) : () -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc)
        %60 = ttcore.load_cached(@main_const_eval_60, [%arg130, %arg477, %arg479]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg479) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg477) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg130) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %61 = ttcore.load_cached(@main_const_eval_61, [%arg123]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg123) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %62 = ttcore.load_cached(@main_const_eval_62, [%arg43]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg43) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %63 = ttcore.load_cached(@main_const_eval_63, [%arg315]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg315) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %64 = ttcore.load_cached(@main_const_eval_64, [%arg37]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg37) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %65 = ttcore.load_cached(@main_const_eval_65, [%arg99]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg99) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %66 = ttcore.load_cached(@main_const_eval_66, [%arg291]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg291) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %67 = ttcore.load_cached(@main_const_eval_67, [%arg295]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg295) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %68 = ttcore.load_cached(@main_const_eval_68, [%arg375]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg375) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %69 = ttcore.load_cached(@main_const_eval_69, [%arg27]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg27) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %70 = ttcore.load_cached(@main_const_eval_70, [%arg261, %arg432, %arg434]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg434) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg432) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg261) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %71 = ttcore.load_cached(@main_const_eval_71, [%arg219]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg219) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %72 = ttcore.load_cached(@main_const_eval_72, [%arg237, %arg440, %arg442]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg442) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg440) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg237) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %73 = ttcore.load_cached(@main_const_eval_73, [%arg127]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg127) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %74 = ttcore.load_cached(@main_const_eval_74, [%arg34, %arg509, %arg511]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg511) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg509) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg34) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %75 = ttcore.load_cached(@main_const_eval_75, [%arg319]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg319) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %76 = ttcore.load_cached(@main_const_eval_76, [%arg321, %arg412, %arg414]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg414) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg412) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg321) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %77 = ttcore.load_cached(@main_const_eval_77, [%arg166, %arg465, %arg467]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg467) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg465) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg166) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %78 = ttcore.load_cached(@main_const_eval_78, [%arg57, %arg500, %arg502]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg502) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg500) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg57) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %79 = ttcore.load_cached(@main_const_eval_79, [%arg337]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg337) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %80 = ttcore.load_cached(@main_const_eval_80, [%arg333, %arg408, %arg410]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg410) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg408) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg333) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %81 = ttcore.load_cached(@main_const_eval_81, [%arg379]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg379) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %82 = ttcore.load_cached(@main_const_eval_82, [%arg283]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg283) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %83 = ttcore.load_cached(@main_const_eval_83, [%arg19]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %84 = ttcore.load_cached(@main_const_eval_84, [%arg75]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg75) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %85 = ttcore.load_cached(@main_const_eval_85, [%arg85]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg85) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %86 = ttcore.load_cached(@main_const_eval_86, [%arg255]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg255) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %87 = ttcore.load_cached(@main_const_eval_87, [%arg271]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg271) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %88 = ttcore.load_cached(@main_const_eval_88, [%arg159]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg159) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %89 = ttcore.load_cached(@main_const_eval_89, [%arg358, %arg401, %arg403]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg403) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg401) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg358) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %90 = ttcore.load_cached(@main_const_eval_90, [%arg139]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg139) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %91 = ttcore.load_cached(@main_const_eval_91, [%arg277]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg277) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %92 = ttcore.load_cached(@main_const_eval_92, [%arg303]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg303) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %93 = ttcore.load_cached(@main_const_eval_93, [%arg370, %arg397, %arg399]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg399) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg397) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg370) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %94 = ttcore.load_cached(@main_const_eval_94, [%arg331]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg331) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %95 = ttcore.load_cached(@main_const_eval_95, [%arg157]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg157) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %96 = ttcore.load_cached(@main_const_eval_96, [%arg327]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg327) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %97 = ttcore.load_cached(@main_const_eval_97, [%arg193]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg193) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %98 = ttcore.load_cached(@main_const_eval_98, [%arg70, %arg497, %arg499]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg499) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg497) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg70) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %99 = ttcore.load_cached(@main_const_eval_99, [%arg322, %arg413, %arg415]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg415) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg413) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg322) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %100 = ttcore.load_cached(@main_const_eval_100, [%arg226, %arg445, %arg447]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg447) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg445) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg226) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %101 = ttcore.load_cached(@main_const_eval_101, []) : () -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc)
        %102 = ttcore.load_cached(@main_const_eval_102, [%arg297, %arg420, %arg422]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg422) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg420) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg297) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %103 = ttcore.load_cached(@main_const_eval_103, [%arg349]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg349) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %104 = ttcore.load_cached(@main_const_eval_104, [%arg279]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg279) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %105 = ttcore.load_cached(@main_const_eval_105, [%arg325]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg325) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %106 = ttcore.load_cached(@main_const_eval_106, [%arg178, %arg461, %arg463]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg463) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg461) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg178) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %107 = ttcore.load_cached(@main_const_eval_107, [%arg262, %arg433, %arg435]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg435) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg433) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg262) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %108 = ttcore.load_cached(@main_const_eval_108, [%arg142, %arg473, %arg475]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg475) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg473) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg142) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %109 = ttcore.load_cached(@main_const_eval_109, [%arg105, %arg484, %arg486]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg486) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg484) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg105) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %110 = ttcore.load_cached(@main_const_eval_110, [%arg175]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg175) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %111 = ttcore.load_cached(@main_const_eval_111, [%arg229]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg229) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %112 = ttcore.load_cached(@main_const_eval_112, [%arg190, %arg457, %arg459]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg459) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg457) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg190) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %113 = ttcore.load_cached(@main_const_eval_113, [%arg369, %arg396, %arg398]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg398) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg396) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg369) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %114 = ttcore.load_cached(@main_const_eval_114, [%arg87]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg87) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %115 = ttcore.load_cached(@main_const_eval_115, [%arg223]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg223) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %116 = ttcore.load_cached(@main_const_eval_116, [%arg346, %arg405, %arg407]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg407) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg405) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg346) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %117 = ttcore.load_cached(@main_const_eval_117, [%arg345, %arg404, %arg406]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg406) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg404) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg345) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %118 = ttcore.load_cached(@main_const_eval_118, [%arg307]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg307) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %119 = ttcore.load_cached(@main_const_eval_119, [%arg145]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg145) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %120 = ttcore.load_cached(@main_const_eval_120, [%arg135]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg135) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %121 = ttcore.load_cached(@main_const_eval_121, [%arg169]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg169) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %122 = ttcore.load_cached(@main_const_eval_122, [%arg391]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x1280x1xbf16, #ttnn_layout33> loc(#loc)
        "ttnn.deallocate"(%arg391) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %123 = ttcore.load_cached(@main_const_eval_123, [%arg73]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg73) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %124 = ttcore.load_cached(@main_const_eval_124, []) : () -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc)
        %125 = ttcore.load_cached(@main_const_eval_125, [%arg351]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg351) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %126 = ttcore.load_cached(@main_const_eval_126, [%arg82, %arg493, %arg495]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg495) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg493) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg82) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %127 = ttcore.load_cached(@main_const_eval_127, [%arg310, %arg417, %arg419]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg419) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg417) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg310) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %128 = ttcore.load_cached(@main_const_eval_128, [%arg91]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg91) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %129 = ttcore.load_cached(@main_const_eval_129, [%arg177, %arg460, %arg462]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg462) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg460) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg177) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %130 = ttcore.load_cached(@main_const_eval_130, [%arg382, %arg393, %arg395]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg395) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg393) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg382) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %131 = ttcore.load_cached(@main_const_eval_131, [%arg357, %arg400, %arg402]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg402) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg400) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg357) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %132 = ttcore.load_cached(@main_const_eval_132, [%arg195]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg195) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %133 = ttcore.load_cached(@main_const_eval_133, [%arg11]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %134 = ttcore.load_cached(@main_const_eval_134, [%arg129, %arg476, %arg478]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg478) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg476) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg129) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %135 = ttcore.load_cached(@main_const_eval_135, [%arg225, %arg444, %arg446]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg446) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg444) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg225) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %136 = ttcore.load_cached(@main_const_eval_136, []) : () -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc)
        %137 = ttcore.load_cached(@main_const_eval_137, [%arg259]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg259) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %138 = ttcore.load_cached(@main_const_eval_138, [%arg199]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg199) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %139 = ttcore.load_cached(@main_const_eval_139, [%arg31]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg31) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %140 = ttcore.load_cached(@main_const_eval_140, [%arg241]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg241) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %141 = ttcore.load_cached(@main_const_eval_141, [%arg121]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg121) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %142 = ttcore.load_cached(@main_const_eval_142, [%arg181]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg181) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %143 = ttcore.load_cached(@main_const_eval_143, [%arg115]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg115) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %144 = ttcore.load_cached(@main_const_eval_144, [%arg39]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg39) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %145:3 = ttcore.load_cached(@main_const_eval_145, [%arg4, %arg7, %arg8, %arg517]) : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280x1280xbf16, #ttnn_layout11>) -> (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<16x1280xbf16, #ttnn_layout36>) loc(#loc)
        "ttnn.deallocate"(%arg517) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %146 = ttcore.load_cached(@main_const_eval_146, [%arg381, %arg392, %arg394]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg394) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg392) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg381) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %147 = ttcore.load_cached(@main_const_eval_147, [%arg25]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg25) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %148 = ttcore.load_cached(@main_const_eval_148, [%arg97]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg97) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %149 = ttcore.load_cached(@main_const_eval_149, [%arg243]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg243) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %150 = ttcore.load_cached(@main_const_eval_150, [%arg214, %arg449, %arg451]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg451) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg449) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg214) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %151 = ttcore.load_cached(@main_const_eval_151, []) : () -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc)
        %152 = ttcore.load_cached(@main_const_eval_152, [%arg367]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg367) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %153 = ttcore.load_cached(@main_const_eval_153, [%arg273, %arg428, %arg430]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg430) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg428) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg273) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %154 = ttcore.load_cached(@main_const_eval_154, [%arg49]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg49) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %155 = ttcore.load_cached(@main_const_eval_155, [%arg187]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg187) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %156 = ttcore.load_cached(@main_const_eval_156, [%arg355]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg355) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %157 = ttcore.load_cached(@main_const_eval_157, [%arg363]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg363) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %158 = ttcore.load_cached(@main_const_eval_158, [%arg313]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg313) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %159 = ttcore.load_cached(@main_const_eval_159, [%arg111]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg111) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %160 = ttcore.load_cached(@main_const_eval_160, [%arg247]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg247) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %161 = ttcore.load_cached(@main_const_eval_161, [%arg265]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg265) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %162 = ttcore.load_cached(@main_const_eval_162, [%arg309, %arg416, %arg418]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg418) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg416) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg309) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %163 = ttcore.load_cached(@main_const_eval_163, [%arg141, %arg472, %arg474]) : (tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>, tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc)
        "ttnn.deallocate"(%arg474) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg472) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg141) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %164 = ttcore.load_cached(@main_const_eval_164, [%arg109]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg109) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %165 = ttcore.load_cached(@main_const_eval_165, []) : () -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc)
        %166 = ttcore.load_cached(@main_const_eval_166, [%arg250, %arg437, %arg439]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg439) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg437) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg250) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %167 = ttcore.load_cached(@main_const_eval_167, [%arg373]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg373) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %168 = ttcore.load_cached(@main_const_eval_168, [%arg147]) : (tensor<5120xbf16, #ttnn_layout6>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg147) <{force = false}> : (tensor<5120xbf16, #ttnn_layout6>) -> () loc(#loc)
        %169 = ttcore.load_cached(@main_const_eval_169, [%arg238, %arg441, %arg443]) : (tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>, tensor<1280x1280xbf16, #ttnn_layout11>) -> tensor<3840x1280xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg443) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg441) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        "ttnn.deallocate"(%arg238) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout11>) -> () loc(#loc)
        %170 = ttcore.load_cached(@main_const_eval_170, [%arg301]) : (tensor<1280xbf16, #ttnn_layout1>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg301) <{force = false}> : (tensor<1280xbf16, #ttnn_layout1>) -> () loc(#loc)
        %171 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1693)
        %172 = "ttnn.to_layout"(%arg390) <{layout = #ttnn.layout<tile>}> : (tensor<1x3x224x224xbf16, #ttnn_layout44>) -> tensor<1x3x224x224xbf16, #ttnn_layout45> loc(#loc1693)
        "ttnn.deallocate"(%arg390) <{force = false}> : (tensor<1x3x224x224xbf16, #ttnn_layout44>) -> () loc(#loc1693)
        %173 = "ttnn.permute"(%172) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x3x224x224xbf16, #ttnn_layout45>) -> tensor<1x224x224x3xbf16, #ttnn_layout46> loc(#loc1696)
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x3x224x224xbf16, #ttnn_layout45>) -> () loc(#loc1696)
        %174 = "ttnn.reshape"(%173) <{shape = [1 : i32, 1 : i32, 50176 : i32, 3 : i32]}> : (tensor<1x224x224x3xbf16, #ttnn_layout46>) -> tensor<1x1x50176x3xbf16, #ttnn_layout47> loc(#loc2329)
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<1x224x224x3xbf16, #ttnn_layout46>) -> () loc(#loc2329)
        %175 = "ttnn.conv2d"(%174, %49, %171) <{batch_size = 1 : i32, compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = true, act_block_h_override = 0, enable_kernel_stride_folding = false, config_tensors_in_dram = true>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 3 : i32, input_height = 224 : i32, input_width = 224 : i32, kernel_size = array<i32: 14, 14>, out_channels = 1280 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 14, 14>}> : (tensor<1x1x50176x3xbf16, #ttnn_layout47>, tensor<1x1x588x1280xbf16, #ttnn_layout18>, !ttnn.device) -> tensor<1x1x256x1280xbf16, #ttnn_layout48> loc(#loc1)
        "ttnn.deallocate"(%174) <{force = false}> : (tensor<1x1x50176x3xbf16, #ttnn_layout47>) -> () loc(#loc1)
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x588x1280xbf16, #ttnn_layout18>) -> () loc(#loc1)
        %176 = "ttnn.reshape"(%175) <{shape = [1 : i32, 16 : i32, 16 : i32, 1280 : i32]}> : (tensor<1x1x256x1280xbf16, #ttnn_layout48>) -> tensor<1x16x16x1280xbf16, #ttnn_layout49> loc(#loc1697)
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<1x1x256x1280xbf16, #ttnn_layout48>) -> () loc(#loc1697)
        %177 = "ttnn.permute"(%176) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x16x16x1280xbf16, #ttnn_layout49>) -> tensor<1x1280x16x16xbf16, #ttnn_layout50> loc(#loc1)
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x16x16x1280xbf16, #ttnn_layout49>) -> () loc(#loc1)
        %178 = "ttnn.reshape"(%177) <{shape = [1 : i32, 1280 : i32, 256 : i32]}> : (tensor<1x1280x16x16xbf16, #ttnn_layout50>) -> tensor<1x1280x256xbf16, #ttnn_layout51> loc(#loc760)
        "ttnn.deallocate"(%177) <{force = false}> : (tensor<1x1280x16x16xbf16, #ttnn_layout50>) -> () loc(#loc760)
        %179 = "ttnn.concat"(%122, %178) <{dim = 2 : si32}> : (tensor<1x1280x1xbf16, #ttnn_layout33>, tensor<1x1280x256xbf16, #ttnn_layout51>) -> tensor<1x1280x257xbf16, #ttnn_layout26> loc(#loc82)
        "ttnn.deallocate"(%178) <{force = false}> : (tensor<1x1280x256xbf16, #ttnn_layout51>) -> () loc(#loc82)
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1280x1xbf16, #ttnn_layout33>) -> () loc(#loc82)
        %180 = "ttnn.add"(%179, %57) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1280x257xbf16, #ttnn_layout26>, tensor<1x1280x257xbf16, #ttnn_layout26>) -> tensor<1x1280x257xbf16, #ttnn_layout26> loc(#loc761)
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<1x1280x257xbf16, #ttnn_layout26>) -> () loc(#loc761)
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x1280x257xbf16, #ttnn_layout26>) -> () loc(#loc761)
        %181 = "ttnn.permute"(%180) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x1280x257xbf16, #ttnn_layout26>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc761)
        "ttnn.deallocate"(%180) <{force = false}> : (tensor<1x1280x257xbf16, #ttnn_layout26>) -> () loc(#loc761)
        %182 = "ttnn.layer_norm"(%181, %arg386, %arg385) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc762)
        "ttnn.deallocate"(%181) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc762)
        "ttnn.deallocate"(%arg386) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc762)
        "ttnn.deallocate"(%arg385) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc762)
        %183 = "ttnn.layer_norm"(%182, %arg384, %arg383) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc763)
        "ttnn.deallocate"(%arg384) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc763)
        "ttnn.deallocate"(%arg383) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc763)
        %184 = "ttnn.reshape"(%183) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc764)
        "ttnn.deallocate"(%183) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc764)
        %185 = "ttnn.matmul"(%184, %130) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1698)
        "ttnn.deallocate"(%184) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1698)
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1698)
        %186 = "ttnn.add"(%185, %146) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1699)
        "ttnn.deallocate"(%185) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1699)
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1699)
        %187 = "ttnn.slice_static"(%186) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1700)
        %188 = "ttnn.slice_static"(%186) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1701)
        %189 = "ttnn.slice_static"(%186) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1702)
        "ttnn.deallocate"(%186) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1702)
        %190 = "ttnn.reshape"(%187) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1703)
        "ttnn.deallocate"(%187) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1703)
        %191 = "ttnn.reshape"(%188) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1704)
        "ttnn.deallocate"(%188) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1704)
        %192 = "ttnn.reshape"(%189) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1705)
        "ttnn.deallocate"(%189) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1705)
        %193 = "ttnn.permute"(%190) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1706)
        "ttnn.deallocate"(%190) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1706)
        %194 = "ttnn.permute"(%191) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1707)
        "ttnn.deallocate"(%191) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1707)
        %195 = "ttnn.permute"(%192) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1708)
        "ttnn.deallocate"(%192) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1708)
        %196 = "ttnn.typecast"(%193) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc766)
        "ttnn.deallocate"(%193) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc766)
        %197 = "ttnn.multiply"(%196, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc767)
        "ttnn.deallocate"(%196) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc767)
        %198 = "ttnn.typecast"(%194) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc768)
        "ttnn.deallocate"(%194) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc768)
        %199 = "ttnn.permute"(%198) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc768)
        "ttnn.deallocate"(%198) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc768)
        %200 = "ttnn.multiply"(%199, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc769)
        "ttnn.deallocate"(%199) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc769)
        %201 = "ttnn.matmul"(%197, %200) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc770)
        "ttnn.deallocate"(%200) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc770)
        "ttnn.deallocate"(%197) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc770)
        %202 = "ttnn.eq"(%201, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc771)
        %203 = "ttnn.logical_not"(%202) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc772)
        "ttnn.deallocate"(%202) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc772)
        %204 = "ttnn.sum"(%203) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc773)
        "ttnn.deallocate"(%203) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc773)
        %205 = "ttnn.ne"(%204, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc773)
        "ttnn.deallocate"(%204) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc773)
        %206 = "ttnn.logical_not"(%205) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc774)
        "ttnn.deallocate"(%205) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc774)
        %207 = "ttnn.reshape"(%206) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc774)
        "ttnn.deallocate"(%206) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc774)
        %208 = "ttnn.softmax"(%201) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc775)
        "ttnn.deallocate"(%201) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc775)
        %209 = "ttnn.repeat"(%207) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc776)
        "ttnn.deallocate"(%207) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc776)
        %210 = "ttnn.typecast"(%209) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1709)
        "ttnn.deallocate"(%209) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1709)
        %211 = "ttnn.where"(%210, %59, %208) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc776)
        "ttnn.deallocate"(%210) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc776)
        "ttnn.deallocate"(%208) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc776)
        %212 = "ttnn.typecast"(%195) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc777)
        "ttnn.deallocate"(%195) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc777)
        %213 = "ttnn.matmul"(%211, %212) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc778)
        "ttnn.deallocate"(%212) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc778)
        "ttnn.deallocate"(%211) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc778)
        %214 = "ttnn.typecast"(%213) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc779)
        "ttnn.deallocate"(%213) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc779)
        %215 = "ttnn.permute"(%214) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1710)
        "ttnn.deallocate"(%214) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1710)
        %216 = "ttnn.reshape"(%215) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc780)
        "ttnn.deallocate"(%215) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc780)
        %217 = "ttnn.matmul"(%216, %arg380) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1711)
        "ttnn.deallocate"(%216) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1711)
        "ttnn.deallocate"(%arg380) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1711)
        %218 = "ttnn.add"(%217, %81) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1712)
        "ttnn.deallocate"(%217) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1712)
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1712)
        %219 = "ttnn.add"(%182, %218) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc782)
        "ttnn.deallocate"(%218) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc782)
        "ttnn.deallocate"(%182) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc782)
        %220 = "ttnn.layer_norm"(%219, %arg378, %arg377) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc783)
        "ttnn.deallocate"(%arg378) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc783)
        "ttnn.deallocate"(%arg377) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc783)
        %221 = "ttnn.reshape"(%220) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc784)
        "ttnn.deallocate"(%220) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc784)
        %222 = "ttnn.matmul"(%221, %arg376) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1713)
        "ttnn.deallocate"(%221) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1713)
        "ttnn.deallocate"(%arg376) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1713)
        %223 = "ttnn.add"(%222, %68) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1714)
        "ttnn.deallocate"(%222) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1714)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1714)
        %224 = "ttnn.gelu"(%223) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1715)
        "ttnn.deallocate"(%223) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1715)
        %225 = "ttnn.reshape"(%224) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc786)
        "ttnn.deallocate"(%224) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc786)
        %226 = "ttnn.matmul"(%225, %arg374) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1716)
        "ttnn.deallocate"(%225) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1716)
        "ttnn.deallocate"(%arg374) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1716)
        %227 = "ttnn.add"(%226, %167) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1717)
        "ttnn.deallocate"(%226) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1717)
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1717)
        %228 = "ttnn.add"(%219, %227) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc788)
        "ttnn.deallocate"(%227) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc788)
        "ttnn.deallocate"(%219) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc788)
        %229 = "ttnn.layer_norm"(%228, %arg372, %arg371) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc789)
        "ttnn.deallocate"(%arg372) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc789)
        "ttnn.deallocate"(%arg371) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc789)
        %230 = "ttnn.reshape"(%229) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc790)
        "ttnn.deallocate"(%229) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc790)
        %231 = "ttnn.matmul"(%230, %93) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1718)
        "ttnn.deallocate"(%230) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1718)
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1718)
        %232 = "ttnn.add"(%231, %113) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1719)
        "ttnn.deallocate"(%231) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1719)
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1719)
        %233 = "ttnn.slice_static"(%232) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1720)
        %234 = "ttnn.slice_static"(%232) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1721)
        %235 = "ttnn.slice_static"(%232) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1722)
        "ttnn.deallocate"(%232) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1722)
        %236 = "ttnn.reshape"(%233) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1723)
        "ttnn.deallocate"(%233) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1723)
        %237 = "ttnn.reshape"(%234) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1724)
        "ttnn.deallocate"(%234) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1724)
        %238 = "ttnn.reshape"(%235) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1725)
        "ttnn.deallocate"(%235) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1725)
        %239 = "ttnn.permute"(%236) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1726)
        "ttnn.deallocate"(%236) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1726)
        %240 = "ttnn.permute"(%237) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1727)
        "ttnn.deallocate"(%237) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1727)
        %241 = "ttnn.permute"(%238) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1728)
        "ttnn.deallocate"(%238) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1728)
        %242 = "ttnn.typecast"(%239) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc792)
        "ttnn.deallocate"(%239) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc792)
        %243 = "ttnn.multiply"(%242, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc793)
        "ttnn.deallocate"(%242) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc793)
        %244 = "ttnn.typecast"(%240) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc794)
        "ttnn.deallocate"(%240) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc794)
        %245 = "ttnn.permute"(%244) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc794)
        "ttnn.deallocate"(%244) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc794)
        %246 = "ttnn.multiply"(%245, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc795)
        "ttnn.deallocate"(%245) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc795)
        %247 = "ttnn.matmul"(%243, %246) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc796)
        "ttnn.deallocate"(%246) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc796)
        "ttnn.deallocate"(%243) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc796)
        %248 = "ttnn.eq"(%247, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc797)
        %249 = "ttnn.logical_not"(%248) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc798)
        "ttnn.deallocate"(%248) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc798)
        %250 = "ttnn.sum"(%249) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc799)
        "ttnn.deallocate"(%249) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc799)
        %251 = "ttnn.ne"(%250, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc799)
        "ttnn.deallocate"(%250) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc799)
        %252 = "ttnn.logical_not"(%251) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc800)
        "ttnn.deallocate"(%251) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc800)
        %253 = "ttnn.reshape"(%252) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc800)
        "ttnn.deallocate"(%252) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc800)
        %254 = "ttnn.softmax"(%247) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc801)
        "ttnn.deallocate"(%247) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc801)
        %255 = "ttnn.repeat"(%253) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc802)
        "ttnn.deallocate"(%253) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc802)
        %256 = "ttnn.typecast"(%255) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1729)
        "ttnn.deallocate"(%255) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1729)
        %257 = "ttnn.where"(%256, %59, %254) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc802)
        "ttnn.deallocate"(%256) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc802)
        "ttnn.deallocate"(%254) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc802)
        %258 = "ttnn.typecast"(%241) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc803)
        "ttnn.deallocate"(%241) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc803)
        %259 = "ttnn.matmul"(%257, %258) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc804)
        "ttnn.deallocate"(%258) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc804)
        "ttnn.deallocate"(%257) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc804)
        %260 = "ttnn.typecast"(%259) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc805)
        "ttnn.deallocate"(%259) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc805)
        %261 = "ttnn.permute"(%260) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1730)
        "ttnn.deallocate"(%260) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1730)
        %262 = "ttnn.reshape"(%261) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc806)
        "ttnn.deallocate"(%261) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc806)
        %263 = "ttnn.matmul"(%262, %arg368) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1731)
        "ttnn.deallocate"(%262) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1731)
        "ttnn.deallocate"(%arg368) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1731)
        %264 = "ttnn.add"(%263, %152) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1732)
        "ttnn.deallocate"(%263) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1732)
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1732)
        %265 = "ttnn.add"(%228, %264) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc808)
        "ttnn.deallocate"(%264) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc808)
        "ttnn.deallocate"(%228) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc808)
        %266 = "ttnn.layer_norm"(%265, %arg366, %arg365) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc809)
        "ttnn.deallocate"(%arg366) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc809)
        "ttnn.deallocate"(%arg365) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc809)
        %267 = "ttnn.reshape"(%266) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc810)
        "ttnn.deallocate"(%266) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc810)
        %268 = "ttnn.matmul"(%267, %arg364) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1733)
        "ttnn.deallocate"(%267) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1733)
        "ttnn.deallocate"(%arg364) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1733)
        %269 = "ttnn.add"(%268, %157) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1734)
        "ttnn.deallocate"(%268) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1734)
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1734)
        %270 = "ttnn.gelu"(%269) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1735)
        "ttnn.deallocate"(%269) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1735)
        %271 = "ttnn.reshape"(%270) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc812)
        "ttnn.deallocate"(%270) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc812)
        %272 = "ttnn.matmul"(%271, %arg362) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1736)
        "ttnn.deallocate"(%271) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1736)
        "ttnn.deallocate"(%arg362) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1736)
        %273 = "ttnn.add"(%272, %27) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1737)
        "ttnn.deallocate"(%272) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1737)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1737)
        %274 = "ttnn.add"(%265, %273) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc814)
        "ttnn.deallocate"(%273) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc814)
        "ttnn.deallocate"(%265) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc814)
        %275 = "ttnn.layer_norm"(%274, %arg360, %arg359) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc815)
        "ttnn.deallocate"(%arg360) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc815)
        "ttnn.deallocate"(%arg359) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc815)
        %276 = "ttnn.reshape"(%275) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc816)
        "ttnn.deallocate"(%275) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc816)
        %277 = "ttnn.matmul"(%276, %89) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1738)
        "ttnn.deallocate"(%276) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1738)
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1738)
        %278 = "ttnn.add"(%277, %131) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1739)
        "ttnn.deallocate"(%277) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1739)
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1739)
        %279 = "ttnn.slice_static"(%278) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1740)
        %280 = "ttnn.slice_static"(%278) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1741)
        %281 = "ttnn.slice_static"(%278) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1742)
        "ttnn.deallocate"(%278) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1742)
        %282 = "ttnn.reshape"(%279) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1743)
        "ttnn.deallocate"(%279) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1743)
        %283 = "ttnn.reshape"(%280) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1744)
        "ttnn.deallocate"(%280) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1744)
        %284 = "ttnn.reshape"(%281) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1745)
        "ttnn.deallocate"(%281) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1745)
        %285 = "ttnn.permute"(%282) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1746)
        "ttnn.deallocate"(%282) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1746)
        %286 = "ttnn.permute"(%283) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1747)
        "ttnn.deallocate"(%283) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1747)
        %287 = "ttnn.permute"(%284) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1748)
        "ttnn.deallocate"(%284) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1748)
        %288 = "ttnn.typecast"(%285) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc818)
        "ttnn.deallocate"(%285) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc818)
        %289 = "ttnn.multiply"(%288, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc819)
        "ttnn.deallocate"(%288) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc819)
        %290 = "ttnn.typecast"(%286) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc820)
        "ttnn.deallocate"(%286) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc820)
        %291 = "ttnn.permute"(%290) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc820)
        "ttnn.deallocate"(%290) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc820)
        %292 = "ttnn.multiply"(%291, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc821)
        "ttnn.deallocate"(%291) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc821)
        %293 = "ttnn.matmul"(%289, %292) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc822)
        "ttnn.deallocate"(%292) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc822)
        "ttnn.deallocate"(%289) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc822)
        %294 = "ttnn.eq"(%293, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc823)
        %295 = "ttnn.logical_not"(%294) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc824)
        "ttnn.deallocate"(%294) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc824)
        %296 = "ttnn.sum"(%295) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc825)
        "ttnn.deallocate"(%295) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc825)
        %297 = "ttnn.ne"(%296, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc825)
        "ttnn.deallocate"(%296) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc825)
        %298 = "ttnn.logical_not"(%297) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc826)
        "ttnn.deallocate"(%297) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc826)
        %299 = "ttnn.reshape"(%298) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc826)
        "ttnn.deallocate"(%298) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc826)
        %300 = "ttnn.softmax"(%293) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc827)
        "ttnn.deallocate"(%293) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc827)
        %301 = "ttnn.repeat"(%299) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc828)
        "ttnn.deallocate"(%299) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc828)
        %302 = "ttnn.typecast"(%301) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1749)
        "ttnn.deallocate"(%301) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1749)
        %303 = "ttnn.where"(%302, %59, %300) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc828)
        "ttnn.deallocate"(%302) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc828)
        "ttnn.deallocate"(%300) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc828)
        %304 = "ttnn.typecast"(%287) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc829)
        "ttnn.deallocate"(%287) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc829)
        %305 = "ttnn.matmul"(%303, %304) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc830)
        "ttnn.deallocate"(%304) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc830)
        "ttnn.deallocate"(%303) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc830)
        %306 = "ttnn.typecast"(%305) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc831)
        "ttnn.deallocate"(%305) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc831)
        %307 = "ttnn.permute"(%306) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1750)
        "ttnn.deallocate"(%306) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1750)
        %308 = "ttnn.reshape"(%307) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc832)
        "ttnn.deallocate"(%307) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc832)
        %309 = "ttnn.matmul"(%308, %arg356) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1751)
        "ttnn.deallocate"(%308) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1751)
        "ttnn.deallocate"(%arg356) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1751)
        %310 = "ttnn.add"(%309, %156) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1752)
        "ttnn.deallocate"(%309) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1752)
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1752)
        %311 = "ttnn.add"(%274, %310) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc834)
        "ttnn.deallocate"(%310) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc834)
        "ttnn.deallocate"(%274) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc834)
        %312 = "ttnn.layer_norm"(%311, %arg354, %arg353) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc835)
        "ttnn.deallocate"(%arg354) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc835)
        "ttnn.deallocate"(%arg353) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc835)
        %313 = "ttnn.reshape"(%312) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc836)
        "ttnn.deallocate"(%312) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc836)
        %314 = "ttnn.matmul"(%313, %arg352) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1753)
        "ttnn.deallocate"(%313) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1753)
        "ttnn.deallocate"(%arg352) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1753)
        %315 = "ttnn.add"(%314, %125) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1754)
        "ttnn.deallocate"(%314) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1754)
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1754)
        %316 = "ttnn.gelu"(%315) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1755)
        "ttnn.deallocate"(%315) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1755)
        %317 = "ttnn.reshape"(%316) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc838)
        "ttnn.deallocate"(%316) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc838)
        %318 = "ttnn.matmul"(%317, %arg350) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1756)
        "ttnn.deallocate"(%317) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1756)
        "ttnn.deallocate"(%arg350) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1756)
        %319 = "ttnn.add"(%318, %103) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1757)
        "ttnn.deallocate"(%318) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1757)
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1757)
        %320 = "ttnn.add"(%311, %319) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc840)
        "ttnn.deallocate"(%319) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc840)
        "ttnn.deallocate"(%311) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc840)
        %321 = "ttnn.layer_norm"(%320, %arg348, %arg347) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc841)
        "ttnn.deallocate"(%arg348) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc841)
        "ttnn.deallocate"(%arg347) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc841)
        %322 = "ttnn.reshape"(%321) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc842)
        "ttnn.deallocate"(%321) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc842)
        %323 = "ttnn.matmul"(%322, %116) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1758)
        "ttnn.deallocate"(%322) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1758)
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1758)
        %324 = "ttnn.add"(%323, %117) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1759)
        "ttnn.deallocate"(%323) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1759)
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1759)
        %325 = "ttnn.slice_static"(%324) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1760)
        %326 = "ttnn.slice_static"(%324) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1761)
        %327 = "ttnn.slice_static"(%324) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1762)
        "ttnn.deallocate"(%324) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1762)
        %328 = "ttnn.reshape"(%325) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1763)
        "ttnn.deallocate"(%325) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1763)
        %329 = "ttnn.reshape"(%326) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1764)
        "ttnn.deallocate"(%326) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1764)
        %330 = "ttnn.reshape"(%327) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1765)
        "ttnn.deallocate"(%327) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1765)
        %331 = "ttnn.permute"(%328) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1766)
        "ttnn.deallocate"(%328) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1766)
        %332 = "ttnn.permute"(%329) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1767)
        "ttnn.deallocate"(%329) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1767)
        %333 = "ttnn.permute"(%330) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1768)
        "ttnn.deallocate"(%330) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1768)
        %334 = "ttnn.typecast"(%331) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc844)
        "ttnn.deallocate"(%331) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc844)
        %335 = "ttnn.multiply"(%334, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc845)
        "ttnn.deallocate"(%334) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc845)
        %336 = "ttnn.typecast"(%332) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc846)
        "ttnn.deallocate"(%332) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc846)
        %337 = "ttnn.permute"(%336) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc846)
        "ttnn.deallocate"(%336) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc846)
        %338 = "ttnn.multiply"(%337, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc847)
        "ttnn.deallocate"(%337) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc847)
        %339 = "ttnn.matmul"(%335, %338) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc848)
        "ttnn.deallocate"(%338) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc848)
        "ttnn.deallocate"(%335) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc848)
        %340 = "ttnn.eq"(%339, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc849)
        %341 = "ttnn.logical_not"(%340) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc850)
        "ttnn.deallocate"(%340) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc850)
        %342 = "ttnn.sum"(%341) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc851)
        "ttnn.deallocate"(%341) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc851)
        %343 = "ttnn.ne"(%342, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc851)
        "ttnn.deallocate"(%342) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc851)
        %344 = "ttnn.logical_not"(%343) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc852)
        "ttnn.deallocate"(%343) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc852)
        %345 = "ttnn.reshape"(%344) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc852)
        "ttnn.deallocate"(%344) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc852)
        %346 = "ttnn.softmax"(%339) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc853)
        "ttnn.deallocate"(%339) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc853)
        %347 = "ttnn.repeat"(%345) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc854)
        "ttnn.deallocate"(%345) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc854)
        %348 = "ttnn.typecast"(%347) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1769)
        "ttnn.deallocate"(%347) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1769)
        %349 = "ttnn.where"(%348, %59, %346) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc854)
        "ttnn.deallocate"(%348) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc854)
        "ttnn.deallocate"(%346) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc854)
        %350 = "ttnn.typecast"(%333) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc855)
        "ttnn.deallocate"(%333) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc855)
        %351 = "ttnn.matmul"(%349, %350) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc856)
        "ttnn.deallocate"(%350) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc856)
        "ttnn.deallocate"(%349) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc856)
        %352 = "ttnn.typecast"(%351) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc857)
        "ttnn.deallocate"(%351) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc857)
        %353 = "ttnn.permute"(%352) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1770)
        "ttnn.deallocate"(%352) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1770)
        %354 = "ttnn.reshape"(%353) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc858)
        "ttnn.deallocate"(%353) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc858)
        %355 = "ttnn.matmul"(%354, %arg344) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1771)
        "ttnn.deallocate"(%354) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1771)
        "ttnn.deallocate"(%arg344) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1771)
        %356 = "ttnn.add"(%355, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1772)
        "ttnn.deallocate"(%355) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1772)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1772)
        %357 = "ttnn.add"(%320, %356) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc860)
        "ttnn.deallocate"(%356) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc860)
        "ttnn.deallocate"(%320) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc860)
        %358 = "ttnn.layer_norm"(%357, %arg342, %arg341) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc861)
        "ttnn.deallocate"(%arg342) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc861)
        "ttnn.deallocate"(%arg341) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc861)
        %359 = "ttnn.reshape"(%358) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc862)
        "ttnn.deallocate"(%358) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc862)
        %360 = "ttnn.matmul"(%359, %arg340) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1773)
        "ttnn.deallocate"(%359) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1773)
        "ttnn.deallocate"(%arg340) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1773)
        %361 = "ttnn.add"(%360, %48) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1774)
        "ttnn.deallocate"(%360) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1774)
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1774)
        %362 = "ttnn.gelu"(%361) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1775)
        "ttnn.deallocate"(%361) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1775)
        %363 = "ttnn.reshape"(%362) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc864)
        "ttnn.deallocate"(%362) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc864)
        %364 = "ttnn.matmul"(%363, %arg338) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1776)
        "ttnn.deallocate"(%363) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1776)
        "ttnn.deallocate"(%arg338) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1776)
        %365 = "ttnn.add"(%364, %79) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1777)
        "ttnn.deallocate"(%364) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1777)
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1777)
        %366 = "ttnn.add"(%357, %365) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc866)
        "ttnn.deallocate"(%365) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc866)
        "ttnn.deallocate"(%357) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc866)
        %367 = "ttnn.layer_norm"(%366, %arg336, %arg335) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc867)
        "ttnn.deallocate"(%arg336) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc867)
        "ttnn.deallocate"(%arg335) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc867)
        %368 = "ttnn.reshape"(%367) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc868)
        "ttnn.deallocate"(%367) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc868)
        %369 = "ttnn.matmul"(%368, %7) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1778)
        "ttnn.deallocate"(%368) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1778)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1778)
        %370 = "ttnn.add"(%369, %80) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1779)
        "ttnn.deallocate"(%369) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1779)
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1779)
        %371 = "ttnn.slice_static"(%370) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1780)
        %372 = "ttnn.slice_static"(%370) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1781)
        %373 = "ttnn.slice_static"(%370) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1782)
        "ttnn.deallocate"(%370) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1782)
        %374 = "ttnn.reshape"(%371) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1783)
        "ttnn.deallocate"(%371) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1783)
        %375 = "ttnn.reshape"(%372) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1784)
        "ttnn.deallocate"(%372) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1784)
        %376 = "ttnn.reshape"(%373) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1785)
        "ttnn.deallocate"(%373) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1785)
        %377 = "ttnn.permute"(%374) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1786)
        "ttnn.deallocate"(%374) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1786)
        %378 = "ttnn.permute"(%375) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1787)
        "ttnn.deallocate"(%375) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1787)
        %379 = "ttnn.permute"(%376) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1788)
        "ttnn.deallocate"(%376) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1788)
        %380 = "ttnn.typecast"(%377) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc870)
        "ttnn.deallocate"(%377) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc870)
        %381 = "ttnn.multiply"(%380, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc871)
        "ttnn.deallocate"(%380) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc871)
        %382 = "ttnn.typecast"(%378) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc872)
        "ttnn.deallocate"(%378) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc872)
        %383 = "ttnn.permute"(%382) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc872)
        "ttnn.deallocate"(%382) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc872)
        %384 = "ttnn.multiply"(%383, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc873)
        "ttnn.deallocate"(%383) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc873)
        %385 = "ttnn.matmul"(%381, %384) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc874)
        "ttnn.deallocate"(%384) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc874)
        "ttnn.deallocate"(%381) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc874)
        %386 = "ttnn.eq"(%385, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc875)
        %387 = "ttnn.logical_not"(%386) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc876)
        "ttnn.deallocate"(%386) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc876)
        %388 = "ttnn.sum"(%387) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc877)
        "ttnn.deallocate"(%387) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc877)
        %389 = "ttnn.ne"(%388, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc877)
        "ttnn.deallocate"(%388) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc877)
        %390 = "ttnn.logical_not"(%389) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc878)
        "ttnn.deallocate"(%389) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc878)
        %391 = "ttnn.reshape"(%390) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc878)
        "ttnn.deallocate"(%390) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc878)
        %392 = "ttnn.softmax"(%385) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc879)
        "ttnn.deallocate"(%385) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc879)
        %393 = "ttnn.repeat"(%391) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc880)
        "ttnn.deallocate"(%391) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc880)
        %394 = "ttnn.typecast"(%393) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1789)
        "ttnn.deallocate"(%393) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1789)
        %395 = "ttnn.where"(%394, %59, %392) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc880)
        "ttnn.deallocate"(%394) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc880)
        "ttnn.deallocate"(%392) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc880)
        %396 = "ttnn.typecast"(%379) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc881)
        "ttnn.deallocate"(%379) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc881)
        %397 = "ttnn.matmul"(%395, %396) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc882)
        "ttnn.deallocate"(%396) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc882)
        "ttnn.deallocate"(%395) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc882)
        %398 = "ttnn.typecast"(%397) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc883)
        "ttnn.deallocate"(%397) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc883)
        %399 = "ttnn.permute"(%398) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1790)
        "ttnn.deallocate"(%398) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1790)
        %400 = "ttnn.reshape"(%399) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc884)
        "ttnn.deallocate"(%399) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc884)
        %401 = "ttnn.matmul"(%400, %arg332) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1791)
        "ttnn.deallocate"(%400) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1791)
        "ttnn.deallocate"(%arg332) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1791)
        %402 = "ttnn.add"(%401, %94) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1792)
        "ttnn.deallocate"(%401) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1792)
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1792)
        %403 = "ttnn.add"(%366, %402) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc886)
        "ttnn.deallocate"(%402) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc886)
        "ttnn.deallocate"(%366) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc886)
        %404 = "ttnn.layer_norm"(%403, %arg330, %arg329) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc887)
        "ttnn.deallocate"(%arg330) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc887)
        "ttnn.deallocate"(%arg329) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc887)
        %405 = "ttnn.reshape"(%404) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc888)
        "ttnn.deallocate"(%404) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc888)
        %406 = "ttnn.matmul"(%405, %arg328) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1793)
        "ttnn.deallocate"(%405) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1793)
        "ttnn.deallocate"(%arg328) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1793)
        %407 = "ttnn.add"(%406, %96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1794)
        "ttnn.deallocate"(%406) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1794)
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1794)
        %408 = "ttnn.gelu"(%407) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1795)
        "ttnn.deallocate"(%407) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1795)
        %409 = "ttnn.reshape"(%408) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc890)
        "ttnn.deallocate"(%408) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc890)
        %410 = "ttnn.matmul"(%409, %arg326) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1796)
        "ttnn.deallocate"(%409) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1796)
        "ttnn.deallocate"(%arg326) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1796)
        %411 = "ttnn.add"(%410, %105) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1797)
        "ttnn.deallocate"(%410) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1797)
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1797)
        %412 = "ttnn.add"(%403, %411) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc892)
        "ttnn.deallocate"(%411) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc892)
        "ttnn.deallocate"(%403) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc892)
        %413 = "ttnn.layer_norm"(%412, %arg324, %arg323) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc893)
        "ttnn.deallocate"(%arg324) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc893)
        "ttnn.deallocate"(%arg323) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc893)
        %414 = "ttnn.reshape"(%413) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc894)
        "ttnn.deallocate"(%413) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc894)
        %415 = "ttnn.matmul"(%414, %99) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1798)
        "ttnn.deallocate"(%414) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1798)
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1798)
        %416 = "ttnn.add"(%415, %76) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1799)
        "ttnn.deallocate"(%415) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1799)
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1799)
        %417 = "ttnn.slice_static"(%416) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1800)
        %418 = "ttnn.slice_static"(%416) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1801)
        %419 = "ttnn.slice_static"(%416) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1802)
        "ttnn.deallocate"(%416) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1802)
        %420 = "ttnn.reshape"(%417) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1803)
        "ttnn.deallocate"(%417) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1803)
        %421 = "ttnn.reshape"(%418) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1804)
        "ttnn.deallocate"(%418) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1804)
        %422 = "ttnn.reshape"(%419) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1805)
        "ttnn.deallocate"(%419) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1805)
        %423 = "ttnn.permute"(%420) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1806)
        "ttnn.deallocate"(%420) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1806)
        %424 = "ttnn.permute"(%421) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1807)
        "ttnn.deallocate"(%421) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1807)
        %425 = "ttnn.permute"(%422) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1808)
        "ttnn.deallocate"(%422) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1808)
        %426 = "ttnn.typecast"(%423) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc896)
        "ttnn.deallocate"(%423) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc896)
        %427 = "ttnn.multiply"(%426, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc897)
        "ttnn.deallocate"(%426) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc897)
        %428 = "ttnn.typecast"(%424) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc898)
        "ttnn.deallocate"(%424) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc898)
        %429 = "ttnn.permute"(%428) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc898)
        "ttnn.deallocate"(%428) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc898)
        %430 = "ttnn.multiply"(%429, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc899)
        "ttnn.deallocate"(%429) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc899)
        %431 = "ttnn.matmul"(%427, %430) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc900)
        "ttnn.deallocate"(%430) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc900)
        "ttnn.deallocate"(%427) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc900)
        %432 = "ttnn.eq"(%431, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc901)
        %433 = "ttnn.logical_not"(%432) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc902)
        "ttnn.deallocate"(%432) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc902)
        %434 = "ttnn.sum"(%433) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc903)
        "ttnn.deallocate"(%433) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc903)
        %435 = "ttnn.ne"(%434, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc903)
        "ttnn.deallocate"(%434) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc903)
        %436 = "ttnn.logical_not"(%435) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc904)
        "ttnn.deallocate"(%435) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc904)
        %437 = "ttnn.reshape"(%436) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc904)
        "ttnn.deallocate"(%436) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc904)
        %438 = "ttnn.softmax"(%431) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc905)
        "ttnn.deallocate"(%431) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc905)
        %439 = "ttnn.repeat"(%437) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc906)
        "ttnn.deallocate"(%437) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc906)
        %440 = "ttnn.typecast"(%439) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1809)
        "ttnn.deallocate"(%439) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1809)
        %441 = "ttnn.where"(%440, %59, %438) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc906)
        "ttnn.deallocate"(%440) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc906)
        "ttnn.deallocate"(%438) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc906)
        %442 = "ttnn.typecast"(%425) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc907)
        "ttnn.deallocate"(%425) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc907)
        %443 = "ttnn.matmul"(%441, %442) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc908)
        "ttnn.deallocate"(%442) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc908)
        "ttnn.deallocate"(%441) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc908)
        %444 = "ttnn.typecast"(%443) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc909)
        "ttnn.deallocate"(%443) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc909)
        %445 = "ttnn.permute"(%444) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1810)
        "ttnn.deallocate"(%444) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1810)
        %446 = "ttnn.reshape"(%445) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc910)
        "ttnn.deallocate"(%445) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc910)
        %447 = "ttnn.matmul"(%446, %arg320) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1811)
        "ttnn.deallocate"(%446) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1811)
        "ttnn.deallocate"(%arg320) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1811)
        %448 = "ttnn.add"(%447, %75) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1812)
        "ttnn.deallocate"(%447) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1812)
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1812)
        %449 = "ttnn.add"(%412, %448) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc912)
        "ttnn.deallocate"(%448) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc912)
        "ttnn.deallocate"(%412) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc912)
        %450 = "ttnn.layer_norm"(%449, %arg318, %arg317) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc913)
        "ttnn.deallocate"(%arg318) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc913)
        "ttnn.deallocate"(%arg317) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc913)
        %451 = "ttnn.reshape"(%450) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc914)
        "ttnn.deallocate"(%450) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc914)
        %452 = "ttnn.matmul"(%451, %arg316) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1813)
        "ttnn.deallocate"(%451) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1813)
        "ttnn.deallocate"(%arg316) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1813)
        %453 = "ttnn.add"(%452, %63) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1814)
        "ttnn.deallocate"(%452) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1814)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1814)
        %454 = "ttnn.gelu"(%453) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1815)
        "ttnn.deallocate"(%453) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1815)
        %455 = "ttnn.reshape"(%454) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc916)
        "ttnn.deallocate"(%454) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc916)
        %456 = "ttnn.matmul"(%455, %arg314) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1816)
        "ttnn.deallocate"(%455) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1816)
        "ttnn.deallocate"(%arg314) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1816)
        %457 = "ttnn.add"(%456, %158) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1817)
        "ttnn.deallocate"(%456) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1817)
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1817)
        %458 = "ttnn.add"(%449, %457) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc918)
        "ttnn.deallocate"(%457) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc918)
        "ttnn.deallocate"(%449) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc918)
        %459 = "ttnn.layer_norm"(%458, %arg312, %arg311) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc919)
        "ttnn.deallocate"(%arg312) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc919)
        "ttnn.deallocate"(%arg311) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc919)
        %460 = "ttnn.reshape"(%459) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc920)
        "ttnn.deallocate"(%459) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc920)
        %461 = "ttnn.matmul"(%460, %127) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1818)
        "ttnn.deallocate"(%460) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1818)
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1818)
        %462 = "ttnn.add"(%461, %162) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1819)
        "ttnn.deallocate"(%461) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1819)
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1819)
        %463 = "ttnn.slice_static"(%462) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1820)
        %464 = "ttnn.slice_static"(%462) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1821)
        %465 = "ttnn.slice_static"(%462) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1822)
        "ttnn.deallocate"(%462) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1822)
        %466 = "ttnn.reshape"(%463) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1823)
        "ttnn.deallocate"(%463) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1823)
        %467 = "ttnn.reshape"(%464) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1824)
        "ttnn.deallocate"(%464) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1824)
        %468 = "ttnn.reshape"(%465) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1825)
        "ttnn.deallocate"(%465) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1825)
        %469 = "ttnn.permute"(%466) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1826)
        "ttnn.deallocate"(%466) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1826)
        %470 = "ttnn.permute"(%467) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1827)
        "ttnn.deallocate"(%467) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1827)
        %471 = "ttnn.permute"(%468) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1828)
        "ttnn.deallocate"(%468) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1828)
        %472 = "ttnn.typecast"(%469) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc922)
        "ttnn.deallocate"(%469) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc922)
        %473 = "ttnn.multiply"(%472, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc923)
        "ttnn.deallocate"(%472) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc923)
        %474 = "ttnn.typecast"(%470) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc924)
        "ttnn.deallocate"(%470) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc924)
        %475 = "ttnn.permute"(%474) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc924)
        "ttnn.deallocate"(%474) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc924)
        %476 = "ttnn.multiply"(%475, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc925)
        "ttnn.deallocate"(%475) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc925)
        %477 = "ttnn.matmul"(%473, %476) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc926)
        "ttnn.deallocate"(%476) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc926)
        "ttnn.deallocate"(%473) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc926)
        %478 = "ttnn.eq"(%477, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc927)
        %479 = "ttnn.logical_not"(%478) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc928)
        "ttnn.deallocate"(%478) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc928)
        %480 = "ttnn.sum"(%479) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc929)
        "ttnn.deallocate"(%479) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc929)
        %481 = "ttnn.ne"(%480, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc929)
        "ttnn.deallocate"(%480) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc929)
        %482 = "ttnn.logical_not"(%481) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc930)
        "ttnn.deallocate"(%481) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc930)
        %483 = "ttnn.reshape"(%482) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc930)
        "ttnn.deallocate"(%482) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc930)
        %484 = "ttnn.softmax"(%477) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc931)
        "ttnn.deallocate"(%477) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc931)
        %485 = "ttnn.repeat"(%483) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc932)
        "ttnn.deallocate"(%483) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc932)
        %486 = "ttnn.typecast"(%485) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1829)
        "ttnn.deallocate"(%485) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1829)
        %487 = "ttnn.where"(%486, %59, %484) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc932)
        "ttnn.deallocate"(%486) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc932)
        "ttnn.deallocate"(%484) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc932)
        %488 = "ttnn.typecast"(%471) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc933)
        "ttnn.deallocate"(%471) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc933)
        %489 = "ttnn.matmul"(%487, %488) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc934)
        "ttnn.deallocate"(%488) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc934)
        "ttnn.deallocate"(%487) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc934)
        %490 = "ttnn.typecast"(%489) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc935)
        "ttnn.deallocate"(%489) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc935)
        %491 = "ttnn.permute"(%490) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1830)
        "ttnn.deallocate"(%490) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1830)
        %492 = "ttnn.reshape"(%491) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc936)
        "ttnn.deallocate"(%491) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc936)
        %493 = "ttnn.matmul"(%492, %arg308) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1831)
        "ttnn.deallocate"(%492) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1831)
        "ttnn.deallocate"(%arg308) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1831)
        %494 = "ttnn.add"(%493, %118) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1832)
        "ttnn.deallocate"(%493) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1832)
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1832)
        %495 = "ttnn.add"(%458, %494) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc938)
        "ttnn.deallocate"(%494) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc938)
        "ttnn.deallocate"(%458) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc938)
        %496 = "ttnn.layer_norm"(%495, %arg306, %arg305) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc939)
        "ttnn.deallocate"(%arg306) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc939)
        "ttnn.deallocate"(%arg305) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc939)
        %497 = "ttnn.reshape"(%496) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc940)
        "ttnn.deallocate"(%496) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc940)
        %498 = "ttnn.matmul"(%497, %arg304) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1833)
        "ttnn.deallocate"(%497) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1833)
        "ttnn.deallocate"(%arg304) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1833)
        %499 = "ttnn.add"(%498, %92) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1834)
        "ttnn.deallocate"(%498) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1834)
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1834)
        %500 = "ttnn.gelu"(%499) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1835)
        "ttnn.deallocate"(%499) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1835)
        %501 = "ttnn.reshape"(%500) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc942)
        "ttnn.deallocate"(%500) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc942)
        %502 = "ttnn.matmul"(%501, %arg302) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1836)
        "ttnn.deallocate"(%501) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1836)
        "ttnn.deallocate"(%arg302) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1836)
        %503 = "ttnn.add"(%502, %170) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1837)
        "ttnn.deallocate"(%502) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1837)
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1837)
        %504 = "ttnn.add"(%495, %503) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc944)
        "ttnn.deallocate"(%503) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc944)
        "ttnn.deallocate"(%495) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc944)
        %505 = "ttnn.layer_norm"(%504, %arg300, %arg299) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc945)
        "ttnn.deallocate"(%arg300) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc945)
        "ttnn.deallocate"(%arg299) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc945)
        %506 = "ttnn.reshape"(%505) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc946)
        "ttnn.deallocate"(%505) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc946)
        %507 = "ttnn.matmul"(%506, %19) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1838)
        "ttnn.deallocate"(%506) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1838)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1838)
        %508 = "ttnn.add"(%507, %102) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1839)
        "ttnn.deallocate"(%507) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1839)
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1839)
        %509 = "ttnn.slice_static"(%508) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1840)
        %510 = "ttnn.slice_static"(%508) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1841)
        %511 = "ttnn.slice_static"(%508) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1842)
        "ttnn.deallocate"(%508) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1842)
        %512 = "ttnn.reshape"(%509) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1843)
        "ttnn.deallocate"(%509) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1843)
        %513 = "ttnn.reshape"(%510) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1844)
        "ttnn.deallocate"(%510) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1844)
        %514 = "ttnn.reshape"(%511) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1845)
        "ttnn.deallocate"(%511) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1845)
        %515 = "ttnn.permute"(%512) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1846)
        "ttnn.deallocate"(%512) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1846)
        %516 = "ttnn.permute"(%513) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1847)
        "ttnn.deallocate"(%513) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1847)
        %517 = "ttnn.permute"(%514) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1848)
        "ttnn.deallocate"(%514) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1848)
        %518 = "ttnn.typecast"(%515) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc948)
        "ttnn.deallocate"(%515) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc948)
        %519 = "ttnn.multiply"(%518, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc949)
        "ttnn.deallocate"(%518) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc949)
        %520 = "ttnn.typecast"(%516) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc950)
        "ttnn.deallocate"(%516) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc950)
        %521 = "ttnn.permute"(%520) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc950)
        "ttnn.deallocate"(%520) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc950)
        %522 = "ttnn.multiply"(%521, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc951)
        "ttnn.deallocate"(%521) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc951)
        %523 = "ttnn.matmul"(%519, %522) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc952)
        "ttnn.deallocate"(%522) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc952)
        "ttnn.deallocate"(%519) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc952)
        %524 = "ttnn.eq"(%523, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc953)
        %525 = "ttnn.logical_not"(%524) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc954)
        "ttnn.deallocate"(%524) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc954)
        %526 = "ttnn.sum"(%525) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc955)
        "ttnn.deallocate"(%525) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc955)
        %527 = "ttnn.ne"(%526, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc955)
        "ttnn.deallocate"(%526) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc955)
        %528 = "ttnn.logical_not"(%527) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc956)
        "ttnn.deallocate"(%527) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc956)
        %529 = "ttnn.reshape"(%528) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc956)
        "ttnn.deallocate"(%528) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc956)
        %530 = "ttnn.softmax"(%523) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc957)
        "ttnn.deallocate"(%523) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc957)
        %531 = "ttnn.repeat"(%529) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc958)
        "ttnn.deallocate"(%529) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc958)
        %532 = "ttnn.typecast"(%531) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1849)
        "ttnn.deallocate"(%531) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1849)
        %533 = "ttnn.where"(%532, %59, %530) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc958)
        "ttnn.deallocate"(%532) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc958)
        "ttnn.deallocate"(%530) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc958)
        %534 = "ttnn.typecast"(%517) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc959)
        "ttnn.deallocate"(%517) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc959)
        %535 = "ttnn.matmul"(%533, %534) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc960)
        "ttnn.deallocate"(%534) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc960)
        "ttnn.deallocate"(%533) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc960)
        %536 = "ttnn.typecast"(%535) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc961)
        "ttnn.deallocate"(%535) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc961)
        %537 = "ttnn.permute"(%536) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1850)
        "ttnn.deallocate"(%536) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1850)
        %538 = "ttnn.reshape"(%537) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc962)
        "ttnn.deallocate"(%537) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc962)
        %539 = "ttnn.matmul"(%538, %arg296) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1851)
        "ttnn.deallocate"(%538) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1851)
        "ttnn.deallocate"(%arg296) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1851)
        %540 = "ttnn.add"(%539, %67) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1852)
        "ttnn.deallocate"(%539) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1852)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1852)
        %541 = "ttnn.add"(%504, %540) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc964)
        "ttnn.deallocate"(%540) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc964)
        "ttnn.deallocate"(%504) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc964)
        %542 = "ttnn.layer_norm"(%541, %arg294, %arg293) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc965)
        "ttnn.deallocate"(%arg294) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc965)
        "ttnn.deallocate"(%arg293) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc965)
        %543 = "ttnn.reshape"(%542) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc966)
        "ttnn.deallocate"(%542) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc966)
        %544 = "ttnn.matmul"(%543, %arg292) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1853)
        "ttnn.deallocate"(%543) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1853)
        "ttnn.deallocate"(%arg292) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1853)
        %545 = "ttnn.add"(%544, %66) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1854)
        "ttnn.deallocate"(%544) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1854)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1854)
        %546 = "ttnn.gelu"(%545) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1855)
        "ttnn.deallocate"(%545) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1855)
        %547 = "ttnn.reshape"(%546) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc968)
        "ttnn.deallocate"(%546) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc968)
        %548 = "ttnn.matmul"(%547, %arg290) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1856)
        "ttnn.deallocate"(%547) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1856)
        "ttnn.deallocate"(%arg290) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1856)
        %549 = "ttnn.add"(%548, %1) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1857)
        "ttnn.deallocate"(%548) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1857)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1857)
        %550 = "ttnn.add"(%541, %549) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc970)
        "ttnn.deallocate"(%549) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc970)
        "ttnn.deallocate"(%541) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc970)
        %551 = "ttnn.layer_norm"(%550, %arg288, %arg287) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc971)
        "ttnn.deallocate"(%arg288) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc971)
        "ttnn.deallocate"(%arg287) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc971)
        %552 = "ttnn.reshape"(%551) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc972)
        "ttnn.deallocate"(%551) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc972)
        %553 = "ttnn.matmul"(%552, %26) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1858)
        "ttnn.deallocate"(%552) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1858)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1858)
        %554 = "ttnn.add"(%553, %14) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1859)
        "ttnn.deallocate"(%553) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1859)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1859)
        %555 = "ttnn.slice_static"(%554) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1860)
        %556 = "ttnn.slice_static"(%554) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1861)
        %557 = "ttnn.slice_static"(%554) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1862)
        "ttnn.deallocate"(%554) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1862)
        %558 = "ttnn.reshape"(%555) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1863)
        "ttnn.deallocate"(%555) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1863)
        %559 = "ttnn.reshape"(%556) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1864)
        "ttnn.deallocate"(%556) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1864)
        %560 = "ttnn.reshape"(%557) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1865)
        "ttnn.deallocate"(%557) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1865)
        %561 = "ttnn.permute"(%558) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1866)
        "ttnn.deallocate"(%558) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1866)
        %562 = "ttnn.permute"(%559) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1867)
        "ttnn.deallocate"(%559) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1867)
        %563 = "ttnn.permute"(%560) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1868)
        "ttnn.deallocate"(%560) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1868)
        %564 = "ttnn.typecast"(%561) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc974)
        "ttnn.deallocate"(%561) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc974)
        %565 = "ttnn.multiply"(%564, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc975)
        "ttnn.deallocate"(%564) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc975)
        %566 = "ttnn.typecast"(%562) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc976)
        "ttnn.deallocate"(%562) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc976)
        %567 = "ttnn.permute"(%566) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc976)
        "ttnn.deallocate"(%566) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc976)
        %568 = "ttnn.multiply"(%567, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc977)
        "ttnn.deallocate"(%567) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc977)
        %569 = "ttnn.matmul"(%565, %568) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc978)
        "ttnn.deallocate"(%568) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc978)
        "ttnn.deallocate"(%565) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc978)
        %570 = "ttnn.eq"(%569, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc979)
        %571 = "ttnn.logical_not"(%570) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc980)
        "ttnn.deallocate"(%570) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc980)
        %572 = "ttnn.sum"(%571) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc981)
        "ttnn.deallocate"(%571) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc981)
        %573 = "ttnn.ne"(%572, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc981)
        "ttnn.deallocate"(%572) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc981)
        %574 = "ttnn.logical_not"(%573) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc982)
        "ttnn.deallocate"(%573) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc982)
        %575 = "ttnn.reshape"(%574) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc982)
        "ttnn.deallocate"(%574) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc982)
        %576 = "ttnn.softmax"(%569) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc983)
        "ttnn.deallocate"(%569) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc983)
        %577 = "ttnn.repeat"(%575) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc984)
        "ttnn.deallocate"(%575) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc984)
        %578 = "ttnn.typecast"(%577) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1869)
        "ttnn.deallocate"(%577) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1869)
        %579 = "ttnn.where"(%578, %59, %576) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc984)
        "ttnn.deallocate"(%578) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc984)
        "ttnn.deallocate"(%576) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc984)
        %580 = "ttnn.typecast"(%563) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc985)
        "ttnn.deallocate"(%563) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc985)
        %581 = "ttnn.matmul"(%579, %580) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc986)
        "ttnn.deallocate"(%580) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc986)
        "ttnn.deallocate"(%579) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc986)
        %582 = "ttnn.typecast"(%581) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc987)
        "ttnn.deallocate"(%581) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc987)
        %583 = "ttnn.permute"(%582) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1870)
        "ttnn.deallocate"(%582) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1870)
        %584 = "ttnn.reshape"(%583) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc988)
        "ttnn.deallocate"(%583) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc988)
        %585 = "ttnn.matmul"(%584, %arg284) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1871)
        "ttnn.deallocate"(%584) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1871)
        "ttnn.deallocate"(%arg284) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1871)
        %586 = "ttnn.add"(%585, %82) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1872)
        "ttnn.deallocate"(%585) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1872)
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1872)
        %587 = "ttnn.add"(%550, %586) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc990)
        "ttnn.deallocate"(%586) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc990)
        "ttnn.deallocate"(%550) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc990)
        %588 = "ttnn.layer_norm"(%587, %arg282, %arg281) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc991)
        "ttnn.deallocate"(%arg282) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc991)
        "ttnn.deallocate"(%arg281) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc991)
        %589 = "ttnn.reshape"(%588) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc992)
        "ttnn.deallocate"(%588) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc992)
        %590 = "ttnn.matmul"(%589, %arg280) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1873)
        "ttnn.deallocate"(%589) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1873)
        "ttnn.deallocate"(%arg280) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1873)
        %591 = "ttnn.add"(%590, %104) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1874)
        "ttnn.deallocate"(%590) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1874)
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1874)
        %592 = "ttnn.gelu"(%591) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1875)
        "ttnn.deallocate"(%591) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1875)
        %593 = "ttnn.reshape"(%592) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc994)
        "ttnn.deallocate"(%592) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc994)
        %594 = "ttnn.matmul"(%593, %arg278) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1876)
        "ttnn.deallocate"(%593) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1876)
        "ttnn.deallocate"(%arg278) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1876)
        %595 = "ttnn.add"(%594, %91) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1877)
        "ttnn.deallocate"(%594) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1877)
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1877)
        %596 = "ttnn.add"(%587, %595) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc996)
        "ttnn.deallocate"(%595) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc996)
        "ttnn.deallocate"(%587) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc996)
        %597 = "ttnn.layer_norm"(%596, %arg276, %arg275) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc997)
        "ttnn.deallocate"(%arg276) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc997)
        "ttnn.deallocate"(%arg275) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc997)
        %598 = "ttnn.reshape"(%597) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc998)
        "ttnn.deallocate"(%597) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc998)
        %599 = "ttnn.matmul"(%598, %8) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1878)
        "ttnn.deallocate"(%598) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1878)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1878)
        %600 = "ttnn.add"(%599, %153) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1879)
        "ttnn.deallocate"(%599) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1879)
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1879)
        %601 = "ttnn.slice_static"(%600) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1880)
        %602 = "ttnn.slice_static"(%600) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1881)
        %603 = "ttnn.slice_static"(%600) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1882)
        "ttnn.deallocate"(%600) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1882)
        %604 = "ttnn.reshape"(%601) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1883)
        "ttnn.deallocate"(%601) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1883)
        %605 = "ttnn.reshape"(%602) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1884)
        "ttnn.deallocate"(%602) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1884)
        %606 = "ttnn.reshape"(%603) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1885)
        "ttnn.deallocate"(%603) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1885)
        %607 = "ttnn.permute"(%604) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1886)
        "ttnn.deallocate"(%604) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1886)
        %608 = "ttnn.permute"(%605) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1887)
        "ttnn.deallocate"(%605) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1887)
        %609 = "ttnn.permute"(%606) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1888)
        "ttnn.deallocate"(%606) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1888)
        %610 = "ttnn.typecast"(%607) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1000)
        "ttnn.deallocate"(%607) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1000)
        %611 = "ttnn.multiply"(%610, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1001)
        "ttnn.deallocate"(%610) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1001)
        %612 = "ttnn.typecast"(%608) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1002)
        "ttnn.deallocate"(%608) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1002)
        %613 = "ttnn.permute"(%612) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1002)
        "ttnn.deallocate"(%612) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1002)
        %614 = "ttnn.multiply"(%613, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1003)
        "ttnn.deallocate"(%613) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1003)
        %615 = "ttnn.matmul"(%611, %614) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1004)
        "ttnn.deallocate"(%614) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1004)
        "ttnn.deallocate"(%611) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1004)
        %616 = "ttnn.eq"(%615, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1005)
        %617 = "ttnn.logical_not"(%616) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1006)
        "ttnn.deallocate"(%616) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1006)
        %618 = "ttnn.sum"(%617) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1007)
        "ttnn.deallocate"(%617) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1007)
        %619 = "ttnn.ne"(%618, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1007)
        "ttnn.deallocate"(%618) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1007)
        %620 = "ttnn.logical_not"(%619) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1008)
        "ttnn.deallocate"(%619) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1008)
        %621 = "ttnn.reshape"(%620) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1008)
        "ttnn.deallocate"(%620) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1008)
        %622 = "ttnn.softmax"(%615) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1009)
        "ttnn.deallocate"(%615) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1009)
        %623 = "ttnn.repeat"(%621) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1010)
        "ttnn.deallocate"(%621) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1010)
        %624 = "ttnn.typecast"(%623) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1889)
        "ttnn.deallocate"(%623) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1889)
        %625 = "ttnn.where"(%624, %59, %622) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1010)
        "ttnn.deallocate"(%624) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1010)
        "ttnn.deallocate"(%622) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1010)
        %626 = "ttnn.typecast"(%609) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1011)
        "ttnn.deallocate"(%609) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1011)
        %627 = "ttnn.matmul"(%625, %626) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1012)
        "ttnn.deallocate"(%626) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1012)
        "ttnn.deallocate"(%625) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1012)
        %628 = "ttnn.typecast"(%627) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1013)
        "ttnn.deallocate"(%627) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1013)
        %629 = "ttnn.permute"(%628) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1890)
        "ttnn.deallocate"(%628) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1890)
        %630 = "ttnn.reshape"(%629) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1014)
        "ttnn.deallocate"(%629) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1014)
        %631 = "ttnn.matmul"(%630, %arg272) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1891)
        "ttnn.deallocate"(%630) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1891)
        "ttnn.deallocate"(%arg272) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1891)
        %632 = "ttnn.add"(%631, %87) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1892)
        "ttnn.deallocate"(%631) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1892)
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1892)
        %633 = "ttnn.add"(%596, %632) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1016)
        "ttnn.deallocate"(%632) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1016)
        "ttnn.deallocate"(%596) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1016)
        %634 = "ttnn.layer_norm"(%633, %arg270, %arg269) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1017)
        "ttnn.deallocate"(%arg270) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1017)
        "ttnn.deallocate"(%arg269) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1017)
        %635 = "ttnn.reshape"(%634) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1018)
        "ttnn.deallocate"(%634) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1018)
        %636 = "ttnn.matmul"(%635, %arg268) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1893)
        "ttnn.deallocate"(%635) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1893)
        "ttnn.deallocate"(%arg268) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1893)
        %637 = "ttnn.add"(%636, %25) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1894)
        "ttnn.deallocate"(%636) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1894)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1894)
        %638 = "ttnn.gelu"(%637) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1895)
        "ttnn.deallocate"(%637) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1895)
        %639 = "ttnn.reshape"(%638) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1020)
        "ttnn.deallocate"(%638) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1020)
        %640 = "ttnn.matmul"(%639, %arg266) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1896)
        "ttnn.deallocate"(%639) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1896)
        "ttnn.deallocate"(%arg266) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1896)
        %641 = "ttnn.add"(%640, %161) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1897)
        "ttnn.deallocate"(%640) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1897)
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1897)
        %642 = "ttnn.add"(%633, %641) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1022)
        "ttnn.deallocate"(%641) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1022)
        "ttnn.deallocate"(%633) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1022)
        %643 = "ttnn.layer_norm"(%642, %arg264, %arg263) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1023)
        "ttnn.deallocate"(%arg264) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1023)
        "ttnn.deallocate"(%arg263) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1023)
        %644 = "ttnn.reshape"(%643) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1024)
        "ttnn.deallocate"(%643) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1024)
        %645 = "ttnn.matmul"(%644, %107) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1898)
        "ttnn.deallocate"(%644) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1898)
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1898)
        %646 = "ttnn.add"(%645, %70) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1899)
        "ttnn.deallocate"(%645) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1899)
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1899)
        %647 = "ttnn.slice_static"(%646) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1900)
        %648 = "ttnn.slice_static"(%646) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1901)
        %649 = "ttnn.slice_static"(%646) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1902)
        "ttnn.deallocate"(%646) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1902)
        %650 = "ttnn.reshape"(%647) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1903)
        "ttnn.deallocate"(%647) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1903)
        %651 = "ttnn.reshape"(%648) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1904)
        "ttnn.deallocate"(%648) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1904)
        %652 = "ttnn.reshape"(%649) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1905)
        "ttnn.deallocate"(%649) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1905)
        %653 = "ttnn.permute"(%650) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1906)
        "ttnn.deallocate"(%650) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1906)
        %654 = "ttnn.permute"(%651) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1907)
        "ttnn.deallocate"(%651) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1907)
        %655 = "ttnn.permute"(%652) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1908)
        "ttnn.deallocate"(%652) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1908)
        %656 = "ttnn.typecast"(%653) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1026)
        "ttnn.deallocate"(%653) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1026)
        %657 = "ttnn.multiply"(%656, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1027)
        "ttnn.deallocate"(%656) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1027)
        %658 = "ttnn.typecast"(%654) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1028)
        "ttnn.deallocate"(%654) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1028)
        %659 = "ttnn.permute"(%658) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1028)
        "ttnn.deallocate"(%658) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1028)
        %660 = "ttnn.multiply"(%659, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1029)
        "ttnn.deallocate"(%659) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1029)
        %661 = "ttnn.matmul"(%657, %660) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1030)
        "ttnn.deallocate"(%660) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1030)
        "ttnn.deallocate"(%657) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1030)
        %662 = "ttnn.eq"(%661, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1031)
        %663 = "ttnn.logical_not"(%662) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1032)
        "ttnn.deallocate"(%662) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1032)
        %664 = "ttnn.sum"(%663) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1033)
        "ttnn.deallocate"(%663) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1033)
        %665 = "ttnn.ne"(%664, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1033)
        "ttnn.deallocate"(%664) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1033)
        %666 = "ttnn.logical_not"(%665) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1034)
        "ttnn.deallocate"(%665) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1034)
        %667 = "ttnn.reshape"(%666) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1034)
        "ttnn.deallocate"(%666) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1034)
        %668 = "ttnn.softmax"(%661) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1035)
        "ttnn.deallocate"(%661) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1035)
        %669 = "ttnn.repeat"(%667) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1036)
        "ttnn.deallocate"(%667) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1036)
        %670 = "ttnn.typecast"(%669) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1909)
        "ttnn.deallocate"(%669) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1909)
        %671 = "ttnn.where"(%670, %59, %668) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1036)
        "ttnn.deallocate"(%670) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1036)
        "ttnn.deallocate"(%668) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1036)
        %672 = "ttnn.typecast"(%655) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1037)
        "ttnn.deallocate"(%655) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1037)
        %673 = "ttnn.matmul"(%671, %672) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1038)
        "ttnn.deallocate"(%672) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1038)
        "ttnn.deallocate"(%671) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1038)
        %674 = "ttnn.typecast"(%673) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1039)
        "ttnn.deallocate"(%673) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1039)
        %675 = "ttnn.permute"(%674) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1910)
        "ttnn.deallocate"(%674) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1910)
        %676 = "ttnn.reshape"(%675) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1040)
        "ttnn.deallocate"(%675) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1040)
        %677 = "ttnn.matmul"(%676, %arg260) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1911)
        "ttnn.deallocate"(%676) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1911)
        "ttnn.deallocate"(%arg260) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1911)
        %678 = "ttnn.add"(%677, %137) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1912)
        "ttnn.deallocate"(%677) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1912)
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1912)
        %679 = "ttnn.add"(%642, %678) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1042)
        "ttnn.deallocate"(%678) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1042)
        "ttnn.deallocate"(%642) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1042)
        %680 = "ttnn.layer_norm"(%679, %arg258, %arg257) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1043)
        "ttnn.deallocate"(%arg258) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1043)
        "ttnn.deallocate"(%arg257) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1043)
        %681 = "ttnn.reshape"(%680) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1044)
        "ttnn.deallocate"(%680) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1044)
        %682 = "ttnn.matmul"(%681, %arg256) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1913)
        "ttnn.deallocate"(%681) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1913)
        "ttnn.deallocate"(%arg256) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1913)
        %683 = "ttnn.add"(%682, %86) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1914)
        "ttnn.deallocate"(%682) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1914)
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1914)
        %684 = "ttnn.gelu"(%683) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1915)
        "ttnn.deallocate"(%683) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1915)
        %685 = "ttnn.reshape"(%684) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1046)
        "ttnn.deallocate"(%684) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1046)
        %686 = "ttnn.matmul"(%685, %arg254) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1916)
        "ttnn.deallocate"(%685) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1916)
        "ttnn.deallocate"(%arg254) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1916)
        %687 = "ttnn.add"(%686, %21) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1917)
        "ttnn.deallocate"(%686) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1917)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1917)
        %688 = "ttnn.add"(%679, %687) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1048)
        "ttnn.deallocate"(%687) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1048)
        "ttnn.deallocate"(%679) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1048)
        %689 = "ttnn.layer_norm"(%688, %arg252, %arg251) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1049)
        "ttnn.deallocate"(%arg252) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1049)
        "ttnn.deallocate"(%arg251) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1049)
        %690 = "ttnn.reshape"(%689) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1050)
        "ttnn.deallocate"(%689) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1050)
        %691 = "ttnn.matmul"(%690, %166) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1918)
        "ttnn.deallocate"(%690) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1918)
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1918)
        %692 = "ttnn.add"(%691, %10) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1919)
        "ttnn.deallocate"(%691) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1919)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1919)
        %693 = "ttnn.slice_static"(%692) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1920)
        %694 = "ttnn.slice_static"(%692) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1921)
        %695 = "ttnn.slice_static"(%692) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1922)
        "ttnn.deallocate"(%692) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1922)
        %696 = "ttnn.reshape"(%693) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1923)
        "ttnn.deallocate"(%693) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1923)
        %697 = "ttnn.reshape"(%694) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1924)
        "ttnn.deallocate"(%694) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1924)
        %698 = "ttnn.reshape"(%695) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1925)
        "ttnn.deallocate"(%695) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1925)
        %699 = "ttnn.permute"(%696) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1926)
        "ttnn.deallocate"(%696) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1926)
        %700 = "ttnn.permute"(%697) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1927)
        "ttnn.deallocate"(%697) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1927)
        %701 = "ttnn.permute"(%698) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1928)
        "ttnn.deallocate"(%698) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1928)
        %702 = "ttnn.typecast"(%699) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1052)
        "ttnn.deallocate"(%699) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1052)
        %703 = "ttnn.multiply"(%702, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1053)
        "ttnn.deallocate"(%702) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1053)
        %704 = "ttnn.typecast"(%700) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1054)
        "ttnn.deallocate"(%700) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1054)
        %705 = "ttnn.permute"(%704) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1054)
        "ttnn.deallocate"(%704) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1054)
        %706 = "ttnn.multiply"(%705, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1055)
        "ttnn.deallocate"(%705) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1055)
        %707 = "ttnn.matmul"(%703, %706) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1056)
        "ttnn.deallocate"(%706) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1056)
        "ttnn.deallocate"(%703) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1056)
        %708 = "ttnn.eq"(%707, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1057)
        %709 = "ttnn.logical_not"(%708) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1058)
        "ttnn.deallocate"(%708) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1058)
        %710 = "ttnn.sum"(%709) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1059)
        "ttnn.deallocate"(%709) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1059)
        %711 = "ttnn.ne"(%710, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1059)
        "ttnn.deallocate"(%710) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1059)
        %712 = "ttnn.logical_not"(%711) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1060)
        "ttnn.deallocate"(%711) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1060)
        %713 = "ttnn.reshape"(%712) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1060)
        "ttnn.deallocate"(%712) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1060)
        %714 = "ttnn.softmax"(%707) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1061)
        "ttnn.deallocate"(%707) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1061)
        %715 = "ttnn.repeat"(%713) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1062)
        "ttnn.deallocate"(%713) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1062)
        %716 = "ttnn.typecast"(%715) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1929)
        "ttnn.deallocate"(%715) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1929)
        %717 = "ttnn.where"(%716, %59, %714) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1062)
        "ttnn.deallocate"(%716) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1062)
        "ttnn.deallocate"(%714) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1062)
        %718 = "ttnn.typecast"(%701) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1063)
        "ttnn.deallocate"(%701) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1063)
        %719 = "ttnn.matmul"(%717, %718) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1064)
        "ttnn.deallocate"(%718) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1064)
        "ttnn.deallocate"(%717) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1064)
        %720 = "ttnn.typecast"(%719) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1065)
        "ttnn.deallocate"(%719) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1065)
        %721 = "ttnn.permute"(%720) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1930)
        "ttnn.deallocate"(%720) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1930)
        %722 = "ttnn.reshape"(%721) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1066)
        "ttnn.deallocate"(%721) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1066)
        %723 = "ttnn.matmul"(%722, %arg248) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1931)
        "ttnn.deallocate"(%722) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1931)
        "ttnn.deallocate"(%arg248) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1931)
        %724 = "ttnn.add"(%723, %160) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1932)
        "ttnn.deallocate"(%723) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1932)
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1932)
        %725 = "ttnn.add"(%688, %724) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1068)
        "ttnn.deallocate"(%724) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1068)
        "ttnn.deallocate"(%688) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1068)
        %726 = "ttnn.layer_norm"(%725, %arg246, %arg245) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1069)
        "ttnn.deallocate"(%arg246) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1069)
        "ttnn.deallocate"(%arg245) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1069)
        %727 = "ttnn.reshape"(%726) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1070)
        "ttnn.deallocate"(%726) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1070)
        %728 = "ttnn.matmul"(%727, %arg244) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1933)
        "ttnn.deallocate"(%727) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1933)
        "ttnn.deallocate"(%arg244) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1933)
        %729 = "ttnn.add"(%728, %149) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1934)
        "ttnn.deallocate"(%728) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1934)
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1934)
        %730 = "ttnn.gelu"(%729) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1935)
        "ttnn.deallocate"(%729) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1935)
        %731 = "ttnn.reshape"(%730) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1072)
        "ttnn.deallocate"(%730) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1072)
        %732 = "ttnn.matmul"(%731, %arg242) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1936)
        "ttnn.deallocate"(%731) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1936)
        "ttnn.deallocate"(%arg242) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1936)
        %733 = "ttnn.add"(%732, %140) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1937)
        "ttnn.deallocate"(%732) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1937)
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1937)
        %734 = "ttnn.add"(%725, %733) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1074)
        "ttnn.deallocate"(%733) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1074)
        "ttnn.deallocate"(%725) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1074)
        %735 = "ttnn.layer_norm"(%734, %arg240, %arg239) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1075)
        "ttnn.deallocate"(%arg240) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1075)
        "ttnn.deallocate"(%arg239) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1075)
        %736 = "ttnn.reshape"(%735) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1076)
        "ttnn.deallocate"(%735) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1076)
        %737 = "ttnn.matmul"(%736, %169) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1938)
        "ttnn.deallocate"(%736) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1938)
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1938)
        %738 = "ttnn.add"(%737, %72) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1939)
        "ttnn.deallocate"(%737) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1939)
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1939)
        %739 = "ttnn.slice_static"(%738) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1940)
        %740 = "ttnn.slice_static"(%738) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1941)
        %741 = "ttnn.slice_static"(%738) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1942)
        "ttnn.deallocate"(%738) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1942)
        %742 = "ttnn.reshape"(%739) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1943)
        "ttnn.deallocate"(%739) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1943)
        %743 = "ttnn.reshape"(%740) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1944)
        "ttnn.deallocate"(%740) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1944)
        %744 = "ttnn.reshape"(%741) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1945)
        "ttnn.deallocate"(%741) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1945)
        %745 = "ttnn.permute"(%742) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1946)
        "ttnn.deallocate"(%742) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1946)
        %746 = "ttnn.permute"(%743) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1947)
        "ttnn.deallocate"(%743) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1947)
        %747 = "ttnn.permute"(%744) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1948)
        "ttnn.deallocate"(%744) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1948)
        %748 = "ttnn.typecast"(%745) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1078)
        "ttnn.deallocate"(%745) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1078)
        %749 = "ttnn.multiply"(%748, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1079)
        "ttnn.deallocate"(%748) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1079)
        %750 = "ttnn.typecast"(%746) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1080)
        "ttnn.deallocate"(%746) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1080)
        %751 = "ttnn.permute"(%750) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1080)
        "ttnn.deallocate"(%750) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1080)
        %752 = "ttnn.multiply"(%751, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1081)
        "ttnn.deallocate"(%751) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1081)
        %753 = "ttnn.matmul"(%749, %752) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1082)
        "ttnn.deallocate"(%752) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1082)
        "ttnn.deallocate"(%749) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1082)
        %754 = "ttnn.eq"(%753, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1083)
        %755 = "ttnn.logical_not"(%754) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1084)
        "ttnn.deallocate"(%754) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1084)
        %756 = "ttnn.sum"(%755) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1085)
        "ttnn.deallocate"(%755) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1085)
        %757 = "ttnn.ne"(%756, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1085)
        "ttnn.deallocate"(%756) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1085)
        %758 = "ttnn.logical_not"(%757) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1086)
        "ttnn.deallocate"(%757) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1086)
        %759 = "ttnn.reshape"(%758) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1086)
        "ttnn.deallocate"(%758) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1086)
        %760 = "ttnn.softmax"(%753) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1087)
        "ttnn.deallocate"(%753) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1087)
        %761 = "ttnn.repeat"(%759) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1088)
        "ttnn.deallocate"(%759) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1088)
        %762 = "ttnn.typecast"(%761) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1949)
        "ttnn.deallocate"(%761) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1949)
        %763 = "ttnn.where"(%762, %59, %760) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1088)
        "ttnn.deallocate"(%762) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1088)
        "ttnn.deallocate"(%760) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1088)
        %764 = "ttnn.typecast"(%747) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1089)
        "ttnn.deallocate"(%747) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1089)
        %765 = "ttnn.matmul"(%763, %764) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1090)
        "ttnn.deallocate"(%764) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1090)
        "ttnn.deallocate"(%763) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1090)
        %766 = "ttnn.typecast"(%765) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1091)
        "ttnn.deallocate"(%765) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1091)
        %767 = "ttnn.permute"(%766) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1950)
        "ttnn.deallocate"(%766) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1950)
        %768 = "ttnn.reshape"(%767) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1092)
        "ttnn.deallocate"(%767) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1092)
        %769 = "ttnn.matmul"(%768, %arg236) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1951)
        "ttnn.deallocate"(%768) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1951)
        "ttnn.deallocate"(%arg236) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1951)
        %770 = "ttnn.add"(%769, %55) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1952)
        "ttnn.deallocate"(%769) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1952)
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1952)
        %771 = "ttnn.add"(%734, %770) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1094)
        "ttnn.deallocate"(%770) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1094)
        "ttnn.deallocate"(%734) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1094)
        %772 = "ttnn.layer_norm"(%771, %arg234, %arg233) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1095)
        "ttnn.deallocate"(%arg234) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1095)
        "ttnn.deallocate"(%arg233) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1095)
        %773 = "ttnn.reshape"(%772) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1096)
        "ttnn.deallocate"(%772) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1096)
        %774 = "ttnn.matmul"(%773, %arg232) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1953)
        "ttnn.deallocate"(%773) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1953)
        "ttnn.deallocate"(%arg232) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1953)
        %775 = "ttnn.add"(%774, %2) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1954)
        "ttnn.deallocate"(%774) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1954)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1954)
        %776 = "ttnn.gelu"(%775) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1955)
        "ttnn.deallocate"(%775) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1955)
        %777 = "ttnn.reshape"(%776) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1098)
        "ttnn.deallocate"(%776) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1098)
        %778 = "ttnn.matmul"(%777, %arg230) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1956)
        "ttnn.deallocate"(%777) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1956)
        "ttnn.deallocate"(%arg230) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1956)
        %779 = "ttnn.add"(%778, %111) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1957)
        "ttnn.deallocate"(%778) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1957)
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1957)
        %780 = "ttnn.add"(%771, %779) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1100)
        "ttnn.deallocate"(%779) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1100)
        "ttnn.deallocate"(%771) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1100)
        %781 = "ttnn.layer_norm"(%780, %arg228, %arg227) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1101)
        "ttnn.deallocate"(%arg228) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1101)
        "ttnn.deallocate"(%arg227) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1101)
        %782 = "ttnn.reshape"(%781) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1102)
        "ttnn.deallocate"(%781) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1102)
        %783 = "ttnn.matmul"(%782, %100) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1958)
        "ttnn.deallocate"(%782) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1958)
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1958)
        %784 = "ttnn.add"(%783, %135) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1959)
        "ttnn.deallocate"(%783) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1959)
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1959)
        %785 = "ttnn.slice_static"(%784) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1960)
        %786 = "ttnn.slice_static"(%784) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1961)
        %787 = "ttnn.slice_static"(%784) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1962)
        "ttnn.deallocate"(%784) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1962)
        %788 = "ttnn.reshape"(%785) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1963)
        "ttnn.deallocate"(%785) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1963)
        %789 = "ttnn.reshape"(%786) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1964)
        "ttnn.deallocate"(%786) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1964)
        %790 = "ttnn.reshape"(%787) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1965)
        "ttnn.deallocate"(%787) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1965)
        %791 = "ttnn.permute"(%788) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1966)
        "ttnn.deallocate"(%788) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1966)
        %792 = "ttnn.permute"(%789) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1967)
        "ttnn.deallocate"(%789) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1967)
        %793 = "ttnn.permute"(%790) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1968)
        "ttnn.deallocate"(%790) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1968)
        %794 = "ttnn.typecast"(%791) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1104)
        "ttnn.deallocate"(%791) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1104)
        %795 = "ttnn.multiply"(%794, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1105)
        "ttnn.deallocate"(%794) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1105)
        %796 = "ttnn.typecast"(%792) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1106)
        "ttnn.deallocate"(%792) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1106)
        %797 = "ttnn.permute"(%796) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1106)
        "ttnn.deallocate"(%796) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1106)
        %798 = "ttnn.multiply"(%797, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1107)
        "ttnn.deallocate"(%797) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1107)
        %799 = "ttnn.matmul"(%795, %798) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1108)
        "ttnn.deallocate"(%798) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1108)
        "ttnn.deallocate"(%795) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1108)
        %800 = "ttnn.eq"(%799, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1109)
        %801 = "ttnn.logical_not"(%800) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1110)
        "ttnn.deallocate"(%800) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1110)
        %802 = "ttnn.sum"(%801) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1111)
        "ttnn.deallocate"(%801) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1111)
        %803 = "ttnn.ne"(%802, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1111)
        "ttnn.deallocate"(%802) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1111)
        %804 = "ttnn.logical_not"(%803) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1112)
        "ttnn.deallocate"(%803) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1112)
        %805 = "ttnn.reshape"(%804) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1112)
        "ttnn.deallocate"(%804) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1112)
        %806 = "ttnn.softmax"(%799) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1113)
        "ttnn.deallocate"(%799) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1113)
        %807 = "ttnn.repeat"(%805) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1114)
        "ttnn.deallocate"(%805) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1114)
        %808 = "ttnn.typecast"(%807) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1969)
        "ttnn.deallocate"(%807) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1969)
        %809 = "ttnn.where"(%808, %59, %806) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1114)
        "ttnn.deallocate"(%808) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1114)
        "ttnn.deallocate"(%806) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1114)
        %810 = "ttnn.typecast"(%793) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1115)
        "ttnn.deallocate"(%793) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1115)
        %811 = "ttnn.matmul"(%809, %810) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1116)
        "ttnn.deallocate"(%810) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1116)
        "ttnn.deallocate"(%809) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1116)
        %812 = "ttnn.typecast"(%811) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1117)
        "ttnn.deallocate"(%811) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1117)
        %813 = "ttnn.permute"(%812) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1970)
        "ttnn.deallocate"(%812) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1970)
        %814 = "ttnn.reshape"(%813) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1118)
        "ttnn.deallocate"(%813) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1118)
        %815 = "ttnn.matmul"(%814, %arg224) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1971)
        "ttnn.deallocate"(%814) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1971)
        "ttnn.deallocate"(%arg224) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1971)
        %816 = "ttnn.add"(%815, %115) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1972)
        "ttnn.deallocate"(%815) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1972)
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1972)
        %817 = "ttnn.add"(%780, %816) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1120)
        "ttnn.deallocate"(%816) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1120)
        "ttnn.deallocate"(%780) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1120)
        %818 = "ttnn.layer_norm"(%817, %arg222, %arg221) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1121)
        "ttnn.deallocate"(%arg222) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1121)
        "ttnn.deallocate"(%arg221) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1121)
        %819 = "ttnn.reshape"(%818) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1122)
        "ttnn.deallocate"(%818) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1122)
        %820 = "ttnn.matmul"(%819, %arg220) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1973)
        "ttnn.deallocate"(%819) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1973)
        "ttnn.deallocate"(%arg220) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1973)
        %821 = "ttnn.add"(%820, %71) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1974)
        "ttnn.deallocate"(%820) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1974)
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1974)
        %822 = "ttnn.gelu"(%821) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1975)
        "ttnn.deallocate"(%821) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1975)
        %823 = "ttnn.reshape"(%822) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1124)
        "ttnn.deallocate"(%822) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1124)
        %824 = "ttnn.matmul"(%823, %arg218) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1976)
        "ttnn.deallocate"(%823) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1976)
        "ttnn.deallocate"(%arg218) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1976)
        %825 = "ttnn.add"(%824, %29) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1977)
        "ttnn.deallocate"(%824) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1977)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1977)
        %826 = "ttnn.add"(%817, %825) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1126)
        "ttnn.deallocate"(%825) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1126)
        "ttnn.deallocate"(%817) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1126)
        %827 = "ttnn.layer_norm"(%826, %arg216, %arg215) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1127)
        "ttnn.deallocate"(%arg216) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1127)
        "ttnn.deallocate"(%arg215) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1127)
        %828 = "ttnn.reshape"(%827) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1128)
        "ttnn.deallocate"(%827) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1128)
        %829 = "ttnn.matmul"(%828, %150) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1978)
        "ttnn.deallocate"(%828) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1978)
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1978)
        %830 = "ttnn.add"(%829, %13) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1979)
        "ttnn.deallocate"(%829) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1979)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1979)
        %831 = "ttnn.slice_static"(%830) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1980)
        %832 = "ttnn.slice_static"(%830) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1981)
        %833 = "ttnn.slice_static"(%830) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1982)
        "ttnn.deallocate"(%830) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1982)
        %834 = "ttnn.reshape"(%831) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1983)
        "ttnn.deallocate"(%831) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1983)
        %835 = "ttnn.reshape"(%832) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1984)
        "ttnn.deallocate"(%832) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1984)
        %836 = "ttnn.reshape"(%833) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1985)
        "ttnn.deallocate"(%833) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1985)
        %837 = "ttnn.permute"(%834) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1986)
        "ttnn.deallocate"(%834) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1986)
        %838 = "ttnn.permute"(%835) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1987)
        "ttnn.deallocate"(%835) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1987)
        %839 = "ttnn.permute"(%836) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1988)
        "ttnn.deallocate"(%836) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1988)
        %840 = "ttnn.typecast"(%837) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1130)
        "ttnn.deallocate"(%837) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1130)
        %841 = "ttnn.multiply"(%840, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1131)
        "ttnn.deallocate"(%840) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1131)
        %842 = "ttnn.typecast"(%838) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1132)
        "ttnn.deallocate"(%838) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1132)
        %843 = "ttnn.permute"(%842) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1132)
        "ttnn.deallocate"(%842) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1132)
        %844 = "ttnn.multiply"(%843, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1133)
        "ttnn.deallocate"(%843) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1133)
        %845 = "ttnn.matmul"(%841, %844) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1134)
        "ttnn.deallocate"(%844) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1134)
        "ttnn.deallocate"(%841) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1134)
        %846 = "ttnn.eq"(%845, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1135)
        %847 = "ttnn.logical_not"(%846) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1136)
        "ttnn.deallocate"(%846) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1136)
        %848 = "ttnn.sum"(%847) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1137)
        "ttnn.deallocate"(%847) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1137)
        %849 = "ttnn.ne"(%848, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1137)
        "ttnn.deallocate"(%848) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1137)
        %850 = "ttnn.logical_not"(%849) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1138)
        "ttnn.deallocate"(%849) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1138)
        %851 = "ttnn.reshape"(%850) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1138)
        "ttnn.deallocate"(%850) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1138)
        %852 = "ttnn.softmax"(%845) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1139)
        "ttnn.deallocate"(%845) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1139)
        %853 = "ttnn.repeat"(%851) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1140)
        "ttnn.deallocate"(%851) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1140)
        %854 = "ttnn.typecast"(%853) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1989)
        "ttnn.deallocate"(%853) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1989)
        %855 = "ttnn.where"(%854, %59, %852) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1140)
        "ttnn.deallocate"(%854) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1140)
        "ttnn.deallocate"(%852) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1140)
        %856 = "ttnn.typecast"(%839) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1141)
        "ttnn.deallocate"(%839) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1141)
        %857 = "ttnn.matmul"(%855, %856) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1142)
        "ttnn.deallocate"(%856) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1142)
        "ttnn.deallocate"(%855) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1142)
        %858 = "ttnn.typecast"(%857) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1143)
        "ttnn.deallocate"(%857) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1143)
        %859 = "ttnn.permute"(%858) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc1990)
        "ttnn.deallocate"(%858) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1990)
        %860 = "ttnn.reshape"(%859) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1144)
        "ttnn.deallocate"(%859) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1144)
        %861 = "ttnn.matmul"(%860, %arg212) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1991)
        "ttnn.deallocate"(%860) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1991)
        "ttnn.deallocate"(%arg212) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1991)
        %862 = "ttnn.add"(%861, %20) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1992)
        "ttnn.deallocate"(%861) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1992)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1992)
        %863 = "ttnn.add"(%826, %862) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1146)
        "ttnn.deallocate"(%862) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1146)
        "ttnn.deallocate"(%826) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1146)
        %864 = "ttnn.layer_norm"(%863, %arg210, %arg209) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1147)
        "ttnn.deallocate"(%arg210) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1147)
        "ttnn.deallocate"(%arg209) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1147)
        %865 = "ttnn.reshape"(%864) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1148)
        "ttnn.deallocate"(%864) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1148)
        %866 = "ttnn.matmul"(%865, %arg208) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1993)
        "ttnn.deallocate"(%865) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1993)
        "ttnn.deallocate"(%arg208) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1993)
        %867 = "ttnn.add"(%866, %40) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1994)
        "ttnn.deallocate"(%866) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1994)
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1994)
        %868 = "ttnn.gelu"(%867) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc1995)
        "ttnn.deallocate"(%867) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1995)
        %869 = "ttnn.reshape"(%868) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1150)
        "ttnn.deallocate"(%868) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1150)
        %870 = "ttnn.matmul"(%869, %arg206) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1996)
        "ttnn.deallocate"(%869) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc1996)
        "ttnn.deallocate"(%arg206) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1996)
        %871 = "ttnn.add"(%870, %41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1997)
        "ttnn.deallocate"(%870) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1997)
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1997)
        %872 = "ttnn.add"(%863, %871) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1152)
        "ttnn.deallocate"(%871) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1152)
        "ttnn.deallocate"(%863) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1152)
        %873 = "ttnn.layer_norm"(%872, %arg204, %arg203) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1153)
        "ttnn.deallocate"(%arg204) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1153)
        "ttnn.deallocate"(%arg203) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1153)
        %874 = "ttnn.reshape"(%873) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1154)
        "ttnn.deallocate"(%873) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1154)
        %875 = "ttnn.matmul"(%874, %11) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc1998)
        "ttnn.deallocate"(%874) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1998)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc1998)
        %876 = "ttnn.add"(%875, %43) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc1999)
        "ttnn.deallocate"(%875) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc1999)
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc1999)
        %877 = "ttnn.slice_static"(%876) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2000)
        %878 = "ttnn.slice_static"(%876) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2001)
        %879 = "ttnn.slice_static"(%876) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2002)
        "ttnn.deallocate"(%876) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2002)
        %880 = "ttnn.reshape"(%877) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2003)
        "ttnn.deallocate"(%877) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2003)
        %881 = "ttnn.reshape"(%878) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2004)
        "ttnn.deallocate"(%878) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2004)
        %882 = "ttnn.reshape"(%879) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2005)
        "ttnn.deallocate"(%879) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2005)
        %883 = "ttnn.permute"(%880) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2006)
        "ttnn.deallocate"(%880) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2006)
        %884 = "ttnn.permute"(%881) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2007)
        "ttnn.deallocate"(%881) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2007)
        %885 = "ttnn.permute"(%882) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2008)
        "ttnn.deallocate"(%882) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2008)
        %886 = "ttnn.typecast"(%883) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1156)
        "ttnn.deallocate"(%883) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1156)
        %887 = "ttnn.multiply"(%886, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1157)
        "ttnn.deallocate"(%886) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1157)
        %888 = "ttnn.typecast"(%884) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1158)
        "ttnn.deallocate"(%884) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1158)
        %889 = "ttnn.permute"(%888) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1158)
        "ttnn.deallocate"(%888) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1158)
        %890 = "ttnn.multiply"(%889, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1159)
        "ttnn.deallocate"(%889) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1159)
        %891 = "ttnn.matmul"(%887, %890) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1160)
        "ttnn.deallocate"(%890) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1160)
        "ttnn.deallocate"(%887) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1160)
        %892 = "ttnn.eq"(%891, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1161)
        %893 = "ttnn.logical_not"(%892) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1162)
        "ttnn.deallocate"(%892) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1162)
        %894 = "ttnn.sum"(%893) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1163)
        "ttnn.deallocate"(%893) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1163)
        %895 = "ttnn.ne"(%894, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1163)
        "ttnn.deallocate"(%894) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1163)
        %896 = "ttnn.logical_not"(%895) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1164)
        "ttnn.deallocate"(%895) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1164)
        %897 = "ttnn.reshape"(%896) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1164)
        "ttnn.deallocate"(%896) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1164)
        %898 = "ttnn.softmax"(%891) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1165)
        "ttnn.deallocate"(%891) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1165)
        %899 = "ttnn.repeat"(%897) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1166)
        "ttnn.deallocate"(%897) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1166)
        %900 = "ttnn.typecast"(%899) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2009)
        "ttnn.deallocate"(%899) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2009)
        %901 = "ttnn.where"(%900, %59, %898) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1166)
        "ttnn.deallocate"(%900) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1166)
        "ttnn.deallocate"(%898) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1166)
        %902 = "ttnn.typecast"(%885) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1167)
        "ttnn.deallocate"(%885) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1167)
        %903 = "ttnn.matmul"(%901, %902) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1168)
        "ttnn.deallocate"(%902) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1168)
        "ttnn.deallocate"(%901) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1168)
        %904 = "ttnn.typecast"(%903) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1169)
        "ttnn.deallocate"(%903) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1169)
        %905 = "ttnn.permute"(%904) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2010)
        "ttnn.deallocate"(%904) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2010)
        %906 = "ttnn.reshape"(%905) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1170)
        "ttnn.deallocate"(%905) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1170)
        %907 = "ttnn.matmul"(%906, %arg200) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2011)
        "ttnn.deallocate"(%906) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2011)
        "ttnn.deallocate"(%arg200) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2011)
        %908 = "ttnn.add"(%907, %138) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2012)
        "ttnn.deallocate"(%907) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2012)
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2012)
        %909 = "ttnn.add"(%872, %908) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1172)
        "ttnn.deallocate"(%908) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1172)
        "ttnn.deallocate"(%872) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1172)
        %910 = "ttnn.layer_norm"(%909, %arg198, %arg197) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1173)
        "ttnn.deallocate"(%arg198) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1173)
        "ttnn.deallocate"(%arg197) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1173)
        %911 = "ttnn.reshape"(%910) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1174)
        "ttnn.deallocate"(%910) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1174)
        %912 = "ttnn.matmul"(%911, %arg196) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2013)
        "ttnn.deallocate"(%911) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2013)
        "ttnn.deallocate"(%arg196) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2013)
        %913 = "ttnn.add"(%912, %132) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2014)
        "ttnn.deallocate"(%912) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2014)
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2014)
        %914 = "ttnn.gelu"(%913) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2015)
        "ttnn.deallocate"(%913) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2015)
        %915 = "ttnn.reshape"(%914) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1176)
        "ttnn.deallocate"(%914) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1176)
        %916 = "ttnn.matmul"(%915, %arg194) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2016)
        "ttnn.deallocate"(%915) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2016)
        "ttnn.deallocate"(%arg194) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2016)
        %917 = "ttnn.add"(%916, %97) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2017)
        "ttnn.deallocate"(%916) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2017)
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2017)
        %918 = "ttnn.add"(%909, %917) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1178)
        "ttnn.deallocate"(%917) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1178)
        "ttnn.deallocate"(%909) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1178)
        %919 = "ttnn.layer_norm"(%918, %arg192, %arg191) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1179)
        "ttnn.deallocate"(%arg192) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1179)
        "ttnn.deallocate"(%arg191) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1179)
        %920 = "ttnn.reshape"(%919) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1180)
        "ttnn.deallocate"(%919) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1180)
        %921 = "ttnn.matmul"(%920, %112) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2018)
        "ttnn.deallocate"(%920) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2018)
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2018)
        %922 = "ttnn.add"(%921, %23) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2019)
        "ttnn.deallocate"(%921) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2019)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2019)
        %923 = "ttnn.slice_static"(%922) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2020)
        %924 = "ttnn.slice_static"(%922) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2021)
        %925 = "ttnn.slice_static"(%922) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2022)
        "ttnn.deallocate"(%922) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2022)
        %926 = "ttnn.reshape"(%923) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2023)
        "ttnn.deallocate"(%923) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2023)
        %927 = "ttnn.reshape"(%924) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2024)
        "ttnn.deallocate"(%924) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2024)
        %928 = "ttnn.reshape"(%925) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2025)
        "ttnn.deallocate"(%925) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2025)
        %929 = "ttnn.permute"(%926) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2026)
        "ttnn.deallocate"(%926) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2026)
        %930 = "ttnn.permute"(%927) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2027)
        "ttnn.deallocate"(%927) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2027)
        %931 = "ttnn.permute"(%928) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2028)
        "ttnn.deallocate"(%928) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2028)
        %932 = "ttnn.typecast"(%929) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1182)
        "ttnn.deallocate"(%929) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1182)
        %933 = "ttnn.multiply"(%932, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1183)
        "ttnn.deallocate"(%932) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1183)
        %934 = "ttnn.typecast"(%930) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1184)
        "ttnn.deallocate"(%930) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1184)
        %935 = "ttnn.permute"(%934) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1184)
        "ttnn.deallocate"(%934) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1184)
        %936 = "ttnn.multiply"(%935, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1185)
        "ttnn.deallocate"(%935) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1185)
        %937 = "ttnn.matmul"(%933, %936) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1186)
        "ttnn.deallocate"(%936) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1186)
        "ttnn.deallocate"(%933) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1186)
        %938 = "ttnn.eq"(%937, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1187)
        %939 = "ttnn.logical_not"(%938) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1188)
        "ttnn.deallocate"(%938) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1188)
        %940 = "ttnn.sum"(%939) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1189)
        "ttnn.deallocate"(%939) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1189)
        %941 = "ttnn.ne"(%940, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1189)
        "ttnn.deallocate"(%940) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1189)
        %942 = "ttnn.logical_not"(%941) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1190)
        "ttnn.deallocate"(%941) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1190)
        %943 = "ttnn.reshape"(%942) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1190)
        "ttnn.deallocate"(%942) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1190)
        %944 = "ttnn.softmax"(%937) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1191)
        "ttnn.deallocate"(%937) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1191)
        %945 = "ttnn.repeat"(%943) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1192)
        "ttnn.deallocate"(%943) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1192)
        %946 = "ttnn.typecast"(%945) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2029)
        "ttnn.deallocate"(%945) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2029)
        %947 = "ttnn.where"(%946, %59, %944) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1192)
        "ttnn.deallocate"(%946) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1192)
        "ttnn.deallocate"(%944) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1192)
        %948 = "ttnn.typecast"(%931) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1193)
        "ttnn.deallocate"(%931) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1193)
        %949 = "ttnn.matmul"(%947, %948) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1194)
        "ttnn.deallocate"(%948) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1194)
        "ttnn.deallocate"(%947) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1194)
        %950 = "ttnn.typecast"(%949) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1195)
        "ttnn.deallocate"(%949) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1195)
        %951 = "ttnn.permute"(%950) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2030)
        "ttnn.deallocate"(%950) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2030)
        %952 = "ttnn.reshape"(%951) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1196)
        "ttnn.deallocate"(%951) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1196)
        %953 = "ttnn.matmul"(%952, %arg188) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2031)
        "ttnn.deallocate"(%952) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2031)
        "ttnn.deallocate"(%arg188) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2031)
        %954 = "ttnn.add"(%953, %155) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2032)
        "ttnn.deallocate"(%953) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2032)
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2032)
        %955 = "ttnn.add"(%918, %954) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1198)
        "ttnn.deallocate"(%954) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1198)
        "ttnn.deallocate"(%918) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1198)
        %956 = "ttnn.layer_norm"(%955, %arg186, %arg185) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1199)
        "ttnn.deallocate"(%arg186) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1199)
        "ttnn.deallocate"(%arg185) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1199)
        %957 = "ttnn.reshape"(%956) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1200)
        "ttnn.deallocate"(%956) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1200)
        %958 = "ttnn.matmul"(%957, %arg184) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2033)
        "ttnn.deallocate"(%957) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2033)
        "ttnn.deallocate"(%arg184) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2033)
        %959 = "ttnn.add"(%958, %56) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2034)
        "ttnn.deallocate"(%958) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2034)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2034)
        %960 = "ttnn.gelu"(%959) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2035)
        "ttnn.deallocate"(%959) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2035)
        %961 = "ttnn.reshape"(%960) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1202)
        "ttnn.deallocate"(%960) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1202)
        %962 = "ttnn.matmul"(%961, %arg182) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2036)
        "ttnn.deallocate"(%961) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2036)
        "ttnn.deallocate"(%arg182) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2036)
        %963 = "ttnn.add"(%962, %142) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2037)
        "ttnn.deallocate"(%962) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2037)
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2037)
        %964 = "ttnn.add"(%955, %963) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1204)
        "ttnn.deallocate"(%963) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1204)
        "ttnn.deallocate"(%955) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1204)
        %965 = "ttnn.layer_norm"(%964, %arg180, %arg179) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1205)
        "ttnn.deallocate"(%arg180) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1205)
        "ttnn.deallocate"(%arg179) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1205)
        %966 = "ttnn.reshape"(%965) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1206)
        "ttnn.deallocate"(%965) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1206)
        %967 = "ttnn.matmul"(%966, %106) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2038)
        "ttnn.deallocate"(%966) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2038)
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2038)
        %968 = "ttnn.add"(%967, %129) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2039)
        "ttnn.deallocate"(%967) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2039)
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2039)
        %969 = "ttnn.slice_static"(%968) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2040)
        %970 = "ttnn.slice_static"(%968) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2041)
        %971 = "ttnn.slice_static"(%968) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2042)
        "ttnn.deallocate"(%968) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2042)
        %972 = "ttnn.reshape"(%969) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2043)
        "ttnn.deallocate"(%969) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2043)
        %973 = "ttnn.reshape"(%970) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2044)
        "ttnn.deallocate"(%970) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2044)
        %974 = "ttnn.reshape"(%971) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2045)
        "ttnn.deallocate"(%971) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2045)
        %975 = "ttnn.permute"(%972) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2046)
        "ttnn.deallocate"(%972) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2046)
        %976 = "ttnn.permute"(%973) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2047)
        "ttnn.deallocate"(%973) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2047)
        %977 = "ttnn.permute"(%974) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2048)
        "ttnn.deallocate"(%974) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2048)
        %978 = "ttnn.typecast"(%975) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1208)
        "ttnn.deallocate"(%975) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1208)
        %979 = "ttnn.multiply"(%978, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1209)
        "ttnn.deallocate"(%978) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1209)
        %980 = "ttnn.typecast"(%976) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1210)
        "ttnn.deallocate"(%976) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1210)
        %981 = "ttnn.permute"(%980) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1210)
        "ttnn.deallocate"(%980) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1210)
        %982 = "ttnn.multiply"(%981, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1211)
        "ttnn.deallocate"(%981) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1211)
        %983 = "ttnn.matmul"(%979, %982) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1212)
        "ttnn.deallocate"(%982) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1212)
        "ttnn.deallocate"(%979) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1212)
        %984 = "ttnn.eq"(%983, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1213)
        %985 = "ttnn.logical_not"(%984) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1214)
        "ttnn.deallocate"(%984) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1214)
        %986 = "ttnn.sum"(%985) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1215)
        "ttnn.deallocate"(%985) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1215)
        %987 = "ttnn.ne"(%986, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1215)
        "ttnn.deallocate"(%986) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1215)
        %988 = "ttnn.logical_not"(%987) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1216)
        "ttnn.deallocate"(%987) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1216)
        %989 = "ttnn.reshape"(%988) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1216)
        "ttnn.deallocate"(%988) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1216)
        %990 = "ttnn.softmax"(%983) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1217)
        "ttnn.deallocate"(%983) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1217)
        %991 = "ttnn.repeat"(%989) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1218)
        "ttnn.deallocate"(%989) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1218)
        %992 = "ttnn.typecast"(%991) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2049)
        "ttnn.deallocate"(%991) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2049)
        %993 = "ttnn.where"(%992, %59, %990) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1218)
        "ttnn.deallocate"(%992) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1218)
        "ttnn.deallocate"(%990) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1218)
        %994 = "ttnn.typecast"(%977) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1219)
        "ttnn.deallocate"(%977) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1219)
        %995 = "ttnn.matmul"(%993, %994) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1220)
        "ttnn.deallocate"(%994) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1220)
        "ttnn.deallocate"(%993) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1220)
        %996 = "ttnn.typecast"(%995) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1221)
        "ttnn.deallocate"(%995) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1221)
        %997 = "ttnn.permute"(%996) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2050)
        "ttnn.deallocate"(%996) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2050)
        %998 = "ttnn.reshape"(%997) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1222)
        "ttnn.deallocate"(%997) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1222)
        %999 = "ttnn.matmul"(%998, %arg176) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2051)
        "ttnn.deallocate"(%998) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2051)
        "ttnn.deallocate"(%arg176) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2051)
        %1000 = "ttnn.add"(%999, %110) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2052)
        "ttnn.deallocate"(%999) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2052)
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2052)
        %1001 = "ttnn.add"(%964, %1000) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1224)
        "ttnn.deallocate"(%1000) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1224)
        "ttnn.deallocate"(%964) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1224)
        %1002 = "ttnn.layer_norm"(%1001, %arg174, %arg173) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1225)
        "ttnn.deallocate"(%arg174) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1225)
        "ttnn.deallocate"(%arg173) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1225)
        %1003 = "ttnn.reshape"(%1002) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1226)
        "ttnn.deallocate"(%1002) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1226)
        %1004 = "ttnn.matmul"(%1003, %arg172) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2053)
        "ttnn.deallocate"(%1003) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2053)
        "ttnn.deallocate"(%arg172) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2053)
        %1005 = "ttnn.add"(%1004, %47) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2054)
        "ttnn.deallocate"(%1004) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2054)
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2054)
        %1006 = "ttnn.gelu"(%1005) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2055)
        "ttnn.deallocate"(%1005) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2055)
        %1007 = "ttnn.reshape"(%1006) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1228)
        "ttnn.deallocate"(%1006) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1228)
        %1008 = "ttnn.matmul"(%1007, %arg170) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2056)
        "ttnn.deallocate"(%1007) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2056)
        "ttnn.deallocate"(%arg170) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2056)
        %1009 = "ttnn.add"(%1008, %121) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2057)
        "ttnn.deallocate"(%1008) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2057)
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2057)
        %1010 = "ttnn.add"(%1001, %1009) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1230)
        "ttnn.deallocate"(%1009) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1230)
        "ttnn.deallocate"(%1001) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1230)
        %1011 = "ttnn.layer_norm"(%1010, %arg168, %arg167) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1231)
        "ttnn.deallocate"(%arg168) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1231)
        "ttnn.deallocate"(%arg167) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1231)
        %1012 = "ttnn.reshape"(%1011) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1232)
        "ttnn.deallocate"(%1011) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1232)
        %1013 = "ttnn.matmul"(%1012, %77) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2058)
        "ttnn.deallocate"(%1012) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2058)
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2058)
        %1014 = "ttnn.add"(%1013, %18) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2059)
        "ttnn.deallocate"(%1013) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2059)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2059)
        %1015 = "ttnn.slice_static"(%1014) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2060)
        %1016 = "ttnn.slice_static"(%1014) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2061)
        %1017 = "ttnn.slice_static"(%1014) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2062)
        "ttnn.deallocate"(%1014) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2062)
        %1018 = "ttnn.reshape"(%1015) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2063)
        "ttnn.deallocate"(%1015) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2063)
        %1019 = "ttnn.reshape"(%1016) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2064)
        "ttnn.deallocate"(%1016) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2064)
        %1020 = "ttnn.reshape"(%1017) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2065)
        "ttnn.deallocate"(%1017) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2065)
        %1021 = "ttnn.permute"(%1018) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2066)
        "ttnn.deallocate"(%1018) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2066)
        %1022 = "ttnn.permute"(%1019) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2067)
        "ttnn.deallocate"(%1019) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2067)
        %1023 = "ttnn.permute"(%1020) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2068)
        "ttnn.deallocate"(%1020) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2068)
        %1024 = "ttnn.typecast"(%1021) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1234)
        "ttnn.deallocate"(%1021) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1234)
        %1025 = "ttnn.multiply"(%1024, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1235)
        "ttnn.deallocate"(%1024) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1235)
        %1026 = "ttnn.typecast"(%1022) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1236)
        "ttnn.deallocate"(%1022) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1236)
        %1027 = "ttnn.permute"(%1026) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1236)
        "ttnn.deallocate"(%1026) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1236)
        %1028 = "ttnn.multiply"(%1027, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1237)
        "ttnn.deallocate"(%1027) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1237)
        %1029 = "ttnn.matmul"(%1025, %1028) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1238)
        "ttnn.deallocate"(%1028) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1238)
        "ttnn.deallocate"(%1025) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1238)
        %1030 = "ttnn.eq"(%1029, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1239)
        %1031 = "ttnn.logical_not"(%1030) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1240)
        "ttnn.deallocate"(%1030) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1240)
        %1032 = "ttnn.sum"(%1031) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1241)
        "ttnn.deallocate"(%1031) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1241)
        %1033 = "ttnn.ne"(%1032, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1241)
        "ttnn.deallocate"(%1032) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1241)
        %1034 = "ttnn.logical_not"(%1033) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1242)
        "ttnn.deallocate"(%1033) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1242)
        %1035 = "ttnn.reshape"(%1034) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1242)
        "ttnn.deallocate"(%1034) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1242)
        %1036 = "ttnn.softmax"(%1029) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1243)
        "ttnn.deallocate"(%1029) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1243)
        %1037 = "ttnn.repeat"(%1035) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1244)
        "ttnn.deallocate"(%1035) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1244)
        %1038 = "ttnn.typecast"(%1037) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2069)
        "ttnn.deallocate"(%1037) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2069)
        %1039 = "ttnn.where"(%1038, %59, %1036) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1244)
        "ttnn.deallocate"(%1038) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1244)
        "ttnn.deallocate"(%1036) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1244)
        %1040 = "ttnn.typecast"(%1023) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1245)
        "ttnn.deallocate"(%1023) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1245)
        %1041 = "ttnn.matmul"(%1039, %1040) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1246)
        "ttnn.deallocate"(%1040) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1246)
        "ttnn.deallocate"(%1039) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1246)
        %1042 = "ttnn.typecast"(%1041) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1247)
        "ttnn.deallocate"(%1041) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1247)
        %1043 = "ttnn.permute"(%1042) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2070)
        "ttnn.deallocate"(%1042) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2070)
        %1044 = "ttnn.reshape"(%1043) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1248)
        "ttnn.deallocate"(%1043) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1248)
        %1045 = "ttnn.matmul"(%1044, %arg164) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2071)
        "ttnn.deallocate"(%1044) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2071)
        "ttnn.deallocate"(%arg164) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2071)
        %1046 = "ttnn.add"(%1045, %34) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2072)
        "ttnn.deallocate"(%1045) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2072)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2072)
        %1047 = "ttnn.add"(%1010, %1046) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1250)
        "ttnn.deallocate"(%1046) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1250)
        "ttnn.deallocate"(%1010) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1250)
        %1048 = "ttnn.layer_norm"(%1047, %arg162, %arg161) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1251)
        "ttnn.deallocate"(%arg162) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1251)
        "ttnn.deallocate"(%arg161) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1251)
        %1049 = "ttnn.reshape"(%1048) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1252)
        "ttnn.deallocate"(%1048) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1252)
        %1050 = "ttnn.matmul"(%1049, %arg160) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2073)
        "ttnn.deallocate"(%1049) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2073)
        "ttnn.deallocate"(%arg160) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2073)
        %1051 = "ttnn.add"(%1050, %88) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2074)
        "ttnn.deallocate"(%1050) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2074)
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2074)
        %1052 = "ttnn.gelu"(%1051) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2075)
        "ttnn.deallocate"(%1051) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2075)
        %1053 = "ttnn.reshape"(%1052) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1254)
        "ttnn.deallocate"(%1052) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1254)
        %1054 = "ttnn.matmul"(%1053, %arg158) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2076)
        "ttnn.deallocate"(%1053) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2076)
        "ttnn.deallocate"(%arg158) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2076)
        %1055 = "ttnn.add"(%1054, %95) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2077)
        "ttnn.deallocate"(%1054) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2077)
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2077)
        %1056 = "ttnn.add"(%1047, %1055) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1256)
        "ttnn.deallocate"(%1055) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1256)
        "ttnn.deallocate"(%1047) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1256)
        %1057 = "ttnn.layer_norm"(%1056, %arg156, %arg155) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1257)
        "ttnn.deallocate"(%arg156) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1257)
        "ttnn.deallocate"(%arg155) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1257)
        %1058 = "ttnn.reshape"(%1057) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1258)
        "ttnn.deallocate"(%1057) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1258)
        %1059 = "ttnn.matmul"(%1058, %50) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2078)
        "ttnn.deallocate"(%1058) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2078)
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2078)
        %1060 = "ttnn.add"(%1059, %58) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2079)
        "ttnn.deallocate"(%1059) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2079)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2079)
        %1061 = "ttnn.slice_static"(%1060) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2080)
        %1062 = "ttnn.slice_static"(%1060) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2081)
        %1063 = "ttnn.slice_static"(%1060) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2082)
        "ttnn.deallocate"(%1060) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2082)
        %1064 = "ttnn.reshape"(%1061) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2083)
        "ttnn.deallocate"(%1061) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2083)
        %1065 = "ttnn.reshape"(%1062) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2084)
        "ttnn.deallocate"(%1062) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2084)
        %1066 = "ttnn.reshape"(%1063) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2085)
        "ttnn.deallocate"(%1063) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2085)
        %1067 = "ttnn.permute"(%1064) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2086)
        "ttnn.deallocate"(%1064) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2086)
        %1068 = "ttnn.permute"(%1065) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2087)
        "ttnn.deallocate"(%1065) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2087)
        %1069 = "ttnn.permute"(%1066) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2088)
        "ttnn.deallocate"(%1066) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2088)
        %1070 = "ttnn.typecast"(%1067) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1260)
        "ttnn.deallocate"(%1067) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1260)
        %1071 = "ttnn.multiply"(%1070, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1261)
        "ttnn.deallocate"(%1070) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1261)
        %1072 = "ttnn.typecast"(%1068) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1262)
        "ttnn.deallocate"(%1068) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1262)
        %1073 = "ttnn.permute"(%1072) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1262)
        "ttnn.deallocate"(%1072) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1262)
        %1074 = "ttnn.multiply"(%1073, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1263)
        "ttnn.deallocate"(%1073) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1263)
        %1075 = "ttnn.matmul"(%1071, %1074) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1264)
        "ttnn.deallocate"(%1074) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1264)
        "ttnn.deallocate"(%1071) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1264)
        %1076 = "ttnn.eq"(%1075, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1265)
        %1077 = "ttnn.logical_not"(%1076) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1266)
        "ttnn.deallocate"(%1076) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1266)
        %1078 = "ttnn.sum"(%1077) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1267)
        "ttnn.deallocate"(%1077) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1267)
        %1079 = "ttnn.ne"(%1078, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1267)
        "ttnn.deallocate"(%1078) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1267)
        %1080 = "ttnn.logical_not"(%1079) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1268)
        "ttnn.deallocate"(%1079) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1268)
        %1081 = "ttnn.reshape"(%1080) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1268)
        "ttnn.deallocate"(%1080) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1268)
        %1082 = "ttnn.softmax"(%1075) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1269)
        "ttnn.deallocate"(%1075) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1269)
        %1083 = "ttnn.repeat"(%1081) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1270)
        "ttnn.deallocate"(%1081) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1270)
        %1084 = "ttnn.typecast"(%1083) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2089)
        "ttnn.deallocate"(%1083) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2089)
        %1085 = "ttnn.where"(%1084, %59, %1082) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1270)
        "ttnn.deallocate"(%1084) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1270)
        "ttnn.deallocate"(%1082) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1270)
        %1086 = "ttnn.typecast"(%1069) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1271)
        "ttnn.deallocate"(%1069) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1271)
        %1087 = "ttnn.matmul"(%1085, %1086) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1272)
        "ttnn.deallocate"(%1086) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1272)
        "ttnn.deallocate"(%1085) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1272)
        %1088 = "ttnn.typecast"(%1087) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1273)
        "ttnn.deallocate"(%1087) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1273)
        %1089 = "ttnn.permute"(%1088) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2090)
        "ttnn.deallocate"(%1088) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2090)
        %1090 = "ttnn.reshape"(%1089) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1274)
        "ttnn.deallocate"(%1089) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1274)
        %1091 = "ttnn.matmul"(%1090, %arg152) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2091)
        "ttnn.deallocate"(%1090) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2091)
        "ttnn.deallocate"(%arg152) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2091)
        %1092 = "ttnn.add"(%1091, %37) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2092)
        "ttnn.deallocate"(%1091) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2092)
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2092)
        %1093 = "ttnn.add"(%1056, %1092) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1276)
        "ttnn.deallocate"(%1092) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1276)
        "ttnn.deallocate"(%1056) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1276)
        %1094 = "ttnn.layer_norm"(%1093, %arg150, %arg149) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1277)
        "ttnn.deallocate"(%arg150) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1277)
        "ttnn.deallocate"(%arg149) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1277)
        %1095 = "ttnn.reshape"(%1094) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1278)
        "ttnn.deallocate"(%1094) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1278)
        %1096 = "ttnn.matmul"(%1095, %arg148) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2093)
        "ttnn.deallocate"(%1095) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2093)
        "ttnn.deallocate"(%arg148) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2093)
        %1097 = "ttnn.add"(%1096, %168) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2094)
        "ttnn.deallocate"(%1096) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2094)
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2094)
        %1098 = "ttnn.gelu"(%1097) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2095)
        "ttnn.deallocate"(%1097) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2095)
        %1099 = "ttnn.reshape"(%1098) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1280)
        "ttnn.deallocate"(%1098) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1280)
        %1100 = "ttnn.matmul"(%1099, %arg146) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2096)
        "ttnn.deallocate"(%1099) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2096)
        "ttnn.deallocate"(%arg146) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2096)
        %1101 = "ttnn.add"(%1100, %119) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2097)
        "ttnn.deallocate"(%1100) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2097)
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2097)
        %1102 = "ttnn.add"(%1093, %1101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1282)
        "ttnn.deallocate"(%1101) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1282)
        "ttnn.deallocate"(%1093) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1282)
        %1103 = "ttnn.layer_norm"(%1102, %arg144, %arg143) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1283)
        "ttnn.deallocate"(%arg144) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1283)
        "ttnn.deallocate"(%arg143) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1283)
        %1104 = "ttnn.reshape"(%1103) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1284)
        "ttnn.deallocate"(%1103) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1284)
        %1105 = "ttnn.matmul"(%1104, %108) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2098)
        "ttnn.deallocate"(%1104) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2098)
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2098)
        %1106 = "ttnn.add"(%1105, %163) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2099)
        "ttnn.deallocate"(%1105) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2099)
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2099)
        %1107 = "ttnn.slice_static"(%1106) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2100)
        %1108 = "ttnn.slice_static"(%1106) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2101)
        %1109 = "ttnn.slice_static"(%1106) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2102)
        "ttnn.deallocate"(%1106) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2102)
        %1110 = "ttnn.reshape"(%1107) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2103)
        "ttnn.deallocate"(%1107) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2103)
        %1111 = "ttnn.reshape"(%1108) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2104)
        "ttnn.deallocate"(%1108) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2104)
        %1112 = "ttnn.reshape"(%1109) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2105)
        "ttnn.deallocate"(%1109) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2105)
        %1113 = "ttnn.permute"(%1110) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2106)
        "ttnn.deallocate"(%1110) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2106)
        %1114 = "ttnn.permute"(%1111) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2107)
        "ttnn.deallocate"(%1111) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2107)
        %1115 = "ttnn.permute"(%1112) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2108)
        "ttnn.deallocate"(%1112) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2108)
        %1116 = "ttnn.typecast"(%1113) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1286)
        "ttnn.deallocate"(%1113) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1286)
        %1117 = "ttnn.multiply"(%1116, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1287)
        "ttnn.deallocate"(%1116) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1287)
        %1118 = "ttnn.typecast"(%1114) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1288)
        "ttnn.deallocate"(%1114) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1288)
        %1119 = "ttnn.permute"(%1118) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1288)
        "ttnn.deallocate"(%1118) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1288)
        %1120 = "ttnn.multiply"(%1119, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1289)
        "ttnn.deallocate"(%1119) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1289)
        %1121 = "ttnn.matmul"(%1117, %1120) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1290)
        "ttnn.deallocate"(%1120) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1290)
        "ttnn.deallocate"(%1117) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1290)
        %1122 = "ttnn.eq"(%1121, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1291)
        %1123 = "ttnn.logical_not"(%1122) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1292)
        "ttnn.deallocate"(%1122) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1292)
        %1124 = "ttnn.sum"(%1123) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1293)
        "ttnn.deallocate"(%1123) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1293)
        %1125 = "ttnn.ne"(%1124, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1293)
        "ttnn.deallocate"(%1124) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1293)
        %1126 = "ttnn.logical_not"(%1125) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1294)
        "ttnn.deallocate"(%1125) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1294)
        %1127 = "ttnn.reshape"(%1126) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1294)
        "ttnn.deallocate"(%1126) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1294)
        %1128 = "ttnn.softmax"(%1121) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1295)
        "ttnn.deallocate"(%1121) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1295)
        %1129 = "ttnn.repeat"(%1127) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1296)
        "ttnn.deallocate"(%1127) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1296)
        %1130 = "ttnn.typecast"(%1129) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2109)
        "ttnn.deallocate"(%1129) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2109)
        %1131 = "ttnn.where"(%1130, %59, %1128) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1296)
        "ttnn.deallocate"(%1130) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1296)
        "ttnn.deallocate"(%1128) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1296)
        %1132 = "ttnn.typecast"(%1115) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1297)
        "ttnn.deallocate"(%1115) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1297)
        %1133 = "ttnn.matmul"(%1131, %1132) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1298)
        "ttnn.deallocate"(%1132) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1298)
        "ttnn.deallocate"(%1131) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1298)
        %1134 = "ttnn.typecast"(%1133) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1299)
        "ttnn.deallocate"(%1133) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1299)
        %1135 = "ttnn.permute"(%1134) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2110)
        "ttnn.deallocate"(%1134) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2110)
        %1136 = "ttnn.reshape"(%1135) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1300)
        "ttnn.deallocate"(%1135) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1300)
        %1137 = "ttnn.matmul"(%1136, %arg140) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2111)
        "ttnn.deallocate"(%1136) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2111)
        "ttnn.deallocate"(%arg140) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2111)
        %1138 = "ttnn.add"(%1137, %90) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2112)
        "ttnn.deallocate"(%1137) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2112)
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2112)
        %1139 = "ttnn.add"(%1102, %1138) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1302)
        "ttnn.deallocate"(%1138) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1302)
        "ttnn.deallocate"(%1102) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1302)
        %1140 = "ttnn.layer_norm"(%1139, %arg138, %arg137) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1303)
        "ttnn.deallocate"(%arg138) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1303)
        "ttnn.deallocate"(%arg137) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1303)
        %1141 = "ttnn.reshape"(%1140) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1304)
        "ttnn.deallocate"(%1140) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1304)
        %1142 = "ttnn.matmul"(%1141, %arg136) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2113)
        "ttnn.deallocate"(%1141) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2113)
        "ttnn.deallocate"(%arg136) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2113)
        %1143 = "ttnn.add"(%1142, %120) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2114)
        "ttnn.deallocate"(%1142) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2114)
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2114)
        %1144 = "ttnn.gelu"(%1143) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2115)
        "ttnn.deallocate"(%1143) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2115)
        %1145 = "ttnn.reshape"(%1144) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1306)
        "ttnn.deallocate"(%1144) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1306)
        %1146 = "ttnn.matmul"(%1145, %arg134) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2116)
        "ttnn.deallocate"(%1145) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2116)
        "ttnn.deallocate"(%arg134) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2116)
        %1147 = "ttnn.add"(%1146, %39) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2117)
        "ttnn.deallocate"(%1146) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2117)
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2117)
        %1148 = "ttnn.add"(%1139, %1147) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1308)
        "ttnn.deallocate"(%1147) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1308)
        "ttnn.deallocate"(%1139) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1308)
        %1149 = "ttnn.layer_norm"(%1148, %arg132, %arg131) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1309)
        "ttnn.deallocate"(%arg132) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1309)
        "ttnn.deallocate"(%arg131) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1309)
        %1150 = "ttnn.reshape"(%1149) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1310)
        "ttnn.deallocate"(%1149) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1310)
        %1151 = "ttnn.matmul"(%1150, %60) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2118)
        "ttnn.deallocate"(%1150) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2118)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2118)
        %1152 = "ttnn.add"(%1151, %134) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2119)
        "ttnn.deallocate"(%1151) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2119)
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2119)
        %1153 = "ttnn.slice_static"(%1152) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2120)
        %1154 = "ttnn.slice_static"(%1152) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2121)
        %1155 = "ttnn.slice_static"(%1152) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2122)
        "ttnn.deallocate"(%1152) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2122)
        %1156 = "ttnn.reshape"(%1153) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2123)
        "ttnn.deallocate"(%1153) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2123)
        %1157 = "ttnn.reshape"(%1154) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2124)
        "ttnn.deallocate"(%1154) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2124)
        %1158 = "ttnn.reshape"(%1155) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2125)
        "ttnn.deallocate"(%1155) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2125)
        %1159 = "ttnn.permute"(%1156) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2126)
        "ttnn.deallocate"(%1156) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2126)
        %1160 = "ttnn.permute"(%1157) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2127)
        "ttnn.deallocate"(%1157) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2127)
        %1161 = "ttnn.permute"(%1158) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2128)
        "ttnn.deallocate"(%1158) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2128)
        %1162 = "ttnn.typecast"(%1159) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1312)
        "ttnn.deallocate"(%1159) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1312)
        %1163 = "ttnn.multiply"(%1162, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1313)
        "ttnn.deallocate"(%1162) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1313)
        %1164 = "ttnn.typecast"(%1160) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1314)
        "ttnn.deallocate"(%1160) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1314)
        %1165 = "ttnn.permute"(%1164) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1314)
        "ttnn.deallocate"(%1164) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1314)
        %1166 = "ttnn.multiply"(%1165, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1315)
        "ttnn.deallocate"(%1165) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1315)
        %1167 = "ttnn.matmul"(%1163, %1166) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1316)
        "ttnn.deallocate"(%1166) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1316)
        "ttnn.deallocate"(%1163) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1316)
        %1168 = "ttnn.eq"(%1167, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1317)
        %1169 = "ttnn.logical_not"(%1168) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1318)
        "ttnn.deallocate"(%1168) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1318)
        %1170 = "ttnn.sum"(%1169) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1319)
        "ttnn.deallocate"(%1169) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1319)
        %1171 = "ttnn.ne"(%1170, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1319)
        "ttnn.deallocate"(%1170) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1319)
        %1172 = "ttnn.logical_not"(%1171) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1320)
        "ttnn.deallocate"(%1171) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1320)
        %1173 = "ttnn.reshape"(%1172) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1320)
        "ttnn.deallocate"(%1172) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1320)
        %1174 = "ttnn.softmax"(%1167) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1321)
        "ttnn.deallocate"(%1167) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1321)
        %1175 = "ttnn.repeat"(%1173) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1322)
        "ttnn.deallocate"(%1173) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1322)
        %1176 = "ttnn.typecast"(%1175) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2129)
        "ttnn.deallocate"(%1175) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2129)
        %1177 = "ttnn.where"(%1176, %59, %1174) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1322)
        "ttnn.deallocate"(%1176) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1322)
        "ttnn.deallocate"(%1174) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1322)
        %1178 = "ttnn.typecast"(%1161) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1323)
        "ttnn.deallocate"(%1161) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1323)
        %1179 = "ttnn.matmul"(%1177, %1178) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1324)
        "ttnn.deallocate"(%1178) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1324)
        "ttnn.deallocate"(%1177) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1324)
        %1180 = "ttnn.typecast"(%1179) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1325)
        "ttnn.deallocate"(%1179) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1325)
        %1181 = "ttnn.permute"(%1180) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2130)
        "ttnn.deallocate"(%1180) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2130)
        %1182 = "ttnn.reshape"(%1181) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1326)
        "ttnn.deallocate"(%1181) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1326)
        %1183 = "ttnn.matmul"(%1182, %arg128) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2131)
        "ttnn.deallocate"(%1182) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2131)
        "ttnn.deallocate"(%arg128) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2131)
        %1184 = "ttnn.add"(%1183, %73) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2132)
        "ttnn.deallocate"(%1183) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2132)
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2132)
        %1185 = "ttnn.add"(%1148, %1184) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1328)
        "ttnn.deallocate"(%1184) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1328)
        "ttnn.deallocate"(%1148) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1328)
        %1186 = "ttnn.layer_norm"(%1185, %arg126, %arg125) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1329)
        "ttnn.deallocate"(%arg126) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1329)
        "ttnn.deallocate"(%arg125) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1329)
        %1187 = "ttnn.reshape"(%1186) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1330)
        "ttnn.deallocate"(%1186) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1330)
        %1188 = "ttnn.matmul"(%1187, %arg124) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2133)
        "ttnn.deallocate"(%1187) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2133)
        "ttnn.deallocate"(%arg124) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2133)
        %1189 = "ttnn.add"(%1188, %61) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2134)
        "ttnn.deallocate"(%1188) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2134)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2134)
        %1190 = "ttnn.gelu"(%1189) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2135)
        "ttnn.deallocate"(%1189) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2135)
        %1191 = "ttnn.reshape"(%1190) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1332)
        "ttnn.deallocate"(%1190) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1332)
        %1192 = "ttnn.matmul"(%1191, %arg122) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2136)
        "ttnn.deallocate"(%1191) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2136)
        "ttnn.deallocate"(%arg122) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2136)
        %1193 = "ttnn.add"(%1192, %141) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2137)
        "ttnn.deallocate"(%1192) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2137)
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2137)
        %1194 = "ttnn.add"(%1185, %1193) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1334)
        "ttnn.deallocate"(%1193) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1334)
        "ttnn.deallocate"(%1185) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1334)
        %1195 = "ttnn.layer_norm"(%1194, %arg120, %arg119) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1335)
        "ttnn.deallocate"(%arg120) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1335)
        "ttnn.deallocate"(%arg119) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1335)
        %1196 = "ttnn.reshape"(%1195) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1336)
        "ttnn.deallocate"(%1195) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1336)
        %1197 = "ttnn.matmul"(%1196, %6) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2138)
        "ttnn.deallocate"(%1196) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2138)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2138)
        %1198 = "ttnn.add"(%1197, %35) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2139)
        "ttnn.deallocate"(%1197) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2139)
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2139)
        %1199 = "ttnn.slice_static"(%1198) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2140)
        %1200 = "ttnn.slice_static"(%1198) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2141)
        %1201 = "ttnn.slice_static"(%1198) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2142)
        "ttnn.deallocate"(%1198) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2142)
        %1202 = "ttnn.reshape"(%1199) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2143)
        "ttnn.deallocate"(%1199) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2143)
        %1203 = "ttnn.reshape"(%1200) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2144)
        "ttnn.deallocate"(%1200) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2144)
        %1204 = "ttnn.reshape"(%1201) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2145)
        "ttnn.deallocate"(%1201) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2145)
        %1205 = "ttnn.permute"(%1202) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2146)
        "ttnn.deallocate"(%1202) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2146)
        %1206 = "ttnn.permute"(%1203) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2147)
        "ttnn.deallocate"(%1203) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2147)
        %1207 = "ttnn.permute"(%1204) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2148)
        "ttnn.deallocate"(%1204) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2148)
        %1208 = "ttnn.typecast"(%1205) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1338)
        "ttnn.deallocate"(%1205) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1338)
        %1209 = "ttnn.multiply"(%1208, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1339)
        "ttnn.deallocate"(%1208) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1339)
        %1210 = "ttnn.typecast"(%1206) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1340)
        "ttnn.deallocate"(%1206) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1340)
        %1211 = "ttnn.permute"(%1210) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1340)
        "ttnn.deallocate"(%1210) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1340)
        %1212 = "ttnn.multiply"(%1211, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1341)
        "ttnn.deallocate"(%1211) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1341)
        %1213 = "ttnn.matmul"(%1209, %1212) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1342)
        "ttnn.deallocate"(%1212) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1342)
        "ttnn.deallocate"(%1209) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1342)
        %1214 = "ttnn.eq"(%1213, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1343)
        %1215 = "ttnn.logical_not"(%1214) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1344)
        "ttnn.deallocate"(%1214) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1344)
        %1216 = "ttnn.sum"(%1215) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1345)
        "ttnn.deallocate"(%1215) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1345)
        %1217 = "ttnn.ne"(%1216, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1345)
        "ttnn.deallocate"(%1216) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1345)
        %1218 = "ttnn.logical_not"(%1217) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1346)
        "ttnn.deallocate"(%1217) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1346)
        %1219 = "ttnn.reshape"(%1218) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1346)
        "ttnn.deallocate"(%1218) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1346)
        %1220 = "ttnn.softmax"(%1213) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1347)
        "ttnn.deallocate"(%1213) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1347)
        %1221 = "ttnn.repeat"(%1219) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1348)
        "ttnn.deallocate"(%1219) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1348)
        %1222 = "ttnn.typecast"(%1221) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2149)
        "ttnn.deallocate"(%1221) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2149)
        %1223 = "ttnn.where"(%1222, %59, %1220) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1348)
        "ttnn.deallocate"(%1222) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1348)
        "ttnn.deallocate"(%1220) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1348)
        %1224 = "ttnn.typecast"(%1207) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1349)
        "ttnn.deallocate"(%1207) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1349)
        %1225 = "ttnn.matmul"(%1223, %1224) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1350)
        "ttnn.deallocate"(%1224) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1350)
        "ttnn.deallocate"(%1223) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1350)
        %1226 = "ttnn.typecast"(%1225) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1351)
        "ttnn.deallocate"(%1225) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1351)
        %1227 = "ttnn.permute"(%1226) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2150)
        "ttnn.deallocate"(%1226) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2150)
        %1228 = "ttnn.reshape"(%1227) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1352)
        "ttnn.deallocate"(%1227) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1352)
        %1229 = "ttnn.matmul"(%1228, %arg116) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2151)
        "ttnn.deallocate"(%1228) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2151)
        "ttnn.deallocate"(%arg116) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2151)
        %1230 = "ttnn.add"(%1229, %143) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2152)
        "ttnn.deallocate"(%1229) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2152)
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2152)
        %1231 = "ttnn.add"(%1194, %1230) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1354)
        "ttnn.deallocate"(%1230) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1354)
        "ttnn.deallocate"(%1194) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1354)
        %1232 = "ttnn.layer_norm"(%1231, %arg114, %arg113) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1355)
        "ttnn.deallocate"(%arg114) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1355)
        "ttnn.deallocate"(%arg113) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1355)
        %1233 = "ttnn.reshape"(%1232) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1356)
        "ttnn.deallocate"(%1232) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1356)
        %1234 = "ttnn.matmul"(%1233, %arg112) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2153)
        "ttnn.deallocate"(%1233) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2153)
        "ttnn.deallocate"(%arg112) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2153)
        %1235 = "ttnn.add"(%1234, %159) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2154)
        "ttnn.deallocate"(%1234) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2154)
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2154)
        %1236 = "ttnn.gelu"(%1235) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2155)
        "ttnn.deallocate"(%1235) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2155)
        %1237 = "ttnn.reshape"(%1236) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1358)
        "ttnn.deallocate"(%1236) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1358)
        %1238 = "ttnn.matmul"(%1237, %arg110) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2156)
        "ttnn.deallocate"(%1237) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2156)
        "ttnn.deallocate"(%arg110) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2156)
        %1239 = "ttnn.add"(%1238, %164) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2157)
        "ttnn.deallocate"(%1238) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2157)
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2157)
        %1240 = "ttnn.add"(%1231, %1239) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1360)
        "ttnn.deallocate"(%1239) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1360)
        "ttnn.deallocate"(%1231) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1360)
        %1241 = "ttnn.layer_norm"(%1240, %arg108, %arg107) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1361)
        "ttnn.deallocate"(%arg108) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1361)
        "ttnn.deallocate"(%arg107) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1361)
        %1242 = "ttnn.reshape"(%1241) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1362)
        "ttnn.deallocate"(%1241) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1362)
        %1243 = "ttnn.matmul"(%1242, %17) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2158)
        "ttnn.deallocate"(%1242) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2158)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2158)
        %1244 = "ttnn.add"(%1243, %109) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2159)
        "ttnn.deallocate"(%1243) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2159)
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2159)
        %1245 = "ttnn.slice_static"(%1244) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2160)
        %1246 = "ttnn.slice_static"(%1244) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2161)
        %1247 = "ttnn.slice_static"(%1244) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2162)
        "ttnn.deallocate"(%1244) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2162)
        %1248 = "ttnn.reshape"(%1245) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2163)
        "ttnn.deallocate"(%1245) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2163)
        %1249 = "ttnn.reshape"(%1246) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2164)
        "ttnn.deallocate"(%1246) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2164)
        %1250 = "ttnn.reshape"(%1247) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2165)
        "ttnn.deallocate"(%1247) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2165)
        %1251 = "ttnn.permute"(%1248) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2166)
        "ttnn.deallocate"(%1248) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2166)
        %1252 = "ttnn.permute"(%1249) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2167)
        "ttnn.deallocate"(%1249) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2167)
        %1253 = "ttnn.permute"(%1250) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2168)
        "ttnn.deallocate"(%1250) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2168)
        %1254 = "ttnn.typecast"(%1251) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1364)
        "ttnn.deallocate"(%1251) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1364)
        %1255 = "ttnn.multiply"(%1254, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1365)
        "ttnn.deallocate"(%1254) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1365)
        %1256 = "ttnn.typecast"(%1252) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1366)
        "ttnn.deallocate"(%1252) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1366)
        %1257 = "ttnn.permute"(%1256) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1366)
        "ttnn.deallocate"(%1256) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1366)
        %1258 = "ttnn.multiply"(%1257, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1367)
        "ttnn.deallocate"(%1257) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1367)
        %1259 = "ttnn.matmul"(%1255, %1258) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1368)
        "ttnn.deallocate"(%1258) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1368)
        "ttnn.deallocate"(%1255) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1368)
        %1260 = "ttnn.eq"(%1259, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1369)
        %1261 = "ttnn.logical_not"(%1260) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1370)
        "ttnn.deallocate"(%1260) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1370)
        %1262 = "ttnn.sum"(%1261) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1371)
        "ttnn.deallocate"(%1261) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1371)
        %1263 = "ttnn.ne"(%1262, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1371)
        "ttnn.deallocate"(%1262) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1371)
        %1264 = "ttnn.logical_not"(%1263) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1372)
        "ttnn.deallocate"(%1263) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1372)
        %1265 = "ttnn.reshape"(%1264) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1372)
        "ttnn.deallocate"(%1264) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1372)
        %1266 = "ttnn.softmax"(%1259) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1373)
        "ttnn.deallocate"(%1259) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1373)
        %1267 = "ttnn.repeat"(%1265) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1374)
        "ttnn.deallocate"(%1265) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1374)
        %1268 = "ttnn.typecast"(%1267) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2169)
        "ttnn.deallocate"(%1267) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2169)
        %1269 = "ttnn.where"(%1268, %59, %1266) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1374)
        "ttnn.deallocate"(%1268) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1374)
        "ttnn.deallocate"(%1266) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1374)
        %1270 = "ttnn.typecast"(%1253) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1375)
        "ttnn.deallocate"(%1253) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1375)
        %1271 = "ttnn.matmul"(%1269, %1270) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1376)
        "ttnn.deallocate"(%1270) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1376)
        "ttnn.deallocate"(%1269) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1376)
        %1272 = "ttnn.typecast"(%1271) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1377)
        "ttnn.deallocate"(%1271) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1377)
        %1273 = "ttnn.permute"(%1272) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2170)
        "ttnn.deallocate"(%1272) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2170)
        %1274 = "ttnn.reshape"(%1273) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1378)
        "ttnn.deallocate"(%1273) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1378)
        %1275 = "ttnn.matmul"(%1274, %arg104) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2171)
        "ttnn.deallocate"(%1274) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2171)
        "ttnn.deallocate"(%arg104) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2171)
        %1276 = "ttnn.add"(%1275, %24) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2172)
        "ttnn.deallocate"(%1275) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2172)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2172)
        %1277 = "ttnn.add"(%1240, %1276) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1380)
        "ttnn.deallocate"(%1276) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1380)
        "ttnn.deallocate"(%1240) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1380)
        %1278 = "ttnn.layer_norm"(%1277, %arg102, %arg101) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1381)
        "ttnn.deallocate"(%arg102) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1381)
        "ttnn.deallocate"(%arg101) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1381)
        %1279 = "ttnn.reshape"(%1278) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1382)
        "ttnn.deallocate"(%1278) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1382)
        %1280 = "ttnn.matmul"(%1279, %arg100) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2173)
        "ttnn.deallocate"(%1279) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2173)
        "ttnn.deallocate"(%arg100) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2173)
        %1281 = "ttnn.add"(%1280, %65) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2174)
        "ttnn.deallocate"(%1280) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2174)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2174)
        %1282 = "ttnn.gelu"(%1281) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2175)
        "ttnn.deallocate"(%1281) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2175)
        %1283 = "ttnn.reshape"(%1282) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1384)
        "ttnn.deallocate"(%1282) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1384)
        %1284 = "ttnn.matmul"(%1283, %arg98) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2176)
        "ttnn.deallocate"(%1283) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2176)
        "ttnn.deallocate"(%arg98) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2176)
        %1285 = "ttnn.add"(%1284, %148) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2177)
        "ttnn.deallocate"(%1284) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2177)
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2177)
        %1286 = "ttnn.add"(%1277, %1285) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1386)
        "ttnn.deallocate"(%1285) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1386)
        "ttnn.deallocate"(%1277) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1386)
        %1287 = "ttnn.layer_norm"(%1286, %arg96, %arg95) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1387)
        "ttnn.deallocate"(%arg96) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1387)
        "ttnn.deallocate"(%arg95) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1387)
        %1288 = "ttnn.reshape"(%1287) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1388)
        "ttnn.deallocate"(%1287) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1388)
        %1289 = "ttnn.matmul"(%1288, %45) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2178)
        "ttnn.deallocate"(%1288) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2178)
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2178)
        %1290 = "ttnn.add"(%1289, %28) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2179)
        "ttnn.deallocate"(%1289) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2179)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2179)
        %1291 = "ttnn.slice_static"(%1290) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2180)
        %1292 = "ttnn.slice_static"(%1290) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2181)
        %1293 = "ttnn.slice_static"(%1290) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2182)
        "ttnn.deallocate"(%1290) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2182)
        %1294 = "ttnn.reshape"(%1291) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2183)
        "ttnn.deallocate"(%1291) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2183)
        %1295 = "ttnn.reshape"(%1292) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2184)
        "ttnn.deallocate"(%1292) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2184)
        %1296 = "ttnn.reshape"(%1293) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2185)
        "ttnn.deallocate"(%1293) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2185)
        %1297 = "ttnn.permute"(%1294) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2186)
        "ttnn.deallocate"(%1294) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2186)
        %1298 = "ttnn.permute"(%1295) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2187)
        "ttnn.deallocate"(%1295) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2187)
        %1299 = "ttnn.permute"(%1296) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2188)
        "ttnn.deallocate"(%1296) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2188)
        %1300 = "ttnn.typecast"(%1297) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1390)
        "ttnn.deallocate"(%1297) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1390)
        %1301 = "ttnn.multiply"(%1300, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1391)
        "ttnn.deallocate"(%1300) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1391)
        %1302 = "ttnn.typecast"(%1298) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1392)
        "ttnn.deallocate"(%1298) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1392)
        %1303 = "ttnn.permute"(%1302) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1392)
        "ttnn.deallocate"(%1302) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1392)
        %1304 = "ttnn.multiply"(%1303, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1393)
        "ttnn.deallocate"(%1303) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1393)
        %1305 = "ttnn.matmul"(%1301, %1304) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1394)
        "ttnn.deallocate"(%1304) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1394)
        "ttnn.deallocate"(%1301) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1394)
        %1306 = "ttnn.eq"(%1305, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1395)
        %1307 = "ttnn.logical_not"(%1306) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1396)
        "ttnn.deallocate"(%1306) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1396)
        %1308 = "ttnn.sum"(%1307) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1397)
        "ttnn.deallocate"(%1307) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1397)
        %1309 = "ttnn.ne"(%1308, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1397)
        "ttnn.deallocate"(%1308) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1397)
        %1310 = "ttnn.logical_not"(%1309) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1398)
        "ttnn.deallocate"(%1309) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1398)
        %1311 = "ttnn.reshape"(%1310) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1398)
        "ttnn.deallocate"(%1310) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1398)
        %1312 = "ttnn.softmax"(%1305) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1399)
        "ttnn.deallocate"(%1305) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1399)
        %1313 = "ttnn.repeat"(%1311) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1400)
        "ttnn.deallocate"(%1311) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1400)
        %1314 = "ttnn.typecast"(%1313) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2189)
        "ttnn.deallocate"(%1313) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2189)
        %1315 = "ttnn.where"(%1314, %59, %1312) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1400)
        "ttnn.deallocate"(%1314) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1400)
        "ttnn.deallocate"(%1312) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1400)
        %1316 = "ttnn.typecast"(%1299) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1401)
        "ttnn.deallocate"(%1299) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1401)
        %1317 = "ttnn.matmul"(%1315, %1316) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1402)
        "ttnn.deallocate"(%1316) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1402)
        "ttnn.deallocate"(%1315) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1402)
        %1318 = "ttnn.typecast"(%1317) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1403)
        "ttnn.deallocate"(%1317) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1403)
        %1319 = "ttnn.permute"(%1318) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2190)
        "ttnn.deallocate"(%1318) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2190)
        %1320 = "ttnn.reshape"(%1319) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1404)
        "ttnn.deallocate"(%1319) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1404)
        %1321 = "ttnn.matmul"(%1320, %arg92) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2191)
        "ttnn.deallocate"(%1320) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2191)
        "ttnn.deallocate"(%arg92) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2191)
        %1322 = "ttnn.add"(%1321, %128) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2192)
        "ttnn.deallocate"(%1321) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2192)
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2192)
        %1323 = "ttnn.add"(%1286, %1322) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1406)
        "ttnn.deallocate"(%1322) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1406)
        "ttnn.deallocate"(%1286) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1406)
        %1324 = "ttnn.layer_norm"(%1323, %arg90, %arg89) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1407)
        "ttnn.deallocate"(%arg90) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1407)
        "ttnn.deallocate"(%arg89) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1407)
        %1325 = "ttnn.reshape"(%1324) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1408)
        "ttnn.deallocate"(%1324) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1408)
        %1326 = "ttnn.matmul"(%1325, %arg88) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2193)
        "ttnn.deallocate"(%1325) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2193)
        "ttnn.deallocate"(%arg88) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2193)
        %1327 = "ttnn.add"(%1326, %114) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2194)
        "ttnn.deallocate"(%1326) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2194)
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2194)
        %1328 = "ttnn.gelu"(%1327) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2195)
        "ttnn.deallocate"(%1327) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2195)
        %1329 = "ttnn.reshape"(%1328) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1410)
        "ttnn.deallocate"(%1328) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1410)
        %1330 = "ttnn.matmul"(%1329, %arg86) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2196)
        "ttnn.deallocate"(%1329) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2196)
        "ttnn.deallocate"(%arg86) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2196)
        %1331 = "ttnn.add"(%1330, %85) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2197)
        "ttnn.deallocate"(%1330) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2197)
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2197)
        %1332 = "ttnn.add"(%1323, %1331) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1412)
        "ttnn.deallocate"(%1331) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1412)
        "ttnn.deallocate"(%1323) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1412)
        %1333 = "ttnn.layer_norm"(%1332, %arg84, %arg83) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1413)
        "ttnn.deallocate"(%arg84) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1413)
        "ttnn.deallocate"(%arg83) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1413)
        %1334 = "ttnn.reshape"(%1333) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1414)
        "ttnn.deallocate"(%1333) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1414)
        %1335 = "ttnn.matmul"(%1334, %126) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2198)
        "ttnn.deallocate"(%1334) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2198)
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2198)
        %1336 = "ttnn.add"(%1335, %33) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2199)
        "ttnn.deallocate"(%1335) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2199)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2199)
        %1337 = "ttnn.slice_static"(%1336) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2200)
        %1338 = "ttnn.slice_static"(%1336) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2201)
        %1339 = "ttnn.slice_static"(%1336) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2202)
        "ttnn.deallocate"(%1336) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2202)
        %1340 = "ttnn.reshape"(%1337) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2203)
        "ttnn.deallocate"(%1337) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2203)
        %1341 = "ttnn.reshape"(%1338) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2204)
        "ttnn.deallocate"(%1338) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2204)
        %1342 = "ttnn.reshape"(%1339) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2205)
        "ttnn.deallocate"(%1339) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2205)
        %1343 = "ttnn.permute"(%1340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2206)
        "ttnn.deallocate"(%1340) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2206)
        %1344 = "ttnn.permute"(%1341) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2207)
        "ttnn.deallocate"(%1341) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2207)
        %1345 = "ttnn.permute"(%1342) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2208)
        "ttnn.deallocate"(%1342) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2208)
        %1346 = "ttnn.typecast"(%1343) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1416)
        "ttnn.deallocate"(%1343) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1416)
        %1347 = "ttnn.multiply"(%1346, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1417)
        "ttnn.deallocate"(%1346) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1417)
        %1348 = "ttnn.typecast"(%1344) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1418)
        "ttnn.deallocate"(%1344) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1418)
        %1349 = "ttnn.permute"(%1348) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1418)
        "ttnn.deallocate"(%1348) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1418)
        %1350 = "ttnn.multiply"(%1349, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1419)
        "ttnn.deallocate"(%1349) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1419)
        %1351 = "ttnn.matmul"(%1347, %1350) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1420)
        "ttnn.deallocate"(%1350) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1420)
        "ttnn.deallocate"(%1347) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1420)
        %1352 = "ttnn.eq"(%1351, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1421)
        %1353 = "ttnn.logical_not"(%1352) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1422)
        "ttnn.deallocate"(%1352) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1422)
        %1354 = "ttnn.sum"(%1353) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1423)
        "ttnn.deallocate"(%1353) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1423)
        %1355 = "ttnn.ne"(%1354, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1423)
        "ttnn.deallocate"(%1354) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1423)
        %1356 = "ttnn.logical_not"(%1355) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1424)
        "ttnn.deallocate"(%1355) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1424)
        %1357 = "ttnn.reshape"(%1356) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1424)
        "ttnn.deallocate"(%1356) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1424)
        %1358 = "ttnn.softmax"(%1351) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1425)
        "ttnn.deallocate"(%1351) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1425)
        %1359 = "ttnn.repeat"(%1357) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1426)
        "ttnn.deallocate"(%1357) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1426)
        %1360 = "ttnn.typecast"(%1359) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2209)
        "ttnn.deallocate"(%1359) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2209)
        %1361 = "ttnn.where"(%1360, %59, %1358) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1426)
        "ttnn.deallocate"(%1360) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1426)
        "ttnn.deallocate"(%1358) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1426)
        %1362 = "ttnn.typecast"(%1345) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1427)
        "ttnn.deallocate"(%1345) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1427)
        %1363 = "ttnn.matmul"(%1361, %1362) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1428)
        "ttnn.deallocate"(%1362) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1428)
        "ttnn.deallocate"(%1361) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1428)
        %1364 = "ttnn.typecast"(%1363) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1429)
        "ttnn.deallocate"(%1363) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1429)
        %1365 = "ttnn.permute"(%1364) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2210)
        "ttnn.deallocate"(%1364) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2210)
        %1366 = "ttnn.reshape"(%1365) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1430)
        "ttnn.deallocate"(%1365) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1430)
        %1367 = "ttnn.matmul"(%1366, %arg80) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2211)
        "ttnn.deallocate"(%1366) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2211)
        "ttnn.deallocate"(%arg80) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2211)
        %1368 = "ttnn.add"(%1367, %38) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2212)
        "ttnn.deallocate"(%1367) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2212)
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2212)
        %1369 = "ttnn.add"(%1332, %1368) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1432)
        "ttnn.deallocate"(%1368) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1432)
        "ttnn.deallocate"(%1332) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1432)
        %1370 = "ttnn.layer_norm"(%1369, %arg78, %arg77) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1433)
        "ttnn.deallocate"(%arg78) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1433)
        "ttnn.deallocate"(%arg77) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1433)
        %1371 = "ttnn.reshape"(%1370) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1434)
        "ttnn.deallocate"(%1370) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1434)
        %1372 = "ttnn.matmul"(%1371, %arg76) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2213)
        "ttnn.deallocate"(%1371) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2213)
        "ttnn.deallocate"(%arg76) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2213)
        %1373 = "ttnn.add"(%1372, %84) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2214)
        "ttnn.deallocate"(%1372) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2214)
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2214)
        %1374 = "ttnn.gelu"(%1373) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2215)
        "ttnn.deallocate"(%1373) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2215)
        %1375 = "ttnn.reshape"(%1374) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1436)
        "ttnn.deallocate"(%1374) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1436)
        %1376 = "ttnn.matmul"(%1375, %arg74) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2216)
        "ttnn.deallocate"(%1375) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2216)
        "ttnn.deallocate"(%arg74) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2216)
        %1377 = "ttnn.add"(%1376, %123) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2217)
        "ttnn.deallocate"(%1376) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2217)
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2217)
        %1378 = "ttnn.add"(%1369, %1377) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1438)
        "ttnn.deallocate"(%1377) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1438)
        "ttnn.deallocate"(%1369) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1438)
        %1379 = "ttnn.layer_norm"(%1378, %arg72, %arg71) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1439)
        "ttnn.deallocate"(%arg72) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1439)
        "ttnn.deallocate"(%arg71) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1439)
        %1380 = "ttnn.reshape"(%1379) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1440)
        "ttnn.deallocate"(%1379) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1440)
        %1381 = "ttnn.matmul"(%1380, %98) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2218)
        "ttnn.deallocate"(%1380) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2218)
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2218)
        %1382 = "ttnn.add"(%1381, %30) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2219)
        "ttnn.deallocate"(%1381) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2219)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2219)
        %1383 = "ttnn.slice_static"(%1382) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2220)
        %1384 = "ttnn.slice_static"(%1382) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2221)
        %1385 = "ttnn.slice_static"(%1382) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2222)
        "ttnn.deallocate"(%1382) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2222)
        %1386 = "ttnn.reshape"(%1383) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2223)
        "ttnn.deallocate"(%1383) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2223)
        %1387 = "ttnn.reshape"(%1384) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2224)
        "ttnn.deallocate"(%1384) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2224)
        %1388 = "ttnn.reshape"(%1385) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2225)
        "ttnn.deallocate"(%1385) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2225)
        %1389 = "ttnn.permute"(%1386) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2226)
        "ttnn.deallocate"(%1386) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2226)
        %1390 = "ttnn.permute"(%1387) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2227)
        "ttnn.deallocate"(%1387) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2227)
        %1391 = "ttnn.permute"(%1388) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2228)
        "ttnn.deallocate"(%1388) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2228)
        %1392 = "ttnn.typecast"(%1389) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1442)
        "ttnn.deallocate"(%1389) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1442)
        %1393 = "ttnn.multiply"(%1392, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1443)
        "ttnn.deallocate"(%1392) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1443)
        %1394 = "ttnn.typecast"(%1390) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1444)
        "ttnn.deallocate"(%1390) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1444)
        %1395 = "ttnn.permute"(%1394) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1444)
        "ttnn.deallocate"(%1394) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1444)
        %1396 = "ttnn.multiply"(%1395, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1445)
        "ttnn.deallocate"(%1395) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1445)
        %1397 = "ttnn.matmul"(%1393, %1396) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1446)
        "ttnn.deallocate"(%1396) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1446)
        "ttnn.deallocate"(%1393) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1446)
        %1398 = "ttnn.eq"(%1397, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1447)
        %1399 = "ttnn.logical_not"(%1398) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1448)
        "ttnn.deallocate"(%1398) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1448)
        %1400 = "ttnn.sum"(%1399) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1449)
        "ttnn.deallocate"(%1399) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1449)
        %1401 = "ttnn.ne"(%1400, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1449)
        "ttnn.deallocate"(%1400) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1449)
        %1402 = "ttnn.logical_not"(%1401) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1450)
        "ttnn.deallocate"(%1401) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1450)
        %1403 = "ttnn.reshape"(%1402) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1450)
        "ttnn.deallocate"(%1402) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1450)
        %1404 = "ttnn.softmax"(%1397) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1451)
        "ttnn.deallocate"(%1397) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1451)
        %1405 = "ttnn.repeat"(%1403) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1452)
        "ttnn.deallocate"(%1403) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1452)
        %1406 = "ttnn.typecast"(%1405) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2229)
        "ttnn.deallocate"(%1405) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2229)
        %1407 = "ttnn.where"(%1406, %59, %1404) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1452)
        "ttnn.deallocate"(%1406) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1452)
        "ttnn.deallocate"(%1404) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1452)
        %1408 = "ttnn.typecast"(%1391) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1453)
        "ttnn.deallocate"(%1391) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1453)
        %1409 = "ttnn.matmul"(%1407, %1408) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1454)
        "ttnn.deallocate"(%1408) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1454)
        "ttnn.deallocate"(%1407) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1454)
        %1410 = "ttnn.typecast"(%1409) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1455)
        "ttnn.deallocate"(%1409) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1455)
        %1411 = "ttnn.permute"(%1410) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2230)
        "ttnn.deallocate"(%1410) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2230)
        %1412 = "ttnn.reshape"(%1411) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1456)
        "ttnn.deallocate"(%1411) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1456)
        %1413 = "ttnn.matmul"(%1412, %arg68) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2231)
        "ttnn.deallocate"(%1412) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2231)
        "ttnn.deallocate"(%arg68) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2231)
        %1414 = "ttnn.add"(%1413, %9) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2232)
        "ttnn.deallocate"(%1413) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2232)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2232)
        %1415 = "ttnn.add"(%1378, %1414) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1458)
        "ttnn.deallocate"(%1414) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1458)
        "ttnn.deallocate"(%1378) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1458)
        %1416 = "ttnn.layer_norm"(%1415, %arg66, %arg65) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1459)
        "ttnn.deallocate"(%arg66) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1459)
        "ttnn.deallocate"(%arg65) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1459)
        %1417 = "ttnn.reshape"(%1416) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1460)
        "ttnn.deallocate"(%1416) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1460)
        %1418 = "ttnn.matmul"(%1417, %arg64) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2233)
        "ttnn.deallocate"(%1417) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2233)
        "ttnn.deallocate"(%arg64) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2233)
        %1419 = "ttnn.add"(%1418, %46) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2234)
        "ttnn.deallocate"(%1418) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2234)
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2234)
        %1420 = "ttnn.gelu"(%1419) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2235)
        "ttnn.deallocate"(%1419) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2235)
        %1421 = "ttnn.reshape"(%1420) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1462)
        "ttnn.deallocate"(%1420) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1462)
        %1422 = "ttnn.matmul"(%1421, %arg62) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2236)
        "ttnn.deallocate"(%1421) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2236)
        "ttnn.deallocate"(%arg62) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2236)
        %1423 = "ttnn.add"(%1422, %3) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2237)
        "ttnn.deallocate"(%1422) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2237)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2237)
        %1424 = "ttnn.add"(%1415, %1423) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1464)
        "ttnn.deallocate"(%1423) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1464)
        "ttnn.deallocate"(%1415) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1464)
        %1425 = "ttnn.layer_norm"(%1424, %arg60, %arg59) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1465)
        "ttnn.deallocate"(%arg60) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1465)
        "ttnn.deallocate"(%arg59) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1465)
        %1426 = "ttnn.reshape"(%1425) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1466)
        "ttnn.deallocate"(%1425) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1466)
        %1427 = "ttnn.matmul"(%1426, %36) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2238)
        "ttnn.deallocate"(%1426) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2238)
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2238)
        %1428 = "ttnn.add"(%1427, %78) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2239)
        "ttnn.deallocate"(%1427) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2239)
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2239)
        %1429 = "ttnn.slice_static"(%1428) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2240)
        %1430 = "ttnn.slice_static"(%1428) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2241)
        %1431 = "ttnn.slice_static"(%1428) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2242)
        "ttnn.deallocate"(%1428) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2242)
        %1432 = "ttnn.reshape"(%1429) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2243)
        "ttnn.deallocate"(%1429) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2243)
        %1433 = "ttnn.reshape"(%1430) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2244)
        "ttnn.deallocate"(%1430) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2244)
        %1434 = "ttnn.reshape"(%1431) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2245)
        "ttnn.deallocate"(%1431) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2245)
        %1435 = "ttnn.permute"(%1432) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2246)
        "ttnn.deallocate"(%1432) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2246)
        %1436 = "ttnn.permute"(%1433) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2247)
        "ttnn.deallocate"(%1433) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2247)
        %1437 = "ttnn.permute"(%1434) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2248)
        "ttnn.deallocate"(%1434) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2248)
        %1438 = "ttnn.typecast"(%1435) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1468)
        "ttnn.deallocate"(%1435) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1468)
        %1439 = "ttnn.multiply"(%1438, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1469)
        "ttnn.deallocate"(%1438) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1469)
        %1440 = "ttnn.typecast"(%1436) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1470)
        "ttnn.deallocate"(%1436) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1470)
        %1441 = "ttnn.permute"(%1440) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1470)
        "ttnn.deallocate"(%1440) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1470)
        %1442 = "ttnn.multiply"(%1441, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1471)
        "ttnn.deallocate"(%1441) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1471)
        %1443 = "ttnn.matmul"(%1439, %1442) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1472)
        "ttnn.deallocate"(%1442) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1472)
        "ttnn.deallocate"(%1439) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1472)
        %1444 = "ttnn.eq"(%1443, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1473)
        %1445 = "ttnn.logical_not"(%1444) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1474)
        "ttnn.deallocate"(%1444) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1474)
        %1446 = "ttnn.sum"(%1445) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1475)
        "ttnn.deallocate"(%1445) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1475)
        %1447 = "ttnn.ne"(%1446, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1475)
        "ttnn.deallocate"(%1446) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1475)
        %1448 = "ttnn.logical_not"(%1447) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1476)
        "ttnn.deallocate"(%1447) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1476)
        %1449 = "ttnn.reshape"(%1448) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1476)
        "ttnn.deallocate"(%1448) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1476)
        %1450 = "ttnn.softmax"(%1443) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1477)
        "ttnn.deallocate"(%1443) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1477)
        %1451 = "ttnn.repeat"(%1449) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1478)
        "ttnn.deallocate"(%1449) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1478)
        %1452 = "ttnn.typecast"(%1451) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2249)
        "ttnn.deallocate"(%1451) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2249)
        %1453 = "ttnn.where"(%1452, %59, %1450) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1478)
        "ttnn.deallocate"(%1452) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1478)
        "ttnn.deallocate"(%1450) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1478)
        %1454 = "ttnn.typecast"(%1437) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1479)
        "ttnn.deallocate"(%1437) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1479)
        %1455 = "ttnn.matmul"(%1453, %1454) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1480)
        "ttnn.deallocate"(%1454) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1480)
        "ttnn.deallocate"(%1453) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1480)
        %1456 = "ttnn.typecast"(%1455) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1481)
        "ttnn.deallocate"(%1455) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1481)
        %1457 = "ttnn.permute"(%1456) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2250)
        "ttnn.deallocate"(%1456) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2250)
        %1458 = "ttnn.reshape"(%1457) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1482)
        "ttnn.deallocate"(%1457) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1482)
        %1459 = "ttnn.matmul"(%1458, %arg56) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2251)
        "ttnn.deallocate"(%1458) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2251)
        "ttnn.deallocate"(%arg56) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2251)
        %1460 = "ttnn.add"(%1459, %31) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2252)
        "ttnn.deallocate"(%1459) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2252)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2252)
        %1461 = "ttnn.add"(%1424, %1460) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1484)
        "ttnn.deallocate"(%1460) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1484)
        "ttnn.deallocate"(%1424) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1484)
        %1462 = "ttnn.layer_norm"(%1461, %arg54, %arg53) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1485)
        "ttnn.deallocate"(%arg54) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1485)
        "ttnn.deallocate"(%arg53) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1485)
        %1463 = "ttnn.reshape"(%1462) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1486)
        "ttnn.deallocate"(%1462) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1486)
        %1464 = "ttnn.matmul"(%1463, %arg52) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2253)
        "ttnn.deallocate"(%1463) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2253)
        "ttnn.deallocate"(%arg52) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2253)
        %1465 = "ttnn.add"(%1464, %52) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2254)
        "ttnn.deallocate"(%1464) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2254)
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2254)
        %1466 = "ttnn.gelu"(%1465) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2255)
        "ttnn.deallocate"(%1465) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2255)
        %1467 = "ttnn.reshape"(%1466) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1488)
        "ttnn.deallocate"(%1466) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1488)
        %1468 = "ttnn.matmul"(%1467, %arg50) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2256)
        "ttnn.deallocate"(%1467) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2256)
        "ttnn.deallocate"(%arg50) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2256)
        %1469 = "ttnn.add"(%1468, %154) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2257)
        "ttnn.deallocate"(%1468) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2257)
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2257)
        %1470 = "ttnn.add"(%1461, %1469) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1490)
        "ttnn.deallocate"(%1469) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1490)
        "ttnn.deallocate"(%1461) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1490)
        %1471 = "ttnn.layer_norm"(%1470, %arg48, %arg47) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1491)
        "ttnn.deallocate"(%arg48) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1491)
        "ttnn.deallocate"(%arg47) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1491)
        %1472 = "ttnn.reshape"(%1471) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1492)
        "ttnn.deallocate"(%1471) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1492)
        %1473 = "ttnn.matmul"(%1472, %16) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2258)
        "ttnn.deallocate"(%1472) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2258)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2258)
        %1474 = "ttnn.add"(%1473, %44) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2259)
        "ttnn.deallocate"(%1473) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2259)
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2259)
        %1475 = "ttnn.slice_static"(%1474) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2260)
        %1476 = "ttnn.slice_static"(%1474) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2261)
        %1477 = "ttnn.slice_static"(%1474) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2262)
        "ttnn.deallocate"(%1474) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2262)
        %1478 = "ttnn.reshape"(%1475) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2263)
        "ttnn.deallocate"(%1475) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2263)
        %1479 = "ttnn.reshape"(%1476) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2264)
        "ttnn.deallocate"(%1476) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2264)
        %1480 = "ttnn.reshape"(%1477) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2265)
        "ttnn.deallocate"(%1477) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2265)
        %1481 = "ttnn.permute"(%1478) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2266)
        "ttnn.deallocate"(%1478) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2266)
        %1482 = "ttnn.permute"(%1479) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2267)
        "ttnn.deallocate"(%1479) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2267)
        %1483 = "ttnn.permute"(%1480) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2268)
        "ttnn.deallocate"(%1480) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2268)
        %1484 = "ttnn.typecast"(%1481) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1494)
        "ttnn.deallocate"(%1481) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1494)
        %1485 = "ttnn.multiply"(%1484, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1495)
        "ttnn.deallocate"(%1484) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1495)
        %1486 = "ttnn.typecast"(%1482) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1496)
        "ttnn.deallocate"(%1482) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1496)
        %1487 = "ttnn.permute"(%1486) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1496)
        "ttnn.deallocate"(%1486) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1496)
        %1488 = "ttnn.multiply"(%1487, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1497)
        "ttnn.deallocate"(%1487) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1497)
        %1489 = "ttnn.matmul"(%1485, %1488) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1498)
        "ttnn.deallocate"(%1488) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1498)
        "ttnn.deallocate"(%1485) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1498)
        %1490 = "ttnn.eq"(%1489, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1499)
        %1491 = "ttnn.logical_not"(%1490) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1500)
        "ttnn.deallocate"(%1490) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1500)
        %1492 = "ttnn.sum"(%1491) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1501)
        "ttnn.deallocate"(%1491) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1501)
        %1493 = "ttnn.ne"(%1492, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1501)
        "ttnn.deallocate"(%1492) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1501)
        %1494 = "ttnn.logical_not"(%1493) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1502)
        "ttnn.deallocate"(%1493) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1502)
        %1495 = "ttnn.reshape"(%1494) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1502)
        "ttnn.deallocate"(%1494) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1502)
        %1496 = "ttnn.softmax"(%1489) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1503)
        "ttnn.deallocate"(%1489) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1503)
        %1497 = "ttnn.repeat"(%1495) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1504)
        "ttnn.deallocate"(%1495) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1504)
        %1498 = "ttnn.typecast"(%1497) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2269)
        "ttnn.deallocate"(%1497) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2269)
        %1499 = "ttnn.where"(%1498, %59, %1496) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1504)
        "ttnn.deallocate"(%1498) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1504)
        "ttnn.deallocate"(%1496) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1504)
        %1500 = "ttnn.typecast"(%1483) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1505)
        "ttnn.deallocate"(%1483) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1505)
        %1501 = "ttnn.matmul"(%1499, %1500) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1506)
        "ttnn.deallocate"(%1500) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1506)
        "ttnn.deallocate"(%1499) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1506)
        %1502 = "ttnn.typecast"(%1501) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1507)
        "ttnn.deallocate"(%1501) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1507)
        %1503 = "ttnn.permute"(%1502) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2270)
        "ttnn.deallocate"(%1502) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2270)
        %1504 = "ttnn.reshape"(%1503) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1508)
        "ttnn.deallocate"(%1503) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1508)
        %1505 = "ttnn.matmul"(%1504, %arg44) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2271)
        "ttnn.deallocate"(%1504) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2271)
        "ttnn.deallocate"(%arg44) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2271)
        %1506 = "ttnn.add"(%1505, %62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2272)
        "ttnn.deallocate"(%1505) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2272)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2272)
        %1507 = "ttnn.add"(%1470, %1506) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1510)
        "ttnn.deallocate"(%1506) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1510)
        "ttnn.deallocate"(%1470) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1510)
        %1508 = "ttnn.layer_norm"(%1507, %arg42, %arg41) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1511)
        "ttnn.deallocate"(%arg42) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1511)
        "ttnn.deallocate"(%arg41) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1511)
        %1509 = "ttnn.reshape"(%1508) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1512)
        "ttnn.deallocate"(%1508) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1512)
        %1510 = "ttnn.matmul"(%1509, %arg40) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2273)
        "ttnn.deallocate"(%1509) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2273)
        "ttnn.deallocate"(%arg40) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2273)
        %1511 = "ttnn.add"(%1510, %144) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2274)
        "ttnn.deallocate"(%1510) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2274)
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2274)
        %1512 = "ttnn.gelu"(%1511) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2275)
        "ttnn.deallocate"(%1511) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2275)
        %1513 = "ttnn.reshape"(%1512) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1514)
        "ttnn.deallocate"(%1512) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1514)
        %1514 = "ttnn.matmul"(%1513, %arg38) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2276)
        "ttnn.deallocate"(%1513) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2276)
        "ttnn.deallocate"(%arg38) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2276)
        %1515 = "ttnn.add"(%1514, %64) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2277)
        "ttnn.deallocate"(%1514) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2277)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2277)
        %1516 = "ttnn.add"(%1507, %1515) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1516)
        "ttnn.deallocate"(%1515) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1516)
        "ttnn.deallocate"(%1507) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1516)
        %1517 = "ttnn.layer_norm"(%1516, %arg36, %arg35) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1517)
        "ttnn.deallocate"(%arg36) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1517)
        "ttnn.deallocate"(%arg35) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1517)
        %1518 = "ttnn.reshape"(%1517) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1518)
        "ttnn.deallocate"(%1517) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1518)
        %1519 = "ttnn.matmul"(%1518, %74) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2278)
        "ttnn.deallocate"(%1518) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2278)
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2278)
        %1520 = "ttnn.add"(%1519, %15) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2279)
        "ttnn.deallocate"(%1519) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2279)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2279)
        %1521 = "ttnn.slice_static"(%1520) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2280)
        %1522 = "ttnn.slice_static"(%1520) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2281)
        %1523 = "ttnn.slice_static"(%1520) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2282)
        "ttnn.deallocate"(%1520) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2282)
        %1524 = "ttnn.reshape"(%1521) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2283)
        "ttnn.deallocate"(%1521) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2283)
        %1525 = "ttnn.reshape"(%1522) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2284)
        "ttnn.deallocate"(%1522) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2284)
        %1526 = "ttnn.reshape"(%1523) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2285)
        "ttnn.deallocate"(%1523) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2285)
        %1527 = "ttnn.permute"(%1524) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2286)
        "ttnn.deallocate"(%1524) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2286)
        %1528 = "ttnn.permute"(%1525) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2287)
        "ttnn.deallocate"(%1525) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2287)
        %1529 = "ttnn.permute"(%1526) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2288)
        "ttnn.deallocate"(%1526) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2288)
        %1530 = "ttnn.typecast"(%1527) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1520)
        "ttnn.deallocate"(%1527) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1520)
        %1531 = "ttnn.multiply"(%1530, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1521)
        "ttnn.deallocate"(%1530) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1521)
        %1532 = "ttnn.typecast"(%1528) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1522)
        "ttnn.deallocate"(%1528) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1522)
        %1533 = "ttnn.permute"(%1532) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1522)
        "ttnn.deallocate"(%1532) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1522)
        %1534 = "ttnn.multiply"(%1533, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1523)
        "ttnn.deallocate"(%1533) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1523)
        %1535 = "ttnn.matmul"(%1531, %1534) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1524)
        "ttnn.deallocate"(%1534) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1524)
        "ttnn.deallocate"(%1531) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1524)
        %1536 = "ttnn.eq"(%1535, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1525)
        %1537 = "ttnn.logical_not"(%1536) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1526)
        "ttnn.deallocate"(%1536) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1526)
        %1538 = "ttnn.sum"(%1537) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1527)
        "ttnn.deallocate"(%1537) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1527)
        %1539 = "ttnn.ne"(%1538, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1527)
        "ttnn.deallocate"(%1538) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1527)
        %1540 = "ttnn.logical_not"(%1539) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1528)
        "ttnn.deallocate"(%1539) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1528)
        %1541 = "ttnn.reshape"(%1540) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1528)
        "ttnn.deallocate"(%1540) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1528)
        %1542 = "ttnn.softmax"(%1535) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1529)
        "ttnn.deallocate"(%1535) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1529)
        %1543 = "ttnn.repeat"(%1541) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1530)
        "ttnn.deallocate"(%1541) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1530)
        %1544 = "ttnn.typecast"(%1543) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2289)
        "ttnn.deallocate"(%1543) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2289)
        %1545 = "ttnn.where"(%1544, %59, %1542) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1530)
        "ttnn.deallocate"(%1544) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1530)
        "ttnn.deallocate"(%1542) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1530)
        %1546 = "ttnn.typecast"(%1529) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1531)
        "ttnn.deallocate"(%1529) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1531)
        %1547 = "ttnn.matmul"(%1545, %1546) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1532)
        "ttnn.deallocate"(%1546) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1532)
        "ttnn.deallocate"(%1545) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1532)
        %1548 = "ttnn.typecast"(%1547) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1533)
        "ttnn.deallocate"(%1547) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1533)
        %1549 = "ttnn.permute"(%1548) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2290)
        "ttnn.deallocate"(%1548) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2290)
        %1550 = "ttnn.reshape"(%1549) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1534)
        "ttnn.deallocate"(%1549) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1534)
        %1551 = "ttnn.matmul"(%1550, %arg32) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2291)
        "ttnn.deallocate"(%1550) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2291)
        "ttnn.deallocate"(%arg32) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2291)
        %1552 = "ttnn.add"(%1551, %139) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2292)
        "ttnn.deallocate"(%1551) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2292)
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2292)
        %1553 = "ttnn.add"(%1516, %1552) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1536)
        "ttnn.deallocate"(%1552) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1536)
        "ttnn.deallocate"(%1516) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1536)
        %1554 = "ttnn.layer_norm"(%1553, %arg30, %arg29) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1537)
        "ttnn.deallocate"(%arg30) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1537)
        "ttnn.deallocate"(%arg29) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1537)
        %1555 = "ttnn.reshape"(%1554) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1538)
        "ttnn.deallocate"(%1554) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1538)
        %1556 = "ttnn.matmul"(%1555, %arg28) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2293)
        "ttnn.deallocate"(%1555) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2293)
        "ttnn.deallocate"(%arg28) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2293)
        %1557 = "ttnn.add"(%1556, %69) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2294)
        "ttnn.deallocate"(%1556) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2294)
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2294)
        %1558 = "ttnn.gelu"(%1557) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2295)
        "ttnn.deallocate"(%1557) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2295)
        %1559 = "ttnn.reshape"(%1558) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1540)
        "ttnn.deallocate"(%1558) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1540)
        %1560 = "ttnn.matmul"(%1559, %arg26) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2296)
        "ttnn.deallocate"(%1559) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2296)
        "ttnn.deallocate"(%arg26) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2296)
        %1561 = "ttnn.add"(%1560, %147) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2297)
        "ttnn.deallocate"(%1560) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2297)
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2297)
        %1562 = "ttnn.add"(%1553, %1561) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1542)
        "ttnn.deallocate"(%1561) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1542)
        "ttnn.deallocate"(%1553) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1542)
        %1563 = "ttnn.layer_norm"(%1562, %arg24, %arg23) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1543)
        "ttnn.deallocate"(%arg24) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1543)
        "ttnn.deallocate"(%arg23) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1543)
        %1564 = "ttnn.reshape"(%1563) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1544)
        "ttnn.deallocate"(%1563) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1544)
        %1565 = "ttnn.matmul"(%1564, %4) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<3840x1280xbf16, #ttnn_layout12>) -> tensor<257x3840xbf16, #ttnn_layout53> loc(#loc2298)
        "ttnn.deallocate"(%1564) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2298)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<3840x1280xbf16, #ttnn_layout12>) -> () loc(#loc2298)
        %1566 = "ttnn.add"(%1565, %54) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x3840xbf16, #ttnn_layout53>, tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x3840xbf16, #ttnn_layout15> loc(#loc2299)
        "ttnn.deallocate"(%1565) <{force = false}> : (tensor<257x3840xbf16, #ttnn_layout53>) -> () loc(#loc2299)
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2299)
        %1567 = "ttnn.slice_static"(%1566) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 257 : i32, 1280 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2300)
        %1568 = "ttnn.slice_static"(%1566) <{begins = [0 : i32, 0 : i32, 1280 : i32], ends = [1 : i32, 257 : i32, 2560 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2301)
        %1569 = "ttnn.slice_static"(%1566) <{begins = [0 : i32, 0 : i32, 2560 : i32], ends = [1 : i32, 257 : i32, 3840 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2302)
        "ttnn.deallocate"(%1566) <{force = false}> : (tensor<1x257x3840xbf16, #ttnn_layout15>) -> () loc(#loc2302)
        %1570 = "ttnn.reshape"(%1567) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2303)
        "ttnn.deallocate"(%1567) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2303)
        %1571 = "ttnn.reshape"(%1568) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2304)
        "ttnn.deallocate"(%1568) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2304)
        %1572 = "ttnn.reshape"(%1569) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2305)
        "ttnn.deallocate"(%1569) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2305)
        %1573 = "ttnn.permute"(%1570) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2306)
        "ttnn.deallocate"(%1570) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2306)
        %1574 = "ttnn.permute"(%1571) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2307)
        "ttnn.deallocate"(%1571) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2307)
        %1575 = "ttnn.permute"(%1572) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc2308)
        "ttnn.deallocate"(%1572) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc2308)
        %1576 = "ttnn.typecast"(%1573) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1546)
        "ttnn.deallocate"(%1573) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1546)
        %1577 = "ttnn.multiply"(%1576, %151) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1547)
        "ttnn.deallocate"(%1576) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1547)
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1547)
        %1578 = "ttnn.typecast"(%1574) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1548)
        "ttnn.deallocate"(%1574) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1548)
        %1579 = "ttnn.permute"(%1578) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1548)
        "ttnn.deallocate"(%1578) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1548)
        %1580 = "ttnn.multiply"(%1579, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x80x257xf32, #ttnn_layout16>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x80x257xf32, #ttnn_layout16> loc(#loc1549)
        "ttnn.deallocate"(%1579) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1549)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1549)
        %1581 = "ttnn.matmul"(%1577, %1580) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>, tensor<1x16x80x257xf32, #ttnn_layout16>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1550)
        "ttnn.deallocate"(%1580) <{force = false}> : (tensor<1x16x80x257xf32, #ttnn_layout16>) -> () loc(#loc1550)
        "ttnn.deallocate"(%1577) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1550)
        %1582 = "ttnn.eq"(%1581, %101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1551)
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1551)
        %1583 = "ttnn.logical_not"(%1582) : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1552)
        "ttnn.deallocate"(%1582) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1552)
        %1584 = "ttnn.sum"(%1583) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1553)
        "ttnn.deallocate"(%1583) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc1553)
        %1585 = "ttnn.ne"(%1584, %136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257xbf16, #ttnn_layout34>, tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1553)
        "ttnn.deallocate"(%1584) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1553)
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1553)
        %1586 = "ttnn.logical_not"(%1585) : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257xbf16, #ttnn_layout34> loc(#loc1554)
        "ttnn.deallocate"(%1585) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1554)
        %1587 = "ttnn.reshape"(%1586) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> tensor<1x16x257x1xbf16, #ttnn_layout57> loc(#loc1554)
        "ttnn.deallocate"(%1586) <{force = false}> : (tensor<1x16x257xbf16, #ttnn_layout34>) -> () loc(#loc1554)
        %1588 = "ttnn.softmax"(%1581) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1555)
        "ttnn.deallocate"(%1581) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1555)
        %1589 = "ttnn.repeat"(%1587) <{repeat_dims = #ttnn.shape<1x1x1x257>}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> tensor<1x16x257x257xbf16, #ttnn_layout56> loc(#loc1556)
        "ttnn.deallocate"(%1587) <{force = false}> : (tensor<1x16x257x1xbf16, #ttnn_layout57>) -> () loc(#loc1556)
        %1590 = "ttnn.typecast"(%1589) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc2309)
        "ttnn.deallocate"(%1589) <{force = false}> : (tensor<1x16x257x257xbf16, #ttnn_layout56>) -> () loc(#loc2309)
        %1591 = "ttnn.where"(%1590, %59, %1588) : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x257xf32, #ttnn_layout32>) -> tensor<1x16x257x257xf32, #ttnn_layout32> loc(#loc1556)
        "ttnn.deallocate"(%1590) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1556)
        "ttnn.deallocate"(%1588) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1556)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1556)
        %1592 = "ttnn.typecast"(%1575) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1557)
        "ttnn.deallocate"(%1575) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc1557)
        %1593 = "ttnn.matmul"(%1591, %1592) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>, tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xf32, #ttnn_layout39> loc(#loc1558)
        "ttnn.deallocate"(%1592) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1558)
        "ttnn.deallocate"(%1591) <{force = false}> : (tensor<1x16x257x257xf32, #ttnn_layout32>) -> () loc(#loc1558)
        %1594 = "ttnn.typecast"(%1593) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> tensor<1x16x257x80xbf16, #ttnn_layout55> loc(#loc1559)
        "ttnn.deallocate"(%1593) <{force = false}> : (tensor<1x16x257x80xf32, #ttnn_layout39>) -> () loc(#loc1559)
        %1595 = "ttnn.permute"(%1594) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> tensor<1x257x16x80xbf16, #ttnn_layout54> loc(#loc2310)
        "ttnn.deallocate"(%1594) <{force = false}> : (tensor<1x16x257x80xbf16, #ttnn_layout55>) -> () loc(#loc2310)
        %1596 = "ttnn.reshape"(%1595) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1560)
        "ttnn.deallocate"(%1595) <{force = false}> : (tensor<1x257x16x80xbf16, #ttnn_layout54>) -> () loc(#loc1560)
        %1597 = "ttnn.matmul"(%1596, %arg20) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2311)
        "ttnn.deallocate"(%1596) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2311)
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2311)
        %1598 = "ttnn.add"(%1597, %83) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2312)
        "ttnn.deallocate"(%1597) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2312)
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2312)
        %1599 = "ttnn.add"(%1562, %1598) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1562)
        "ttnn.deallocate"(%1598) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1562)
        "ttnn.deallocate"(%1562) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1562)
        %1600 = "ttnn.layer_norm"(%1599, %arg18, %arg17) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1563)
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1563)
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1563)
        %1601 = "ttnn.reshape"(%1600) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1564)
        "ttnn.deallocate"(%1600) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1564)
        %1602 = "ttnn.matmul"(%1601, %arg16) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc2313)
        "ttnn.deallocate"(%1601) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2313)
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc2313)
        %1603 = "ttnn.add"(%1602, %42) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2314)
        "ttnn.deallocate"(%1602) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2314)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2314)
        %1604 = "ttnn.gelu"(%1603) : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<1x257x5120xbf16, #ttnn_layout7> loc(#loc2315)
        "ttnn.deallocate"(%1603) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc2315)
        %1605 = "ttnn.reshape"(%1604) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> tensor<257x5120xbf16, #ttnn_layout58> loc(#loc1566)
        "ttnn.deallocate"(%1604) <{force = false}> : (tensor<1x257x5120xbf16, #ttnn_layout7>) -> () loc(#loc1566)
        %1606 = "ttnn.matmul"(%1605, %arg14) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x5120xbf16, #ttnn_layout58>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2316)
        "ttnn.deallocate"(%1605) <{force = false}> : (tensor<257x5120xbf16, #ttnn_layout58>) -> () loc(#loc2316)
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2316)
        %1607 = "ttnn.add"(%1606, %5) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2317)
        "ttnn.deallocate"(%1606) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2317)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2317)
        %1608 = "ttnn.add"(%1599, %1607) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1568)
        "ttnn.deallocate"(%1607) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1568)
        "ttnn.deallocate"(%1599) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1568)
        %1609 = "ttnn.reshape"(%1608) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1568)
        "ttnn.deallocate"(%1608) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1568)
        %1610 = "ttnn.matmul"(%1609, %arg12) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc2318)
        "ttnn.deallocate"(%1609) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2318)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc2318)
        %1611 = "ttnn.add"(%1610, %133) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc2319)
        "ttnn.deallocate"(%1610) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc2319)
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc2319)
        %1612 = "ttnn.layer_norm"(%1611, %arg10, %arg9) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1570)
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1570)
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1570)
        %1613 = "ttnn.reshape"(%1612) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc201)
        "ttnn.deallocate"(%1612) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc201)
        %1614 = "ttnn.concat"(%1613, %145#2) <{dim = 0 : si32}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc201)
        "ttnn.deallocate"(%1613) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc201)
        "ttnn.deallocate"(%145#2) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc201)
        %1615 = "ttnn.matmul"(%1614, %arg516) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<273x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1571)
        "ttnn.deallocate"(%arg516) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1571)
        %1616 = "ttnn.typecast"(%1615) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> tensor<273x1280xf32, #ttnn_layout59> loc(#loc1572)
        "ttnn.deallocate"(%1615) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1572)
        %1617 = "ttnn.reshape"(%1616) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xf32, #ttnn_layout59>) -> tensor<1x273x20x64xf32, #ttnn_layout60> loc(#loc1572)
        "ttnn.deallocate"(%1616) <{force = false}> : (tensor<273x1280xf32, #ttnn_layout59>) -> () loc(#loc1572)
        %1618 = "ttnn.permute"(%1617) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc1572)
        "ttnn.deallocate"(%1617) <{force = false}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> () loc(#loc1572)
        %1619 = "ttnn.multiply"(%1618, %165) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x64x273xf32, #ttnn_layout40>, tensor<1x20x64x273xf32, #ttnn_layout40>) -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc1573)
        "ttnn.deallocate"(%1618) <{force = false}> : (tensor<1x20x64x273xf32, #ttnn_layout40>) -> () loc(#loc1573)
        %1620 = "ttnn.matmul"(%145#1, %1619) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x64x273xf32, #ttnn_layout40>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1574)
        "ttnn.deallocate"(%1619) <{force = false}> : (tensor<1x20x64x273xf32, #ttnn_layout40>) -> () loc(#loc1574)
        "ttnn.deallocate"(%145#1) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1574)
        %1621 = "ttnn.eq"(%1620, %53) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1575)
        %1622 = "ttnn.logical_not"(%1621) : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1576)
        "ttnn.deallocate"(%1621) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc1576)
        %1623 = "ttnn.sum"(%1622) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1577)
        "ttnn.deallocate"(%1622) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc1577)
        %1624 = "ttnn.ne"(%1623, %0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16xbf16, #ttnn_layout>, tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1577)
        "ttnn.deallocate"(%1623) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1577)
        %1625 = "ttnn.logical_not"(%1624) : (tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1578)
        "ttnn.deallocate"(%1624) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1578)
        %1626 = "ttnn.reshape"(%1625) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16x1xbf16, #ttnn_layout62> loc(#loc1578)
        "ttnn.deallocate"(%1625) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1578)
        %1627 = "ttnn.softmax"(%1620) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1579)
        "ttnn.deallocate"(%1620) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1579)
        %1628 = "ttnn.repeat"(%1626) <{repeat_dims = #ttnn.shape<1x1x1x273>}> : (tensor<1x20x16x1xbf16, #ttnn_layout62>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1580)
        "ttnn.deallocate"(%1626) <{force = false}> : (tensor<1x20x16x1xbf16, #ttnn_layout62>) -> () loc(#loc1580)
        %1629 = "ttnn.typecast"(%1628) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc2320)
        "ttnn.deallocate"(%1628) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc2320)
        %1630 = "ttnn.where"(%1629, %124, %1627) : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1580)
        "ttnn.deallocate"(%1629) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1580)
        "ttnn.deallocate"(%1627) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1580)
        %1631 = "ttnn.matmul"(%1614, %arg6) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<273x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1581)
        "ttnn.deallocate"(%1614) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1581)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1581)
        %1632 = "ttnn.typecast"(%1631) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> tensor<273x1280xf32, #ttnn_layout59> loc(#loc1582)
        "ttnn.deallocate"(%1631) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1582)
        %1633 = "ttnn.reshape"(%1632) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xf32, #ttnn_layout59>) -> tensor<1x273x20x64xf32, #ttnn_layout60> loc(#loc1582)
        "ttnn.deallocate"(%1632) <{force = false}> : (tensor<273x1280xf32, #ttnn_layout59>) -> () loc(#loc1582)
        %1634 = "ttnn.permute"(%1633) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> tensor<1x20x273x64xf32, #ttnn_layout63> loc(#loc1582)
        "ttnn.deallocate"(%1633) <{force = false}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> () loc(#loc1582)
        %1635 = "ttnn.matmul"(%1630, %1634) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x273x64xf32, #ttnn_layout63>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1583)
        "ttnn.deallocate"(%1634) <{force = false}> : (tensor<1x20x273x64xf32, #ttnn_layout63>) -> () loc(#loc1583)
        "ttnn.deallocate"(%1630) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1583)
        %1636 = "ttnn.typecast"(%1635) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> tensor<1x20x16x64xbf16, #ttnn_layout38> loc(#loc1584)
        "ttnn.deallocate"(%1635) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1584)
        %1637 = "ttnn.concatenate_heads"(%1636) : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1585)
        "ttnn.deallocate"(%1636) <{force = false}> : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> () loc(#loc1585)
        %1638 = "ttnn.reshape"(%1637) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1585)
        "ttnn.deallocate"(%1637) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1585)
        %1639 = "ttnn.matmul"(%1638, %arg5) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1586)
        "ttnn.deallocate"(%1638) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1586)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1586)
        %1640 = "ttnn.reshape"(%1639) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1587)
        "ttnn.deallocate"(%1639) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1587)
        %1641 = "ttnn.divide"(%1640, %22) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1588)
        "ttnn.deallocate"(%1640) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1588)
        %1642 = "ttnn.add"(%1641, %arg4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1589)
        "ttnn.deallocate"(%1641) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1589)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1589)
        %1643 = "ttnn.layer_norm"(%1642, %arg521, %arg520) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1590)
        "ttnn.deallocate"(%arg521) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1590)
        "ttnn.deallocate"(%arg520) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1590)
        %1644 = "ttnn.reshape"(%1643) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1591)
        "ttnn.deallocate"(%1643) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1591)
        %1645 = "ttnn.matmul"(%1644, %arg519) <{activation = "gelu", compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<16x5120xbf16, #ttnn_layout64> loc(#loc1592)
        "ttnn.deallocate"(%1644) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1592)
        "ttnn.deallocate"(%arg519) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1592)
        %1646 = "ttnn.matmul"(%1645, %arg518) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x5120xbf16, #ttnn_layout64>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1593)
        "ttnn.deallocate"(%1645) <{force = false}> : (tensor<16x5120xbf16, #ttnn_layout64>) -> () loc(#loc1593)
        "ttnn.deallocate"(%arg518) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1593)
        %1647 = "ttnn.reshape"(%1646) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1594)
        "ttnn.deallocate"(%1646) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1594)
        %1648 = "ttnn.add"(%1647, %1642) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1595)
        "ttnn.deallocate"(%1647) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1595)
        "ttnn.deallocate"(%1642) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1595)
        %1649 = "ttnn.layer_norm"(%1648, %arg525, %arg524) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1596)
        "ttnn.deallocate"(%arg525) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1596)
        "ttnn.deallocate"(%arg524) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1596)
        %1650 = "ttnn.reshape"(%1649) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1597)
        %1651 = "ttnn.matmul"(%1650, %arg529) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1598)
        "ttnn.deallocate"(%1650) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1598)
        "ttnn.deallocate"(%arg529) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1598)
        %1652 = "ttnn.typecast"(%1651) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<16x1280xf32, #ttnn_layout65> loc(#loc1599)
        "ttnn.deallocate"(%1651) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1599)
        %1653 = "ttnn.reshape"(%1652) <{shape = [1 : i32, 16 : i32, 20 : i32, 64 : i32]}> : (tensor<16x1280xf32, #ttnn_layout65>) -> tensor<1x16x20x64xf32, #ttnn_layout66> loc(#loc1599)
        "ttnn.deallocate"(%1652) <{force = false}> : (tensor<16x1280xf32, #ttnn_layout65>) -> () loc(#loc1599)
        %1654 = "ttnn.permute"(%1653) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x20x64xf32, #ttnn_layout66>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1599)
        "ttnn.deallocate"(%1653) <{force = false}> : (tensor<1x16x20x64xf32, #ttnn_layout66>) -> () loc(#loc1599)
        %1655 = "ttnn.multiply"(%1654, %145#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x16x64xf32, #ttnn_layout35>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1600)
        "ttnn.deallocate"(%1654) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1600)
        %1656 = "ttnn.layer_norm"(%1611, %arg527, %arg526) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1601)
        "ttnn.deallocate"(%arg527) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1601)
        "ttnn.deallocate"(%arg526) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1601)
        %1657 = "ttnn.reshape"(%1656) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1602)
        "ttnn.deallocate"(%1656) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1602)
        %1658 = "ttnn.reshape"(%1649) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1602)
        "ttnn.deallocate"(%1649) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1602)
        %1659 = "ttnn.concat"(%1657, %1658) <{dim = 0 : si32}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1602)
        "ttnn.deallocate"(%1658) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1602)
        "ttnn.deallocate"(%1657) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1602)
        %1660 = "ttnn.matmul"(%1659, %arg528) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<273x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1603)
        "ttnn.deallocate"(%arg528) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1603)
        %1661 = "ttnn.typecast"(%1660) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> tensor<273x1280xf32, #ttnn_layout59> loc(#loc1604)
        "ttnn.deallocate"(%1660) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1604)
        %1662 = "ttnn.reshape"(%1661) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xf32, #ttnn_layout59>) -> tensor<1x273x20x64xf32, #ttnn_layout60> loc(#loc1604)
        "ttnn.deallocate"(%1661) <{force = false}> : (tensor<273x1280xf32, #ttnn_layout59>) -> () loc(#loc1604)
        %1663 = "ttnn.permute"(%1662) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc1604)
        "ttnn.deallocate"(%1662) <{force = false}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> () loc(#loc1604)
        %1664 = "ttnn.multiply"(%1663, %165) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x64x273xf32, #ttnn_layout40>, tensor<1x20x64x273xf32, #ttnn_layout40>) -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc1605)
        "ttnn.deallocate"(%1663) <{force = false}> : (tensor<1x20x64x273xf32, #ttnn_layout40>) -> () loc(#loc1605)
        %1665 = "ttnn.matmul"(%1655, %1664) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x64x273xf32, #ttnn_layout40>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1606)
        "ttnn.deallocate"(%1664) <{force = false}> : (tensor<1x20x64x273xf32, #ttnn_layout40>) -> () loc(#loc1606)
        "ttnn.deallocate"(%1655) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1606)
        %1666 = "ttnn.eq"(%1665, %53) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1607)
        %1667 = "ttnn.logical_not"(%1666) : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1608)
        "ttnn.deallocate"(%1666) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc1608)
        %1668 = "ttnn.sum"(%1667) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1609)
        "ttnn.deallocate"(%1667) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc1609)
        %1669 = "ttnn.ne"(%1668, %0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16xbf16, #ttnn_layout>, tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1609)
        "ttnn.deallocate"(%1668) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1609)
        %1670 = "ttnn.logical_not"(%1669) : (tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1610)
        "ttnn.deallocate"(%1669) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1610)
        %1671 = "ttnn.reshape"(%1670) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16x1xbf16, #ttnn_layout62> loc(#loc1610)
        "ttnn.deallocate"(%1670) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1610)
        %1672 = "ttnn.softmax"(%1665) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1611)
        "ttnn.deallocate"(%1665) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1611)
        %1673 = "ttnn.repeat"(%1671) <{repeat_dims = #ttnn.shape<1x1x1x273>}> : (tensor<1x20x16x1xbf16, #ttnn_layout62>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1612)
        "ttnn.deallocate"(%1671) <{force = false}> : (tensor<1x20x16x1xbf16, #ttnn_layout62>) -> () loc(#loc1612)
        %1674 = "ttnn.typecast"(%1673) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc2321)
        "ttnn.deallocate"(%1673) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc2321)
        %1675 = "ttnn.where"(%1674, %124, %1672) : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1612)
        "ttnn.deallocate"(%1674) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1612)
        "ttnn.deallocate"(%1672) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1612)
        %1676 = "ttnn.matmul"(%1659, %arg523) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<273x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1613)
        "ttnn.deallocate"(%1659) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1613)
        "ttnn.deallocate"(%arg523) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1613)
        %1677 = "ttnn.typecast"(%1676) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> tensor<273x1280xf32, #ttnn_layout59> loc(#loc1614)
        "ttnn.deallocate"(%1676) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1614)
        %1678 = "ttnn.reshape"(%1677) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xf32, #ttnn_layout59>) -> tensor<1x273x20x64xf32, #ttnn_layout60> loc(#loc1614)
        "ttnn.deallocate"(%1677) <{force = false}> : (tensor<273x1280xf32, #ttnn_layout59>) -> () loc(#loc1614)
        %1679 = "ttnn.permute"(%1678) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> tensor<1x20x273x64xf32, #ttnn_layout63> loc(#loc1614)
        "ttnn.deallocate"(%1678) <{force = false}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> () loc(#loc1614)
        %1680 = "ttnn.matmul"(%1675, %1679) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x273x64xf32, #ttnn_layout63>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1615)
        "ttnn.deallocate"(%1679) <{force = false}> : (tensor<1x20x273x64xf32, #ttnn_layout63>) -> () loc(#loc1615)
        "ttnn.deallocate"(%1675) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1615)
        %1681 = "ttnn.typecast"(%1680) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> tensor<1x20x16x64xbf16, #ttnn_layout38> loc(#loc1616)
        "ttnn.deallocate"(%1680) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1616)
        %1682 = "ttnn.concatenate_heads"(%1681) : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1617)
        "ttnn.deallocate"(%1681) <{force = false}> : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> () loc(#loc1617)
        %1683 = "ttnn.reshape"(%1682) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1617)
        "ttnn.deallocate"(%1682) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1617)
        %1684 = "ttnn.matmul"(%1683, %arg522) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1618)
        "ttnn.deallocate"(%1683) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1618)
        "ttnn.deallocate"(%arg522) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1618)
        %1685 = "ttnn.reshape"(%1684) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1619)
        "ttnn.deallocate"(%1684) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1619)
        %1686 = "ttnn.divide"(%1685, %22) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1620)
        "ttnn.deallocate"(%1685) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1620)
        %1687 = "ttnn.add"(%1686, %1648) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1621)
        "ttnn.deallocate"(%1686) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1621)
        "ttnn.deallocate"(%1648) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1621)
        %1688 = "ttnn.layer_norm"(%1687, %arg533, %arg532) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1622)
        "ttnn.deallocate"(%arg533) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1622)
        "ttnn.deallocate"(%arg532) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1622)
        %1689 = "ttnn.reshape"(%1688) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1623)
        "ttnn.deallocate"(%1688) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1623)
        %1690 = "ttnn.matmul"(%1689, %arg531) <{activation = "gelu", compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<16x5120xbf16, #ttnn_layout64> loc(#loc1624)
        "ttnn.deallocate"(%1689) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1624)
        "ttnn.deallocate"(%arg531) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1624)
        %1691 = "ttnn.matmul"(%1690, %arg530) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x5120xbf16, #ttnn_layout64>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1625)
        "ttnn.deallocate"(%1690) <{force = false}> : (tensor<16x5120xbf16, #ttnn_layout64>) -> () loc(#loc1625)
        "ttnn.deallocate"(%arg530) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1625)
        %1692 = "ttnn.reshape"(%1691) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1626)
        "ttnn.deallocate"(%1691) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1626)
        %1693 = "ttnn.add"(%1692, %1687) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1627)
        "ttnn.deallocate"(%1692) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1627)
        "ttnn.deallocate"(%1687) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1627)
        %1694 = "ttnn.layer_norm"(%1693, %arg537, %arg536) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1628)
        "ttnn.deallocate"(%arg537) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1628)
        "ttnn.deallocate"(%arg536) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1628)
        %1695 = "ttnn.reshape"(%1694) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1629)
        %1696 = "ttnn.matmul"(%1695, %arg541) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1630)
        "ttnn.deallocate"(%1695) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1630)
        "ttnn.deallocate"(%arg541) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1630)
        %1697 = "ttnn.typecast"(%1696) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<16x1280xf32, #ttnn_layout65> loc(#loc1631)
        "ttnn.deallocate"(%1696) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1631)
        %1698 = "ttnn.reshape"(%1697) <{shape = [1 : i32, 16 : i32, 20 : i32, 64 : i32]}> : (tensor<16x1280xf32, #ttnn_layout65>) -> tensor<1x16x20x64xf32, #ttnn_layout66> loc(#loc1631)
        "ttnn.deallocate"(%1697) <{force = false}> : (tensor<16x1280xf32, #ttnn_layout65>) -> () loc(#loc1631)
        %1699 = "ttnn.permute"(%1698) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x20x64xf32, #ttnn_layout66>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1631)
        "ttnn.deallocate"(%1698) <{force = false}> : (tensor<1x16x20x64xf32, #ttnn_layout66>) -> () loc(#loc1631)
        %1700 = "ttnn.multiply"(%1699, %145#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x16x64xf32, #ttnn_layout35>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1632)
        "ttnn.deallocate"(%1699) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1632)
        %1701 = "ttnn.layer_norm"(%1611, %arg539, %arg538) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1633)
        "ttnn.deallocate"(%arg539) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1633)
        "ttnn.deallocate"(%arg538) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1633)
        %1702 = "ttnn.reshape"(%1701) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1634)
        "ttnn.deallocate"(%1701) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1634)
        %1703 = "ttnn.reshape"(%1694) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1634)
        "ttnn.deallocate"(%1694) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1634)
        %1704 = "ttnn.concat"(%1702, %1703) <{dim = 0 : si32}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1634)
        "ttnn.deallocate"(%1703) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1634)
        "ttnn.deallocate"(%1702) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1634)
        %1705 = "ttnn.matmul"(%1704, %arg540) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<273x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1635)
        "ttnn.deallocate"(%arg540) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1635)
        %1706 = "ttnn.typecast"(%1705) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> tensor<273x1280xf32, #ttnn_layout59> loc(#loc1636)
        "ttnn.deallocate"(%1705) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1636)
        %1707 = "ttnn.reshape"(%1706) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xf32, #ttnn_layout59>) -> tensor<1x273x20x64xf32, #ttnn_layout60> loc(#loc1636)
        "ttnn.deallocate"(%1706) <{force = false}> : (tensor<273x1280xf32, #ttnn_layout59>) -> () loc(#loc1636)
        %1708 = "ttnn.permute"(%1707) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc1636)
        "ttnn.deallocate"(%1707) <{force = false}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> () loc(#loc1636)
        %1709 = "ttnn.multiply"(%1708, %165) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x64x273xf32, #ttnn_layout40>, tensor<1x20x64x273xf32, #ttnn_layout40>) -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc1637)
        "ttnn.deallocate"(%1708) <{force = false}> : (tensor<1x20x64x273xf32, #ttnn_layout40>) -> () loc(#loc1637)
        %1710 = "ttnn.matmul"(%1700, %1709) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x64x273xf32, #ttnn_layout40>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1638)
        "ttnn.deallocate"(%1709) <{force = false}> : (tensor<1x20x64x273xf32, #ttnn_layout40>) -> () loc(#loc1638)
        "ttnn.deallocate"(%1700) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1638)
        %1711 = "ttnn.eq"(%1710, %53) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1639)
        %1712 = "ttnn.logical_not"(%1711) : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1640)
        "ttnn.deallocate"(%1711) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc1640)
        %1713 = "ttnn.sum"(%1712) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1641)
        "ttnn.deallocate"(%1712) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc1641)
        %1714 = "ttnn.ne"(%1713, %0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16xbf16, #ttnn_layout>, tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1641)
        "ttnn.deallocate"(%1713) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1641)
        %1715 = "ttnn.logical_not"(%1714) : (tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1642)
        "ttnn.deallocate"(%1714) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1642)
        %1716 = "ttnn.reshape"(%1715) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16x1xbf16, #ttnn_layout62> loc(#loc1642)
        "ttnn.deallocate"(%1715) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1642)
        %1717 = "ttnn.softmax"(%1710) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1643)
        "ttnn.deallocate"(%1710) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1643)
        %1718 = "ttnn.repeat"(%1716) <{repeat_dims = #ttnn.shape<1x1x1x273>}> : (tensor<1x20x16x1xbf16, #ttnn_layout62>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1644)
        "ttnn.deallocate"(%1716) <{force = false}> : (tensor<1x20x16x1xbf16, #ttnn_layout62>) -> () loc(#loc1644)
        %1719 = "ttnn.typecast"(%1718) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc2322)
        "ttnn.deallocate"(%1718) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc2322)
        %1720 = "ttnn.where"(%1719, %124, %1717) : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1644)
        "ttnn.deallocate"(%1719) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1644)
        "ttnn.deallocate"(%1717) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1644)
        %1721 = "ttnn.matmul"(%1704, %arg535) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<273x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1645)
        "ttnn.deallocate"(%1704) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1645)
        "ttnn.deallocate"(%arg535) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1645)
        %1722 = "ttnn.typecast"(%1721) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> tensor<273x1280xf32, #ttnn_layout59> loc(#loc1646)
        "ttnn.deallocate"(%1721) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1646)
        %1723 = "ttnn.reshape"(%1722) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xf32, #ttnn_layout59>) -> tensor<1x273x20x64xf32, #ttnn_layout60> loc(#loc1646)
        "ttnn.deallocate"(%1722) <{force = false}> : (tensor<273x1280xf32, #ttnn_layout59>) -> () loc(#loc1646)
        %1724 = "ttnn.permute"(%1723) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> tensor<1x20x273x64xf32, #ttnn_layout63> loc(#loc1646)
        "ttnn.deallocate"(%1723) <{force = false}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> () loc(#loc1646)
        %1725 = "ttnn.matmul"(%1720, %1724) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x273x64xf32, #ttnn_layout63>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1647)
        "ttnn.deallocate"(%1724) <{force = false}> : (tensor<1x20x273x64xf32, #ttnn_layout63>) -> () loc(#loc1647)
        "ttnn.deallocate"(%1720) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1647)
        %1726 = "ttnn.typecast"(%1725) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> tensor<1x20x16x64xbf16, #ttnn_layout38> loc(#loc1648)
        "ttnn.deallocate"(%1725) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1648)
        %1727 = "ttnn.concatenate_heads"(%1726) : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1649)
        "ttnn.deallocate"(%1726) <{force = false}> : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> () loc(#loc1649)
        %1728 = "ttnn.reshape"(%1727) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1649)
        "ttnn.deallocate"(%1727) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1649)
        %1729 = "ttnn.matmul"(%1728, %arg534) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1650)
        "ttnn.deallocate"(%1728) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1650)
        "ttnn.deallocate"(%arg534) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1650)
        %1730 = "ttnn.reshape"(%1729) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1651)
        "ttnn.deallocate"(%1729) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1651)
        %1731 = "ttnn.divide"(%1730, %22) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1652)
        "ttnn.deallocate"(%1730) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1652)
        %1732 = "ttnn.add"(%1731, %1693) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1653)
        "ttnn.deallocate"(%1731) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1653)
        "ttnn.deallocate"(%1693) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1653)
        %1733 = "ttnn.layer_norm"(%1732, %arg545, %arg544) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1654)
        "ttnn.deallocate"(%arg545) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1654)
        "ttnn.deallocate"(%arg544) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1654)
        %1734 = "ttnn.reshape"(%1733) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1655)
        "ttnn.deallocate"(%1733) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1655)
        %1735 = "ttnn.matmul"(%1734, %arg543) <{activation = "gelu", compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<16x5120xbf16, #ttnn_layout64> loc(#loc1656)
        "ttnn.deallocate"(%1734) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1656)
        "ttnn.deallocate"(%arg543) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1656)
        %1736 = "ttnn.matmul"(%1735, %arg542) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x5120xbf16, #ttnn_layout64>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1657)
        "ttnn.deallocate"(%1735) <{force = false}> : (tensor<16x5120xbf16, #ttnn_layout64>) -> () loc(#loc1657)
        "ttnn.deallocate"(%arg542) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc1657)
        %1737 = "ttnn.reshape"(%1736) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1658)
        "ttnn.deallocate"(%1736) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1658)
        %1738 = "ttnn.add"(%1737, %1732) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1659)
        "ttnn.deallocate"(%1737) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1659)
        "ttnn.deallocate"(%1732) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1659)
        %1739 = "ttnn.layer_norm"(%1738, %arg549, %arg548) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1660)
        "ttnn.deallocate"(%arg549) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1660)
        "ttnn.deallocate"(%arg548) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1660)
        %1740 = "ttnn.reshape"(%1739) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1661)
        %1741 = "ttnn.matmul"(%1740, %arg553) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1662)
        "ttnn.deallocate"(%1740) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1662)
        "ttnn.deallocate"(%arg553) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1662)
        %1742 = "ttnn.typecast"(%1741) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<16x1280xf32, #ttnn_layout65> loc(#loc1663)
        "ttnn.deallocate"(%1741) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1663)
        %1743 = "ttnn.reshape"(%1742) <{shape = [1 : i32, 16 : i32, 20 : i32, 64 : i32]}> : (tensor<16x1280xf32, #ttnn_layout65>) -> tensor<1x16x20x64xf32, #ttnn_layout66> loc(#loc1663)
        "ttnn.deallocate"(%1742) <{force = false}> : (tensor<16x1280xf32, #ttnn_layout65>) -> () loc(#loc1663)
        %1744 = "ttnn.permute"(%1743) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x20x64xf32, #ttnn_layout66>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1663)
        "ttnn.deallocate"(%1743) <{force = false}> : (tensor<1x16x20x64xf32, #ttnn_layout66>) -> () loc(#loc1663)
        %1745 = "ttnn.multiply"(%1744, %145#0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x16x64xf32, #ttnn_layout35>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1664)
        "ttnn.deallocate"(%1744) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1664)
        "ttnn.deallocate"(%145#0) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1664)
        %1746 = "ttnn.layer_norm"(%1611, %arg551, %arg550) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16, #ttnn_layout2>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x257x1280xbf16, #ttnn_layout2> loc(#loc1665)
        "ttnn.deallocate"(%1611) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1665)
        "ttnn.deallocate"(%arg551) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1665)
        "ttnn.deallocate"(%arg550) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1665)
        %1747 = "ttnn.reshape"(%1746) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> tensor<257x1280xbf16, #ttnn_layout52> loc(#loc1666)
        "ttnn.deallocate"(%1746) <{force = false}> : (tensor<1x257x1280xbf16, #ttnn_layout2>) -> () loc(#loc1666)
        %1748 = "ttnn.reshape"(%1739) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1666)
        "ttnn.deallocate"(%1739) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1666)
        %1749 = "ttnn.concat"(%1747, %1748) <{dim = 0 : si32}> : (tensor<257x1280xbf16, #ttnn_layout52>, tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1666)
        "ttnn.deallocate"(%1748) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1666)
        "ttnn.deallocate"(%1747) <{force = false}> : (tensor<257x1280xbf16, #ttnn_layout52>) -> () loc(#loc1666)
        %1750 = "ttnn.matmul"(%1749, %arg552) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<273x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1667)
        "ttnn.deallocate"(%arg552) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1667)
        %1751 = "ttnn.typecast"(%1750) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> tensor<273x1280xf32, #ttnn_layout59> loc(#loc1668)
        "ttnn.deallocate"(%1750) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1668)
        %1752 = "ttnn.reshape"(%1751) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xf32, #ttnn_layout59>) -> tensor<1x273x20x64xf32, #ttnn_layout60> loc(#loc1668)
        "ttnn.deallocate"(%1751) <{force = false}> : (tensor<273x1280xf32, #ttnn_layout59>) -> () loc(#loc1668)
        %1753 = "ttnn.permute"(%1752) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc1668)
        "ttnn.deallocate"(%1752) <{force = false}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> () loc(#loc1668)
        %1754 = "ttnn.multiply"(%1753, %165) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x64x273xf32, #ttnn_layout40>, tensor<1x20x64x273xf32, #ttnn_layout40>) -> tensor<1x20x64x273xf32, #ttnn_layout40> loc(#loc1669)
        "ttnn.deallocate"(%1753) <{force = false}> : (tensor<1x20x64x273xf32, #ttnn_layout40>) -> () loc(#loc1669)
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x20x64x273xf32, #ttnn_layout40>) -> () loc(#loc1669)
        %1755 = "ttnn.matmul"(%1745, %1754) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>, tensor<1x20x64x273xf32, #ttnn_layout40>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1670)
        "ttnn.deallocate"(%1754) <{force = false}> : (tensor<1x20x64x273xf32, #ttnn_layout40>) -> () loc(#loc1670)
        "ttnn.deallocate"(%1745) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1670)
        %1756 = "ttnn.eq"(%1755, %53) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1671)
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1671)
        %1757 = "ttnn.logical_not"(%1756) : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1672)
        "ttnn.deallocate"(%1756) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc1672)
        %1758 = "ttnn.sum"(%1757) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1673)
        "ttnn.deallocate"(%1757) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc1673)
        %1759 = "ttnn.ne"(%1758, %0) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16xbf16, #ttnn_layout>, tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1673)
        "ttnn.deallocate"(%1758) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1673)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1673)
        %1760 = "ttnn.logical_not"(%1759) : (tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16xbf16, #ttnn_layout> loc(#loc1674)
        "ttnn.deallocate"(%1759) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1674)
        %1761 = "ttnn.reshape"(%1760) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> tensor<1x20x16x1xbf16, #ttnn_layout62> loc(#loc1674)
        "ttnn.deallocate"(%1760) <{force = false}> : (tensor<1x20x16xbf16, #ttnn_layout>) -> () loc(#loc1674)
        %1762 = "ttnn.softmax"(%1755) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, dimension = 3 : si32, numericStable = true}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1675)
        "ttnn.deallocate"(%1755) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1675)
        %1763 = "ttnn.repeat"(%1761) <{repeat_dims = #ttnn.shape<1x1x1x273>}> : (tensor<1x20x16x1xbf16, #ttnn_layout62>) -> tensor<1x20x16x273xbf16, #ttnn_layout61> loc(#loc1676)
        "ttnn.deallocate"(%1761) <{force = false}> : (tensor<1x20x16x1xbf16, #ttnn_layout62>) -> () loc(#loc1676)
        %1764 = "ttnn.typecast"(%1763) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc2323)
        "ttnn.deallocate"(%1763) <{force = false}> : (tensor<1x20x16x273xbf16, #ttnn_layout61>) -> () loc(#loc2323)
        %1765 = "ttnn.where"(%1764, %124, %1762) : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x16x273xf32, #ttnn_layout23>) -> tensor<1x20x16x273xf32, #ttnn_layout23> loc(#loc1676)
        "ttnn.deallocate"(%1764) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1676)
        "ttnn.deallocate"(%1762) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1676)
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1676)
        %1766 = "ttnn.matmul"(%1749, %arg547) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<273x1280xbf16, #ttnn_layout52>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<273x1280xbf16, #ttnn_layout52> loc(#loc1677)
        "ttnn.deallocate"(%1749) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1677)
        "ttnn.deallocate"(%arg547) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1677)
        %1767 = "ttnn.typecast"(%1766) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> tensor<273x1280xf32, #ttnn_layout59> loc(#loc1678)
        "ttnn.deallocate"(%1766) <{force = false}> : (tensor<273x1280xbf16, #ttnn_layout52>) -> () loc(#loc1678)
        %1768 = "ttnn.reshape"(%1767) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xf32, #ttnn_layout59>) -> tensor<1x273x20x64xf32, #ttnn_layout60> loc(#loc1678)
        "ttnn.deallocate"(%1767) <{force = false}> : (tensor<273x1280xf32, #ttnn_layout59>) -> () loc(#loc1678)
        %1769 = "ttnn.permute"(%1768) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> tensor<1x20x273x64xf32, #ttnn_layout63> loc(#loc1678)
        "ttnn.deallocate"(%1768) <{force = false}> : (tensor<1x273x20x64xf32, #ttnn_layout60>) -> () loc(#loc1678)
        %1770 = "ttnn.matmul"(%1765, %1769) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>, tensor<1x20x273x64xf32, #ttnn_layout63>) -> tensor<1x20x16x64xf32, #ttnn_layout35> loc(#loc1679)
        "ttnn.deallocate"(%1769) <{force = false}> : (tensor<1x20x273x64xf32, #ttnn_layout63>) -> () loc(#loc1679)
        "ttnn.deallocate"(%1765) <{force = false}> : (tensor<1x20x16x273xf32, #ttnn_layout23>) -> () loc(#loc1679)
        %1771 = "ttnn.typecast"(%1770) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> tensor<1x20x16x64xbf16, #ttnn_layout38> loc(#loc1680)
        "ttnn.deallocate"(%1770) <{force = false}> : (tensor<1x20x16x64xf32, #ttnn_layout35>) -> () loc(#loc1680)
        %1772 = "ttnn.concatenate_heads"(%1771) : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1681)
        "ttnn.deallocate"(%1771) <{force = false}> : (tensor<1x20x16x64xbf16, #ttnn_layout38>) -> () loc(#loc1681)
        %1773 = "ttnn.reshape"(%1772) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1681)
        "ttnn.deallocate"(%1772) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1681)
        %1774 = "ttnn.matmul"(%1773, %arg546) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<1280x1280xbf16, #ttnn_layout14>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1682)
        "ttnn.deallocate"(%1773) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1682)
        "ttnn.deallocate"(%arg546) <{force = false}> : (tensor<1280x1280xbf16, #ttnn_layout14>) -> () loc(#loc1682)
        %1775 = "ttnn.reshape"(%1774) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1683)
        "ttnn.deallocate"(%1774) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1683)
        %1776 = "ttnn.divide"(%1775, %22) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1684)
        "ttnn.deallocate"(%1775) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1684)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1684)
        %1777 = "ttnn.add"(%1776, %1738) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1685)
        "ttnn.deallocate"(%1776) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1685)
        "ttnn.deallocate"(%1738) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1685)
        %1778 = "ttnn.layer_norm"(%1777, %arg557, %arg556) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16, #ttnn_layout5>, tensor<1280xbf16, #ttnn_layout4>, tensor<1280xbf16, #ttnn_layout4>) -> tensor<1x16x1280xbf16, #ttnn_layout5> loc(#loc1686)
        "ttnn.deallocate"(%arg557) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1686)
        "ttnn.deallocate"(%arg556) <{force = false}> : (tensor<1280xbf16, #ttnn_layout4>) -> () loc(#loc1686)
        %1779 = "ttnn.reshape"(%1778) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc1687)
        "ttnn.deallocate"(%1778) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc1687)
        %1780 = "ttnn.matmul"(%1779, %arg555) <{activation = "gelu", compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<5120x1280xbf16, #ttnn_layout43>) -> tensor<16x5120xbf16, #ttnn_layout64> loc(#loc1688)
        "ttnn.deallocate"(%1779) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc1688)
        "ttnn.deallocate"(%arg555) <{force = false}> : (tensor<5120x1280xbf16, #ttnn_layout43>) -> () loc(#loc1688)
        %1781 = "ttnn.reshape"(%1777) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc2324)
        "ttnn.deallocate"(%1777) <{force = false}> : (tensor<1x16x1280xbf16, #ttnn_layout5>) -> () loc(#loc2324)
        %1782 = "ttnn.matmul"(%1780, %arg554) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x5120xbf16, #ttnn_layout64>, tensor<1280x5120xbf16, #ttnn_layout42>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc2325)
        "ttnn.deallocate"(%1780) <{force = false}> : (tensor<16x5120xbf16, #ttnn_layout64>) -> () loc(#loc2325)
        "ttnn.deallocate"(%arg554) <{force = false}> : (tensor<1280x5120xbf16, #ttnn_layout42>) -> () loc(#loc2325)
        %1783 = "ttnn.add"(%1782, %1781) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<16x1280xbf16, #ttnn_layout36>) -> tensor<16x1280xbf16, #ttnn_layout36> loc(#loc2326)
        "ttnn.deallocate"(%1782) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc2326)
        "ttnn.deallocate"(%1781) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc2326)
        %1784 = "ttnn.matmul"(%1783, %arg3) <{compute_config = #ttnn.device_compute_kernel_config<math_fidelity = hifi4, fp32_dest_acc_en = true>, transpose_a = false, transpose_b = true}> : (tensor<16x1280xbf16, #ttnn_layout36>, tensor<2048x1280xbf16, #ttnn_layout41>) -> tensor<16x2048xbf16, #ttnn_layout67> loc(#loc2327)
        "ttnn.deallocate"(%1783) <{force = false}> : (tensor<16x1280xbf16, #ttnn_layout36>) -> () loc(#loc2327)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<2048x1280xbf16, #ttnn_layout41>) -> () loc(#loc2327)
        %1785 = "ttnn.add"(%1784, %51) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<16x2048xbf16, #ttnn_layout67>, tensor<1x16x2048xbf16, #ttnn_layout20>) -> tensor<1x16x2048xbf16, #ttnn_layout20> loc(#loc2328)
        "ttnn.deallocate"(%1784) <{force = false}> : (tensor<16x2048xbf16, #ttnn_layout67>) -> () loc(#loc2328)
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x16x2048xbf16, #ttnn_layout20>) -> () loc(#loc2328)
        %1786 = "ttnn.layer_norm"(%1785, %arg1, %arg0) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x2048xbf16, #ttnn_layout20>, tensor<2048xbf16, #ttnn_layout22>, tensor<2048xbf16, #ttnn_layout22>) -> tensor<1x16x2048xbf16, #ttnn_layout20> loc(#loc1692)
        "ttnn.deallocate"(%1785) <{force = false}> : (tensor<1x16x2048xbf16, #ttnn_layout20>) -> () loc(#loc1692)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<2048xbf16, #ttnn_layout22>) -> () loc(#loc1692)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<2048xbf16, #ttnn_layout22>) -> () loc(#loc1692)
        return %1786 : tensor<1x16x2048xbf16, #ttnn_layout20> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("convolution.2299")
#loc2 = loc("broadcast.4864")
#loc3 = loc("broadcast.6394")
#loc4 = loc("broadcast.10754")
#loc6 = loc("broadcast.11994")
#loc10 = loc("broadcast.10647")
#loc11 = loc("broadcast.5926")
#loc12 = loc("broadcast.5902")
#loc13 = loc("broadcast.5881")
#loc15 = loc("broadcast.3517")
#loc16 = loc("broadcast.6856")
#loc17 = loc("broadcast.6832")
#loc18 = loc("broadcast.6811")
#loc19 = loc("broadcast.4996")
#loc20 = loc("broadcast.4972")
#loc21 = loc("broadcast.4951")
#loc22 = loc("broadcast.11506")
#loc23 = loc("broadcast.11482")
#loc24 = loc("broadcast.11461")
#loc27 = loc("broadcast.8096")
#loc28 = loc("broadcast.8072")
#loc29 = loc("broadcast.8051")
#loc31 = loc("broadcast.6927")
#loc32 = loc("broadcast.5794")
#loc33 = loc("broadcast.7476")
#loc34 = loc("broadcast.7452")
#loc35 = loc("broadcast.7431")
#loc36 = loc("broadcast.9717")
#loc37 = loc("broadcast.5464")
#loc39 = loc("broadcast.3004")
#loc40 = loc("broadcast.9956")
#loc41 = loc("broadcast.9932")
#loc42 = loc("broadcast.9911")
#loc43 = loc("broadcast.6724")
#loc44 = loc("broadcast.10576")
#loc45 = loc("broadcast.10552")
#loc46 = loc("broadcast.10531")
#loc47 = loc("broadcast.10957")
#loc48 = loc("broadcast.10266")
#loc49 = loc("broadcast.10242")
#loc50 = loc("broadcast.10221")
#loc51 = loc("broadcast.8167")
#loc52 = loc("broadcast.9336")
#loc53 = loc("broadcast.9312")
#loc54 = loc("broadcast.9291")
#loc56 = loc("broadcast.8477")
#loc57 = loc("broadcast.10337")
#loc58 = loc("broadcast.8894")
#loc59 = loc("broadcast.7014")
#loc60 = loc("broadcast.7034")
#loc61 = loc("broadcast.11974")
#loc62 = loc("broadcast.7166")
#loc63 = loc("broadcast.7142")
#loc64 = loc("broadcast.7121")
#loc65 = loc("broadcast.11196")
#loc66 = loc("broadcast.11172")
#loc67 = loc("broadcast.11151")
#loc69 = loc("broadcast.10734")
#loc70 = loc("broadcast.7944")
#loc71 = loc("broadcast.3604")
#loc73 = loc("broadcast.13561")
#loc74 = loc("broadcast.11044")
#loc75 = loc("broadcast.11816")
#loc76 = loc("broadcast.11792")
#loc77 = loc("broadcast.11771")
#loc78 = loc("broadcast.6307")
#loc79 = loc("broadcast.7634")
#loc80 = loc("convert.2292")
#loc81 = loc("gather.2293")
#loc82 = loc("concatenate.2308")
#loc83 = loc("broadcast.8406")
#loc84 = loc("broadcast.8382")
#loc85 = loc("broadcast.8361")
#loc87 = loc("broadcast.9184")
#loc88 = loc("broadcast.11267")
#loc89 = loc("broadcast.4224")
#loc90 = loc("broadcast.11374")
#loc91 = loc("broadcast.9804")
#loc92 = loc("broadcast.4844")
#loc93 = loc("broadcast.4757")
#loc94 = loc("broadcast.2674")
#loc95 = loc("broadcast.11664")
#loc96 = loc("broadcast.5616")
#loc97 = loc("broadcast.5592")
#loc98 = loc("broadcast.5571")
#loc99 = loc("broadcast.6704")
#loc100 = loc("broadcast.6236")
#loc101 = loc("broadcast.6212")
#loc102 = loc("broadcast.6191")
#loc103 = loc("broadcast.9097")
#loc105 = loc("broadcast.4137")
#loc106 = loc("broadcast.4066")
#loc107 = loc("broadcast.4042")
#loc108 = loc("broadcast.4021")
#loc110 = loc("broadcast.10886")
#loc111 = loc("broadcast.10862")
#loc112 = loc("broadcast.10841")
#loc113 = loc("broadcast.3624")
#loc114 = loc("broadcast.3756")
#loc115 = loc("broadcast.3732")
#loc116 = loc("broadcast.3711")
#loc117 = loc("broadcast.2587")
#loc118 = loc("broadcast.5067")
#loc119 = loc("broadcast.11887")
#loc120 = loc("broadcast.10424")
#loc121 = loc("broadcast.10134")
#loc122 = loc("broadcast.5774")
#loc123 = loc("broadcast.5377")
#loc124 = loc("broadcast.8254")
#loc126 = loc("broadcast.8787")
#loc127 = loc("broadcast.5174")
#loc128 = loc("broadcast.4534")
#loc130 = loc("broadcast.3827")
#loc131 = loc("broadcast.8274")
#loc132 = loc("broadcast.3914")
#loc133 = loc("broadcast.7344")
#loc137 = loc("broadcast.4686")
#loc138 = loc("broadcast.4662")
#loc139 = loc("broadcast.4641")
#loc140 = loc("broadcast.3314")
#loc141 = loc("broadcast.5154")
#loc142 = loc("broadcast.3934")
#loc146 = loc("broadcast.9646")
#loc147 = loc("broadcast.9622")
#loc148 = loc("broadcast.9601")
#loc149 = loc("broadcast.7857")
#loc150 = loc("broadcast.6414")
#loc152 = loc("broadcast.2826")
#loc153 = loc("broadcast.2802")
#loc154 = loc("broadcast.2781")
#loc155 = loc("broadcast.10114")
#loc156 = loc("broadcast.6617")
#loc158 = loc("broadcast.3446")
#loc159 = loc("broadcast.3422")
#loc160 = loc("broadcast.3401")
#loc161 = loc("broadcast.4447")
#loc162 = loc("broadcast.8584")
#loc163 = loc("broadcast.8874")
#loc164 = loc("broadcast.7964")
#loc165 = loc("reshape.2303")
#loc166 = loc("broadcast.10444")
#loc167 = loc("broadcast.3294")
#loc170 = loc("broadcast.10027")
#loc171 = loc("broadcast.7786")
#loc172 = loc("broadcast.7762")
#loc173 = loc("broadcast.7741")
#loc175 = loc("broadcast.3136")
#loc176 = loc("broadcast.3112")
#loc177 = loc("broadcast.3091")
#loc178 = loc("broadcast.7324")
#loc179 = loc("broadcast.12004")
#loc180 = loc("broadcast.9026")
#loc181 = loc("broadcast.9002")
#loc182 = loc("broadcast.8981")
#loc183 = loc("broadcast.6546")
#loc184 = loc("broadcast.6522")
#loc185 = loc("broadcast.6501")
#loc186 = loc("broadcast.5687")
#loc187 = loc("broadcast.7237")
#loc188 = loc("broadcast.11577")
#loc189 = loc("broadcast.6104")
#loc190 = loc("broadcast.9204")
#loc191 = loc("broadcast.7654")
#loc192 = loc("broadcast.9407")
#loc193 = loc("broadcast.11354")
#loc194 = loc("custom-call.137")
#loc195 = loc("reshape.12111")
#loc196 = loc("dot.12112")
#loc197 = loc("reshape.12114")
#loc198 = loc("transpose.12115")
#loc199 = loc("convert.12116")
#loc200 = loc("multiply.12118")
#loc201 = loc("concatenate.12083")
#loc202 = loc("broadcast.2516")
#loc203 = loc("broadcast.2492")
#loc204 = loc("broadcast.2471")
#loc205 = loc("broadcast.11684")
#loc206 = loc("broadcast.9824")
#loc207 = loc("broadcast.6084")
#loc209 = loc("broadcast.2897")
#loc210 = loc("broadcast.5306")
#loc211 = loc("broadcast.5282")
#loc212 = loc("broadcast.5261")
#loc213 = loc("broadcast.11064")
#loc214 = loc("broadcast.7547")
#loc215 = loc("broadcast.3207")
#loc216 = loc("broadcast.2984")
#loc217 = loc("broadcast.4244")
#loc218 = loc("broadcast.9494")
#loc219 = loc("broadcast.5997")
#loc220 = loc("broadcast.5484")
#loc221 = loc("broadcast.4376")
#loc222 = loc("broadcast.4352")
#loc223 = loc("broadcast.4331")
#loc224 = loc("broadcast.8716")
#loc225 = loc("broadcast.8692")
#loc226 = loc("broadcast.8671")
#loc227 = loc("broadcast.9514")
#loc229 = loc("broadcast.2694")
#loc230 = loc("broadcast.8564")
#loc232 = loc("broadcast.4554")
#loc760 = loc("reshape.2300")
#loc761 = loc("add.2311")
#loc762 = loc("custom-call.2388")
#loc763 = loc("custom-call.2465")
#loc764 = loc("reshape.2511")
#loc765 = loc("add.2517")
#loc766 = loc("convert.2520")
#loc767 = loc("multiply.2522")
#loc768 = loc("convert.2496")
#loc769 = loc("multiply.2499")
#loc770 = loc("dot.2523")
#loc771 = loc("compare.2552")
#loc772 = loc("not.2554")
#loc773 = loc("reduce.2566")
#loc774 = loc("not.2572")
#loc775 = loc("divide.2541")
#loc776 = loc("select.2576")
#loc777 = loc("convert.2475")
#loc778 = loc("dot.2577")
#loc779 = loc("convert.2579")
#loc780 = loc("reshape.2582")
#loc781 = loc("add.2588")
#loc782 = loc("add.2591")
#loc783 = loc("custom-call.2668")
#loc784 = loc("reshape.2669")
#loc785 = loc("add.2675")
#loc786 = loc("custom-call.2688")
#loc787 = loc("add.2695")
#loc788 = loc("add.2698")
#loc789 = loc("custom-call.2775")
#loc790 = loc("reshape.2821")
#loc791 = loc("add.2827")
#loc792 = loc("convert.2830")
#loc793 = loc("multiply.2832")
#loc794 = loc("convert.2806")
#loc795 = loc("multiply.2809")
#loc796 = loc("dot.2833")
#loc797 = loc("compare.2862")
#loc798 = loc("not.2864")
#loc799 = loc("reduce.2876")
#loc800 = loc("not.2882")
#loc801 = loc("divide.2851")
#loc802 = loc("select.2886")
#loc803 = loc("convert.2785")
#loc804 = loc("dot.2887")
#loc805 = loc("convert.2889")
#loc806 = loc("reshape.2892")
#loc807 = loc("add.2898")
#loc808 = loc("add.2901")
#loc809 = loc("custom-call.2978")
#loc810 = loc("reshape.2979")
#loc811 = loc("add.2985")
#loc812 = loc("custom-call.2998")
#loc813 = loc("add.3005")
#loc814 = loc("add.3008")
#loc815 = loc("custom-call.3085")
#loc816 = loc("reshape.3131")
#loc817 = loc("add.3137")
#loc818 = loc("convert.3140")
#loc819 = loc("multiply.3142")
#loc820 = loc("convert.3116")
#loc821 = loc("multiply.3119")
#loc822 = loc("dot.3143")
#loc823 = loc("compare.3172")
#loc824 = loc("not.3174")
#loc825 = loc("reduce.3186")
#loc826 = loc("not.3192")
#loc827 = loc("divide.3161")
#loc828 = loc("select.3196")
#loc829 = loc("convert.3095")
#loc830 = loc("dot.3197")
#loc831 = loc("convert.3199")
#loc832 = loc("reshape.3202")
#loc833 = loc("add.3208")
#loc834 = loc("add.3211")
#loc835 = loc("custom-call.3288")
#loc836 = loc("reshape.3289")
#loc837 = loc("add.3295")
#loc838 = loc("custom-call.3308")
#loc839 = loc("add.3315")
#loc840 = loc("add.3318")
#loc841 = loc("custom-call.3395")
#loc842 = loc("reshape.3441")
#loc843 = loc("add.3447")
#loc844 = loc("convert.3450")
#loc845 = loc("multiply.3452")
#loc846 = loc("convert.3426")
#loc847 = loc("multiply.3429")
#loc848 = loc("dot.3453")
#loc849 = loc("compare.3482")
#loc850 = loc("not.3484")
#loc851 = loc("reduce.3496")
#loc852 = loc("not.3502")
#loc853 = loc("divide.3471")
#loc854 = loc("select.3506")
#loc855 = loc("convert.3405")
#loc856 = loc("dot.3507")
#loc857 = loc("convert.3509")
#loc858 = loc("reshape.3512")
#loc859 = loc("add.3518")
#loc860 = loc("add.3521")
#loc861 = loc("custom-call.3598")
#loc862 = loc("reshape.3599")
#loc863 = loc("add.3605")
#loc864 = loc("custom-call.3618")
#loc865 = loc("add.3625")
#loc866 = loc("add.3628")
#loc867 = loc("custom-call.3705")
#loc868 = loc("reshape.3751")
#loc869 = loc("add.3757")
#loc870 = loc("convert.3760")
#loc871 = loc("multiply.3762")
#loc872 = loc("convert.3736")
#loc873 = loc("multiply.3739")
#loc874 = loc("dot.3763")
#loc875 = loc("compare.3792")
#loc876 = loc("not.3794")
#loc877 = loc("reduce.3806")
#loc878 = loc("not.3812")
#loc879 = loc("divide.3781")
#loc880 = loc("select.3816")
#loc881 = loc("convert.3715")
#loc882 = loc("dot.3817")
#loc883 = loc("convert.3819")
#loc884 = loc("reshape.3822")
#loc885 = loc("add.3828")
#loc886 = loc("add.3831")
#loc887 = loc("custom-call.3908")
#loc888 = loc("reshape.3909")
#loc889 = loc("add.3915")
#loc890 = loc("custom-call.3928")
#loc891 = loc("add.3935")
#loc892 = loc("add.3938")
#loc893 = loc("custom-call.4015")
#loc894 = loc("reshape.4061")
#loc895 = loc("add.4067")
#loc896 = loc("convert.4070")
#loc897 = loc("multiply.4072")
#loc898 = loc("convert.4046")
#loc899 = loc("multiply.4049")
#loc900 = loc("dot.4073")
#loc901 = loc("compare.4102")
#loc902 = loc("not.4104")
#loc903 = loc("reduce.4116")
#loc904 = loc("not.4122")
#loc905 = loc("divide.4091")
#loc906 = loc("select.4126")
#loc907 = loc("convert.4025")
#loc908 = loc("dot.4127")
#loc909 = loc("convert.4129")
#loc910 = loc("reshape.4132")
#loc911 = loc("add.4138")
#loc912 = loc("add.4141")
#loc913 = loc("custom-call.4218")
#loc914 = loc("reshape.4219")
#loc915 = loc("add.4225")
#loc916 = loc("custom-call.4238")
#loc917 = loc("add.4245")
#loc918 = loc("add.4248")
#loc919 = loc("custom-call.4325")
#loc920 = loc("reshape.4371")
#loc921 = loc("add.4377")
#loc922 = loc("convert.4380")
#loc923 = loc("multiply.4382")
#loc924 = loc("convert.4356")
#loc925 = loc("multiply.4359")
#loc926 = loc("dot.4383")
#loc927 = loc("compare.4412")
#loc928 = loc("not.4414")
#loc929 = loc("reduce.4426")
#loc930 = loc("not.4432")
#loc931 = loc("divide.4401")
#loc932 = loc("select.4436")
#loc933 = loc("convert.4335")
#loc934 = loc("dot.4437")
#loc935 = loc("convert.4439")
#loc936 = loc("reshape.4442")
#loc937 = loc("add.4448")
#loc938 = loc("add.4451")
#loc939 = loc("custom-call.4528")
#loc940 = loc("reshape.4529")
#loc941 = loc("add.4535")
#loc942 = loc("custom-call.4548")
#loc943 = loc("add.4555")
#loc944 = loc("add.4558")
#loc945 = loc("custom-call.4635")
#loc946 = loc("reshape.4681")
#loc947 = loc("add.4687")
#loc948 = loc("convert.4690")
#loc949 = loc("multiply.4692")
#loc950 = loc("convert.4666")
#loc951 = loc("multiply.4669")
#loc952 = loc("dot.4693")
#loc953 = loc("compare.4722")
#loc954 = loc("not.4724")
#loc955 = loc("reduce.4736")
#loc956 = loc("not.4742")
#loc957 = loc("divide.4711")
#loc958 = loc("select.4746")
#loc959 = loc("convert.4645")
#loc960 = loc("dot.4747")
#loc961 = loc("convert.4749")
#loc962 = loc("reshape.4752")
#loc963 = loc("add.4758")
#loc964 = loc("add.4761")
#loc965 = loc("custom-call.4838")
#loc966 = loc("reshape.4839")
#loc967 = loc("add.4845")
#loc968 = loc("custom-call.4858")
#loc969 = loc("add.4865")
#loc970 = loc("add.4868")
#loc971 = loc("custom-call.4945")
#loc972 = loc("reshape.4991")
#loc973 = loc("add.4997")
#loc974 = loc("convert.5000")
#loc975 = loc("multiply.5002")
#loc976 = loc("convert.4976")
#loc977 = loc("multiply.4979")
#loc978 = loc("dot.5003")
#loc979 = loc("compare.5032")
#loc980 = loc("not.5034")
#loc981 = loc("reduce.5046")
#loc982 = loc("not.5052")
#loc983 = loc("divide.5021")
#loc984 = loc("select.5056")
#loc985 = loc("convert.4955")
#loc986 = loc("dot.5057")
#loc987 = loc("convert.5059")
#loc988 = loc("reshape.5062")
#loc989 = loc("add.5068")
#loc990 = loc("add.5071")
#loc991 = loc("custom-call.5148")
#loc992 = loc("reshape.5149")
#loc993 = loc("add.5155")
#loc994 = loc("custom-call.5168")
#loc995 = loc("add.5175")
#loc996 = loc("add.5178")
#loc997 = loc("custom-call.5255")
#loc998 = loc("reshape.5301")
#loc999 = loc("add.5307")
#loc1000 = loc("convert.5310")
#loc1001 = loc("multiply.5312")
#loc1002 = loc("convert.5286")
#loc1003 = loc("multiply.5289")
#loc1004 = loc("dot.5313")
#loc1005 = loc("compare.5342")
#loc1006 = loc("not.5344")
#loc1007 = loc("reduce.5356")
#loc1008 = loc("not.5362")
#loc1009 = loc("divide.5331")
#loc1010 = loc("select.5366")
#loc1011 = loc("convert.5265")
#loc1012 = loc("dot.5367")
#loc1013 = loc("convert.5369")
#loc1014 = loc("reshape.5372")
#loc1015 = loc("add.5378")
#loc1016 = loc("add.5381")
#loc1017 = loc("custom-call.5458")
#loc1018 = loc("reshape.5459")
#loc1019 = loc("add.5465")
#loc1020 = loc("custom-call.5478")
#loc1021 = loc("add.5485")
#loc1022 = loc("add.5488")
#loc1023 = loc("custom-call.5565")
#loc1024 = loc("reshape.5611")
#loc1025 = loc("add.5617")
#loc1026 = loc("convert.5620")
#loc1027 = loc("multiply.5622")
#loc1028 = loc("convert.5596")
#loc1029 = loc("multiply.5599")
#loc1030 = loc("dot.5623")
#loc1031 = loc("compare.5652")
#loc1032 = loc("not.5654")
#loc1033 = loc("reduce.5666")
#loc1034 = loc("not.5672")
#loc1035 = loc("divide.5641")
#loc1036 = loc("select.5676")
#loc1037 = loc("convert.5575")
#loc1038 = loc("dot.5677")
#loc1039 = loc("convert.5679")
#loc1040 = loc("reshape.5682")
#loc1041 = loc("add.5688")
#loc1042 = loc("add.5691")
#loc1043 = loc("custom-call.5768")
#loc1044 = loc("reshape.5769")
#loc1045 = loc("add.5775")
#loc1046 = loc("custom-call.5788")
#loc1047 = loc("add.5795")
#loc1048 = loc("add.5798")
#loc1049 = loc("custom-call.5875")
#loc1050 = loc("reshape.5921")
#loc1051 = loc("add.5927")
#loc1052 = loc("convert.5930")
#loc1053 = loc("multiply.5932")
#loc1054 = loc("convert.5906")
#loc1055 = loc("multiply.5909")
#loc1056 = loc("dot.5933")
#loc1057 = loc("compare.5962")
#loc1058 = loc("not.5964")
#loc1059 = loc("reduce.5976")
#loc1060 = loc("not.5982")
#loc1061 = loc("divide.5951")
#loc1062 = loc("select.5986")
#loc1063 = loc("convert.5885")
#loc1064 = loc("dot.5987")
#loc1065 = loc("convert.5989")
#loc1066 = loc("reshape.5992")
#loc1067 = loc("add.5998")
#loc1068 = loc("add.6001")
#loc1069 = loc("custom-call.6078")
#loc1070 = loc("reshape.6079")
#loc1071 = loc("add.6085")
#loc1072 = loc("custom-call.6098")
#loc1073 = loc("add.6105")
#loc1074 = loc("add.6108")
#loc1075 = loc("custom-call.6185")
#loc1076 = loc("reshape.6231")
#loc1077 = loc("add.6237")
#loc1078 = loc("convert.6240")
#loc1079 = loc("multiply.6242")
#loc1080 = loc("convert.6216")
#loc1081 = loc("multiply.6219")
#loc1082 = loc("dot.6243")
#loc1083 = loc("compare.6272")
#loc1084 = loc("not.6274")
#loc1085 = loc("reduce.6286")
#loc1086 = loc("not.6292")
#loc1087 = loc("divide.6261")
#loc1088 = loc("select.6296")
#loc1089 = loc("convert.6195")
#loc1090 = loc("dot.6297")
#loc1091 = loc("convert.6299")
#loc1092 = loc("reshape.6302")
#loc1093 = loc("add.6308")
#loc1094 = loc("add.6311")
#loc1095 = loc("custom-call.6388")
#loc1096 = loc("reshape.6389")
#loc1097 = loc("add.6395")
#loc1098 = loc("custom-call.6408")
#loc1099 = loc("add.6415")
#loc1100 = loc("add.6418")
#loc1101 = loc("custom-call.6495")
#loc1102 = loc("reshape.6541")
#loc1103 = loc("add.6547")
#loc1104 = loc("convert.6550")
#loc1105 = loc("multiply.6552")
#loc1106 = loc("convert.6526")
#loc1107 = loc("multiply.6529")
#loc1108 = loc("dot.6553")
#loc1109 = loc("compare.6582")
#loc1110 = loc("not.6584")
#loc1111 = loc("reduce.6596")
#loc1112 = loc("not.6602")
#loc1113 = loc("divide.6571")
#loc1114 = loc("select.6606")
#loc1115 = loc("convert.6505")
#loc1116 = loc("dot.6607")
#loc1117 = loc("convert.6609")
#loc1118 = loc("reshape.6612")
#loc1119 = loc("add.6618")
#loc1120 = loc("add.6621")
#loc1121 = loc("custom-call.6698")
#loc1122 = loc("reshape.6699")
#loc1123 = loc("add.6705")
#loc1124 = loc("custom-call.6718")
#loc1125 = loc("add.6725")
#loc1126 = loc("add.6728")
#loc1127 = loc("custom-call.6805")
#loc1128 = loc("reshape.6851")
#loc1129 = loc("add.6857")
#loc1130 = loc("convert.6860")
#loc1131 = loc("multiply.6862")
#loc1132 = loc("convert.6836")
#loc1133 = loc("multiply.6839")
#loc1134 = loc("dot.6863")
#loc1135 = loc("compare.6892")
#loc1136 = loc("not.6894")
#loc1137 = loc("reduce.6906")
#loc1138 = loc("not.6912")
#loc1139 = loc("divide.6881")
#loc1140 = loc("select.6916")
#loc1141 = loc("convert.6815")
#loc1142 = loc("dot.6917")
#loc1143 = loc("convert.6919")
#loc1144 = loc("reshape.6922")
#loc1145 = loc("add.6928")
#loc1146 = loc("add.6931")
#loc1147 = loc("custom-call.7008")
#loc1148 = loc("reshape.7009")
#loc1149 = loc("add.7015")
#loc1150 = loc("custom-call.7028")
#loc1151 = loc("add.7035")
#loc1152 = loc("add.7038")
#loc1153 = loc("custom-call.7115")
#loc1154 = loc("reshape.7161")
#loc1155 = loc("add.7167")
#loc1156 = loc("convert.7170")
#loc1157 = loc("multiply.7172")
#loc1158 = loc("convert.7146")
#loc1159 = loc("multiply.7149")
#loc1160 = loc("dot.7173")
#loc1161 = loc("compare.7202")
#loc1162 = loc("not.7204")
#loc1163 = loc("reduce.7216")
#loc1164 = loc("not.7222")
#loc1165 = loc("divide.7191")
#loc1166 = loc("select.7226")
#loc1167 = loc("convert.7125")
#loc1168 = loc("dot.7227")
#loc1169 = loc("convert.7229")
#loc1170 = loc("reshape.7232")
#loc1171 = loc("add.7238")
#loc1172 = loc("add.7241")
#loc1173 = loc("custom-call.7318")
#loc1174 = loc("reshape.7319")
#loc1175 = loc("add.7325")
#loc1176 = loc("custom-call.7338")
#loc1177 = loc("add.7345")
#loc1178 = loc("add.7348")
#loc1179 = loc("custom-call.7425")
#loc1180 = loc("reshape.7471")
#loc1181 = loc("add.7477")
#loc1182 = loc("convert.7480")
#loc1183 = loc("multiply.7482")
#loc1184 = loc("convert.7456")
#loc1185 = loc("multiply.7459")
#loc1186 = loc("dot.7483")
#loc1187 = loc("compare.7512")
#loc1188 = loc("not.7514")
#loc1189 = loc("reduce.7526")
#loc1190 = loc("not.7532")
#loc1191 = loc("divide.7501")
#loc1192 = loc("select.7536")
#loc1193 = loc("convert.7435")
#loc1194 = loc("dot.7537")
#loc1195 = loc("convert.7539")
#loc1196 = loc("reshape.7542")
#loc1197 = loc("add.7548")
#loc1198 = loc("add.7551")
#loc1199 = loc("custom-call.7628")
#loc1200 = loc("reshape.7629")
#loc1201 = loc("add.7635")
#loc1202 = loc("custom-call.7648")
#loc1203 = loc("add.7655")
#loc1204 = loc("add.7658")
#loc1205 = loc("custom-call.7735")
#loc1206 = loc("reshape.7781")
#loc1207 = loc("add.7787")
#loc1208 = loc("convert.7790")
#loc1209 = loc("multiply.7792")
#loc1210 = loc("convert.7766")
#loc1211 = loc("multiply.7769")
#loc1212 = loc("dot.7793")
#loc1213 = loc("compare.7822")
#loc1214 = loc("not.7824")
#loc1215 = loc("reduce.7836")
#loc1216 = loc("not.7842")
#loc1217 = loc("divide.7811")
#loc1218 = loc("select.7846")
#loc1219 = loc("convert.7745")
#loc1220 = loc("dot.7847")
#loc1221 = loc("convert.7849")
#loc1222 = loc("reshape.7852")
#loc1223 = loc("add.7858")
#loc1224 = loc("add.7861")
#loc1225 = loc("custom-call.7938")
#loc1226 = loc("reshape.7939")
#loc1227 = loc("add.7945")
#loc1228 = loc("custom-call.7958")
#loc1229 = loc("add.7965")
#loc1230 = loc("add.7968")
#loc1231 = loc("custom-call.8045")
#loc1232 = loc("reshape.8091")
#loc1233 = loc("add.8097")
#loc1234 = loc("convert.8100")
#loc1235 = loc("multiply.8102")
#loc1236 = loc("convert.8076")
#loc1237 = loc("multiply.8079")
#loc1238 = loc("dot.8103")
#loc1239 = loc("compare.8132")
#loc1240 = loc("not.8134")
#loc1241 = loc("reduce.8146")
#loc1242 = loc("not.8152")
#loc1243 = loc("divide.8121")
#loc1244 = loc("select.8156")
#loc1245 = loc("convert.8055")
#loc1246 = loc("dot.8157")
#loc1247 = loc("convert.8159")
#loc1248 = loc("reshape.8162")
#loc1249 = loc("add.8168")
#loc1250 = loc("add.8171")
#loc1251 = loc("custom-call.8248")
#loc1252 = loc("reshape.8249")
#loc1253 = loc("add.8255")
#loc1254 = loc("custom-call.8268")
#loc1255 = loc("add.8275")
#loc1256 = loc("add.8278")
#loc1257 = loc("custom-call.8355")
#loc1258 = loc("reshape.8401")
#loc1259 = loc("add.8407")
#loc1260 = loc("convert.8410")
#loc1261 = loc("multiply.8412")
#loc1262 = loc("convert.8386")
#loc1263 = loc("multiply.8389")
#loc1264 = loc("dot.8413")
#loc1265 = loc("compare.8442")
#loc1266 = loc("not.8444")
#loc1267 = loc("reduce.8456")
#loc1268 = loc("not.8462")
#loc1269 = loc("divide.8431")
#loc1270 = loc("select.8466")
#loc1271 = loc("convert.8365")
#loc1272 = loc("dot.8467")
#loc1273 = loc("convert.8469")
#loc1274 = loc("reshape.8472")
#loc1275 = loc("add.8478")
#loc1276 = loc("add.8481")
#loc1277 = loc("custom-call.8558")
#loc1278 = loc("reshape.8559")
#loc1279 = loc("add.8565")
#loc1280 = loc("custom-call.8578")
#loc1281 = loc("add.8585")
#loc1282 = loc("add.8588")
#loc1283 = loc("custom-call.8665")
#loc1284 = loc("reshape.8711")
#loc1285 = loc("add.8717")
#loc1286 = loc("convert.8720")
#loc1287 = loc("multiply.8722")
#loc1288 = loc("convert.8696")
#loc1289 = loc("multiply.8699")
#loc1290 = loc("dot.8723")
#loc1291 = loc("compare.8752")
#loc1292 = loc("not.8754")
#loc1293 = loc("reduce.8766")
#loc1294 = loc("not.8772")
#loc1295 = loc("divide.8741")
#loc1296 = loc("select.8776")
#loc1297 = loc("convert.8675")
#loc1298 = loc("dot.8777")
#loc1299 = loc("convert.8779")
#loc1300 = loc("reshape.8782")
#loc1301 = loc("add.8788")
#loc1302 = loc("add.8791")
#loc1303 = loc("custom-call.8868")
#loc1304 = loc("reshape.8869")
#loc1305 = loc("add.8875")
#loc1306 = loc("custom-call.8888")
#loc1307 = loc("add.8895")
#loc1308 = loc("add.8898")
#loc1309 = loc("custom-call.8975")
#loc1310 = loc("reshape.9021")
#loc1311 = loc("add.9027")
#loc1312 = loc("convert.9030")
#loc1313 = loc("multiply.9032")
#loc1314 = loc("convert.9006")
#loc1315 = loc("multiply.9009")
#loc1316 = loc("dot.9033")
#loc1317 = loc("compare.9062")
#loc1318 = loc("not.9064")
#loc1319 = loc("reduce.9076")
#loc1320 = loc("not.9082")
#loc1321 = loc("divide.9051")
#loc1322 = loc("select.9086")
#loc1323 = loc("convert.8985")
#loc1324 = loc("dot.9087")
#loc1325 = loc("convert.9089")
#loc1326 = loc("reshape.9092")
#loc1327 = loc("add.9098")
#loc1328 = loc("add.9101")
#loc1329 = loc("custom-call.9178")
#loc1330 = loc("reshape.9179")
#loc1331 = loc("add.9185")
#loc1332 = loc("custom-call.9198")
#loc1333 = loc("add.9205")
#loc1334 = loc("add.9208")
#loc1335 = loc("custom-call.9285")
#loc1336 = loc("reshape.9331")
#loc1337 = loc("add.9337")
#loc1338 = loc("convert.9340")
#loc1339 = loc("multiply.9342")
#loc1340 = loc("convert.9316")
#loc1341 = loc("multiply.9319")
#loc1342 = loc("dot.9343")
#loc1343 = loc("compare.9372")
#loc1344 = loc("not.9374")
#loc1345 = loc("reduce.9386")
#loc1346 = loc("not.9392")
#loc1347 = loc("divide.9361")
#loc1348 = loc("select.9396")
#loc1349 = loc("convert.9295")
#loc1350 = loc("dot.9397")
#loc1351 = loc("convert.9399")
#loc1352 = loc("reshape.9402")
#loc1353 = loc("add.9408")
#loc1354 = loc("add.9411")
#loc1355 = loc("custom-call.9488")
#loc1356 = loc("reshape.9489")
#loc1357 = loc("add.9495")
#loc1358 = loc("custom-call.9508")
#loc1359 = loc("add.9515")
#loc1360 = loc("add.9518")
#loc1361 = loc("custom-call.9595")
#loc1362 = loc("reshape.9641")
#loc1363 = loc("add.9647")
#loc1364 = loc("convert.9650")
#loc1365 = loc("multiply.9652")
#loc1366 = loc("convert.9626")
#loc1367 = loc("multiply.9629")
#loc1368 = loc("dot.9653")
#loc1369 = loc("compare.9682")
#loc1370 = loc("not.9684")
#loc1371 = loc("reduce.9696")
#loc1372 = loc("not.9702")
#loc1373 = loc("divide.9671")
#loc1374 = loc("select.9706")
#loc1375 = loc("convert.9605")
#loc1376 = loc("dot.9707")
#loc1377 = loc("convert.9709")
#loc1378 = loc("reshape.9712")
#loc1379 = loc("add.9718")
#loc1380 = loc("add.9721")
#loc1381 = loc("custom-call.9798")
#loc1382 = loc("reshape.9799")
#loc1383 = loc("add.9805")
#loc1384 = loc("custom-call.9818")
#loc1385 = loc("add.9825")
#loc1386 = loc("add.9828")
#loc1387 = loc("custom-call.9905")
#loc1388 = loc("reshape.9951")
#loc1389 = loc("add.9957")
#loc1390 = loc("convert.9960")
#loc1391 = loc("multiply.9962")
#loc1392 = loc("convert.9936")
#loc1393 = loc("multiply.9939")
#loc1394 = loc("dot.9963")
#loc1395 = loc("compare.9992")
#loc1396 = loc("not.9994")
#loc1397 = loc("reduce.10006")
#loc1398 = loc("not.10012")
#loc1399 = loc("divide.9981")
#loc1400 = loc("select.10016")
#loc1401 = loc("convert.9915")
#loc1402 = loc("dot.10017")
#loc1403 = loc("convert.10019")
#loc1404 = loc("reshape.10022")
#loc1405 = loc("add.10028")
#loc1406 = loc("add.10031")
#loc1407 = loc("custom-call.10108")
#loc1408 = loc("reshape.10109")
#loc1409 = loc("add.10115")
#loc1410 = loc("custom-call.10128")
#loc1411 = loc("add.10135")
#loc1412 = loc("add.10138")
#loc1413 = loc("custom-call.10215")
#loc1414 = loc("reshape.10261")
#loc1415 = loc("add.10267")
#loc1416 = loc("convert.10270")
#loc1417 = loc("multiply.10272")
#loc1418 = loc("convert.10246")
#loc1419 = loc("multiply.10249")
#loc1420 = loc("dot.10273")
#loc1421 = loc("compare.10302")
#loc1422 = loc("not.10304")
#loc1423 = loc("reduce.10316")
#loc1424 = loc("not.10322")
#loc1425 = loc("divide.10291")
#loc1426 = loc("select.10326")
#loc1427 = loc("convert.10225")
#loc1428 = loc("dot.10327")
#loc1429 = loc("convert.10329")
#loc1430 = loc("reshape.10332")
#loc1431 = loc("add.10338")
#loc1432 = loc("add.10341")
#loc1433 = loc("custom-call.10418")
#loc1434 = loc("reshape.10419")
#loc1435 = loc("add.10425")
#loc1436 = loc("custom-call.10438")
#loc1437 = loc("add.10445")
#loc1438 = loc("add.10448")
#loc1439 = loc("custom-call.10525")
#loc1440 = loc("reshape.10571")
#loc1441 = loc("add.10577")
#loc1442 = loc("convert.10580")
#loc1443 = loc("multiply.10582")
#loc1444 = loc("convert.10556")
#loc1445 = loc("multiply.10559")
#loc1446 = loc("dot.10583")
#loc1447 = loc("compare.10612")
#loc1448 = loc("not.10614")
#loc1449 = loc("reduce.10626")
#loc1450 = loc("not.10632")
#loc1451 = loc("divide.10601")
#loc1452 = loc("select.10636")
#loc1453 = loc("convert.10535")
#loc1454 = loc("dot.10637")
#loc1455 = loc("convert.10639")
#loc1456 = loc("reshape.10642")
#loc1457 = loc("add.10648")
#loc1458 = loc("add.10651")
#loc1459 = loc("custom-call.10728")
#loc1460 = loc("reshape.10729")
#loc1461 = loc("add.10735")
#loc1462 = loc("custom-call.10748")
#loc1463 = loc("add.10755")
#loc1464 = loc("add.10758")
#loc1465 = loc("custom-call.10835")
#loc1466 = loc("reshape.10881")
#loc1467 = loc("add.10887")
#loc1468 = loc("convert.10890")
#loc1469 = loc("multiply.10892")
#loc1470 = loc("convert.10866")
#loc1471 = loc("multiply.10869")
#loc1472 = loc("dot.10893")
#loc1473 = loc("compare.10922")
#loc1474 = loc("not.10924")
#loc1475 = loc("reduce.10936")
#loc1476 = loc("not.10942")
#loc1477 = loc("divide.10911")
#loc1478 = loc("select.10946")
#loc1479 = loc("convert.10845")
#loc1480 = loc("dot.10947")
#loc1481 = loc("convert.10949")
#loc1482 = loc("reshape.10952")
#loc1483 = loc("add.10958")
#loc1484 = loc("add.10961")
#loc1485 = loc("custom-call.11038")
#loc1486 = loc("reshape.11039")
#loc1487 = loc("add.11045")
#loc1488 = loc("custom-call.11058")
#loc1489 = loc("add.11065")
#loc1490 = loc("add.11068")
#loc1491 = loc("custom-call.11145")
#loc1492 = loc("reshape.11191")
#loc1493 = loc("add.11197")
#loc1494 = loc("convert.11200")
#loc1495 = loc("multiply.11202")
#loc1496 = loc("convert.11176")
#loc1497 = loc("multiply.11179")
#loc1498 = loc("dot.11203")
#loc1499 = loc("compare.11232")
#loc1500 = loc("not.11234")
#loc1501 = loc("reduce.11246")
#loc1502 = loc("not.11252")
#loc1503 = loc("divide.11221")
#loc1504 = loc("select.11256")
#loc1505 = loc("convert.11155")
#loc1506 = loc("dot.11257")
#loc1507 = loc("convert.11259")
#loc1508 = loc("reshape.11262")
#loc1509 = loc("add.11268")
#loc1510 = loc("add.11271")
#loc1511 = loc("custom-call.11348")
#loc1512 = loc("reshape.11349")
#loc1513 = loc("add.11355")
#loc1514 = loc("custom-call.11368")
#loc1515 = loc("add.11375")
#loc1516 = loc("add.11378")
#loc1517 = loc("custom-call.11455")
#loc1518 = loc("reshape.11501")
#loc1519 = loc("add.11507")
#loc1520 = loc("convert.11510")
#loc1521 = loc("multiply.11512")
#loc1522 = loc("convert.11486")
#loc1523 = loc("multiply.11489")
#loc1524 = loc("dot.11513")
#loc1525 = loc("compare.11542")
#loc1526 = loc("not.11544")
#loc1527 = loc("reduce.11556")
#loc1528 = loc("not.11562")
#loc1529 = loc("divide.11531")
#loc1530 = loc("select.11566")
#loc1531 = loc("convert.11465")
#loc1532 = loc("dot.11567")
#loc1533 = loc("convert.11569")
#loc1534 = loc("reshape.11572")
#loc1535 = loc("add.11578")
#loc1536 = loc("add.11581")
#loc1537 = loc("custom-call.11658")
#loc1538 = loc("reshape.11659")
#loc1539 = loc("add.11665")
#loc1540 = loc("custom-call.11678")
#loc1541 = loc("add.11685")
#loc1542 = loc("add.11688")
#loc1543 = loc("custom-call.11765")
#loc1544 = loc("reshape.11811")
#loc1545 = loc("add.11817")
#loc1546 = loc("convert.11820")
#loc1547 = loc("multiply.11822")
#loc1548 = loc("convert.11796")
#loc1549 = loc("multiply.11799")
#loc1550 = loc("dot.11823")
#loc1551 = loc("compare.11852")
#loc1552 = loc("not.11854")
#loc1553 = loc("reduce.11866")
#loc1554 = loc("not.11872")
#loc1555 = loc("divide.11841")
#loc1556 = loc("select.11876")
#loc1557 = loc("convert.11775")
#loc1558 = loc("dot.11877")
#loc1559 = loc("convert.11879")
#loc1560 = loc("reshape.11882")
#loc1561 = loc("add.11888")
#loc1562 = loc("add.11891")
#loc1563 = loc("custom-call.11968")
#loc1564 = loc("reshape.11969")
#loc1565 = loc("add.11975")
#loc1566 = loc("custom-call.11988")
#loc1567 = loc("add.11995")
#loc1568 = loc("add.11998")
#loc1569 = loc("add.12005")
#loc1570 = loc("custom-call.12082")
#loc1571 = loc("dot.12097")
#loc1572 = loc("convert.12101")
#loc1573 = loc("multiply.12104")
#loc1574 = loc("dot.12119")
#loc1575 = loc("compare.12148")
#loc1576 = loc("not.12150")
#loc1577 = loc("reduce.12162")
#loc1578 = loc("not.12168")
#loc1579 = loc("divide.12137")
#loc1580 = loc("select.12172")
#loc1581 = loc("dot.12085")
#loc1582 = loc("convert.12089")
#loc1583 = loc("dot.12173")
#loc1584 = loc("convert.12175")
#loc1585 = loc("reshape.12178")
#loc1586 = loc("dot.12179")
#loc1587 = loc("reshape.12180")
#loc1588 = loc("divide.12182")
#loc1589 = loc("add.12185")
#loc1590 = loc("custom-call.12285")
#loc1591 = loc("reshape.12286")
#loc1592 = loc("dot.12287")
#loc1593 = loc("dot.12303")
#loc1594 = loc("reshape.12304")
#loc1595 = loc("add.12307")
#loc1596 = loc("custom-call.12408")
#loc1597 = loc("reshape.12527")
#loc1598 = loc("dot.12528")
#loc1599 = loc("convert.12532")
#loc1600 = loc("multiply.12534")
#loc1601 = loc("custom-call.12498")
#loc1602 = loc("concatenate.12499")
#loc1603 = loc("dot.12513")
#loc1604 = loc("convert.12517")
#loc1605 = loc("multiply.12520")
#loc1606 = loc("dot.12535")
#loc1607 = loc("compare.12564")
#loc1608 = loc("not.12566")
#loc1609 = loc("reduce.12578")
#loc1610 = loc("not.12584")
#loc1611 = loc("divide.12553")
#loc1612 = loc("select.12588")
#loc1613 = loc("dot.12501")
#loc1614 = loc("convert.12505")
#loc1615 = loc("dot.12589")
#loc1616 = loc("convert.12591")
#loc1617 = loc("reshape.12594")
#loc1618 = loc("dot.12595")
#loc1619 = loc("reshape.12596")
#loc1620 = loc("divide.12598")
#loc1621 = loc("add.12601")
#loc1622 = loc("custom-call.12701")
#loc1623 = loc("reshape.12702")
#loc1624 = loc("dot.12703")
#loc1625 = loc("dot.12719")
#loc1626 = loc("reshape.12720")
#loc1627 = loc("add.12723")
#loc1628 = loc("custom-call.12824")
#loc1629 = loc("reshape.12943")
#loc1630 = loc("dot.12944")
#loc1631 = loc("convert.12948")
#loc1632 = loc("multiply.12950")
#loc1633 = loc("custom-call.12914")
#loc1634 = loc("concatenate.12915")
#loc1635 = loc("dot.12929")
#loc1636 = loc("convert.12933")
#loc1637 = loc("multiply.12936")
#loc1638 = loc("dot.12951")
#loc1639 = loc("compare.12980")
#loc1640 = loc("not.12982")
#loc1641 = loc("reduce.12994")
#loc1642 = loc("not.13000")
#loc1643 = loc("divide.12969")
#loc1644 = loc("select.13004")
#loc1645 = loc("dot.12917")
#loc1646 = loc("convert.12921")
#loc1647 = loc("dot.13005")
#loc1648 = loc("convert.13007")
#loc1649 = loc("reshape.13010")
#loc1650 = loc("dot.13011")
#loc1651 = loc("reshape.13012")
#loc1652 = loc("divide.13014")
#loc1653 = loc("add.13017")
#loc1654 = loc("custom-call.13117")
#loc1655 = loc("reshape.13118")
#loc1656 = loc("dot.13119")
#loc1657 = loc("dot.13135")
#loc1658 = loc("reshape.13136")
#loc1659 = loc("add.13139")
#loc1660 = loc("custom-call.13240")
#loc1661 = loc("reshape.13359")
#loc1662 = loc("dot.13360")
#loc1663 = loc("convert.13364")
#loc1664 = loc("multiply.13366")
#loc1665 = loc("custom-call.13330")
#loc1666 = loc("concatenate.13331")
#loc1667 = loc("dot.13345")
#loc1668 = loc("convert.13349")
#loc1669 = loc("multiply.13352")
#loc1670 = loc("dot.13367")
#loc1671 = loc("compare.13396")
#loc1672 = loc("not.13398")
#loc1673 = loc("reduce.13410")
#loc1674 = loc("not.13416")
#loc1675 = loc("divide.13385")
#loc1676 = loc("select.13420")
#loc1677 = loc("dot.13333")
#loc1678 = loc("convert.13337")
#loc1679 = loc("dot.13421")
#loc1680 = loc("convert.13423")
#loc1681 = loc("reshape.13426")
#loc1682 = loc("dot.13427")
#loc1683 = loc("reshape.13428")
#loc1684 = loc("divide.13430")
#loc1685 = loc("add.13433")
#loc1686 = loc("custom-call.13533")
#loc1687 = loc("reshape.13534")
#loc1688 = loc("dot.13535")
#loc1689 = loc("reshape.13556")
#loc1690 = loc("add.13555")
#loc1691 = loc("add.13562")
#loc1692 = loc("custom-call.13639")
#loc1693 = loc("convolution.2299_input_in_0_layout"(#loc1))
#loc1694 = loc("convolution.2299_prepare_conv2d_weight"(#loc1))
#loc1695 = loc("gather.2293_workaround"(#loc81))
#loc1696 = loc("convolution.2299_input"(#loc1))
#loc1697 = loc("convolution.2299_reshape"(#loc1))
#loc1698 = loc("add.2517_decomp_matmul"(#loc765))
#loc1699 = loc("add.2517_decomp_add"(#loc765))
#loc1700 = loc("add.2517_split_q"(#loc765))
#loc1701 = loc("add.2517_split_k"(#loc765))
#loc1702 = loc("add.2517_split_v"(#loc765))
#loc1703 = loc("add.2517_reshape_q"(#loc765))
#loc1704 = loc("add.2517_reshape_k"(#loc765))
#loc1705 = loc("add.2517_reshape_v"(#loc765))
#loc1706 = loc("add.2517_permute_q"(#loc765))
#loc1707 = loc("add.2517_permute_k"(#loc765))
#loc1708 = loc("add.2517_permute_v"(#loc765))
#loc1709 = loc("select.2576_workaround"(#loc776))
#loc1710 = loc("reshape.2582_concat_heads"(#loc780))
#loc1711 = loc("add.2588_decomp_matmul"(#loc781))
#loc1712 = loc("add.2588_decomp_add"(#loc781))
#loc1713 = loc("add.2675_decomp_matmul"(#loc785))
#loc1714 = loc("add.2675_decomp_add"(#loc785))
#loc1715 = loc("add.2675_decomp_gelu"(#loc785))
#loc1716 = loc("add.2695_decomp_matmul"(#loc787))
#loc1717 = loc("add.2695_decomp_add"(#loc787))
#loc1718 = loc("add.2827_decomp_matmul"(#loc791))
#loc1719 = loc("add.2827_decomp_add"(#loc791))
#loc1720 = loc("add.2827_split_q"(#loc791))
#loc1721 = loc("add.2827_split_k"(#loc791))
#loc1722 = loc("add.2827_split_v"(#loc791))
#loc1723 = loc("add.2827_reshape_q"(#loc791))
#loc1724 = loc("add.2827_reshape_k"(#loc791))
#loc1725 = loc("add.2827_reshape_v"(#loc791))
#loc1726 = loc("add.2827_permute_q"(#loc791))
#loc1727 = loc("add.2827_permute_k"(#loc791))
#loc1728 = loc("add.2827_permute_v"(#loc791))
#loc1729 = loc("select.2886_workaround"(#loc802))
#loc1730 = loc("reshape.2892_concat_heads"(#loc806))
#loc1731 = loc("add.2898_decomp_matmul"(#loc807))
#loc1732 = loc("add.2898_decomp_add"(#loc807))
#loc1733 = loc("add.2985_decomp_matmul"(#loc811))
#loc1734 = loc("add.2985_decomp_add"(#loc811))
#loc1735 = loc("add.2985_decomp_gelu"(#loc811))
#loc1736 = loc("add.3005_decomp_matmul"(#loc813))
#loc1737 = loc("add.3005_decomp_add"(#loc813))
#loc1738 = loc("add.3137_decomp_matmul"(#loc817))
#loc1739 = loc("add.3137_decomp_add"(#loc817))
#loc1740 = loc("add.3137_split_q"(#loc817))
#loc1741 = loc("add.3137_split_k"(#loc817))
#loc1742 = loc("add.3137_split_v"(#loc817))
#loc1743 = loc("add.3137_reshape_q"(#loc817))
#loc1744 = loc("add.3137_reshape_k"(#loc817))
#loc1745 = loc("add.3137_reshape_v"(#loc817))
#loc1746 = loc("add.3137_permute_q"(#loc817))
#loc1747 = loc("add.3137_permute_k"(#loc817))
#loc1748 = loc("add.3137_permute_v"(#loc817))
#loc1749 = loc("select.3196_workaround"(#loc828))
#loc1750 = loc("reshape.3202_concat_heads"(#loc832))
#loc1751 = loc("add.3208_decomp_matmul"(#loc833))
#loc1752 = loc("add.3208_decomp_add"(#loc833))
#loc1753 = loc("add.3295_decomp_matmul"(#loc837))
#loc1754 = loc("add.3295_decomp_add"(#loc837))
#loc1755 = loc("add.3295_decomp_gelu"(#loc837))
#loc1756 = loc("add.3315_decomp_matmul"(#loc839))
#loc1757 = loc("add.3315_decomp_add"(#loc839))
#loc1758 = loc("add.3447_decomp_matmul"(#loc843))
#loc1759 = loc("add.3447_decomp_add"(#loc843))
#loc1760 = loc("add.3447_split_q"(#loc843))
#loc1761 = loc("add.3447_split_k"(#loc843))
#loc1762 = loc("add.3447_split_v"(#loc843))
#loc1763 = loc("add.3447_reshape_q"(#loc843))
#loc1764 = loc("add.3447_reshape_k"(#loc843))
#loc1765 = loc("add.3447_reshape_v"(#loc843))
#loc1766 = loc("add.3447_permute_q"(#loc843))
#loc1767 = loc("add.3447_permute_k"(#loc843))
#loc1768 = loc("add.3447_permute_v"(#loc843))
#loc1769 = loc("select.3506_workaround"(#loc854))
#loc1770 = loc("reshape.3512_concat_heads"(#loc858))
#loc1771 = loc("add.3518_decomp_matmul"(#loc859))
#loc1772 = loc("add.3518_decomp_add"(#loc859))
#loc1773 = loc("add.3605_decomp_matmul"(#loc863))
#loc1774 = loc("add.3605_decomp_add"(#loc863))
#loc1775 = loc("add.3605_decomp_gelu"(#loc863))
#loc1776 = loc("add.3625_decomp_matmul"(#loc865))
#loc1777 = loc("add.3625_decomp_add"(#loc865))
#loc1778 = loc("add.3757_decomp_matmul"(#loc869))
#loc1779 = loc("add.3757_decomp_add"(#loc869))
#loc1780 = loc("add.3757_split_q"(#loc869))
#loc1781 = loc("add.3757_split_k"(#loc869))
#loc1782 = loc("add.3757_split_v"(#loc869))
#loc1783 = loc("add.3757_reshape_q"(#loc869))
#loc1784 = loc("add.3757_reshape_k"(#loc869))
#loc1785 = loc("add.3757_reshape_v"(#loc869))
#loc1786 = loc("add.3757_permute_q"(#loc869))
#loc1787 = loc("add.3757_permute_k"(#loc869))
#loc1788 = loc("add.3757_permute_v"(#loc869))
#loc1789 = loc("select.3816_workaround"(#loc880))
#loc1790 = loc("reshape.3822_concat_heads"(#loc884))
#loc1791 = loc("add.3828_decomp_matmul"(#loc885))
#loc1792 = loc("add.3828_decomp_add"(#loc885))
#loc1793 = loc("add.3915_decomp_matmul"(#loc889))
#loc1794 = loc("add.3915_decomp_add"(#loc889))
#loc1795 = loc("add.3915_decomp_gelu"(#loc889))
#loc1796 = loc("add.3935_decomp_matmul"(#loc891))
#loc1797 = loc("add.3935_decomp_add"(#loc891))
#loc1798 = loc("add.4067_decomp_matmul"(#loc895))
#loc1799 = loc("add.4067_decomp_add"(#loc895))
#loc1800 = loc("add.4067_split_q"(#loc895))
#loc1801 = loc("add.4067_split_k"(#loc895))
#loc1802 = loc("add.4067_split_v"(#loc895))
#loc1803 = loc("add.4067_reshape_q"(#loc895))
#loc1804 = loc("add.4067_reshape_k"(#loc895))
#loc1805 = loc("add.4067_reshape_v"(#loc895))
#loc1806 = loc("add.4067_permute_q"(#loc895))
#loc1807 = loc("add.4067_permute_k"(#loc895))
#loc1808 = loc("add.4067_permute_v"(#loc895))
#loc1809 = loc("select.4126_workaround"(#loc906))
#loc1810 = loc("reshape.4132_concat_heads"(#loc910))
#loc1811 = loc("add.4138_decomp_matmul"(#loc911))
#loc1812 = loc("add.4138_decomp_add"(#loc911))
#loc1813 = loc("add.4225_decomp_matmul"(#loc915))
#loc1814 = loc("add.4225_decomp_add"(#loc915))
#loc1815 = loc("add.4225_decomp_gelu"(#loc915))
#loc1816 = loc("add.4245_decomp_matmul"(#loc917))
#loc1817 = loc("add.4245_decomp_add"(#loc917))
#loc1818 = loc("add.4377_decomp_matmul"(#loc921))
#loc1819 = loc("add.4377_decomp_add"(#loc921))
#loc1820 = loc("add.4377_split_q"(#loc921))
#loc1821 = loc("add.4377_split_k"(#loc921))
#loc1822 = loc("add.4377_split_v"(#loc921))
#loc1823 = loc("add.4377_reshape_q"(#loc921))
#loc1824 = loc("add.4377_reshape_k"(#loc921))
#loc1825 = loc("add.4377_reshape_v"(#loc921))
#loc1826 = loc("add.4377_permute_q"(#loc921))
#loc1827 = loc("add.4377_permute_k"(#loc921))
#loc1828 = loc("add.4377_permute_v"(#loc921))
#loc1829 = loc("select.4436_workaround"(#loc932))
#loc1830 = loc("reshape.4442_concat_heads"(#loc936))
#loc1831 = loc("add.4448_decomp_matmul"(#loc937))
#loc1832 = loc("add.4448_decomp_add"(#loc937))
#loc1833 = loc("add.4535_decomp_matmul"(#loc941))
#loc1834 = loc("add.4535_decomp_add"(#loc941))
#loc1835 = loc("add.4535_decomp_gelu"(#loc941))
#loc1836 = loc("add.4555_decomp_matmul"(#loc943))
#loc1837 = loc("add.4555_decomp_add"(#loc943))
#loc1838 = loc("add.4687_decomp_matmul"(#loc947))
#loc1839 = loc("add.4687_decomp_add"(#loc947))
#loc1840 = loc("add.4687_split_q"(#loc947))
#loc1841 = loc("add.4687_split_k"(#loc947))
#loc1842 = loc("add.4687_split_v"(#loc947))
#loc1843 = loc("add.4687_reshape_q"(#loc947))
#loc1844 = loc("add.4687_reshape_k"(#loc947))
#loc1845 = loc("add.4687_reshape_v"(#loc947))
#loc1846 = loc("add.4687_permute_q"(#loc947))
#loc1847 = loc("add.4687_permute_k"(#loc947))
#loc1848 = loc("add.4687_permute_v"(#loc947))
#loc1849 = loc("select.4746_workaround"(#loc958))
#loc1850 = loc("reshape.4752_concat_heads"(#loc962))
#loc1851 = loc("add.4758_decomp_matmul"(#loc963))
#loc1852 = loc("add.4758_decomp_add"(#loc963))
#loc1853 = loc("add.4845_decomp_matmul"(#loc967))
#loc1854 = loc("add.4845_decomp_add"(#loc967))
#loc1855 = loc("add.4845_decomp_gelu"(#loc967))
#loc1856 = loc("add.4865_decomp_matmul"(#loc969))
#loc1857 = loc("add.4865_decomp_add"(#loc969))
#loc1858 = loc("add.4997_decomp_matmul"(#loc973))
#loc1859 = loc("add.4997_decomp_add"(#loc973))
#loc1860 = loc("add.4997_split_q"(#loc973))
#loc1861 = loc("add.4997_split_k"(#loc973))
#loc1862 = loc("add.4997_split_v"(#loc973))
#loc1863 = loc("add.4997_reshape_q"(#loc973))
#loc1864 = loc("add.4997_reshape_k"(#loc973))
#loc1865 = loc("add.4997_reshape_v"(#loc973))
#loc1866 = loc("add.4997_permute_q"(#loc973))
#loc1867 = loc("add.4997_permute_k"(#loc973))
#loc1868 = loc("add.4997_permute_v"(#loc973))
#loc1869 = loc("select.5056_workaround"(#loc984))
#loc1870 = loc("reshape.5062_concat_heads"(#loc988))
#loc1871 = loc("add.5068_decomp_matmul"(#loc989))
#loc1872 = loc("add.5068_decomp_add"(#loc989))
#loc1873 = loc("add.5155_decomp_matmul"(#loc993))
#loc1874 = loc("add.5155_decomp_add"(#loc993))
#loc1875 = loc("add.5155_decomp_gelu"(#loc993))
#loc1876 = loc("add.5175_decomp_matmul"(#loc995))
#loc1877 = loc("add.5175_decomp_add"(#loc995))
#loc1878 = loc("add.5307_decomp_matmul"(#loc999))
#loc1879 = loc("add.5307_decomp_add"(#loc999))
#loc1880 = loc("add.5307_split_q"(#loc999))
#loc1881 = loc("add.5307_split_k"(#loc999))
#loc1882 = loc("add.5307_split_v"(#loc999))
#loc1883 = loc("add.5307_reshape_q"(#loc999))
#loc1884 = loc("add.5307_reshape_k"(#loc999))
#loc1885 = loc("add.5307_reshape_v"(#loc999))
#loc1886 = loc("add.5307_permute_q"(#loc999))
#loc1887 = loc("add.5307_permute_k"(#loc999))
#loc1888 = loc("add.5307_permute_v"(#loc999))
#loc1889 = loc("select.5366_workaround"(#loc1010))
#loc1890 = loc("reshape.5372_concat_heads"(#loc1014))
#loc1891 = loc("add.5378_decomp_matmul"(#loc1015))
#loc1892 = loc("add.5378_decomp_add"(#loc1015))
#loc1893 = loc("add.5465_decomp_matmul"(#loc1019))
#loc1894 = loc("add.5465_decomp_add"(#loc1019))
#loc1895 = loc("add.5465_decomp_gelu"(#loc1019))
#loc1896 = loc("add.5485_decomp_matmul"(#loc1021))
#loc1897 = loc("add.5485_decomp_add"(#loc1021))
#loc1898 = loc("add.5617_decomp_matmul"(#loc1025))
#loc1899 = loc("add.5617_decomp_add"(#loc1025))
#loc1900 = loc("add.5617_split_q"(#loc1025))
#loc1901 = loc("add.5617_split_k"(#loc1025))
#loc1902 = loc("add.5617_split_v"(#loc1025))
#loc1903 = loc("add.5617_reshape_q"(#loc1025))
#loc1904 = loc("add.5617_reshape_k"(#loc1025))
#loc1905 = loc("add.5617_reshape_v"(#loc1025))
#loc1906 = loc("add.5617_permute_q"(#loc1025))
#loc1907 = loc("add.5617_permute_k"(#loc1025))
#loc1908 = loc("add.5617_permute_v"(#loc1025))
#loc1909 = loc("select.5676_workaround"(#loc1036))
#loc1910 = loc("reshape.5682_concat_heads"(#loc1040))
#loc1911 = loc("add.5688_decomp_matmul"(#loc1041))
#loc1912 = loc("add.5688_decomp_add"(#loc1041))
#loc1913 = loc("add.5775_decomp_matmul"(#loc1045))
#loc1914 = loc("add.5775_decomp_add"(#loc1045))
#loc1915 = loc("add.5775_decomp_gelu"(#loc1045))
#loc1916 = loc("add.5795_decomp_matmul"(#loc1047))
#loc1917 = loc("add.5795_decomp_add"(#loc1047))
#loc1918 = loc("add.5927_decomp_matmul"(#loc1051))
#loc1919 = loc("add.5927_decomp_add"(#loc1051))
#loc1920 = loc("add.5927_split_q"(#loc1051))
#loc1921 = loc("add.5927_split_k"(#loc1051))
#loc1922 = loc("add.5927_split_v"(#loc1051))
#loc1923 = loc("add.5927_reshape_q"(#loc1051))
#loc1924 = loc("add.5927_reshape_k"(#loc1051))
#loc1925 = loc("add.5927_reshape_v"(#loc1051))
#loc1926 = loc("add.5927_permute_q"(#loc1051))
#loc1927 = loc("add.5927_permute_k"(#loc1051))
#loc1928 = loc("add.5927_permute_v"(#loc1051))
#loc1929 = loc("select.5986_workaround"(#loc1062))
#loc1930 = loc("reshape.5992_concat_heads"(#loc1066))
#loc1931 = loc("add.5998_decomp_matmul"(#loc1067))
#loc1932 = loc("add.5998_decomp_add"(#loc1067))
#loc1933 = loc("add.6085_decomp_matmul"(#loc1071))
#loc1934 = loc("add.6085_decomp_add"(#loc1071))
#loc1935 = loc("add.6085_decomp_gelu"(#loc1071))
#loc1936 = loc("add.6105_decomp_matmul"(#loc1073))
#loc1937 = loc("add.6105_decomp_add"(#loc1073))
#loc1938 = loc("add.6237_decomp_matmul"(#loc1077))
#loc1939 = loc("add.6237_decomp_add"(#loc1077))
#loc1940 = loc("add.6237_split_q"(#loc1077))
#loc1941 = loc("add.6237_split_k"(#loc1077))
#loc1942 = loc("add.6237_split_v"(#loc1077))
#loc1943 = loc("add.6237_reshape_q"(#loc1077))
#loc1944 = loc("add.6237_reshape_k"(#loc1077))
#loc1945 = loc("add.6237_reshape_v"(#loc1077))
#loc1946 = loc("add.6237_permute_q"(#loc1077))
#loc1947 = loc("add.6237_permute_k"(#loc1077))
#loc1948 = loc("add.6237_permute_v"(#loc1077))
#loc1949 = loc("select.6296_workaround"(#loc1088))
#loc1950 = loc("reshape.6302_concat_heads"(#loc1092))
#loc1951 = loc("add.6308_decomp_matmul"(#loc1093))
#loc1952 = loc("add.6308_decomp_add"(#loc1093))
#loc1953 = loc("add.6395_decomp_matmul"(#loc1097))
#loc1954 = loc("add.6395_decomp_add"(#loc1097))
#loc1955 = loc("add.6395_decomp_gelu"(#loc1097))
#loc1956 = loc("add.6415_decomp_matmul"(#loc1099))
#loc1957 = loc("add.6415_decomp_add"(#loc1099))
#loc1958 = loc("add.6547_decomp_matmul"(#loc1103))
#loc1959 = loc("add.6547_decomp_add"(#loc1103))
#loc1960 = loc("add.6547_split_q"(#loc1103))
#loc1961 = loc("add.6547_split_k"(#loc1103))
#loc1962 = loc("add.6547_split_v"(#loc1103))
#loc1963 = loc("add.6547_reshape_q"(#loc1103))
#loc1964 = loc("add.6547_reshape_k"(#loc1103))
#loc1965 = loc("add.6547_reshape_v"(#loc1103))
#loc1966 = loc("add.6547_permute_q"(#loc1103))
#loc1967 = loc("add.6547_permute_k"(#loc1103))
#loc1968 = loc("add.6547_permute_v"(#loc1103))
#loc1969 = loc("select.6606_workaround"(#loc1114))
#loc1970 = loc("reshape.6612_concat_heads"(#loc1118))
#loc1971 = loc("add.6618_decomp_matmul"(#loc1119))
#loc1972 = loc("add.6618_decomp_add"(#loc1119))
#loc1973 = loc("add.6705_decomp_matmul"(#loc1123))
#loc1974 = loc("add.6705_decomp_add"(#loc1123))
#loc1975 = loc("add.6705_decomp_gelu"(#loc1123))
#loc1976 = loc("add.6725_decomp_matmul"(#loc1125))
#loc1977 = loc("add.6725_decomp_add"(#loc1125))
#loc1978 = loc("add.6857_decomp_matmul"(#loc1129))
#loc1979 = loc("add.6857_decomp_add"(#loc1129))
#loc1980 = loc("add.6857_split_q"(#loc1129))
#loc1981 = loc("add.6857_split_k"(#loc1129))
#loc1982 = loc("add.6857_split_v"(#loc1129))
#loc1983 = loc("add.6857_reshape_q"(#loc1129))
#loc1984 = loc("add.6857_reshape_k"(#loc1129))
#loc1985 = loc("add.6857_reshape_v"(#loc1129))
#loc1986 = loc("add.6857_permute_q"(#loc1129))
#loc1987 = loc("add.6857_permute_k"(#loc1129))
#loc1988 = loc("add.6857_permute_v"(#loc1129))
#loc1989 = loc("select.6916_workaround"(#loc1140))
#loc1990 = loc("reshape.6922_concat_heads"(#loc1144))
#loc1991 = loc("add.6928_decomp_matmul"(#loc1145))
#loc1992 = loc("add.6928_decomp_add"(#loc1145))
#loc1993 = loc("add.7015_decomp_matmul"(#loc1149))
#loc1994 = loc("add.7015_decomp_add"(#loc1149))
#loc1995 = loc("add.7015_decomp_gelu"(#loc1149))
#loc1996 = loc("add.7035_decomp_matmul"(#loc1151))
#loc1997 = loc("add.7035_decomp_add"(#loc1151))
#loc1998 = loc("add.7167_decomp_matmul"(#loc1155))
#loc1999 = loc("add.7167_decomp_add"(#loc1155))
#loc2000 = loc("add.7167_split_q"(#loc1155))
#loc2001 = loc("add.7167_split_k"(#loc1155))
#loc2002 = loc("add.7167_split_v"(#loc1155))
#loc2003 = loc("add.7167_reshape_q"(#loc1155))
#loc2004 = loc("add.7167_reshape_k"(#loc1155))
#loc2005 = loc("add.7167_reshape_v"(#loc1155))
#loc2006 = loc("add.7167_permute_q"(#loc1155))
#loc2007 = loc("add.7167_permute_k"(#loc1155))
#loc2008 = loc("add.7167_permute_v"(#loc1155))
#loc2009 = loc("select.7226_workaround"(#loc1166))
#loc2010 = loc("reshape.7232_concat_heads"(#loc1170))
#loc2011 = loc("add.7238_decomp_matmul"(#loc1171))
#loc2012 = loc("add.7238_decomp_add"(#loc1171))
#loc2013 = loc("add.7325_decomp_matmul"(#loc1175))
#loc2014 = loc("add.7325_decomp_add"(#loc1175))
#loc2015 = loc("add.7325_decomp_gelu"(#loc1175))
#loc2016 = loc("add.7345_decomp_matmul"(#loc1177))
#loc2017 = loc("add.7345_decomp_add"(#loc1177))
#loc2018 = loc("add.7477_decomp_matmul"(#loc1181))
#loc2019 = loc("add.7477_decomp_add"(#loc1181))
#loc2020 = loc("add.7477_split_q"(#loc1181))
#loc2021 = loc("add.7477_split_k"(#loc1181))
#loc2022 = loc("add.7477_split_v"(#loc1181))
#loc2023 = loc("add.7477_reshape_q"(#loc1181))
#loc2024 = loc("add.7477_reshape_k"(#loc1181))
#loc2025 = loc("add.7477_reshape_v"(#loc1181))
#loc2026 = loc("add.7477_permute_q"(#loc1181))
#loc2027 = loc("add.7477_permute_k"(#loc1181))
#loc2028 = loc("add.7477_permute_v"(#loc1181))
#loc2029 = loc("select.7536_workaround"(#loc1192))
#loc2030 = loc("reshape.7542_concat_heads"(#loc1196))
#loc2031 = loc("add.7548_decomp_matmul"(#loc1197))
#loc2032 = loc("add.7548_decomp_add"(#loc1197))
#loc2033 = loc("add.7635_decomp_matmul"(#loc1201))
#loc2034 = loc("add.7635_decomp_add"(#loc1201))
#loc2035 = loc("add.7635_decomp_gelu"(#loc1201))
#loc2036 = loc("add.7655_decomp_matmul"(#loc1203))
#loc2037 = loc("add.7655_decomp_add"(#loc1203))
#loc2038 = loc("add.7787_decomp_matmul"(#loc1207))
#loc2039 = loc("add.7787_decomp_add"(#loc1207))
#loc2040 = loc("add.7787_split_q"(#loc1207))
#loc2041 = loc("add.7787_split_k"(#loc1207))
#loc2042 = loc("add.7787_split_v"(#loc1207))
#loc2043 = loc("add.7787_reshape_q"(#loc1207))
#loc2044 = loc("add.7787_reshape_k"(#loc1207))
#loc2045 = loc("add.7787_reshape_v"(#loc1207))
#loc2046 = loc("add.7787_permute_q"(#loc1207))
#loc2047 = loc("add.7787_permute_k"(#loc1207))
#loc2048 = loc("add.7787_permute_v"(#loc1207))
#loc2049 = loc("select.7846_workaround"(#loc1218))
#loc2050 = loc("reshape.7852_concat_heads"(#loc1222))
#loc2051 = loc("add.7858_decomp_matmul"(#loc1223))
#loc2052 = loc("add.7858_decomp_add"(#loc1223))
#loc2053 = loc("add.7945_decomp_matmul"(#loc1227))
#loc2054 = loc("add.7945_decomp_add"(#loc1227))
#loc2055 = loc("add.7945_decomp_gelu"(#loc1227))
#loc2056 = loc("add.7965_decomp_matmul"(#loc1229))
#loc2057 = loc("add.7965_decomp_add"(#loc1229))
#loc2058 = loc("add.8097_decomp_matmul"(#loc1233))
#loc2059 = loc("add.8097_decomp_add"(#loc1233))
#loc2060 = loc("add.8097_split_q"(#loc1233))
#loc2061 = loc("add.8097_split_k"(#loc1233))
#loc2062 = loc("add.8097_split_v"(#loc1233))
#loc2063 = loc("add.8097_reshape_q"(#loc1233))
#loc2064 = loc("add.8097_reshape_k"(#loc1233))
#loc2065 = loc("add.8097_reshape_v"(#loc1233))
#loc2066 = loc("add.8097_permute_q"(#loc1233))
#loc2067 = loc("add.8097_permute_k"(#loc1233))
#loc2068 = loc("add.8097_permute_v"(#loc1233))
#loc2069 = loc("select.8156_workaround"(#loc1244))
#loc2070 = loc("reshape.8162_concat_heads"(#loc1248))
#loc2071 = loc("add.8168_decomp_matmul"(#loc1249))
#loc2072 = loc("add.8168_decomp_add"(#loc1249))
#loc2073 = loc("add.8255_decomp_matmul"(#loc1253))
#loc2074 = loc("add.8255_decomp_add"(#loc1253))
#loc2075 = loc("add.8255_decomp_gelu"(#loc1253))
#loc2076 = loc("add.8275_decomp_matmul"(#loc1255))
#loc2077 = loc("add.8275_decomp_add"(#loc1255))
#loc2078 = loc("add.8407_decomp_matmul"(#loc1259))
#loc2079 = loc("add.8407_decomp_add"(#loc1259))
#loc2080 = loc("add.8407_split_q"(#loc1259))
#loc2081 = loc("add.8407_split_k"(#loc1259))
#loc2082 = loc("add.8407_split_v"(#loc1259))
#loc2083 = loc("add.8407_reshape_q"(#loc1259))
#loc2084 = loc("add.8407_reshape_k"(#loc1259))
#loc2085 = loc("add.8407_reshape_v"(#loc1259))
#loc2086 = loc("add.8407_permute_q"(#loc1259))
#loc2087 = loc("add.8407_permute_k"(#loc1259))
#loc2088 = loc("add.8407_permute_v"(#loc1259))
#loc2089 = loc("select.8466_workaround"(#loc1270))
#loc2090 = loc("reshape.8472_concat_heads"(#loc1274))
#loc2091 = loc("add.8478_decomp_matmul"(#loc1275))
#loc2092 = loc("add.8478_decomp_add"(#loc1275))
#loc2093 = loc("add.8565_decomp_matmul"(#loc1279))
#loc2094 = loc("add.8565_decomp_add"(#loc1279))
#loc2095 = loc("add.8565_decomp_gelu"(#loc1279))
#loc2096 = loc("add.8585_decomp_matmul"(#loc1281))
#loc2097 = loc("add.8585_decomp_add"(#loc1281))
#loc2098 = loc("add.8717_decomp_matmul"(#loc1285))
#loc2099 = loc("add.8717_decomp_add"(#loc1285))
#loc2100 = loc("add.8717_split_q"(#loc1285))
#loc2101 = loc("add.8717_split_k"(#loc1285))
#loc2102 = loc("add.8717_split_v"(#loc1285))
#loc2103 = loc("add.8717_reshape_q"(#loc1285))
#loc2104 = loc("add.8717_reshape_k"(#loc1285))
#loc2105 = loc("add.8717_reshape_v"(#loc1285))
#loc2106 = loc("add.8717_permute_q"(#loc1285))
#loc2107 = loc("add.8717_permute_k"(#loc1285))
#loc2108 = loc("add.8717_permute_v"(#loc1285))
#loc2109 = loc("select.8776_workaround"(#loc1296))
#loc2110 = loc("reshape.8782_concat_heads"(#loc1300))
#loc2111 = loc("add.8788_decomp_matmul"(#loc1301))
#loc2112 = loc("add.8788_decomp_add"(#loc1301))
#loc2113 = loc("add.8875_decomp_matmul"(#loc1305))
#loc2114 = loc("add.8875_decomp_add"(#loc1305))
#loc2115 = loc("add.8875_decomp_gelu"(#loc1305))
#loc2116 = loc("add.8895_decomp_matmul"(#loc1307))
#loc2117 = loc("add.8895_decomp_add"(#loc1307))
#loc2118 = loc("add.9027_decomp_matmul"(#loc1311))
#loc2119 = loc("add.9027_decomp_add"(#loc1311))
#loc2120 = loc("add.9027_split_q"(#loc1311))
#loc2121 = loc("add.9027_split_k"(#loc1311))
#loc2122 = loc("add.9027_split_v"(#loc1311))
#loc2123 = loc("add.9027_reshape_q"(#loc1311))
#loc2124 = loc("add.9027_reshape_k"(#loc1311))
#loc2125 = loc("add.9027_reshape_v"(#loc1311))
#loc2126 = loc("add.9027_permute_q"(#loc1311))
#loc2127 = loc("add.9027_permute_k"(#loc1311))
#loc2128 = loc("add.9027_permute_v"(#loc1311))
#loc2129 = loc("select.9086_workaround"(#loc1322))
#loc2130 = loc("reshape.9092_concat_heads"(#loc1326))
#loc2131 = loc("add.9098_decomp_matmul"(#loc1327))
#loc2132 = loc("add.9098_decomp_add"(#loc1327))
#loc2133 = loc("add.9185_decomp_matmul"(#loc1331))
#loc2134 = loc("add.9185_decomp_add"(#loc1331))
#loc2135 = loc("add.9185_decomp_gelu"(#loc1331))
#loc2136 = loc("add.9205_decomp_matmul"(#loc1333))
#loc2137 = loc("add.9205_decomp_add"(#loc1333))
#loc2138 = loc("add.9337_decomp_matmul"(#loc1337))
#loc2139 = loc("add.9337_decomp_add"(#loc1337))
#loc2140 = loc("add.9337_split_q"(#loc1337))
#loc2141 = loc("add.9337_split_k"(#loc1337))
#loc2142 = loc("add.9337_split_v"(#loc1337))
#loc2143 = loc("add.9337_reshape_q"(#loc1337))
#loc2144 = loc("add.9337_reshape_k"(#loc1337))
#loc2145 = loc("add.9337_reshape_v"(#loc1337))
#loc2146 = loc("add.9337_permute_q"(#loc1337))
#loc2147 = loc("add.9337_permute_k"(#loc1337))
#loc2148 = loc("add.9337_permute_v"(#loc1337))
#loc2149 = loc("select.9396_workaround"(#loc1348))
#loc2150 = loc("reshape.9402_concat_heads"(#loc1352))
#loc2151 = loc("add.9408_decomp_matmul"(#loc1353))
#loc2152 = loc("add.9408_decomp_add"(#loc1353))
#loc2153 = loc("add.9495_decomp_matmul"(#loc1357))
#loc2154 = loc("add.9495_decomp_add"(#loc1357))
#loc2155 = loc("add.9495_decomp_gelu"(#loc1357))
#loc2156 = loc("add.9515_decomp_matmul"(#loc1359))
#loc2157 = loc("add.9515_decomp_add"(#loc1359))
#loc2158 = loc("add.9647_decomp_matmul"(#loc1363))
#loc2159 = loc("add.9647_decomp_add"(#loc1363))
#loc2160 = loc("add.9647_split_q"(#loc1363))
#loc2161 = loc("add.9647_split_k"(#loc1363))
#loc2162 = loc("add.9647_split_v"(#loc1363))
#loc2163 = loc("add.9647_reshape_q"(#loc1363))
#loc2164 = loc("add.9647_reshape_k"(#loc1363))
#loc2165 = loc("add.9647_reshape_v"(#loc1363))
#loc2166 = loc("add.9647_permute_q"(#loc1363))
#loc2167 = loc("add.9647_permute_k"(#loc1363))
#loc2168 = loc("add.9647_permute_v"(#loc1363))
#loc2169 = loc("select.9706_workaround"(#loc1374))
#loc2170 = loc("reshape.9712_concat_heads"(#loc1378))
#loc2171 = loc("add.9718_decomp_matmul"(#loc1379))
#loc2172 = loc("add.9718_decomp_add"(#loc1379))
#loc2173 = loc("add.9805_decomp_matmul"(#loc1383))
#loc2174 = loc("add.9805_decomp_add"(#loc1383))
#loc2175 = loc("add.9805_decomp_gelu"(#loc1383))
#loc2176 = loc("add.9825_decomp_matmul"(#loc1385))
#loc2177 = loc("add.9825_decomp_add"(#loc1385))
#loc2178 = loc("add.9957_decomp_matmul"(#loc1389))
#loc2179 = loc("add.9957_decomp_add"(#loc1389))
#loc2180 = loc("add.9957_split_q"(#loc1389))
#loc2181 = loc("add.9957_split_k"(#loc1389))
#loc2182 = loc("add.9957_split_v"(#loc1389))
#loc2183 = loc("add.9957_reshape_q"(#loc1389))
#loc2184 = loc("add.9957_reshape_k"(#loc1389))
#loc2185 = loc("add.9957_reshape_v"(#loc1389))
#loc2186 = loc("add.9957_permute_q"(#loc1389))
#loc2187 = loc("add.9957_permute_k"(#loc1389))
#loc2188 = loc("add.9957_permute_v"(#loc1389))
#loc2189 = loc("select.10016_workaround"(#loc1400))
#loc2190 = loc("reshape.10022_concat_heads"(#loc1404))
#loc2191 = loc("add.10028_decomp_matmul"(#loc1405))
#loc2192 = loc("add.10028_decomp_add"(#loc1405))
#loc2193 = loc("add.10115_decomp_matmul"(#loc1409))
#loc2194 = loc("add.10115_decomp_add"(#loc1409))
#loc2195 = loc("add.10115_decomp_gelu"(#loc1409))
#loc2196 = loc("add.10135_decomp_matmul"(#loc1411))
#loc2197 = loc("add.10135_decomp_add"(#loc1411))
#loc2198 = loc("add.10267_decomp_matmul"(#loc1415))
#loc2199 = loc("add.10267_decomp_add"(#loc1415))
#loc2200 = loc("add.10267_split_q"(#loc1415))
#loc2201 = loc("add.10267_split_k"(#loc1415))
#loc2202 = loc("add.10267_split_v"(#loc1415))
#loc2203 = loc("add.10267_reshape_q"(#loc1415))
#loc2204 = loc("add.10267_reshape_k"(#loc1415))
#loc2205 = loc("add.10267_reshape_v"(#loc1415))
#loc2206 = loc("add.10267_permute_q"(#loc1415))
#loc2207 = loc("add.10267_permute_k"(#loc1415))
#loc2208 = loc("add.10267_permute_v"(#loc1415))
#loc2209 = loc("select.10326_workaround"(#loc1426))
#loc2210 = loc("reshape.10332_concat_heads"(#loc1430))
#loc2211 = loc("add.10338_decomp_matmul"(#loc1431))
#loc2212 = loc("add.10338_decomp_add"(#loc1431))
#loc2213 = loc("add.10425_decomp_matmul"(#loc1435))
#loc2214 = loc("add.10425_decomp_add"(#loc1435))
#loc2215 = loc("add.10425_decomp_gelu"(#loc1435))
#loc2216 = loc("add.10445_decomp_matmul"(#loc1437))
#loc2217 = loc("add.10445_decomp_add"(#loc1437))
#loc2218 = loc("add.10577_decomp_matmul"(#loc1441))
#loc2219 = loc("add.10577_decomp_add"(#loc1441))
#loc2220 = loc("add.10577_split_q"(#loc1441))
#loc2221 = loc("add.10577_split_k"(#loc1441))
#loc2222 = loc("add.10577_split_v"(#loc1441))
#loc2223 = loc("add.10577_reshape_q"(#loc1441))
#loc2224 = loc("add.10577_reshape_k"(#loc1441))
#loc2225 = loc("add.10577_reshape_v"(#loc1441))
#loc2226 = loc("add.10577_permute_q"(#loc1441))
#loc2227 = loc("add.10577_permute_k"(#loc1441))
#loc2228 = loc("add.10577_permute_v"(#loc1441))
#loc2229 = loc("select.10636_workaround"(#loc1452))
#loc2230 = loc("reshape.10642_concat_heads"(#loc1456))
#loc2231 = loc("add.10648_decomp_matmul"(#loc1457))
#loc2232 = loc("add.10648_decomp_add"(#loc1457))
#loc2233 = loc("add.10735_decomp_matmul"(#loc1461))
#loc2234 = loc("add.10735_decomp_add"(#loc1461))
#loc2235 = loc("add.10735_decomp_gelu"(#loc1461))
#loc2236 = loc("add.10755_decomp_matmul"(#loc1463))
#loc2237 = loc("add.10755_decomp_add"(#loc1463))
#loc2238 = loc("add.10887_decomp_matmul"(#loc1467))
#loc2239 = loc("add.10887_decomp_add"(#loc1467))
#loc2240 = loc("add.10887_split_q"(#loc1467))
#loc2241 = loc("add.10887_split_k"(#loc1467))
#loc2242 = loc("add.10887_split_v"(#loc1467))
#loc2243 = loc("add.10887_reshape_q"(#loc1467))
#loc2244 = loc("add.10887_reshape_k"(#loc1467))
#loc2245 = loc("add.10887_reshape_v"(#loc1467))
#loc2246 = loc("add.10887_permute_q"(#loc1467))
#loc2247 = loc("add.10887_permute_k"(#loc1467))
#loc2248 = loc("add.10887_permute_v"(#loc1467))
#loc2249 = loc("select.10946_workaround"(#loc1478))
#loc2250 = loc("reshape.10952_concat_heads"(#loc1482))
#loc2251 = loc("add.10958_decomp_matmul"(#loc1483))
#loc2252 = loc("add.10958_decomp_add"(#loc1483))
#loc2253 = loc("add.11045_decomp_matmul"(#loc1487))
#loc2254 = loc("add.11045_decomp_add"(#loc1487))
#loc2255 = loc("add.11045_decomp_gelu"(#loc1487))
#loc2256 = loc("add.11065_decomp_matmul"(#loc1489))
#loc2257 = loc("add.11065_decomp_add"(#loc1489))
#loc2258 = loc("add.11197_decomp_matmul"(#loc1493))
#loc2259 = loc("add.11197_decomp_add"(#loc1493))
#loc2260 = loc("add.11197_split_q"(#loc1493))
#loc2261 = loc("add.11197_split_k"(#loc1493))
#loc2262 = loc("add.11197_split_v"(#loc1493))
#loc2263 = loc("add.11197_reshape_q"(#loc1493))
#loc2264 = loc("add.11197_reshape_k"(#loc1493))
#loc2265 = loc("add.11197_reshape_v"(#loc1493))
#loc2266 = loc("add.11197_permute_q"(#loc1493))
#loc2267 = loc("add.11197_permute_k"(#loc1493))
#loc2268 = loc("add.11197_permute_v"(#loc1493))
#loc2269 = loc("select.11256_workaround"(#loc1504))
#loc2270 = loc("reshape.11262_concat_heads"(#loc1508))
#loc2271 = loc("add.11268_decomp_matmul"(#loc1509))
#loc2272 = loc("add.11268_decomp_add"(#loc1509))
#loc2273 = loc("add.11355_decomp_matmul"(#loc1513))
#loc2274 = loc("add.11355_decomp_add"(#loc1513))
#loc2275 = loc("add.11355_decomp_gelu"(#loc1513))
#loc2276 = loc("add.11375_decomp_matmul"(#loc1515))
#loc2277 = loc("add.11375_decomp_add"(#loc1515))
#loc2278 = loc("add.11507_decomp_matmul"(#loc1519))
#loc2279 = loc("add.11507_decomp_add"(#loc1519))
#loc2280 = loc("add.11507_split_q"(#loc1519))
#loc2281 = loc("add.11507_split_k"(#loc1519))
#loc2282 = loc("add.11507_split_v"(#loc1519))
#loc2283 = loc("add.11507_reshape_q"(#loc1519))
#loc2284 = loc("add.11507_reshape_k"(#loc1519))
#loc2285 = loc("add.11507_reshape_v"(#loc1519))
#loc2286 = loc("add.11507_permute_q"(#loc1519))
#loc2287 = loc("add.11507_permute_k"(#loc1519))
#loc2288 = loc("add.11507_permute_v"(#loc1519))
#loc2289 = loc("select.11566_workaround"(#loc1530))
#loc2290 = loc("reshape.11572_concat_heads"(#loc1534))
#loc2291 = loc("add.11578_decomp_matmul"(#loc1535))
#loc2292 = loc("add.11578_decomp_add"(#loc1535))
#loc2293 = loc("add.11665_decomp_matmul"(#loc1539))
#loc2294 = loc("add.11665_decomp_add"(#loc1539))
#loc2295 = loc("add.11665_decomp_gelu"(#loc1539))
#loc2296 = loc("add.11685_decomp_matmul"(#loc1541))
#loc2297 = loc("add.11685_decomp_add"(#loc1541))
#loc2298 = loc("add.11817_decomp_matmul"(#loc1545))
#loc2299 = loc("add.11817_decomp_add"(#loc1545))
#loc2300 = loc("add.11817_split_q"(#loc1545))
#loc2301 = loc("add.11817_split_k"(#loc1545))
#loc2302 = loc("add.11817_split_v"(#loc1545))
#loc2303 = loc("add.11817_reshape_q"(#loc1545))
#loc2304 = loc("add.11817_reshape_k"(#loc1545))
#loc2305 = loc("add.11817_reshape_v"(#loc1545))
#loc2306 = loc("add.11817_permute_q"(#loc1545))
#loc2307 = loc("add.11817_permute_k"(#loc1545))
#loc2308 = loc("add.11817_permute_v"(#loc1545))
#loc2309 = loc("select.11876_workaround"(#loc1556))
#loc2310 = loc("reshape.11882_concat_heads"(#loc1560))
#loc2311 = loc("add.11888_decomp_matmul"(#loc1561))
#loc2312 = loc("add.11888_decomp_add"(#loc1561))
#loc2313 = loc("add.11975_decomp_matmul"(#loc1565))
#loc2314 = loc("add.11975_decomp_add"(#loc1565))
#loc2315 = loc("add.11975_decomp_gelu"(#loc1565))
#loc2316 = loc("add.11995_decomp_matmul"(#loc1567))
#loc2317 = loc("add.11995_decomp_add"(#loc1567))
#loc2318 = loc("add.12005_decomp_matmul"(#loc1569))
#loc2319 = loc("add.12005_decomp_add"(#loc1569))
#loc2320 = loc("select.12172_workaround"(#loc1580))
#loc2321 = loc("select.12588_workaround"(#loc1612))
#loc2322 = loc("select.13004_workaround"(#loc1644))
#loc2323 = loc("select.13420_workaround"(#loc1676))
#loc2324 = loc("reshape.13556_tm1"(#loc1689))
#loc2325 = loc("add.13555_decomp_matmul"(#loc1690))
#loc2326 = loc("add.13555_decomp_add"(#loc1690))
#loc2327 = loc("add.13562_decomp_matmul"(#loc1691))
#loc2328 = loc("add.13562_decomp_add"(#loc1691))
#loc2329 = loc("convolution.2299_input_reshape"(#loc1696))
