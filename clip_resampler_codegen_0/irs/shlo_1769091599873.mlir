#loc1 = loc("p0.2")
#loc2 = loc("p1.7")
#loc3 = loc("p2.15")
#loc4 = loc("p3.19")
#loc5 = loc("p4.32")
#loc6 = loc("p5.38")
#loc7 = loc("p6.43")
#loc8 = loc("p7.49")
#loc9 = loc("p8.54")
#loc10 = loc("p9.139")
#loc11 = loc("p10.144")
#loc12 = loc("p11.152")
#loc13 = loc("p12.156")
#loc14 = loc("p13.163")
#loc15 = loc("p14.167")
#loc16 = loc("p15.173")
#loc17 = loc("p16.177")
#loc18 = loc("p17.183")
#loc19 = loc("p18.188")
#loc20 = loc("p19.197")
#loc21 = loc("p20.201")
#loc22 = loc("p21.207")
#loc23 = loc("p22.211")
#loc24 = loc("p23.217")
#loc25 = loc("p24.222")
#loc26 = loc("p25.231")
#loc27 = loc("p26.235")
#loc28 = loc("p27.241")
#loc29 = loc("p28.245")
#loc30 = loc("p29.251")
#loc31 = loc("p30.256")
#loc32 = loc("p31.265")
#loc33 = loc("p32.269")
#loc34 = loc("p33.275")
#loc35 = loc("p34.279")
#loc36 = loc("p35.285")
#loc37 = loc("p36.290")
#loc38 = loc("p37.299")
#loc39 = loc("p38.303")
#loc40 = loc("p39.309")
#loc41 = loc("p40.313")
#loc42 = loc("p41.319")
#loc43 = loc("p42.324")
#loc44 = loc("p43.333")
#loc45 = loc("p44.337")
#loc46 = loc("p45.343")
#loc47 = loc("p46.347")
#loc48 = loc("p47.353")
#loc49 = loc("p48.358")
#loc50 = loc("p49.367")
#loc51 = loc("p50.371")
#loc52 = loc("p51.377")
#loc53 = loc("p52.381")
#loc54 = loc("p53.387")
#loc55 = loc("p54.392")
#loc56 = loc("p55.401")
#loc57 = loc("p56.405")
#loc58 = loc("p57.411")
#loc59 = loc("p58.415")
#loc60 = loc("p59.421")
#loc61 = loc("p60.426")
#loc62 = loc("p61.435")
#loc63 = loc("p62.439")
#loc64 = loc("p63.445")
#loc65 = loc("p64.449")
#loc66 = loc("p65.455")
#loc67 = loc("p66.460")
#loc68 = loc("p67.469")
#loc69 = loc("p68.473")
#loc70 = loc("p69.479")
#loc71 = loc("p70.483")
#loc72 = loc("p71.489")
#loc73 = loc("p72.494")
#loc74 = loc("p73.503")
#loc75 = loc("p74.507")
#loc76 = loc("p75.513")
#loc77 = loc("p76.517")
#loc78 = loc("p77.523")
#loc79 = loc("p78.528")
#loc80 = loc("p79.537")
#loc81 = loc("p80.541")
#loc82 = loc("p81.547")
#loc83 = loc("p82.551")
#loc84 = loc("p83.557")
#loc85 = loc("p84.562")
#loc86 = loc("p85.571")
#loc87 = loc("p86.575")
#loc88 = loc("p87.581")
#loc89 = loc("p88.585")
#loc90 = loc("p89.591")
#loc91 = loc("p90.596")
#loc92 = loc("p91.605")
#loc93 = loc("p92.609")
#loc94 = loc("p93.615")
#loc95 = loc("p94.619")
#loc96 = loc("p95.625")
#loc97 = loc("p96.630")
#loc98 = loc("p97.639")
#loc99 = loc("p98.643")
#loc100 = loc("p99.649")
#loc101 = loc("p100.653")
#loc102 = loc("p101.659")
#loc103 = loc("p102.664")
#loc104 = loc("p103.673")
#loc105 = loc("p104.677")
#loc106 = loc("p105.683")
#loc107 = loc("p106.687")
#loc108 = loc("p107.693")
#loc109 = loc("p108.698")
#loc110 = loc("p109.707")
#loc111 = loc("p110.711")
#loc112 = loc("p111.717")
#loc113 = loc("p112.721")
#loc114 = loc("p113.727")
#loc115 = loc("p114.732")
#loc116 = loc("p115.741")
#loc117 = loc("p116.745")
#loc118 = loc("p117.751")
#loc119 = loc("p118.755")
#loc120 = loc("p119.761")
#loc121 = loc("p120.766")
#loc122 = loc("p121.775")
#loc123 = loc("p122.779")
#loc124 = loc("p123.785")
#loc125 = loc("p124.789")
#loc126 = loc("p125.795")
#loc127 = loc("p126.800")
#loc128 = loc("p127.809")
#loc129 = loc("p128.813")
#loc130 = loc("p129.819")
#loc131 = loc("p130.823")
#loc132 = loc("p131.829")
#loc133 = loc("p132.834")
#loc134 = loc("p133.843")
#loc135 = loc("p134.847")
#loc136 = loc("p135.853")
#loc137 = loc("p136.857")
#loc138 = loc("p137.863")
#loc139 = loc("p138.868")
#loc140 = loc("p139.877")
#loc141 = loc("p140.881")
#loc142 = loc("p141.887")
#loc143 = loc("p142.891")
#loc144 = loc("p143.897")
#loc145 = loc("p144.902")
#loc146 = loc("p145.911")
#loc147 = loc("p146.915")
#loc148 = loc("p147.921")
#loc149 = loc("p148.925")
#loc150 = loc("p149.931")
#loc151 = loc("p150.936")
#loc152 = loc("p151.945")
#loc153 = loc("p152.949")
#loc154 = loc("p153.955")
#loc155 = loc("p154.959")
#loc156 = loc("p155.965")
#loc157 = loc("p156.970")
#loc158 = loc("p157.979")
#loc159 = loc("p158.983")
#loc160 = loc("p159.989")
#loc161 = loc("p160.993")
#loc162 = loc("p161.999")
#loc163 = loc("p162.1004")
#loc164 = loc("p163.1013")
#loc165 = loc("p164.1017")
#loc166 = loc("p165.1023")
#loc167 = loc("p166.1027")
#loc168 = loc("p167.1033")
#loc169 = loc("p168.1038")
#loc170 = loc("p169.1047")
#loc171 = loc("p170.1051")
#loc172 = loc("p171.1057")
#loc173 = loc("p172.1061")
#loc174 = loc("p173.1067")
#loc175 = loc("p174.1072")
#loc176 = loc("p175.1081")
#loc177 = loc("p176.1085")
#loc178 = loc("p177.1091")
#loc179 = loc("p178.1095")
#loc180 = loc("p179.1101")
#loc181 = loc("p180.1106")
#loc182 = loc("p181.1115")
#loc183 = loc("p182.1119")
#loc184 = loc("p183.1125")
#loc185 = loc("p184.1129")
#loc186 = loc("p185.1135")
#loc187 = loc("p186.1140")
#loc188 = loc("p187.1149")
#loc189 = loc("p188.1153")
#loc190 = loc("p189.1159")
#loc191 = loc("p190.1163")
#loc192 = loc("p191.1169")
#loc193 = loc("p192.1174")
#loc194 = loc("p193.1183")
#loc195 = loc("p194.1187")
#loc196 = loc("p195.1193")
#loc197 = loc("p196.1197")
#loc198 = loc("p197.1203")
#loc199 = loc("p198.1208")
#loc200 = loc("p199.1217")
#loc201 = loc("p200.1221")
#loc202 = loc("p201.1227")
#loc203 = loc("p202.1231")
#loc204 = loc("p203.1237")
#loc205 = loc("p204.1242")
#loc206 = loc("p205.1251")
#loc207 = loc("p206.1255")
#loc208 = loc("p207.1261")
#loc209 = loc("p208.1265")
#loc210 = loc("p209.1271")
#loc211 = loc("p210.1276")
#loc212 = loc("p211.1285")
#loc213 = loc("p212.1289")
#loc214 = loc("p213.1295")
#loc215 = loc("p214.1299")
#loc216 = loc("p215.1305")
#loc217 = loc("p216.1310")
#loc218 = loc("p217.1319")
#loc219 = loc("p218.1323")
#loc220 = loc("p219.1329")
#loc221 = loc("p220.1333")
#loc222 = loc("p221.1339")
#loc223 = loc("p222.1344")
#loc224 = loc("p223.1353")
#loc225 = loc("p224.1357")
#loc226 = loc("p225.1363")
#loc227 = loc("p226.1367")
#loc228 = loc("p227.1373")
#loc229 = loc("p228.1378")
#loc230 = loc("p229.1387")
#loc231 = loc("p230.1391")
#loc232 = loc("p231.1397")
#loc233 = loc("p232.1401")
#loc234 = loc("p233.1407")
#loc235 = loc("p234.1412")
#loc236 = loc("p235.1421")
#loc237 = loc("p236.1425")
#loc238 = loc("p237.1431")
#loc239 = loc("p238.1435")
#loc240 = loc("p239.1441")
#loc241 = loc("p240.1446")
#loc242 = loc("p241.1455")
#loc243 = loc("p242.1459")
#loc244 = loc("p243.1465")
#loc245 = loc("p244.1469")
#loc246 = loc("p245.1475")
#loc247 = loc("p246.1480")
#loc248 = loc("p247.1489")
#loc249 = loc("p248.1493")
#loc250 = loc("p249.1499")
#loc251 = loc("p250.1503")
#loc252 = loc("p251.1509")
#loc253 = loc("p252.1514")
#loc254 = loc("p253.1523")
#loc255 = loc("p254.1527")
#loc256 = loc("p255.1533")
#loc257 = loc("p256.1537")
#loc258 = loc("p257.1543")
#loc259 = loc("p258.1548")
#loc260 = loc("p259.1557")
#loc261 = loc("p260.1561")
#loc262 = loc("p261.1567")
#loc263 = loc("p262.1571")
#loc264 = loc("p263.1577")
#loc265 = loc("p264.1582")
#loc266 = loc("p265.1591")
#loc267 = loc("p266.1595")
#loc268 = loc("p267.1601")
#loc269 = loc("p268.1605")
#loc270 = loc("p269.1611")
#loc271 = loc("p270.1616")
#loc272 = loc("p271.1625")
#loc273 = loc("p272.1629")
#loc274 = loc("p273.1635")
#loc275 = loc("p274.1639")
#loc276 = loc("p275.1645")
#loc277 = loc("p276.1650")
#loc278 = loc("p277.1659")
#loc279 = loc("p278.1663")
#loc280 = loc("p279.1669")
#loc281 = loc("p280.1673")
#loc282 = loc("p281.1679")
#loc283 = loc("p282.1684")
#loc284 = loc("p283.1693")
#loc285 = loc("p284.1697")
#loc286 = loc("p285.1703")
#loc287 = loc("p286.1707")
#loc288 = loc("p287.1713")
#loc289 = loc("p288.1718")
#loc290 = loc("p289.1727")
#loc291 = loc("p290.1731")
#loc292 = loc("p291.1737")
#loc293 = loc("p292.1741")
#loc294 = loc("p293.1747")
#loc295 = loc("p294.1752")
#loc296 = loc("p295.1761")
#loc297 = loc("p296.1765")
#loc298 = loc("p297.1771")
#loc299 = loc("p298.1775")
#loc300 = loc("p299.1781")
#loc301 = loc("p300.1786")
#loc302 = loc("p301.1795")
#loc303 = loc("p302.1799")
#loc304 = loc("p303.1805")
#loc305 = loc("p304.1809")
#loc306 = loc("p305.1815")
#loc307 = loc("p306.1820")
#loc308 = loc("p307.1829")
#loc309 = loc("p308.1833")
#loc310 = loc("p309.1839")
#loc311 = loc("p310.1843")
#loc312 = loc("p311.1849")
#loc313 = loc("p312.1854")
#loc314 = loc("p313.1863")
#loc315 = loc("p314.1867")
#loc316 = loc("p315.1873")
#loc317 = loc("p316.1877")
#loc318 = loc("p317.1883")
#loc319 = loc("p318.1888")
#loc320 = loc("p319.1897")
#loc321 = loc("p320.1901")
#loc322 = loc("p321.1907")
#loc323 = loc("p322.1911")
#loc324 = loc("p323.1917")
#loc325 = loc("p324.1922")
#loc326 = loc("p325.1931")
#loc327 = loc("p326.1935")
#loc328 = loc("p327.1941")
#loc329 = loc("p328.1945")
#loc330 = loc("p329.1951")
#loc331 = loc("p330.1956")
#loc332 = loc("p331.1965")
#loc333 = loc("p332.1969")
#loc334 = loc("p333.1975")
#loc335 = loc("p334.1979")
#loc336 = loc("p335.1985")
#loc337 = loc("p336.1990")
#loc338 = loc("p337.1999")
#loc339 = loc("p338.2003")
#loc340 = loc("p339.2009")
#loc341 = loc("p340.2013")
#loc342 = loc("p341.2019")
#loc343 = loc("p342.2024")
#loc344 = loc("p343.2033")
#loc345 = loc("p344.2037")
#loc346 = loc("p345.2043")
#loc347 = loc("p346.2047")
#loc348 = loc("p347.2053")
#loc349 = loc("p348.2058")
#loc350 = loc("p349.2067")
#loc351 = loc("p350.2071")
#loc352 = loc("p351.2077")
#loc353 = loc("p352.2081")
#loc354 = loc("p353.2087")
#loc355 = loc("p354.2092")
#loc356 = loc("p355.2101")
#loc357 = loc("p356.2105")
#loc358 = loc("p357.2111")
#loc359 = loc("p358.2115")
#loc360 = loc("p359.2121")
#loc361 = loc("p360.2126")
#loc362 = loc("p361.2135")
#loc363 = loc("p362.2139")
#loc364 = loc("p363.2145")
#loc365 = loc("p364.2149")
#loc366 = loc("p365.2155")
#loc367 = loc("p366.2160")
#loc368 = loc("p367.2169")
#loc369 = loc("p368.2173")
#loc370 = loc("p369.2179")
#loc371 = loc("p370.2183")
#loc372 = loc("p371.2189")
#loc373 = loc("p372.2194")
#loc374 = loc("p373.2203")
#loc375 = loc("p374.2207")
#loc376 = loc("p375.2213")
#loc377 = loc("p376.2217")
#loc378 = loc("p377.2223")
#loc379 = loc("p378.2228")
#loc380 = loc("p379.2237")
#loc381 = loc("p380.2241")
#loc382 = loc("p381.2247")
#loc383 = loc("p382.2251")
#loc384 = loc("p383.2257")
#loc385 = loc("p384.2262")
#loc386 = loc("p385.2270")
#loc387 = loc("p386.2275")
#loc388 = loc("p387.2283")
#loc389 = loc("p388.2288")
#loc390 = loc("p389.2295")
#loc391 = loc("p390.2297")
#loc392 = loc("p391.2302")
#loc393 = loc("p392.2478")
#loc394 = loc("p393.2482")
#loc395 = loc("p394.2502")
#loc396 = loc("p395.2506")
#loc397 = loc("p396.2788")
#loc398 = loc("p397.2792")
#loc399 = loc("p398.2812")
#loc400 = loc("p399.2816")
#loc401 = loc("p400.3098")
#loc402 = loc("p401.3102")
#loc403 = loc("p402.3122")
#loc404 = loc("p403.3126")
#loc405 = loc("p404.3408")
#loc406 = loc("p405.3412")
#loc407 = loc("p406.3432")
#loc408 = loc("p407.3436")
#loc409 = loc("p408.3718")
#loc410 = loc("p409.3722")
#loc411 = loc("p410.3742")
#loc412 = loc("p411.3746")
#loc413 = loc("p412.4028")
#loc414 = loc("p413.4032")
#loc415 = loc("p414.4052")
#loc416 = loc("p415.4056")
#loc417 = loc("p416.4338")
#loc418 = loc("p417.4342")
#loc419 = loc("p418.4362")
#loc420 = loc("p419.4366")
#loc421 = loc("p420.4648")
#loc422 = loc("p421.4652")
#loc423 = loc("p422.4672")
#loc424 = loc("p423.4676")
#loc425 = loc("p424.4958")
#loc426 = loc("p425.4962")
#loc427 = loc("p426.4982")
#loc428 = loc("p427.4986")
#loc429 = loc("p428.5268")
#loc430 = loc("p429.5272")
#loc431 = loc("p430.5292")
#loc432 = loc("p431.5296")
#loc433 = loc("p432.5578")
#loc434 = loc("p433.5582")
#loc435 = loc("p434.5602")
#loc436 = loc("p435.5606")
#loc437 = loc("p436.5888")
#loc438 = loc("p437.5892")
#loc439 = loc("p438.5912")
#loc440 = loc("p439.5916")
#loc441 = loc("p440.6198")
#loc442 = loc("p441.6202")
#loc443 = loc("p442.6222")
#loc444 = loc("p443.6226")
#loc445 = loc("p444.6508")
#loc446 = loc("p445.6512")
#loc447 = loc("p446.6532")
#loc448 = loc("p447.6536")
#loc449 = loc("p448.6818")
#loc450 = loc("p449.6822")
#loc451 = loc("p450.6842")
#loc452 = loc("p451.6846")
#loc453 = loc("p452.7128")
#loc454 = loc("p453.7132")
#loc455 = loc("p454.7152")
#loc456 = loc("p455.7156")
#loc457 = loc("p456.7438")
#loc458 = loc("p457.7442")
#loc459 = loc("p458.7462")
#loc460 = loc("p459.7466")
#loc461 = loc("p460.7748")
#loc462 = loc("p461.7752")
#loc463 = loc("p462.7772")
#loc464 = loc("p463.7776")
#loc465 = loc("p464.8058")
#loc466 = loc("p465.8062")
#loc467 = loc("p466.8082")
#loc468 = loc("p467.8086")
#loc469 = loc("p468.8368")
#loc470 = loc("p469.8372")
#loc471 = loc("p470.8392")
#loc472 = loc("p471.8396")
#loc473 = loc("p472.8678")
#loc474 = loc("p473.8682")
#loc475 = loc("p474.8702")
#loc476 = loc("p475.8706")
#loc477 = loc("p476.8988")
#loc478 = loc("p477.8992")
#loc479 = loc("p478.9012")
#loc480 = loc("p479.9016")
#loc481 = loc("p480.9298")
#loc482 = loc("p481.9302")
#loc483 = loc("p482.9322")
#loc484 = loc("p483.9326")
#loc485 = loc("p484.9608")
#loc486 = loc("p485.9612")
#loc487 = loc("p486.9632")
#loc488 = loc("p487.9636")
#loc489 = loc("p488.9918")
#loc490 = loc("p489.9922")
#loc491 = loc("p490.9942")
#loc492 = loc("p491.9946")
#loc493 = loc("p492.10228")
#loc494 = loc("p493.10232")
#loc495 = loc("p494.10252")
#loc496 = loc("p495.10256")
#loc497 = loc("p496.10538")
#loc498 = loc("p497.10542")
#loc499 = loc("p498.10562")
#loc500 = loc("p499.10566")
#loc501 = loc("p500.10848")
#loc502 = loc("p501.10852")
#loc503 = loc("p502.10872")
#loc504 = loc("p503.10876")
#loc505 = loc("p504.11158")
#loc506 = loc("p505.11162")
#loc507 = loc("p506.11182")
#loc508 = loc("p507.11186")
#loc509 = loc("p508.11468")
#loc510 = loc("p509.11472")
#loc511 = loc("p510.11492")
#loc512 = loc("p511.11496")
#loc513 = loc("p512.11778")
#loc514 = loc("p513.11782")
#loc515 = loc("p514.11802")
#loc516 = loc("p515.11806")
#loc517 = loc("p516.12091")
#loc518 = loc("p517.12106")
#loc519 = loc("p518.12186")
#loc520 = loc("p519.12191")
#loc521 = loc("p520.12197")
#loc522 = loc("p521.12202")
#loc523 = loc("p522.12309")
#loc524 = loc("p523.12314")
#loc525 = loc("p524.12320")
#loc526 = loc("p525.12325")
#loc527 = loc("p526.12410")
#loc528 = loc("p527.12415")
#loc529 = loc("p528.12507")
#loc530 = loc("p529.12522")
#loc531 = loc("p530.12602")
#loc532 = loc("p531.12607")
#loc533 = loc("p532.12613")
#loc534 = loc("p533.12618")
#loc535 = loc("p534.12725")
#loc536 = loc("p535.12730")
#loc537 = loc("p536.12736")
#loc538 = loc("p537.12741")
#loc539 = loc("p538.12826")
#loc540 = loc("p539.12831")
#loc541 = loc("p540.12923")
#loc542 = loc("p541.12938")
#loc543 = loc("p542.13018")
#loc544 = loc("p543.13023")
#loc545 = loc("p544.13029")
#loc546 = loc("p545.13034")
#loc547 = loc("p546.13141")
#loc548 = loc("p547.13146")
#loc549 = loc("p548.13152")
#loc550 = loc("p549.13157")
#loc551 = loc("p550.13242")
#loc552 = loc("p551.13247")
#loc553 = loc("p552.13339")
#loc554 = loc("p553.13354")
#loc555 = loc("p554.13434")
#loc556 = loc("p555.13439")
#loc557 = loc("p556.13445")
#loc558 = loc("p557.13450")
#loc645 = loc("reduce.2566")
#loc766 = loc("reduce.2876")
#loc887 = loc("reduce.3186")
#loc1008 = loc("reduce.3496")
#loc1129 = loc("reduce.3806")
#loc1250 = loc("reduce.4116")
#loc1371 = loc("reduce.4426")
#loc1492 = loc("reduce.4736")
#loc1613 = loc("reduce.5046")
#loc1734 = loc("reduce.5356")
#loc1855 = loc("reduce.5666")
#loc1976 = loc("reduce.5976")
#loc2097 = loc("reduce.6286")
#loc2218 = loc("reduce.6596")
#loc2339 = loc("reduce.6906")
#loc2460 = loc("reduce.7216")
#loc2581 = loc("reduce.7526")
#loc2702 = loc("reduce.7836")
#loc2823 = loc("reduce.8146")
#loc2944 = loc("reduce.8456")
#loc3065 = loc("reduce.8766")
#loc3186 = loc("reduce.9076")
#loc3307 = loc("reduce.9386")
#loc3428 = loc("reduce.9696")
#loc3549 = loc("reduce.10006")
#loc3670 = loc("reduce.10316")
#loc3791 = loc("reduce.10626")
#loc3912 = loc("reduce.10936")
#loc4033 = loc("reduce.11246")
#loc4154 = loc("reduce.11556")
#loc4275 = loc("reduce.11866")
#loc4388 = loc("reduce.12162")
#loc4486 = loc("reduce.12578")
#loc4584 = loc("reduce.12994")
#loc4682 = loc("reduce.13410")
#loc4759 = loc("custom-call.13457")
#loc4760 = loc("custom-call.13454")
#loc4761 = loc("custom-call.13449")
#loc4779 = loc("custom-call.13164")
#loc4780 = loc("custom-call.13161")
#loc4781 = loc("custom-call.13156")
#loc4799 = loc("custom-call.12748")
#loc4800 = loc("custom-call.12745")
#loc4801 = loc("custom-call.12740")
#loc4819 = loc("custom-call.12625")
#loc4820 = loc("custom-call.12622")
#loc4821 = loc("custom-call.12617")
#loc4839 = loc("custom-call.12422")
#loc4840 = loc("custom-call.12419")
#loc4841 = loc("custom-call.12414")
#loc4859 = loc("custom-call.13254")
#loc4860 = loc("custom-call.13251")
#loc4861 = loc("custom-call.13246")
#loc4879 = loc("custom-call.12332")
#loc4880 = loc("custom-call.12329")
#loc4881 = loc("custom-call.12324")
#loc4899 = loc("custom-call.12289")
#loc4905 = loc("custom-call.12006")
#loc4906 = loc("custom-call.148")
#loc4907 = loc("custom-call.143")
#loc4925 = loc("custom-call.11976")
#loc4931 = loc("custom-call.11666")
#loc4937 = loc("custom-call.11582")
#loc4938 = loc("custom-call.260")
#loc4939 = loc("custom-call.255")
#loc4957 = loc("custom-call.11379")
#loc4958 = loc("custom-call.294")
#loc4959 = loc("custom-call.289")
#loc4977 = loc("custom-call.11356")
#loc4983 = loc("custom-call.11046")
#loc4989 = loc("custom-call.10962")
#loc4990 = loc("custom-call.396")
#loc4991 = loc("custom-call.391")
#loc5009 = loc("custom-call.10736")
#loc5015 = loc("custom-call.10652")
#loc5016 = loc("custom-call.464")
#loc5017 = loc("custom-call.459")
#loc5035 = loc("custom-call.13041")
#loc5036 = loc("custom-call.13038")
#loc5037 = loc("custom-call.13033")
#loc5055 = loc("custom-call.10449")
#loc5056 = loc("custom-call.498")
#loc5057 = loc("custom-call.493")
#loc5075 = loc("custom-call.10426")
#loc5081 = loc("custom-call.10342")
#loc5082 = loc("custom-call.532")
#loc5083 = loc("custom-call.527")
#loc5101 = loc("custom-call.12838")
#loc5102 = loc("custom-call.12835")
#loc5103 = loc("custom-call.12830")
#loc5121 = loc("custom-call.10139")
#loc5122 = loc("custom-call.566")
#loc5123 = loc("custom-call.561")
#loc5141 = loc("custom-call.10032")
#loc5142 = loc("custom-call.600")
#loc5143 = loc("custom-call.595")
#loc5161 = loc("custom-call.9722")
#loc5162 = loc("custom-call.668")
#loc5163 = loc("custom-call.663")
#loc5181 = loc("custom-call.9496")
#loc5187 = loc("custom-call.8899")
#loc5188 = loc("custom-call.838")
#loc5189 = loc("custom-call.833")
#loc5207 = loc("custom-call.8876")
#loc5213 = loc("custom-call.13563")
#loc5214 = loc("custom-call.11")
#loc5215 = loc("custom-call.6")
#loc5233 = loc("custom-call.8589")
#loc5234 = loc("custom-call.906")
#loc5235 = loc("custom-call.901")
#loc5253 = loc("custom-call.8482")
#loc5254 = loc("custom-call.940")
#loc5255 = loc("custom-call.935")
#loc5273 = loc("custom-call.4762")
#loc5274 = loc("custom-call.1756")
#loc5275 = loc("custom-call.1751")
#loc5293 = loc("custom-call.2312")
#loc5294 = loc("custom-call.2279")
#loc5295 = loc("custom-call.2274")
#loc5313 = loc("custom-call.4536")
#loc5319 = loc("custom-call.3522")
#loc5320 = loc("custom-call.2028")
#loc5321 = loc("custom-call.2023")
#loc5339 = loc("custom-call.2592")
#loc5340 = loc("custom-call.2232")
#loc5341 = loc("custom-call.2227")
#loc5359 = loc("custom-call.10759")
#loc5360 = loc("custom-call.430")
#loc5361 = loc("custom-call.425")
#loc5379 = loc("custom-call.61")
#loc5380 = loc("custom-call.58")
#loc5381 = loc("custom-call.53")
#loc5399 = loc("custom-call.9209")
#loc5400 = loc("custom-call.770")
#loc5401 = loc("custom-call.765")
#loc5419 = loc("custom-call.4249")
#loc5420 = loc("custom-call.1858")
#loc5421 = loc("custom-call.1853")
#loc5439 = loc("custom-call.12705")
#loc5445 = loc("custom-call.6419")
#loc5446 = loc("custom-call.1382")
#loc5447 = loc("custom-call.1377")
#loc5465 = loc("custom-call.3939")
#loc5466 = loc("custom-call.1926")
#loc5467 = loc("custom-call.1921")
#loc5485 = loc("custom-call.9102")
#loc5486 = loc("custom-call.804")
#loc5487 = loc("custom-call.799")
#loc5505 = loc("custom-call.3916")
#loc5511 = loc("custom-call.5489")
#loc5512 = loc("custom-call.1586")
#loc5513 = loc("custom-call.1581")
#loc5531 = loc("custom-call.13121")
#loc5537 = loc("custom-call.6002")
#loc5538 = loc("custom-call.1484")
#loc5539 = loc("custom-call.1479")
#loc5557 = loc("custom-call.6312")
#loc5558 = loc("custom-call.1416")
#loc5559 = loc("custom-call.1411")
#loc5577 = loc("custom-call.4452")
#loc5578 = loc("custom-call.1824")
#loc5579 = loc("custom-call.1819")
#loc5597 = loc("custom-call.3629")
#loc5598 = loc("custom-call.1994")
#loc5599 = loc("custom-call.1989")
#loc5617 = loc("custom-call.7326")
#loc5623 = loc("custom-call.2389")
#loc5624 = loc("custom-call.2266")
#loc5625 = loc("custom-call.2261")
#loc5643 = loc("custom-call.5156")
#loc5649 = loc("custom-call.11272")
#loc5650 = loc("custom-call.328")
#loc5651 = loc("custom-call.323")
#loc5669 = loc("custom-call.4869")
#loc5670 = loc("custom-call.1722")
#loc5671 = loc("custom-call.1717")
#loc5689 = loc("custom-call.5799")
#loc5690 = loc("custom-call.1518")
#loc5691 = loc("custom-call.1513")
#loc5709 = loc("custom-call.4142")
#loc5710 = loc("custom-call.1892")
#loc5711 = loc("custom-call.1887")
#loc5729 = loc("custom-call.3319")
#loc5730 = loc("custom-call.2062")
#loc5731 = loc("custom-call.2057")
#loc5749 = loc("custom-call.13537")
#loc5755 = loc("custom-call.9829")
#loc5756 = loc("custom-call.634")
#loc5757 = loc("custom-call.629")
#loc5775 = loc("custom-call.8566")
#loc5781 = loc("custom-call.2902")
#loc5782 = loc("custom-call.2164")
#loc5783 = loc("custom-call.2159")
#loc5801 = loc("custom-call.2986")
#loc5807 = loc("custom-call.2676")
#loc5813 = loc("custom-call.6729")
#loc5814 = loc("custom-call.1314")
#loc5815 = loc("custom-call.1309")
#loc5833 = loc("custom-call.11069")
#loc5834 = loc("custom-call.362")
#loc5835 = loc("custom-call.357")
#loc5853 = loc("custom-call.4846")
#loc5859 = loc("custom-call.12209")
#loc5860 = loc("custom-call.12206")
#loc5861 = loc("custom-call.12201")
#loc5879 = loc("custom-call.9806")
#loc5885 = loc("custom-call.3832")
#loc5886 = loc("custom-call.1960")
#loc5887 = loc("custom-call.1955")
#loc5905 = loc("custom-call.3296")
#loc5911 = loc("custom-call.7016")
#loc5917 = loc("custom-call.7862")
#loc5918 = loc("custom-call.1076")
#loc5919 = loc("custom-call.1071")
#loc5937 = loc("custom-call.5179")
#loc5938 = loc("custom-call.1654")
#loc5939 = loc("custom-call.1649")
#loc5957 = loc("custom-call.5382")
#loc5958 = loc("custom-call.1620")
#loc5959 = loc("custom-call.1615")
#loc5977 = loc("custom-call.5466")
#loc5983 = loc("custom-call.5072")
#loc5984 = loc("custom-call.1688")
#loc5985 = loc("custom-call.1683")
#loc6003 = loc("custom-call.3212")
#loc6004 = loc("custom-call.2096")
#loc6005 = loc("custom-call.2091")
#loc6023 = loc("custom-call.5692")
#loc6024 = loc("custom-call.1552")
#loc6025 = loc("custom-call.1547")
#loc6043 = loc("custom-call.11892")
#loc6044 = loc("custom-call.192")
#loc6045 = loc("custom-call.187")
#loc6063 = loc("custom-call.9519")
#loc6064 = loc("custom-call.702")
#loc6065 = loc("custom-call.697")
#loc6083 = loc("custom-call.4226")
#loc6089 = loc("custom-call.5776")
#loc6095 = loc("custom-call.3009")
#loc6096 = loc("custom-call.2130")
#loc6097 = loc("custom-call.2125")
#loc6115 = loc("custom-call.6086")
#loc6121 = loc("custom-call.6396")
#loc6127 = loc("custom-call.8279")
#loc6128 = loc("custom-call.974")
#loc6129 = loc("custom-call.969")
#loc6147 = loc("custom-call.7039")
#loc6148 = loc("custom-call.1246")
#loc6149 = loc("custom-call.1241")
#loc6167 = loc("custom-call.3606")
#loc6173 = loc("custom-call.6622")
#loc6174 = loc("custom-call.1348")
#loc6175 = loc("custom-call.1343")
#loc6193 = loc("custom-call.9412")
#loc6194 = loc("custom-call.736")
#loc6195 = loc("custom-call.731")
#loc6213 = loc("custom-call.8792")
#loc6214 = loc("custom-call.872")
#loc6215 = loc("custom-call.867")
#loc6233 = loc("custom-call.6706")
#loc6239 = loc("custom-call.6109")
#loc6240 = loc("custom-call.1450")
#loc6241 = loc("custom-call.1445")
#loc6259 = loc("custom-call.6932")
#loc6260 = loc("custom-call.1280")
#loc6261 = loc("custom-call.1275")
#loc6279 = loc("custom-call.7242")
#loc6280 = loc("custom-call.1212")
#loc6281 = loc("custom-call.1207")
#loc6299 = loc("custom-call.7969")
#loc6300 = loc("custom-call.1042")
#loc6301 = loc("custom-call.1037")
#loc6319 = loc("custom-call.7349")
#loc6320 = loc("custom-call.1178")
#loc6321 = loc("custom-call.1173")
#loc6339 = loc("custom-call.10116")
#loc6345 = loc("custom-call.9186")
#loc6351 = loc("custom-call.7552")
#loc6352 = loc("custom-call.1144")
#loc6353 = loc("custom-call.1139")
#loc6371 = loc("custom-call.2699")
#loc6372 = loc("custom-call.2198")
#loc6373 = loc("custom-call.2193")
#loc6391 = loc("custom-call.7636")
#loc6397 = loc("custom-call.7659")
#loc6398 = loc("custom-call.1110")
#loc6399 = loc("custom-call.1105")
#loc6417 = loc("custom-call.4559")
#loc6418 = loc("custom-call.1790")
#loc6419 = loc("custom-call.1785")
#loc6437 = loc("custom-call.7946")
#loc6443 = loc("custom-call.11689")
#loc6444 = loc("custom-call.226")
#loc6445 = loc("custom-call.221")
#loc6463 = loc("custom-call.8172")
#loc6464 = loc("custom-call.1008")
#loc6465 = loc("custom-call.1003")
#loc6483 = loc("custom-call.8256")
module @SyncTensorsGraph.13641 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> loc("p0.2"), %arg1: tensor<2048xbf16> loc("p1.7"), %arg2: tensor<2048xbf16> loc("p2.15"), %arg3: tensor<2048x1280xbf16> loc("p3.19"), %arg4: tensor<1x16x1280xbf16> loc("p4.32"), %arg5: tensor<1280x1280xbf16> loc("p5.38"), %arg6: tensor<1280x1280xbf16> loc("p6.43"), %arg7: tensor<1280xbf16> loc("p7.49"), %arg8: tensor<1280xbf16> loc("p8.54"), %arg9: tensor<1280xbf16> loc("p9.139"), %arg10: tensor<1280xbf16> loc("p10.144"), %arg11: tensor<1280xbf16> loc("p11.152"), %arg12: tensor<1280x1280xbf16> loc("p12.156"), %arg13: tensor<1280xbf16> loc("p13.163"), %arg14: tensor<1280x5120xbf16> loc("p14.167"), %arg15: tensor<5120xbf16> loc("p15.173"), %arg16: tensor<5120x1280xbf16> loc("p16.177"), %arg17: tensor<1280xbf16> loc("p17.183"), %arg18: tensor<1280xbf16> loc("p18.188"), %arg19: tensor<1280xbf16> loc("p19.197"), %arg20: tensor<1280x1280xbf16> loc("p20.201"), %arg21: tensor<1280xbf16> loc("p21.207"), %arg22: tensor<1280x1280xbf16> loc("p22.211"), %arg23: tensor<1280xbf16> loc("p23.217"), %arg24: tensor<1280xbf16> loc("p24.222"), %arg25: tensor<1280xbf16> loc("p25.231"), %arg26: tensor<1280x5120xbf16> loc("p26.235"), %arg27: tensor<5120xbf16> loc("p27.241"), %arg28: tensor<5120x1280xbf16> loc("p28.245"), %arg29: tensor<1280xbf16> loc("p29.251"), %arg30: tensor<1280xbf16> loc("p30.256"), %arg31: tensor<1280xbf16> loc("p31.265"), %arg32: tensor<1280x1280xbf16> loc("p32.269"), %arg33: tensor<1280xbf16> loc("p33.275"), %arg34: tensor<1280x1280xbf16> loc("p34.279"), %arg35: tensor<1280xbf16> loc("p35.285"), %arg36: tensor<1280xbf16> loc("p36.290"), %arg37: tensor<1280xbf16> loc("p37.299"), %arg38: tensor<1280x5120xbf16> loc("p38.303"), %arg39: tensor<5120xbf16> loc("p39.309"), %arg40: tensor<5120x1280xbf16> loc("p40.313"), %arg41: tensor<1280xbf16> loc("p41.319"), %arg42: tensor<1280xbf16> loc("p42.324"), %arg43: tensor<1280xbf16> loc("p43.333"), %arg44: tensor<1280x1280xbf16> loc("p44.337"), %arg45: tensor<1280xbf16> loc("p45.343"), %arg46: tensor<1280x1280xbf16> loc("p46.347"), %arg47: tensor<1280xbf16> loc("p47.353"), %arg48: tensor<1280xbf16> loc("p48.358"), %arg49: tensor<1280xbf16> loc("p49.367"), %arg50: tensor<1280x5120xbf16> loc("p50.371"), %arg51: tensor<5120xbf16> loc("p51.377"), %arg52: tensor<5120x1280xbf16> loc("p52.381"), %arg53: tensor<1280xbf16> loc("p53.387"), %arg54: tensor<1280xbf16> loc("p54.392"), %arg55: tensor<1280xbf16> loc("p55.401"), %arg56: tensor<1280x1280xbf16> loc("p56.405"), %arg57: tensor<1280xbf16> loc("p57.411"), %arg58: tensor<1280x1280xbf16> loc("p58.415"), %arg59: tensor<1280xbf16> loc("p59.421"), %arg60: tensor<1280xbf16> loc("p60.426"), %arg61: tensor<1280xbf16> loc("p61.435"), %arg62: tensor<1280x5120xbf16> loc("p62.439"), %arg63: tensor<5120xbf16> loc("p63.445"), %arg64: tensor<5120x1280xbf16> loc("p64.449"), %arg65: tensor<1280xbf16> loc("p65.455"), %arg66: tensor<1280xbf16> loc("p66.460"), %arg67: tensor<1280xbf16> loc("p67.469"), %arg68: tensor<1280x1280xbf16> loc("p68.473"), %arg69: tensor<1280xbf16> loc("p69.479"), %arg70: tensor<1280x1280xbf16> loc("p70.483"), %arg71: tensor<1280xbf16> loc("p71.489"), %arg72: tensor<1280xbf16> loc("p72.494"), %arg73: tensor<1280xbf16> loc("p73.503"), %arg74: tensor<1280x5120xbf16> loc("p74.507"), %arg75: tensor<5120xbf16> loc("p75.513"), %arg76: tensor<5120x1280xbf16> loc("p76.517"), %arg77: tensor<1280xbf16> loc("p77.523"), %arg78: tensor<1280xbf16> loc("p78.528"), %arg79: tensor<1280xbf16> loc("p79.537"), %arg80: tensor<1280x1280xbf16> loc("p80.541"), %arg81: tensor<1280xbf16> loc("p81.547"), %arg82: tensor<1280x1280xbf16> loc("p82.551"), %arg83: tensor<1280xbf16> loc("p83.557"), %arg84: tensor<1280xbf16> loc("p84.562"), %arg85: tensor<1280xbf16> loc("p85.571"), %arg86: tensor<1280x5120xbf16> loc("p86.575"), %arg87: tensor<5120xbf16> loc("p87.581"), %arg88: tensor<5120x1280xbf16> loc("p88.585"), %arg89: tensor<1280xbf16> loc("p89.591"), %arg90: tensor<1280xbf16> loc("p90.596"), %arg91: tensor<1280xbf16> loc("p91.605"), %arg92: tensor<1280x1280xbf16> loc("p92.609"), %arg93: tensor<1280xbf16> loc("p93.615"), %arg94: tensor<1280x1280xbf16> loc("p94.619"), %arg95: tensor<1280xbf16> loc("p95.625"), %arg96: tensor<1280xbf16> loc("p96.630"), %arg97: tensor<1280xbf16> loc("p97.639"), %arg98: tensor<1280x5120xbf16> loc("p98.643"), %arg99: tensor<5120xbf16> loc("p99.649"), %arg100: tensor<5120x1280xbf16> loc("p100.653"), %arg101: tensor<1280xbf16> loc("p101.659"), %arg102: tensor<1280xbf16> loc("p102.664"), %arg103: tensor<1280xbf16> loc("p103.673"), %arg104: tensor<1280x1280xbf16> loc("p104.677"), %arg105: tensor<1280xbf16> loc("p105.683"), %arg106: tensor<1280x1280xbf16> loc("p106.687"), %arg107: tensor<1280xbf16> loc("p107.693"), %arg108: tensor<1280xbf16> loc("p108.698"), %arg109: tensor<1280xbf16> loc("p109.707"), %arg110: tensor<1280x5120xbf16> loc("p110.711"), %arg111: tensor<5120xbf16> loc("p111.717"), %arg112: tensor<5120x1280xbf16> loc("p112.721"), %arg113: tensor<1280xbf16> loc("p113.727"), %arg114: tensor<1280xbf16> loc("p114.732"), %arg115: tensor<1280xbf16> loc("p115.741"), %arg116: tensor<1280x1280xbf16> loc("p116.745"), %arg117: tensor<1280xbf16> loc("p117.751"), %arg118: tensor<1280x1280xbf16> loc("p118.755"), %arg119: tensor<1280xbf16> loc("p119.761"), %arg120: tensor<1280xbf16> loc("p120.766"), %arg121: tensor<1280xbf16> loc("p121.775"), %arg122: tensor<1280x5120xbf16> loc("p122.779"), %arg123: tensor<5120xbf16> loc("p123.785"), %arg124: tensor<5120x1280xbf16> loc("p124.789"), %arg125: tensor<1280xbf16> loc("p125.795"), %arg126: tensor<1280xbf16> loc("p126.800"), %arg127: tensor<1280xbf16> loc("p127.809"), %arg128: tensor<1280x1280xbf16> loc("p128.813"), %arg129: tensor<1280xbf16> loc("p129.819"), %arg130: tensor<1280x1280xbf16> loc("p130.823"), %arg131: tensor<1280xbf16> loc("p131.829"), %arg132: tensor<1280xbf16> loc("p132.834"), %arg133: tensor<1280xbf16> loc("p133.843"), %arg134: tensor<1280x5120xbf16> loc("p134.847"), %arg135: tensor<5120xbf16> loc("p135.853"), %arg136: tensor<5120x1280xbf16> loc("p136.857"), %arg137: tensor<1280xbf16> loc("p137.863"), %arg138: tensor<1280xbf16> loc("p138.868"), %arg139: tensor<1280xbf16> loc("p139.877"), %arg140: tensor<1280x1280xbf16> loc("p140.881"), %arg141: tensor<1280xbf16> loc("p141.887"), %arg142: tensor<1280x1280xbf16> loc("p142.891"), %arg143: tensor<1280xbf16> loc("p143.897"), %arg144: tensor<1280xbf16> loc("p144.902"), %arg145: tensor<1280xbf16> loc("p145.911"), %arg146: tensor<1280x5120xbf16> loc("p146.915"), %arg147: tensor<5120xbf16> loc("p147.921"), %arg148: tensor<5120x1280xbf16> loc("p148.925"), %arg149: tensor<1280xbf16> loc("p149.931"), %arg150: tensor<1280xbf16> loc("p150.936"), %arg151: tensor<1280xbf16> loc("p151.945"), %arg152: tensor<1280x1280xbf16> loc("p152.949"), %arg153: tensor<1280xbf16> loc("p153.955"), %arg154: tensor<1280x1280xbf16> loc("p154.959"), %arg155: tensor<1280xbf16> loc("p155.965"), %arg156: tensor<1280xbf16> loc("p156.970"), %arg157: tensor<1280xbf16> loc("p157.979"), %arg158: tensor<1280x5120xbf16> loc("p158.983"), %arg159: tensor<5120xbf16> loc("p159.989"), %arg160: tensor<5120x1280xbf16> loc("p160.993"), %arg161: tensor<1280xbf16> loc("p161.999"), %arg162: tensor<1280xbf16> loc("p162.1004"), %arg163: tensor<1280xbf16> loc("p163.1013"), %arg164: tensor<1280x1280xbf16> loc("p164.1017"), %arg165: tensor<1280xbf16> loc("p165.1023"), %arg166: tensor<1280x1280xbf16> loc("p166.1027"), %arg167: tensor<1280xbf16> loc("p167.1033"), %arg168: tensor<1280xbf16> loc("p168.1038"), %arg169: tensor<1280xbf16> loc("p169.1047"), %arg170: tensor<1280x5120xbf16> loc("p170.1051"), %arg171: tensor<5120xbf16> loc("p171.1057"), %arg172: tensor<5120x1280xbf16> loc("p172.1061"), %arg173: tensor<1280xbf16> loc("p173.1067"), %arg174: tensor<1280xbf16> loc("p174.1072"), %arg175: tensor<1280xbf16> loc("p175.1081"), %arg176: tensor<1280x1280xbf16> loc("p176.1085"), %arg177: tensor<1280xbf16> loc("p177.1091"), %arg178: tensor<1280x1280xbf16> loc("p178.1095"), %arg179: tensor<1280xbf16> loc("p179.1101"), %arg180: tensor<1280xbf16> loc("p180.1106"), %arg181: tensor<1280xbf16> loc("p181.1115"), %arg182: tensor<1280x5120xbf16> loc("p182.1119"), %arg183: tensor<5120xbf16> loc("p183.1125"), %arg184: tensor<5120x1280xbf16> loc("p184.1129"), %arg185: tensor<1280xbf16> loc("p185.1135"), %arg186: tensor<1280xbf16> loc("p186.1140"), %arg187: tensor<1280xbf16> loc("p187.1149"), %arg188: tensor<1280x1280xbf16> loc("p188.1153"), %arg189: tensor<1280xbf16> loc("p189.1159"), %arg190: tensor<1280x1280xbf16> loc("p190.1163"), %arg191: tensor<1280xbf16> loc("p191.1169"), %arg192: tensor<1280xbf16> loc("p192.1174"), %arg193: tensor<1280xbf16> loc("p193.1183"), %arg194: tensor<1280x5120xbf16> loc("p194.1187"), %arg195: tensor<5120xbf16> loc("p195.1193"), %arg196: tensor<5120x1280xbf16> loc("p196.1197"), %arg197: tensor<1280xbf16> loc("p197.1203"), %arg198: tensor<1280xbf16> loc("p198.1208"), %arg199: tensor<1280xbf16> loc("p199.1217"), %arg200: tensor<1280x1280xbf16> loc("p200.1221"), %arg201: tensor<1280xbf16> loc("p201.1227"), %arg202: tensor<1280x1280xbf16> loc("p202.1231"), %arg203: tensor<1280xbf16> loc("p203.1237"), %arg204: tensor<1280xbf16> loc("p204.1242"), %arg205: tensor<1280xbf16> loc("p205.1251"), %arg206: tensor<1280x5120xbf16> loc("p206.1255"), %arg207: tensor<5120xbf16> loc("p207.1261"), %arg208: tensor<5120x1280xbf16> loc("p208.1265"), %arg209: tensor<1280xbf16> loc("p209.1271"), %arg210: tensor<1280xbf16> loc("p210.1276"), %arg211: tensor<1280xbf16> loc("p211.1285"), %arg212: tensor<1280x1280xbf16> loc("p212.1289"), %arg213: tensor<1280xbf16> loc("p213.1295"), %arg214: tensor<1280x1280xbf16> loc("p214.1299"), %arg215: tensor<1280xbf16> loc("p215.1305"), %arg216: tensor<1280xbf16> loc("p216.1310"), %arg217: tensor<1280xbf16> loc("p217.1319"), %arg218: tensor<1280x5120xbf16> loc("p218.1323"), %arg219: tensor<5120xbf16> loc("p219.1329"), %arg220: tensor<5120x1280xbf16> loc("p220.1333"), %arg221: tensor<1280xbf16> loc("p221.1339"), %arg222: tensor<1280xbf16> loc("p222.1344"), %arg223: tensor<1280xbf16> loc("p223.1353"), %arg224: tensor<1280x1280xbf16> loc("p224.1357"), %arg225: tensor<1280xbf16> loc("p225.1363"), %arg226: tensor<1280x1280xbf16> loc("p226.1367"), %arg227: tensor<1280xbf16> loc("p227.1373"), %arg228: tensor<1280xbf16> loc("p228.1378"), %arg229: tensor<1280xbf16> loc("p229.1387"), %arg230: tensor<1280x5120xbf16> loc("p230.1391"), %arg231: tensor<5120xbf16> loc("p231.1397"), %arg232: tensor<5120x1280xbf16> loc("p232.1401"), %arg233: tensor<1280xbf16> loc("p233.1407"), %arg234: tensor<1280xbf16> loc("p234.1412"), %arg235: tensor<1280xbf16> loc("p235.1421"), %arg236: tensor<1280x1280xbf16> loc("p236.1425"), %arg237: tensor<1280xbf16> loc("p237.1431"), %arg238: tensor<1280x1280xbf16> loc("p238.1435"), %arg239: tensor<1280xbf16> loc("p239.1441"), %arg240: tensor<1280xbf16> loc("p240.1446"), %arg241: tensor<1280xbf16> loc("p241.1455"), %arg242: tensor<1280x5120xbf16> loc("p242.1459"), %arg243: tensor<5120xbf16> loc("p243.1465"), %arg244: tensor<5120x1280xbf16> loc("p244.1469"), %arg245: tensor<1280xbf16> loc("p245.1475"), %arg246: tensor<1280xbf16> loc("p246.1480"), %arg247: tensor<1280xbf16> loc("p247.1489"), %arg248: tensor<1280x1280xbf16> loc("p248.1493"), %arg249: tensor<1280xbf16> loc("p249.1499"), %arg250: tensor<1280x1280xbf16> loc("p250.1503"), %arg251: tensor<1280xbf16> loc("p251.1509"), %arg252: tensor<1280xbf16> loc("p252.1514"), %arg253: tensor<1280xbf16> loc("p253.1523"), %arg254: tensor<1280x5120xbf16> loc("p254.1527"), %arg255: tensor<5120xbf16> loc("p255.1533"), %arg256: tensor<5120x1280xbf16> loc("p256.1537"), %arg257: tensor<1280xbf16> loc("p257.1543"), %arg258: tensor<1280xbf16> loc("p258.1548"), %arg259: tensor<1280xbf16> loc("p259.1557"), %arg260: tensor<1280x1280xbf16> loc("p260.1561"), %arg261: tensor<1280xbf16> loc("p261.1567"), %arg262: tensor<1280x1280xbf16> loc("p262.1571"), %arg263: tensor<1280xbf16> loc("p263.1577"), %arg264: tensor<1280xbf16> loc("p264.1582"), %arg265: tensor<1280xbf16> loc("p265.1591"), %arg266: tensor<1280x5120xbf16> loc("p266.1595"), %arg267: tensor<5120xbf16> loc("p267.1601"), %arg268: tensor<5120x1280xbf16> loc("p268.1605"), %arg269: tensor<1280xbf16> loc("p269.1611"), %arg270: tensor<1280xbf16> loc("p270.1616"), %arg271: tensor<1280xbf16> loc("p271.1625"), %arg272: tensor<1280x1280xbf16> loc("p272.1629"), %arg273: tensor<1280xbf16> loc("p273.1635"), %arg274: tensor<1280x1280xbf16> loc("p274.1639"), %arg275: tensor<1280xbf16> loc("p275.1645"), %arg276: tensor<1280xbf16> loc("p276.1650"), %arg277: tensor<1280xbf16> loc("p277.1659"), %arg278: tensor<1280x5120xbf16> loc("p278.1663"), %arg279: tensor<5120xbf16> loc("p279.1669"), %arg280: tensor<5120x1280xbf16> loc("p280.1673"), %arg281: tensor<1280xbf16> loc("p281.1679"), %arg282: tensor<1280xbf16> loc("p282.1684"), %arg283: tensor<1280xbf16> loc("p283.1693"), %arg284: tensor<1280x1280xbf16> loc("p284.1697"), %arg285: tensor<1280xbf16> loc("p285.1703"), %arg286: tensor<1280x1280xbf16> loc("p286.1707"), %arg287: tensor<1280xbf16> loc("p287.1713"), %arg288: tensor<1280xbf16> loc("p288.1718"), %arg289: tensor<1280xbf16> loc("p289.1727"), %arg290: tensor<1280x5120xbf16> loc("p290.1731"), %arg291: tensor<5120xbf16> loc("p291.1737"), %arg292: tensor<5120x1280xbf16> loc("p292.1741"), %arg293: tensor<1280xbf16> loc("p293.1747"), %arg294: tensor<1280xbf16> loc("p294.1752"), %arg295: tensor<1280xbf16> loc("p295.1761"), %arg296: tensor<1280x1280xbf16> loc("p296.1765"), %arg297: tensor<1280xbf16> loc("p297.1771"), %arg298: tensor<1280x1280xbf16> loc("p298.1775"), %arg299: tensor<1280xbf16> loc("p299.1781"), %arg300: tensor<1280xbf16> loc("p300.1786"), %arg301: tensor<1280xbf16> loc("p301.1795"), %arg302: tensor<1280x5120xbf16> loc("p302.1799"), %arg303: tensor<5120xbf16> loc("p303.1805"), %arg304: tensor<5120x1280xbf16> loc("p304.1809"), %arg305: tensor<1280xbf16> loc("p305.1815"), %arg306: tensor<1280xbf16> loc("p306.1820"), %arg307: tensor<1280xbf16> loc("p307.1829"), %arg308: tensor<1280x1280xbf16> loc("p308.1833"), %arg309: tensor<1280xbf16> loc("p309.1839"), %arg310: tensor<1280x1280xbf16> loc("p310.1843"), %arg311: tensor<1280xbf16> loc("p311.1849"), %arg312: tensor<1280xbf16> loc("p312.1854"), %arg313: tensor<1280xbf16> loc("p313.1863"), %arg314: tensor<1280x5120xbf16> loc("p314.1867"), %arg315: tensor<5120xbf16> loc("p315.1873"), %arg316: tensor<5120x1280xbf16> loc("p316.1877"), %arg317: tensor<1280xbf16> loc("p317.1883"), %arg318: tensor<1280xbf16> loc("p318.1888"), %arg319: tensor<1280xbf16> loc("p319.1897"), %arg320: tensor<1280x1280xbf16> loc("p320.1901"), %arg321: tensor<1280xbf16> loc("p321.1907"), %arg322: tensor<1280x1280xbf16> loc("p322.1911"), %arg323: tensor<1280xbf16> loc("p323.1917"), %arg324: tensor<1280xbf16> loc("p324.1922"), %arg325: tensor<1280xbf16> loc("p325.1931"), %arg326: tensor<1280x5120xbf16> loc("p326.1935"), %arg327: tensor<5120xbf16> loc("p327.1941"), %arg328: tensor<5120x1280xbf16> loc("p328.1945"), %arg329: tensor<1280xbf16> loc("p329.1951"), %arg330: tensor<1280xbf16> loc("p330.1956"), %arg331: tensor<1280xbf16> loc("p331.1965"), %arg332: tensor<1280x1280xbf16> loc("p332.1969"), %arg333: tensor<1280xbf16> loc("p333.1975"), %arg334: tensor<1280x1280xbf16> loc("p334.1979"), %arg335: tensor<1280xbf16> loc("p335.1985"), %arg336: tensor<1280xbf16> loc("p336.1990"), %arg337: tensor<1280xbf16> loc("p337.1999"), %arg338: tensor<1280x5120xbf16> loc("p338.2003"), %arg339: tensor<5120xbf16> loc("p339.2009"), %arg340: tensor<5120x1280xbf16> loc("p340.2013"), %arg341: tensor<1280xbf16> loc("p341.2019"), %arg342: tensor<1280xbf16> loc("p342.2024"), %arg343: tensor<1280xbf16> loc("p343.2033"), %arg344: tensor<1280x1280xbf16> loc("p344.2037"), %arg345: tensor<1280xbf16> loc("p345.2043"), %arg346: tensor<1280x1280xbf16> loc("p346.2047"), %arg347: tensor<1280xbf16> loc("p347.2053"), %arg348: tensor<1280xbf16> loc("p348.2058"), %arg349: tensor<1280xbf16> loc("p349.2067"), %arg350: tensor<1280x5120xbf16> loc("p350.2071"), %arg351: tensor<5120xbf16> loc("p351.2077"), %arg352: tensor<5120x1280xbf16> loc("p352.2081"), %arg353: tensor<1280xbf16> loc("p353.2087"), %arg354: tensor<1280xbf16> loc("p354.2092"), %arg355: tensor<1280xbf16> loc("p355.2101"), %arg356: tensor<1280x1280xbf16> loc("p356.2105"), %arg357: tensor<1280xbf16> loc("p357.2111"), %arg358: tensor<1280x1280xbf16> loc("p358.2115"), %arg359: tensor<1280xbf16> loc("p359.2121"), %arg360: tensor<1280xbf16> loc("p360.2126"), %arg361: tensor<1280xbf16> loc("p361.2135"), %arg362: tensor<1280x5120xbf16> loc("p362.2139"), %arg363: tensor<5120xbf16> loc("p363.2145"), %arg364: tensor<5120x1280xbf16> loc("p364.2149"), %arg365: tensor<1280xbf16> loc("p365.2155"), %arg366: tensor<1280xbf16> loc("p366.2160"), %arg367: tensor<1280xbf16> loc("p367.2169"), %arg368: tensor<1280x1280xbf16> loc("p368.2173"), %arg369: tensor<1280xbf16> loc("p369.2179"), %arg370: tensor<1280x1280xbf16> loc("p370.2183"), %arg371: tensor<1280xbf16> loc("p371.2189"), %arg372: tensor<1280xbf16> loc("p372.2194"), %arg373: tensor<1280xbf16> loc("p373.2203"), %arg374: tensor<1280x5120xbf16> loc("p374.2207"), %arg375: tensor<5120xbf16> loc("p375.2213"), %arg376: tensor<5120x1280xbf16> loc("p376.2217"), %arg377: tensor<1280xbf16> loc("p377.2223"), %arg378: tensor<1280xbf16> loc("p378.2228"), %arg379: tensor<1280xbf16> loc("p379.2237"), %arg380: tensor<1280x1280xbf16> loc("p380.2241"), %arg381: tensor<1280xbf16> loc("p381.2247"), %arg382: tensor<1280x1280xbf16> loc("p382.2251"), %arg383: tensor<1280xbf16> loc("p383.2257"), %arg384: tensor<1280xbf16> loc("p384.2262"), %arg385: tensor<1280xbf16> loc("p385.2270"), %arg386: tensor<1280xbf16> loc("p386.2275"), %arg387: tensor<1x257xi64> loc("p387.2283"), %arg388: tensor<257x1280xbf16> loc("p388.2288"), %arg389: tensor<1280x3x14x14xbf16> loc("p389.2295"), %arg390: tensor<1x3x224x224xbf16> loc("p390.2297"), %arg391: tensor<1280xbf16> loc("p391.2302"), %arg392: tensor<1280xbf16> loc("p392.2478"), %arg393: tensor<1280x1280xbf16> loc("p393.2482"), %arg394: tensor<1280xbf16> loc("p394.2502"), %arg395: tensor<1280x1280xbf16> loc("p395.2506"), %arg396: tensor<1280xbf16> loc("p396.2788"), %arg397: tensor<1280x1280xbf16> loc("p397.2792"), %arg398: tensor<1280xbf16> loc("p398.2812"), %arg399: tensor<1280x1280xbf16> loc("p399.2816"), %arg400: tensor<1280xbf16> loc("p400.3098"), %arg401: tensor<1280x1280xbf16> loc("p401.3102"), %arg402: tensor<1280xbf16> loc("p402.3122"), %arg403: tensor<1280x1280xbf16> loc("p403.3126"), %arg404: tensor<1280xbf16> loc("p404.3408"), %arg405: tensor<1280x1280xbf16> loc("p405.3412"), %arg406: tensor<1280xbf16> loc("p406.3432"), %arg407: tensor<1280x1280xbf16> loc("p407.3436"), %arg408: tensor<1280xbf16> loc("p408.3718"), %arg409: tensor<1280x1280xbf16> loc("p409.3722"), %arg410: tensor<1280xbf16> loc("p410.3742"), %arg411: tensor<1280x1280xbf16> loc("p411.3746"), %arg412: tensor<1280xbf16> loc("p412.4028"), %arg413: tensor<1280x1280xbf16> loc("p413.4032"), %arg414: tensor<1280xbf16> loc("p414.4052"), %arg415: tensor<1280x1280xbf16> loc("p415.4056"), %arg416: tensor<1280xbf16> loc("p416.4338"), %arg417: tensor<1280x1280xbf16> loc("p417.4342"), %arg418: tensor<1280xbf16> loc("p418.4362"), %arg419: tensor<1280x1280xbf16> loc("p419.4366"), %arg420: tensor<1280xbf16> loc("p420.4648"), %arg421: tensor<1280x1280xbf16> loc("p421.4652"), %arg422: tensor<1280xbf16> loc("p422.4672"), %arg423: tensor<1280x1280xbf16> loc("p423.4676"), %arg424: tensor<1280xbf16> loc("p424.4958"), %arg425: tensor<1280x1280xbf16> loc("p425.4962"), %arg426: tensor<1280xbf16> loc("p426.4982"), %arg427: tensor<1280x1280xbf16> loc("p427.4986"), %arg428: tensor<1280xbf16> loc("p428.5268"), %arg429: tensor<1280x1280xbf16> loc("p429.5272"), %arg430: tensor<1280xbf16> loc("p430.5292"), %arg431: tensor<1280x1280xbf16> loc("p431.5296"), %arg432: tensor<1280xbf16> loc("p432.5578"), %arg433: tensor<1280x1280xbf16> loc("p433.5582"), %arg434: tensor<1280xbf16> loc("p434.5602"), %arg435: tensor<1280x1280xbf16> loc("p435.5606"), %arg436: tensor<1280xbf16> loc("p436.5888"), %arg437: tensor<1280x1280xbf16> loc("p437.5892"), %arg438: tensor<1280xbf16> loc("p438.5912"), %arg439: tensor<1280x1280xbf16> loc("p439.5916"), %arg440: tensor<1280xbf16> loc("p440.6198"), %arg441: tensor<1280x1280xbf16> loc("p441.6202"), %arg442: tensor<1280xbf16> loc("p442.6222"), %arg443: tensor<1280x1280xbf16> loc("p443.6226"), %arg444: tensor<1280xbf16> loc("p444.6508"), %arg445: tensor<1280x1280xbf16> loc("p445.6512"), %arg446: tensor<1280xbf16> loc("p446.6532"), %arg447: tensor<1280x1280xbf16> loc("p447.6536"), %arg448: tensor<1280xbf16> loc("p448.6818"), %arg449: tensor<1280x1280xbf16> loc("p449.6822"), %arg450: tensor<1280xbf16> loc("p450.6842"), %arg451: tensor<1280x1280xbf16> loc("p451.6846"), %arg452: tensor<1280xbf16> loc("p452.7128"), %arg453: tensor<1280x1280xbf16> loc("p453.7132"), %arg454: tensor<1280xbf16> loc("p454.7152"), %arg455: tensor<1280x1280xbf16> loc("p455.7156"), %arg456: tensor<1280xbf16> loc("p456.7438"), %arg457: tensor<1280x1280xbf16> loc("p457.7442"), %arg458: tensor<1280xbf16> loc("p458.7462"), %arg459: tensor<1280x1280xbf16> loc("p459.7466"), %arg460: tensor<1280xbf16> loc("p460.7748"), %arg461: tensor<1280x1280xbf16> loc("p461.7752"), %arg462: tensor<1280xbf16> loc("p462.7772"), %arg463: tensor<1280x1280xbf16> loc("p463.7776"), %arg464: tensor<1280xbf16> loc("p464.8058"), %arg465: tensor<1280x1280xbf16> loc("p465.8062"), %arg466: tensor<1280xbf16> loc("p466.8082"), %arg467: tensor<1280x1280xbf16> loc("p467.8086"), %arg468: tensor<1280xbf16> loc("p468.8368"), %arg469: tensor<1280x1280xbf16> loc("p469.8372"), %arg470: tensor<1280xbf16> loc("p470.8392"), %arg471: tensor<1280x1280xbf16> loc("p471.8396"), %arg472: tensor<1280xbf16> loc("p472.8678"), %arg473: tensor<1280x1280xbf16> loc("p473.8682"), %arg474: tensor<1280xbf16> loc("p474.8702"), %arg475: tensor<1280x1280xbf16> loc("p475.8706"), %arg476: tensor<1280xbf16> loc("p476.8988"), %arg477: tensor<1280x1280xbf16> loc("p477.8992"), %arg478: tensor<1280xbf16> loc("p478.9012"), %arg479: tensor<1280x1280xbf16> loc("p479.9016"), %arg480: tensor<1280xbf16> loc("p480.9298"), %arg481: tensor<1280x1280xbf16> loc("p481.9302"), %arg482: tensor<1280xbf16> loc("p482.9322"), %arg483: tensor<1280x1280xbf16> loc("p483.9326"), %arg484: tensor<1280xbf16> loc("p484.9608"), %arg485: tensor<1280x1280xbf16> loc("p485.9612"), %arg486: tensor<1280xbf16> loc("p486.9632"), %arg487: tensor<1280x1280xbf16> loc("p487.9636"), %arg488: tensor<1280xbf16> loc("p488.9918"), %arg489: tensor<1280x1280xbf16> loc("p489.9922"), %arg490: tensor<1280xbf16> loc("p490.9942"), %arg491: tensor<1280x1280xbf16> loc("p491.9946"), %arg492: tensor<1280xbf16> loc("p492.10228"), %arg493: tensor<1280x1280xbf16> loc("p493.10232"), %arg494: tensor<1280xbf16> loc("p494.10252"), %arg495: tensor<1280x1280xbf16> loc("p495.10256"), %arg496: tensor<1280xbf16> loc("p496.10538"), %arg497: tensor<1280x1280xbf16> loc("p497.10542"), %arg498: tensor<1280xbf16> loc("p498.10562"), %arg499: tensor<1280x1280xbf16> loc("p499.10566"), %arg500: tensor<1280xbf16> loc("p500.10848"), %arg501: tensor<1280x1280xbf16> loc("p501.10852"), %arg502: tensor<1280xbf16> loc("p502.10872"), %arg503: tensor<1280x1280xbf16> loc("p503.10876"), %arg504: tensor<1280xbf16> loc("p504.11158"), %arg505: tensor<1280x1280xbf16> loc("p505.11162"), %arg506: tensor<1280xbf16> loc("p506.11182"), %arg507: tensor<1280x1280xbf16> loc("p507.11186"), %arg508: tensor<1280xbf16> loc("p508.11468"), %arg509: tensor<1280x1280xbf16> loc("p509.11472"), %arg510: tensor<1280xbf16> loc("p510.11492"), %arg511: tensor<1280x1280xbf16> loc("p511.11496"), %arg512: tensor<1280xbf16> loc("p512.11778"), %arg513: tensor<1280x1280xbf16> loc("p513.11782"), %arg514: tensor<1280xbf16> loc("p514.11802"), %arg515: tensor<1280x1280xbf16> loc("p515.11806"), %arg516: tensor<1280x1280xbf16> loc("p516.12091"), %arg517: tensor<1280x1280xbf16> loc("p517.12106"), %arg518: tensor<1280x5120xbf16> loc("p518.12186"), %arg519: tensor<5120x1280xbf16> loc("p519.12191"), %arg520: tensor<1280xbf16> loc("p520.12197"), %arg521: tensor<1280xbf16> loc("p521.12202"), %arg522: tensor<1280x1280xbf16> loc("p522.12309"), %arg523: tensor<1280x1280xbf16> loc("p523.12314"), %arg524: tensor<1280xbf16> loc("p524.12320"), %arg525: tensor<1280xbf16> loc("p525.12325"), %arg526: tensor<1280xbf16> loc("p526.12410"), %arg527: tensor<1280xbf16> loc("p527.12415"), %arg528: tensor<1280x1280xbf16> loc("p528.12507"), %arg529: tensor<1280x1280xbf16> loc("p529.12522"), %arg530: tensor<1280x5120xbf16> loc("p530.12602"), %arg531: tensor<5120x1280xbf16> loc("p531.12607"), %arg532: tensor<1280xbf16> loc("p532.12613"), %arg533: tensor<1280xbf16> loc("p533.12618"), %arg534: tensor<1280x1280xbf16> loc("p534.12725"), %arg535: tensor<1280x1280xbf16> loc("p535.12730"), %arg536: tensor<1280xbf16> loc("p536.12736"), %arg537: tensor<1280xbf16> loc("p537.12741"), %arg538: tensor<1280xbf16> loc("p538.12826"), %arg539: tensor<1280xbf16> loc("p539.12831"), %arg540: tensor<1280x1280xbf16> loc("p540.12923"), %arg541: tensor<1280x1280xbf16> loc("p541.12938"), %arg542: tensor<1280x5120xbf16> loc("p542.13018"), %arg543: tensor<5120x1280xbf16> loc("p543.13023"), %arg544: tensor<1280xbf16> loc("p544.13029"), %arg545: tensor<1280xbf16> loc("p545.13034"), %arg546: tensor<1280x1280xbf16> loc("p546.13141"), %arg547: tensor<1280x1280xbf16> loc("p547.13146"), %arg548: tensor<1280xbf16> loc("p548.13152"), %arg549: tensor<1280xbf16> loc("p549.13157"), %arg550: tensor<1280xbf16> loc("p550.13242"), %arg551: tensor<1280xbf16> loc("p551.13247"), %arg552: tensor<1280x1280xbf16> loc("p552.13339"), %arg553: tensor<1280x1280xbf16> loc("p553.13354"), %arg554: tensor<1280x5120xbf16> loc("p554.13434"), %arg555: tensor<5120x1280xbf16> loc("p555.13439"), %arg556: tensor<1280xbf16> loc("p556.13445"), %arg557: tensor<1280xbf16> loc("p557.13450")) -> tensor<1x16x2048xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x1280xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x20x16x273xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0xFFF0000000000000> : tensor<1x20x16x273xf64> loc(#loc)
    %cst_2 = stablehlo.constant dense<0.353553385> : tensor<1x20x64x273xf32> loc(#loc)
    %cst_3 = stablehlo.constant dense<0.000000e+00> : tensor<1x16x257x257xf32> loc(#loc)
    %cst_4 = stablehlo.constant dense<0xFFF0000000000000> : tensor<1x16x257x257xf64> loc(#loc)
    %cst_5 = stablehlo.constant dense<0.334370166> : tensor<1x16x80x257xf32> loc(#loc)
    %cst_6 = stablehlo.constant dense<0.334370166> : tensor<1x16x257x80xf32> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.353553385> : tensor<1x20x16x64xf32> loc(#loc)
    %cst_8 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_9 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<true> : tensor<i1> loc(#loc)
    %c_10 = stablehlo.constant dense<false> : tensor<i1> loc(#loc)
    %0 = stablehlo.custom_call @tt.mark_argument(%arg4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_latents"}} : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc559)
    %1 = stablehlo.reshape %arg8 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc560)
    %2 = stablehlo.custom_call @tt.mark_argument(%1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ln1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc561)
    %3 = stablehlo.reshape %2 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc562)
    %4 = stablehlo.reshape %arg7 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc563)
    %5 = stablehlo.custom_call @tt.mark_argument(%4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ln1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc564)
    %6 = stablehlo.reshape %5 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc565)
    %7 = stablehlo.composite "tenstorrent.layer_norm" %0, %3, %6 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_27} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc566)
    %8 = stablehlo.reshape %7 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc567)
    %9 = stablehlo.reshape %arg517 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc568)
    %10 = stablehlo.custom_call @tt.mark_argument(%9) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_attn_to_q_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc569)
    %11 = stablehlo.reshape %10 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc570)
    %12 = stablehlo.transpose %11, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc571)
    %13 = stablehlo.dot_general %8, %12, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc572)
    %14 = stablehlo.reshape %13 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc573)
    %15 = stablehlo.transpose %14, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc574)
    %16 = stablehlo.convert %15 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc575)
    %17 = stablehlo.multiply %16, %cst_7 : tensor<1x20x16x64xf32> loc(#loc576)
    %18 = stablehlo.reshape %arg391 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc577)
    %19 = stablehlo.custom_call @tt.mark_argument(%18) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_embeddings_class_embedding"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc578)
    %20 = stablehlo.custom_call @tt.mark_argument(%arg390) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x3x224x224xbf16>) -> tensor<1x3x224x224xbf16> loc(#loc579)
    %21 = stablehlo.custom_call @tt.mark_argument(%arg389) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_embeddings_patch_embedding_weight"}} : (tensor<1280x3x14x14xbf16>) -> tensor<1280x3x14x14xbf16> loc(#loc580)
    %22 = stablehlo.convolution(%20, %21) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [14, 14]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x3x224x224xbf16>, tensor<1280x3x14x14xbf16>) -> tensor<1x1280x16x16xbf16> loc(#loc581)
    %23 = stablehlo.reshape %22 : (tensor<1x1280x16x16xbf16>) -> tensor<1x1280x256xbf16> loc(#loc582)
    %24 = stablehlo.transpose %23, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "bf16[1,256,1280]{1,2,0}"} : (tensor<1x1280x256xbf16>) -> tensor<1x256x1280xbf16> loc(#loc583)
    %25 = stablehlo.concatenate %19, %24, dim = 1 : (tensor<1x1x1280xbf16>, tensor<1x256x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc584)
    %26 = stablehlo.reshape %arg388 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc585)
    %27 = stablehlo.custom_call @tt.mark_argument(%26) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_embeddings_position_embedding_weight"}} : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc586)
    %28 = stablehlo.reshape %27 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc587)
    %29 = stablehlo.reshape %arg387 : (tensor<1x257xi64>) -> tensor<1x1x257xi64> loc(#loc588)
    %30 = stablehlo.custom_call @tt.mark_argument(%29) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___image_encoder_vision_model_embeddings_position_ids"}} : (tensor<1x1x257xi64>) -> tensor<1x1x257xi64> loc(#loc589)
    %31 = stablehlo.reshape %30 : (tensor<1x1x257xi64>) -> tensor<257xi64> loc(#loc590)
    %32 = stablehlo.convert %31 : (tensor<257xi64>) -> tensor<257xui32> loc(#loc591)
    %33 = "stablehlo.gather"(%28, %32) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1280>}> : (tensor<257x1280xbf16>, tensor<257xui32>) -> tensor<257x1280xbf16> loc(#loc592)
    %34 = stablehlo.reshape %33 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc593)
    %35 = stablehlo.add %25, %34 : tensor<1x257x1280xbf16> loc(#loc594)
    %36 = stablehlo.reshape %arg386 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc595)
    %37 = stablehlo.custom_call @tt.mark_argument(%36) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc596)
    %38 = stablehlo.reshape %37 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc597)
    %39 = stablehlo.reshape %arg385 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc598)
    %40 = stablehlo.custom_call @tt.mark_argument(%39) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc599)
    %41 = stablehlo.reshape %40 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc600)
    %42 = stablehlo.composite "tenstorrent.layer_norm" %35, %38, %41 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_23} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc601)
    %43 = stablehlo.reshape %arg384 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc602)
    %44 = stablehlo.custom_call @tt.mark_argument(%43) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc603)
    %45 = stablehlo.reshape %44 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc604)
    %46 = stablehlo.reshape %arg383 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc605)
    %47 = stablehlo.custom_call @tt.mark_argument(%46) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc606)
    %48 = stablehlo.reshape %47 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc607)
    %49 = stablehlo.composite "tenstorrent.layer_norm" %42, %45, %48 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_38} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc608)
    %50 = stablehlo.reshape %49 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc609)
    %51 = stablehlo.reshape %arg395 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc610)
    %52 = stablehlo.custom_call @tt.mark_argument(%51) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc611)
    %53 = stablehlo.reshape %52 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc612)
    %54 = stablehlo.transpose %53, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc613)
    %55 = stablehlo.dot_general %50, %54, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc614)
    %56 = stablehlo.reshape %55 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc615)
    %57 = stablehlo.reshape %arg394 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc616)
    %58 = stablehlo.custom_call @tt.mark_argument(%57) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc617)
    %59 = stablehlo.reshape %58 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc618)
    %60 = stablehlo.broadcast_in_dim %59, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc619)
    %61 = stablehlo.add %56, %60 : tensor<1x257x1280xbf16> loc(#loc620)
    %62 = stablehlo.reshape %61 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc621)
    %63 = stablehlo.transpose %62, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc622)
    %64 = stablehlo.convert %63 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc623)
    %65 = stablehlo.multiply %64, %cst_6 : tensor<1x16x257x80xf32> loc(#loc624)
    %66 = stablehlo.reshape %arg393 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc625)
    %67 = stablehlo.custom_call @tt.mark_argument(%66) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc626)
    %68 = stablehlo.reshape %67 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc627)
    %69 = stablehlo.transpose %68, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc628)
    %70 = stablehlo.dot_general %50, %69, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc629)
    %71 = stablehlo.reshape %70 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc630)
    %72 = stablehlo.reshape %arg392 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc631)
    %73 = stablehlo.custom_call @tt.mark_argument(%72) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc632)
    %74 = stablehlo.reshape %73 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc633)
    %75 = stablehlo.broadcast_in_dim %74, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc634)
    %76 = stablehlo.add %71, %75 : tensor<1x257x1280xbf16> loc(#loc635)
    %77 = stablehlo.reshape %76 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc636)
    %78 = stablehlo.transpose %77, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc637)
    %79 = stablehlo.convert %78 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc638)
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc639)
    %81 = stablehlo.multiply %80, %cst_5 : tensor<1x16x80x257xf32> loc(#loc640)
    %82 = stablehlo.dot_general %65, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc641)
    %83 = stablehlo.convert %82 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc642)
    %84 = stablehlo.compare  EQ, %83, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc643)
    %85 = stablehlo.not %84 : tensor<1x16x257x257xi1> loc(#loc644)
    %86 = stablehlo.reduce(%85 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.2566"), %arg559: tensor<i1> loc("reduce.2566"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc646)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc647)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc645)
    %87 = stablehlo.reshape %86 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc648)
    %88 = stablehlo.not %87 : tensor<1x16x257x1xi1> loc(#loc649)
    %89 = stablehlo.reshape %88 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc650)
    %90 = stablehlo.broadcast_in_dim %89, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc651)
    %91 = stablehlo.reduce(%82 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc652)
    %92 = stablehlo.broadcast_in_dim %91, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc653)
    %93 = stablehlo.subtract %82, %92 : tensor<1x16x257x257xf32> loc(#loc654)
    %94 = stablehlo.exponential %93 : tensor<1x16x257x257xf32> loc(#loc655)
    %95 = stablehlo.reduce(%94 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc656)
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc657)
    %97 = stablehlo.divide %94, %96 : tensor<1x16x257x257xf32> loc(#loc658)
    %98 = stablehlo.select %90, %cst_3, %97 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc659)
    %99 = stablehlo.reshape %arg382 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc660)
    %100 = stablehlo.custom_call @tt.mark_argument(%99) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc661)
    %101 = stablehlo.reshape %100 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc662)
    %102 = stablehlo.transpose %101, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc663)
    %103 = stablehlo.dot_general %50, %102, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc664)
    %104 = stablehlo.reshape %103 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc665)
    %105 = stablehlo.reshape %arg381 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc666)
    %106 = stablehlo.custom_call @tt.mark_argument(%105) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc667)
    %107 = stablehlo.reshape %106 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc668)
    %108 = stablehlo.broadcast_in_dim %107, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc669)
    %109 = stablehlo.add %104, %108 : tensor<1x257x1280xbf16> loc(#loc670)
    %110 = stablehlo.reshape %109 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc671)
    %111 = stablehlo.transpose %110, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc672)
    %112 = stablehlo.convert %111 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc673)
    %113 = stablehlo.dot_general %98, %112, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc674)
    %114 = stablehlo.convert %113 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc675)
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc676)
    %116 = stablehlo.reshape %115 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc677)
    %117 = stablehlo.reshape %arg380 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc678)
    %118 = stablehlo.custom_call @tt.mark_argument(%117) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc679)
    %119 = stablehlo.reshape %118 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc680)
    %120 = stablehlo.transpose %119, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc681)
    %121 = stablehlo.dot_general %116, %120, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc682)
    %122 = stablehlo.reshape %121 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc683)
    %123 = stablehlo.reshape %arg379 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc684)
    %124 = stablehlo.custom_call @tt.mark_argument(%123) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc685)
    %125 = stablehlo.reshape %124 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc686)
    %126 = stablehlo.broadcast_in_dim %125, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc687)
    %127 = stablehlo.add %122, %126 : tensor<1x257x1280xbf16> loc(#loc688)
    %128 = stablehlo.add %42, %127 : tensor<1x257x1280xbf16> loc(#loc689)
    %129 = stablehlo.reshape %arg378 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc690)
    %130 = stablehlo.custom_call @tt.mark_argument(%129) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc691)
    %131 = stablehlo.reshape %130 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc692)
    %132 = stablehlo.reshape %arg377 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc693)
    %133 = stablehlo.custom_call @tt.mark_argument(%132) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc694)
    %134 = stablehlo.reshape %133 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc695)
    %135 = stablehlo.composite "tenstorrent.layer_norm" %128, %131, %134 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_25} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc696)
    %136 = stablehlo.reshape %135 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc697)
    %137 = stablehlo.reshape %arg376 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc698)
    %138 = stablehlo.custom_call @tt.mark_argument(%137) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc699)
    %139 = stablehlo.reshape %138 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc700)
    %140 = stablehlo.transpose %139, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc701)
    %141 = stablehlo.dot_general %136, %140, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc702)
    %142 = stablehlo.reshape %141 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc703)
    %143 = stablehlo.reshape %arg375 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc704)
    %144 = stablehlo.custom_call @tt.mark_argument(%143) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc705)
    %145 = stablehlo.reshape %144 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc706)
    %146 = stablehlo.broadcast_in_dim %145, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc707)
    %147 = stablehlo.add %142, %146 : tensor<1x257x5120xbf16> loc(#loc708)
    %148 = stablehlo.composite "tenstorrent.gelu" %147 {decomposition = @tenstorrent.gelu.impl_17} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc709)
    %149 = stablehlo.reshape %148 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc710)
    %150 = stablehlo.reshape %arg374 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc711)
    %151 = stablehlo.custom_call @tt.mark_argument(%150) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc712)
    %152 = stablehlo.reshape %151 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc713)
    %153 = stablehlo.transpose %152, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc714)
    %154 = stablehlo.dot_general %149, %153, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc715)
    %155 = stablehlo.reshape %154 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc716)
    %156 = stablehlo.reshape %arg373 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc717)
    %157 = stablehlo.custom_call @tt.mark_argument(%156) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc718)
    %158 = stablehlo.reshape %157 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc719)
    %159 = stablehlo.broadcast_in_dim %158, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc720)
    %160 = stablehlo.add %155, %159 : tensor<1x257x1280xbf16> loc(#loc721)
    %161 = stablehlo.add %128, %160 : tensor<1x257x1280xbf16> loc(#loc722)
    %162 = stablehlo.reshape %arg372 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc723)
    %163 = stablehlo.custom_call @tt.mark_argument(%162) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc724)
    %164 = stablehlo.reshape %163 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc725)
    %165 = stablehlo.reshape %arg371 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc726)
    %166 = stablehlo.custom_call @tt.mark_argument(%165) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc727)
    %167 = stablehlo.reshape %166 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc728)
    %168 = stablehlo.composite "tenstorrent.layer_norm" %161, %164, %167 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_70} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc729)
    %169 = stablehlo.reshape %168 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc730)
    %170 = stablehlo.reshape %arg399 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc731)
    %171 = stablehlo.custom_call @tt.mark_argument(%170) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc732)
    %172 = stablehlo.reshape %171 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc733)
    %173 = stablehlo.transpose %172, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc734)
    %174 = stablehlo.dot_general %169, %173, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc735)
    %175 = stablehlo.reshape %174 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc736)
    %176 = stablehlo.reshape %arg398 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc737)
    %177 = stablehlo.custom_call @tt.mark_argument(%176) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc738)
    %178 = stablehlo.reshape %177 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc739)
    %179 = stablehlo.broadcast_in_dim %178, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc740)
    %180 = stablehlo.add %175, %179 : tensor<1x257x1280xbf16> loc(#loc741)
    %181 = stablehlo.reshape %180 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc742)
    %182 = stablehlo.transpose %181, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc743)
    %183 = stablehlo.convert %182 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc744)
    %184 = stablehlo.multiply %183, %cst_6 : tensor<1x16x257x80xf32> loc(#loc745)
    %185 = stablehlo.reshape %arg397 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc746)
    %186 = stablehlo.custom_call @tt.mark_argument(%185) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc747)
    %187 = stablehlo.reshape %186 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc748)
    %188 = stablehlo.transpose %187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc749)
    %189 = stablehlo.dot_general %169, %188, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc750)
    %190 = stablehlo.reshape %189 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc751)
    %191 = stablehlo.reshape %arg396 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc752)
    %192 = stablehlo.custom_call @tt.mark_argument(%191) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc753)
    %193 = stablehlo.reshape %192 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc754)
    %194 = stablehlo.broadcast_in_dim %193, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc755)
    %195 = stablehlo.add %190, %194 : tensor<1x257x1280xbf16> loc(#loc756)
    %196 = stablehlo.reshape %195 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc757)
    %197 = stablehlo.transpose %196, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc758)
    %198 = stablehlo.convert %197 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc759)
    %199 = stablehlo.transpose %198, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc760)
    %200 = stablehlo.multiply %199, %cst_5 : tensor<1x16x80x257xf32> loc(#loc761)
    %201 = stablehlo.dot_general %184, %200, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc762)
    %202 = stablehlo.convert %201 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc763)
    %203 = stablehlo.compare  EQ, %202, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc764)
    %204 = stablehlo.not %203 : tensor<1x16x257x257xi1> loc(#loc765)
    %205 = stablehlo.reduce(%204 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.2876"), %arg559: tensor<i1> loc("reduce.2876"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc767)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc768)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc766)
    %206 = stablehlo.reshape %205 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc769)
    %207 = stablehlo.not %206 : tensor<1x16x257x1xi1> loc(#loc770)
    %208 = stablehlo.reshape %207 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc771)
    %209 = stablehlo.broadcast_in_dim %208, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc772)
    %210 = stablehlo.reduce(%201 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc773)
    %211 = stablehlo.broadcast_in_dim %210, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc774)
    %212 = stablehlo.subtract %201, %211 : tensor<1x16x257x257xf32> loc(#loc775)
    %213 = stablehlo.exponential %212 : tensor<1x16x257x257xf32> loc(#loc776)
    %214 = stablehlo.reduce(%213 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc777)
    %215 = stablehlo.broadcast_in_dim %214, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc778)
    %216 = stablehlo.divide %213, %215 : tensor<1x16x257x257xf32> loc(#loc779)
    %217 = stablehlo.select %209, %cst_3, %216 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc780)
    %218 = stablehlo.reshape %arg370 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc781)
    %219 = stablehlo.custom_call @tt.mark_argument(%218) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc782)
    %220 = stablehlo.reshape %219 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc783)
    %221 = stablehlo.transpose %220, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc784)
    %222 = stablehlo.dot_general %169, %221, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc785)
    %223 = stablehlo.reshape %222 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc786)
    %224 = stablehlo.reshape %arg369 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc787)
    %225 = stablehlo.custom_call @tt.mark_argument(%224) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc788)
    %226 = stablehlo.reshape %225 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc789)
    %227 = stablehlo.broadcast_in_dim %226, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc790)
    %228 = stablehlo.add %223, %227 : tensor<1x257x1280xbf16> loc(#loc791)
    %229 = stablehlo.reshape %228 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc792)
    %230 = stablehlo.transpose %229, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc793)
    %231 = stablehlo.convert %230 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc794)
    %232 = stablehlo.dot_general %217, %231, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc795)
    %233 = stablehlo.convert %232 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc796)
    %234 = stablehlo.transpose %233, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc797)
    %235 = stablehlo.reshape %234 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc798)
    %236 = stablehlo.reshape %arg368 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc799)
    %237 = stablehlo.custom_call @tt.mark_argument(%236) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc800)
    %238 = stablehlo.reshape %237 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc801)
    %239 = stablehlo.transpose %238, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc802)
    %240 = stablehlo.dot_general %235, %239, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc803)
    %241 = stablehlo.reshape %240 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc804)
    %242 = stablehlo.reshape %arg367 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc805)
    %243 = stablehlo.custom_call @tt.mark_argument(%242) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc806)
    %244 = stablehlo.reshape %243 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc807)
    %245 = stablehlo.broadcast_in_dim %244, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc808)
    %246 = stablehlo.add %241, %245 : tensor<1x257x1280xbf16> loc(#loc809)
    %247 = stablehlo.add %161, %246 : tensor<1x257x1280xbf16> loc(#loc810)
    %248 = stablehlo.reshape %arg366 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc811)
    %249 = stablehlo.custom_call @tt.mark_argument(%248) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc812)
    %250 = stablehlo.reshape %249 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc813)
    %251 = stablehlo.reshape %arg365 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc814)
    %252 = stablehlo.custom_call @tt.mark_argument(%251) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc815)
    %253 = stablehlo.reshape %252 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc816)
    %254 = stablehlo.composite "tenstorrent.layer_norm" %247, %250, %253 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_45} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc817)
    %255 = stablehlo.reshape %254 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc818)
    %256 = stablehlo.reshape %arg364 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc819)
    %257 = stablehlo.custom_call @tt.mark_argument(%256) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc820)
    %258 = stablehlo.reshape %257 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc821)
    %259 = stablehlo.transpose %258, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc822)
    %260 = stablehlo.dot_general %255, %259, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc823)
    %261 = stablehlo.reshape %260 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc824)
    %262 = stablehlo.reshape %arg363 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc825)
    %263 = stablehlo.custom_call @tt.mark_argument(%262) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc826)
    %264 = stablehlo.reshape %263 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc827)
    %265 = stablehlo.broadcast_in_dim %264, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc828)
    %266 = stablehlo.add %261, %265 : tensor<1x257x5120xbf16> loc(#loc829)
    %267 = stablehlo.composite "tenstorrent.gelu" %266 {decomposition = @tenstorrent.gelu.impl_16} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc830)
    %268 = stablehlo.reshape %267 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc831)
    %269 = stablehlo.reshape %arg362 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc832)
    %270 = stablehlo.custom_call @tt.mark_argument(%269) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc833)
    %271 = stablehlo.reshape %270 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc834)
    %272 = stablehlo.transpose %271, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc835)
    %273 = stablehlo.dot_general %268, %272, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc836)
    %274 = stablehlo.reshape %273 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc837)
    %275 = stablehlo.reshape %arg361 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc838)
    %276 = stablehlo.custom_call @tt.mark_argument(%275) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc839)
    %277 = stablehlo.reshape %276 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc840)
    %278 = stablehlo.broadcast_in_dim %277, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc841)
    %279 = stablehlo.add %274, %278 : tensor<1x257x1280xbf16> loc(#loc842)
    %280 = stablehlo.add %247, %279 : tensor<1x257x1280xbf16> loc(#loc843)
    %281 = stablehlo.reshape %arg360 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc844)
    %282 = stablehlo.custom_call @tt.mark_argument(%281) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc845)
    %283 = stablehlo.reshape %282 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc846)
    %284 = stablehlo.reshape %arg359 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc847)
    %285 = stablehlo.custom_call @tt.mark_argument(%284) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc848)
    %286 = stablehlo.reshape %285 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc849)
    %287 = stablehlo.composite "tenstorrent.layer_norm" %280, %283, %286 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_58} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc850)
    %288 = stablehlo.reshape %287 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc851)
    %289 = stablehlo.reshape %arg403 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc852)
    %290 = stablehlo.custom_call @tt.mark_argument(%289) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc853)
    %291 = stablehlo.reshape %290 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc854)
    %292 = stablehlo.transpose %291, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc855)
    %293 = stablehlo.dot_general %288, %292, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc856)
    %294 = stablehlo.reshape %293 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc857)
    %295 = stablehlo.reshape %arg402 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc858)
    %296 = stablehlo.custom_call @tt.mark_argument(%295) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc859)
    %297 = stablehlo.reshape %296 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc860)
    %298 = stablehlo.broadcast_in_dim %297, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc861)
    %299 = stablehlo.add %294, %298 : tensor<1x257x1280xbf16> loc(#loc862)
    %300 = stablehlo.reshape %299 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc863)
    %301 = stablehlo.transpose %300, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc864)
    %302 = stablehlo.convert %301 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc865)
    %303 = stablehlo.multiply %302, %cst_6 : tensor<1x16x257x80xf32> loc(#loc866)
    %304 = stablehlo.reshape %arg401 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc867)
    %305 = stablehlo.custom_call @tt.mark_argument(%304) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc868)
    %306 = stablehlo.reshape %305 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc869)
    %307 = stablehlo.transpose %306, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc870)
    %308 = stablehlo.dot_general %288, %307, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc871)
    %309 = stablehlo.reshape %308 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc872)
    %310 = stablehlo.reshape %arg400 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc873)
    %311 = stablehlo.custom_call @tt.mark_argument(%310) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc874)
    %312 = stablehlo.reshape %311 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc875)
    %313 = stablehlo.broadcast_in_dim %312, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc876)
    %314 = stablehlo.add %309, %313 : tensor<1x257x1280xbf16> loc(#loc877)
    %315 = stablehlo.reshape %314 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc878)
    %316 = stablehlo.transpose %315, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc879)
    %317 = stablehlo.convert %316 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc880)
    %318 = stablehlo.transpose %317, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc881)
    %319 = stablehlo.multiply %318, %cst_5 : tensor<1x16x80x257xf32> loc(#loc882)
    %320 = stablehlo.dot_general %303, %319, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc883)
    %321 = stablehlo.convert %320 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc884)
    %322 = stablehlo.compare  EQ, %321, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc885)
    %323 = stablehlo.not %322 : tensor<1x16x257x257xi1> loc(#loc886)
    %324 = stablehlo.reduce(%323 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.3186"), %arg559: tensor<i1> loc("reduce.3186"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc888)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc889)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc887)
    %325 = stablehlo.reshape %324 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc890)
    %326 = stablehlo.not %325 : tensor<1x16x257x1xi1> loc(#loc891)
    %327 = stablehlo.reshape %326 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc892)
    %328 = stablehlo.broadcast_in_dim %327, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc893)
    %329 = stablehlo.reduce(%320 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc894)
    %330 = stablehlo.broadcast_in_dim %329, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc895)
    %331 = stablehlo.subtract %320, %330 : tensor<1x16x257x257xf32> loc(#loc896)
    %332 = stablehlo.exponential %331 : tensor<1x16x257x257xf32> loc(#loc897)
    %333 = stablehlo.reduce(%332 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc898)
    %334 = stablehlo.broadcast_in_dim %333, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc899)
    %335 = stablehlo.divide %332, %334 : tensor<1x16x257x257xf32> loc(#loc900)
    %336 = stablehlo.select %328, %cst_3, %335 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc901)
    %337 = stablehlo.reshape %arg358 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc902)
    %338 = stablehlo.custom_call @tt.mark_argument(%337) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc903)
    %339 = stablehlo.reshape %338 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc904)
    %340 = stablehlo.transpose %339, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc905)
    %341 = stablehlo.dot_general %288, %340, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc906)
    %342 = stablehlo.reshape %341 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc907)
    %343 = stablehlo.reshape %arg357 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc908)
    %344 = stablehlo.custom_call @tt.mark_argument(%343) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc909)
    %345 = stablehlo.reshape %344 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc910)
    %346 = stablehlo.broadcast_in_dim %345, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc911)
    %347 = stablehlo.add %342, %346 : tensor<1x257x1280xbf16> loc(#loc912)
    %348 = stablehlo.reshape %347 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc913)
    %349 = stablehlo.transpose %348, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc914)
    %350 = stablehlo.convert %349 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc915)
    %351 = stablehlo.dot_general %336, %350, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc916)
    %352 = stablehlo.convert %351 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc917)
    %353 = stablehlo.transpose %352, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc918)
    %354 = stablehlo.reshape %353 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc919)
    %355 = stablehlo.reshape %arg356 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc920)
    %356 = stablehlo.custom_call @tt.mark_argument(%355) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc921)
    %357 = stablehlo.reshape %356 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc922)
    %358 = stablehlo.transpose %357, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc923)
    %359 = stablehlo.dot_general %354, %358, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc924)
    %360 = stablehlo.reshape %359 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc925)
    %361 = stablehlo.reshape %arg355 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc926)
    %362 = stablehlo.custom_call @tt.mark_argument(%361) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc927)
    %363 = stablehlo.reshape %362 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc928)
    %364 = stablehlo.broadcast_in_dim %363, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc929)
    %365 = stablehlo.add %360, %364 : tensor<1x257x1280xbf16> loc(#loc930)
    %366 = stablehlo.add %280, %365 : tensor<1x257x1280xbf16> loc(#loc931)
    %367 = stablehlo.reshape %arg354 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc932)
    %368 = stablehlo.custom_call @tt.mark_argument(%367) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc933)
    %369 = stablehlo.reshape %368 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc934)
    %370 = stablehlo.reshape %arg353 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc935)
    %371 = stablehlo.custom_call @tt.mark_argument(%370) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc936)
    %372 = stablehlo.reshape %371 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc937)
    %373 = stablehlo.composite "tenstorrent.layer_norm" %366, %369, %372 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_54} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc938)
    %374 = stablehlo.reshape %373 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc939)
    %375 = stablehlo.reshape %arg352 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc940)
    %376 = stablehlo.custom_call @tt.mark_argument(%375) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc941)
    %377 = stablehlo.reshape %376 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc942)
    %378 = stablehlo.transpose %377, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc943)
    %379 = stablehlo.dot_general %374, %378, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc944)
    %380 = stablehlo.reshape %379 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc945)
    %381 = stablehlo.reshape %arg351 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc946)
    %382 = stablehlo.custom_call @tt.mark_argument(%381) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc947)
    %383 = stablehlo.reshape %382 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc948)
    %384 = stablehlo.broadcast_in_dim %383, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc949)
    %385 = stablehlo.add %380, %384 : tensor<1x257x5120xbf16> loc(#loc950)
    %386 = stablehlo.composite "tenstorrent.gelu" %385 {decomposition = @tenstorrent.gelu.impl_20} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc951)
    %387 = stablehlo.reshape %386 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc952)
    %388 = stablehlo.reshape %arg350 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc953)
    %389 = stablehlo.custom_call @tt.mark_argument(%388) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc954)
    %390 = stablehlo.reshape %389 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc955)
    %391 = stablehlo.transpose %390, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc956)
    %392 = stablehlo.dot_general %387, %391, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc957)
    %393 = stablehlo.reshape %392 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc958)
    %394 = stablehlo.reshape %arg349 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc959)
    %395 = stablehlo.custom_call @tt.mark_argument(%394) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc960)
    %396 = stablehlo.reshape %395 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc961)
    %397 = stablehlo.broadcast_in_dim %396, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc962)
    %398 = stablehlo.add %393, %397 : tensor<1x257x1280xbf16> loc(#loc963)
    %399 = stablehlo.add %366, %398 : tensor<1x257x1280xbf16> loc(#loc964)
    %400 = stablehlo.reshape %arg348 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc965)
    %401 = stablehlo.custom_call @tt.mark_argument(%400) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc966)
    %402 = stablehlo.reshape %401 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc967)
    %403 = stablehlo.reshape %arg347 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc968)
    %404 = stablehlo.custom_call @tt.mark_argument(%403) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc969)
    %405 = stablehlo.reshape %404 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc970)
    %406 = stablehlo.composite "tenstorrent.layer_norm" %399, %402, %405 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_43} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc971)
    %407 = stablehlo.reshape %406 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc972)
    %408 = stablehlo.reshape %arg407 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc973)
    %409 = stablehlo.custom_call @tt.mark_argument(%408) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc974)
    %410 = stablehlo.reshape %409 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc975)
    %411 = stablehlo.transpose %410, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc976)
    %412 = stablehlo.dot_general %407, %411, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc977)
    %413 = stablehlo.reshape %412 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc978)
    %414 = stablehlo.reshape %arg406 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc979)
    %415 = stablehlo.custom_call @tt.mark_argument(%414) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc980)
    %416 = stablehlo.reshape %415 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc981)
    %417 = stablehlo.broadcast_in_dim %416, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc982)
    %418 = stablehlo.add %413, %417 : tensor<1x257x1280xbf16> loc(#loc983)
    %419 = stablehlo.reshape %418 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc984)
    %420 = stablehlo.transpose %419, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc985)
    %421 = stablehlo.convert %420 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc986)
    %422 = stablehlo.multiply %421, %cst_6 : tensor<1x16x257x80xf32> loc(#loc987)
    %423 = stablehlo.reshape %arg405 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc988)
    %424 = stablehlo.custom_call @tt.mark_argument(%423) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc989)
    %425 = stablehlo.reshape %424 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc990)
    %426 = stablehlo.transpose %425, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc991)
    %427 = stablehlo.dot_general %407, %426, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc992)
    %428 = stablehlo.reshape %427 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc993)
    %429 = stablehlo.reshape %arg404 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc994)
    %430 = stablehlo.custom_call @tt.mark_argument(%429) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc995)
    %431 = stablehlo.reshape %430 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc996)
    %432 = stablehlo.broadcast_in_dim %431, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc997)
    %433 = stablehlo.add %428, %432 : tensor<1x257x1280xbf16> loc(#loc998)
    %434 = stablehlo.reshape %433 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc999)
    %435 = stablehlo.transpose %434, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1000)
    %436 = stablehlo.convert %435 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1001)
    %437 = stablehlo.transpose %436, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1002)
    %438 = stablehlo.multiply %437, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1003)
    %439 = stablehlo.dot_general %422, %438, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1004)
    %440 = stablehlo.convert %439 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1005)
    %441 = stablehlo.compare  EQ, %440, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1006)
    %442 = stablehlo.not %441 : tensor<1x16x257x257xi1> loc(#loc1007)
    %443 = stablehlo.reduce(%442 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.3496"), %arg559: tensor<i1> loc("reduce.3496"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1009)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc1010)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1008)
    %444 = stablehlo.reshape %443 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1011)
    %445 = stablehlo.not %444 : tensor<1x16x257x1xi1> loc(#loc1012)
    %446 = stablehlo.reshape %445 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1013)
    %447 = stablehlo.broadcast_in_dim %446, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1014)
    %448 = stablehlo.reduce(%439 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1015)
    %449 = stablehlo.broadcast_in_dim %448, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1016)
    %450 = stablehlo.subtract %439, %449 : tensor<1x16x257x257xf32> loc(#loc1017)
    %451 = stablehlo.exponential %450 : tensor<1x16x257x257xf32> loc(#loc1018)
    %452 = stablehlo.reduce(%451 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1019)
    %453 = stablehlo.broadcast_in_dim %452, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1020)
    %454 = stablehlo.divide %451, %453 : tensor<1x16x257x257xf32> loc(#loc1021)
    %455 = stablehlo.select %447, %cst_3, %454 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1022)
    %456 = stablehlo.reshape %arg346 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1023)
    %457 = stablehlo.custom_call @tt.mark_argument(%456) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1024)
    %458 = stablehlo.reshape %457 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1025)
    %459 = stablehlo.transpose %458, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1026)
    %460 = stablehlo.dot_general %407, %459, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1027)
    %461 = stablehlo.reshape %460 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1028)
    %462 = stablehlo.reshape %arg345 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1029)
    %463 = stablehlo.custom_call @tt.mark_argument(%462) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1030)
    %464 = stablehlo.reshape %463 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1031)
    %465 = stablehlo.broadcast_in_dim %464, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1032)
    %466 = stablehlo.add %461, %465 : tensor<1x257x1280xbf16> loc(#loc1033)
    %467 = stablehlo.reshape %466 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1034)
    %468 = stablehlo.transpose %467, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1035)
    %469 = stablehlo.convert %468 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1036)
    %470 = stablehlo.dot_general %455, %469, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1037)
    %471 = stablehlo.convert %470 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1038)
    %472 = stablehlo.transpose %471, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1039)
    %473 = stablehlo.reshape %472 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1040)
    %474 = stablehlo.reshape %arg344 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1041)
    %475 = stablehlo.custom_call @tt.mark_argument(%474) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1042)
    %476 = stablehlo.reshape %475 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1043)
    %477 = stablehlo.transpose %476, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1044)
    %478 = stablehlo.dot_general %473, %477, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1045)
    %479 = stablehlo.reshape %478 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1046)
    %480 = stablehlo.reshape %arg343 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1047)
    %481 = stablehlo.custom_call @tt.mark_argument(%480) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1048)
    %482 = stablehlo.reshape %481 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1049)
    %483 = stablehlo.broadcast_in_dim %482, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1050)
    %484 = stablehlo.add %479, %483 : tensor<1x257x1280xbf16> loc(#loc1051)
    %485 = stablehlo.add %399, %484 : tensor<1x257x1280xbf16> loc(#loc1052)
    %486 = stablehlo.reshape %arg342 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1053)
    %487 = stablehlo.custom_call @tt.mark_argument(%486) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1054)
    %488 = stablehlo.reshape %487 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1055)
    %489 = stablehlo.reshape %arg341 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1056)
    %490 = stablehlo.custom_call @tt.mark_argument(%489) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1057)
    %491 = stablehlo.reshape %490 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1058)
    %492 = stablehlo.composite "tenstorrent.layer_norm" %485, %488, %491 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_24} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1059)
    %493 = stablehlo.reshape %492 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1060)
    %494 = stablehlo.reshape %arg340 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1061)
    %495 = stablehlo.custom_call @tt.mark_argument(%494) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1062)
    %496 = stablehlo.reshape %495 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc1063)
    %497 = stablehlo.transpose %496, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1064)
    %498 = stablehlo.dot_general %493, %497, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1065)
    %499 = stablehlo.reshape %498 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1066)
    %500 = stablehlo.reshape %arg339 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1067)
    %501 = stablehlo.custom_call @tt.mark_argument(%500) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1068)
    %502 = stablehlo.reshape %501 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1069)
    %503 = stablehlo.broadcast_in_dim %502, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1070)
    %504 = stablehlo.add %499, %503 : tensor<1x257x5120xbf16> loc(#loc1071)
    %505 = stablehlo.composite "tenstorrent.gelu" %504 {decomposition = @tenstorrent.gelu.impl_27} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1072)
    %506 = stablehlo.reshape %505 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1073)
    %507 = stablehlo.reshape %arg338 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1074)
    %508 = stablehlo.custom_call @tt.mark_argument(%507) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1075)
    %509 = stablehlo.reshape %508 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc1076)
    %510 = stablehlo.transpose %509, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1077)
    %511 = stablehlo.dot_general %506, %510, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1078)
    %512 = stablehlo.reshape %511 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1079)
    %513 = stablehlo.reshape %arg337 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1080)
    %514 = stablehlo.custom_call @tt.mark_argument(%513) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1081)
    %515 = stablehlo.reshape %514 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1082)
    %516 = stablehlo.broadcast_in_dim %515, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1083)
    %517 = stablehlo.add %512, %516 : tensor<1x257x1280xbf16> loc(#loc1084)
    %518 = stablehlo.add %485, %517 : tensor<1x257x1280xbf16> loc(#loc1085)
    %519 = stablehlo.reshape %arg336 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1086)
    %520 = stablehlo.custom_call @tt.mark_argument(%519) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1087)
    %521 = stablehlo.reshape %520 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1088)
    %522 = stablehlo.reshape %arg335 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1089)
    %523 = stablehlo.custom_call @tt.mark_argument(%522) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1090)
    %524 = stablehlo.reshape %523 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1091)
    %525 = stablehlo.composite "tenstorrent.layer_norm" %518, %521, %524 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_37} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1092)
    %526 = stablehlo.reshape %525 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1093)
    %527 = stablehlo.reshape %arg411 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1094)
    %528 = stablehlo.custom_call @tt.mark_argument(%527) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1095)
    %529 = stablehlo.reshape %528 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1096)
    %530 = stablehlo.transpose %529, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1097)
    %531 = stablehlo.dot_general %526, %530, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1098)
    %532 = stablehlo.reshape %531 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1099)
    %533 = stablehlo.reshape %arg410 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1100)
    %534 = stablehlo.custom_call @tt.mark_argument(%533) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1101)
    %535 = stablehlo.reshape %534 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1102)
    %536 = stablehlo.broadcast_in_dim %535, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1103)
    %537 = stablehlo.add %532, %536 : tensor<1x257x1280xbf16> loc(#loc1104)
    %538 = stablehlo.reshape %537 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1105)
    %539 = stablehlo.transpose %538, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1106)
    %540 = stablehlo.convert %539 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1107)
    %541 = stablehlo.multiply %540, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1108)
    %542 = stablehlo.reshape %arg409 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1109)
    %543 = stablehlo.custom_call @tt.mark_argument(%542) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1110)
    %544 = stablehlo.reshape %543 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1111)
    %545 = stablehlo.transpose %544, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1112)
    %546 = stablehlo.dot_general %526, %545, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1113)
    %547 = stablehlo.reshape %546 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1114)
    %548 = stablehlo.reshape %arg408 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1115)
    %549 = stablehlo.custom_call @tt.mark_argument(%548) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1116)
    %550 = stablehlo.reshape %549 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1117)
    %551 = stablehlo.broadcast_in_dim %550, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1118)
    %552 = stablehlo.add %547, %551 : tensor<1x257x1280xbf16> loc(#loc1119)
    %553 = stablehlo.reshape %552 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1120)
    %554 = stablehlo.transpose %553, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1121)
    %555 = stablehlo.convert %554 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1122)
    %556 = stablehlo.transpose %555, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1123)
    %557 = stablehlo.multiply %556, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1124)
    %558 = stablehlo.dot_general %541, %557, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1125)
    %559 = stablehlo.convert %558 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1126)
    %560 = stablehlo.compare  EQ, %559, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1127)
    %561 = stablehlo.not %560 : tensor<1x16x257x257xi1> loc(#loc1128)
    %562 = stablehlo.reduce(%561 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.3806"), %arg559: tensor<i1> loc("reduce.3806"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1130)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc1131)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1129)
    %563 = stablehlo.reshape %562 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1132)
    %564 = stablehlo.not %563 : tensor<1x16x257x1xi1> loc(#loc1133)
    %565 = stablehlo.reshape %564 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1134)
    %566 = stablehlo.broadcast_in_dim %565, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1135)
    %567 = stablehlo.reduce(%558 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1136)
    %568 = stablehlo.broadcast_in_dim %567, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1137)
    %569 = stablehlo.subtract %558, %568 : tensor<1x16x257x257xf32> loc(#loc1138)
    %570 = stablehlo.exponential %569 : tensor<1x16x257x257xf32> loc(#loc1139)
    %571 = stablehlo.reduce(%570 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1140)
    %572 = stablehlo.broadcast_in_dim %571, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1141)
    %573 = stablehlo.divide %570, %572 : tensor<1x16x257x257xf32> loc(#loc1142)
    %574 = stablehlo.select %566, %cst_3, %573 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1143)
    %575 = stablehlo.reshape %arg334 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1144)
    %576 = stablehlo.custom_call @tt.mark_argument(%575) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1145)
    %577 = stablehlo.reshape %576 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1146)
    %578 = stablehlo.transpose %577, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1147)
    %579 = stablehlo.dot_general %526, %578, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1148)
    %580 = stablehlo.reshape %579 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1149)
    %581 = stablehlo.reshape %arg333 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1150)
    %582 = stablehlo.custom_call @tt.mark_argument(%581) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1151)
    %583 = stablehlo.reshape %582 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1152)
    %584 = stablehlo.broadcast_in_dim %583, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1153)
    %585 = stablehlo.add %580, %584 : tensor<1x257x1280xbf16> loc(#loc1154)
    %586 = stablehlo.reshape %585 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1155)
    %587 = stablehlo.transpose %586, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1156)
    %588 = stablehlo.convert %587 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1157)
    %589 = stablehlo.dot_general %574, %588, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1158)
    %590 = stablehlo.convert %589 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1159)
    %591 = stablehlo.transpose %590, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1160)
    %592 = stablehlo.reshape %591 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1161)
    %593 = stablehlo.reshape %arg332 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1162)
    %594 = stablehlo.custom_call @tt.mark_argument(%593) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1163)
    %595 = stablehlo.reshape %594 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1164)
    %596 = stablehlo.transpose %595, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1165)
    %597 = stablehlo.dot_general %592, %596, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1166)
    %598 = stablehlo.reshape %597 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1167)
    %599 = stablehlo.reshape %arg331 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1168)
    %600 = stablehlo.custom_call @tt.mark_argument(%599) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1169)
    %601 = stablehlo.reshape %600 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1170)
    %602 = stablehlo.broadcast_in_dim %601, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1171)
    %603 = stablehlo.add %598, %602 : tensor<1x257x1280xbf16> loc(#loc1172)
    %604 = stablehlo.add %518, %603 : tensor<1x257x1280xbf16> loc(#loc1173)
    %605 = stablehlo.reshape %arg330 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1174)
    %606 = stablehlo.custom_call @tt.mark_argument(%605) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1175)
    %607 = stablehlo.reshape %606 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1176)
    %608 = stablehlo.reshape %arg329 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1177)
    %609 = stablehlo.custom_call @tt.mark_argument(%608) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1178)
    %610 = stablehlo.reshape %609 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1179)
    %611 = stablehlo.composite "tenstorrent.layer_norm" %604, %607, %610 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_49} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1180)
    %612 = stablehlo.reshape %611 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1181)
    %613 = stablehlo.reshape %arg328 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1182)
    %614 = stablehlo.custom_call @tt.mark_argument(%613) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1183)
    %615 = stablehlo.reshape %614 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc1184)
    %616 = stablehlo.transpose %615, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1185)
    %617 = stablehlo.dot_general %612, %616, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1186)
    %618 = stablehlo.reshape %617 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1187)
    %619 = stablehlo.reshape %arg327 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1188)
    %620 = stablehlo.custom_call @tt.mark_argument(%619) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1189)
    %621 = stablehlo.reshape %620 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1190)
    %622 = stablehlo.broadcast_in_dim %621, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1191)
    %623 = stablehlo.add %618, %622 : tensor<1x257x5120xbf16> loc(#loc1192)
    %624 = stablehlo.composite "tenstorrent.gelu" %623 {decomposition = @tenstorrent.gelu.impl_10} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1193)
    %625 = stablehlo.reshape %624 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1194)
    %626 = stablehlo.reshape %arg326 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1195)
    %627 = stablehlo.custom_call @tt.mark_argument(%626) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1196)
    %628 = stablehlo.reshape %627 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc1197)
    %629 = stablehlo.transpose %628, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1198)
    %630 = stablehlo.dot_general %625, %629, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1199)
    %631 = stablehlo.reshape %630 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1200)
    %632 = stablehlo.reshape %arg325 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1201)
    %633 = stablehlo.custom_call @tt.mark_argument(%632) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1202)
    %634 = stablehlo.reshape %633 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1203)
    %635 = stablehlo.broadcast_in_dim %634, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1204)
    %636 = stablehlo.add %631, %635 : tensor<1x257x1280xbf16> loc(#loc1205)
    %637 = stablehlo.add %604, %636 : tensor<1x257x1280xbf16> loc(#loc1206)
    %638 = stablehlo.reshape %arg324 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1207)
    %639 = stablehlo.custom_call @tt.mark_argument(%638) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1208)
    %640 = stablehlo.reshape %639 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1209)
    %641 = stablehlo.reshape %arg323 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1210)
    %642 = stablehlo.custom_call @tt.mark_argument(%641) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1211)
    %643 = stablehlo.reshape %642 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1212)
    %644 = stablehlo.composite "tenstorrent.layer_norm" %637, %640, %643 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_31} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1213)
    %645 = stablehlo.reshape %644 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1214)
    %646 = stablehlo.reshape %arg415 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1215)
    %647 = stablehlo.custom_call @tt.mark_argument(%646) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1216)
    %648 = stablehlo.reshape %647 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1217)
    %649 = stablehlo.transpose %648, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1218)
    %650 = stablehlo.dot_general %645, %649, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1219)
    %651 = stablehlo.reshape %650 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1220)
    %652 = stablehlo.reshape %arg414 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1221)
    %653 = stablehlo.custom_call @tt.mark_argument(%652) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1222)
    %654 = stablehlo.reshape %653 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1223)
    %655 = stablehlo.broadcast_in_dim %654, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1224)
    %656 = stablehlo.add %651, %655 : tensor<1x257x1280xbf16> loc(#loc1225)
    %657 = stablehlo.reshape %656 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1226)
    %658 = stablehlo.transpose %657, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1227)
    %659 = stablehlo.convert %658 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1228)
    %660 = stablehlo.multiply %659, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1229)
    %661 = stablehlo.reshape %arg413 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1230)
    %662 = stablehlo.custom_call @tt.mark_argument(%661) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1231)
    %663 = stablehlo.reshape %662 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1232)
    %664 = stablehlo.transpose %663, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1233)
    %665 = stablehlo.dot_general %645, %664, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1234)
    %666 = stablehlo.reshape %665 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1235)
    %667 = stablehlo.reshape %arg412 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1236)
    %668 = stablehlo.custom_call @tt.mark_argument(%667) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1237)
    %669 = stablehlo.reshape %668 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1238)
    %670 = stablehlo.broadcast_in_dim %669, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1239)
    %671 = stablehlo.add %666, %670 : tensor<1x257x1280xbf16> loc(#loc1240)
    %672 = stablehlo.reshape %671 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1241)
    %673 = stablehlo.transpose %672, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1242)
    %674 = stablehlo.convert %673 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1243)
    %675 = stablehlo.transpose %674, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1244)
    %676 = stablehlo.multiply %675, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1245)
    %677 = stablehlo.dot_general %660, %676, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1246)
    %678 = stablehlo.convert %677 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1247)
    %679 = stablehlo.compare  EQ, %678, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1248)
    %680 = stablehlo.not %679 : tensor<1x16x257x257xi1> loc(#loc1249)
    %681 = stablehlo.reduce(%680 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.4116"), %arg559: tensor<i1> loc("reduce.4116"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1251)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc1252)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1250)
    %682 = stablehlo.reshape %681 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1253)
    %683 = stablehlo.not %682 : tensor<1x16x257x1xi1> loc(#loc1254)
    %684 = stablehlo.reshape %683 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1255)
    %685 = stablehlo.broadcast_in_dim %684, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1256)
    %686 = stablehlo.reduce(%677 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1257)
    %687 = stablehlo.broadcast_in_dim %686, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1258)
    %688 = stablehlo.subtract %677, %687 : tensor<1x16x257x257xf32> loc(#loc1259)
    %689 = stablehlo.exponential %688 : tensor<1x16x257x257xf32> loc(#loc1260)
    %690 = stablehlo.reduce(%689 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1261)
    %691 = stablehlo.broadcast_in_dim %690, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1262)
    %692 = stablehlo.divide %689, %691 : tensor<1x16x257x257xf32> loc(#loc1263)
    %693 = stablehlo.select %685, %cst_3, %692 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1264)
    %694 = stablehlo.reshape %arg322 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1265)
    %695 = stablehlo.custom_call @tt.mark_argument(%694) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1266)
    %696 = stablehlo.reshape %695 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1267)
    %697 = stablehlo.transpose %696, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1268)
    %698 = stablehlo.dot_general %645, %697, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1269)
    %699 = stablehlo.reshape %698 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1270)
    %700 = stablehlo.reshape %arg321 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1271)
    %701 = stablehlo.custom_call @tt.mark_argument(%700) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1272)
    %702 = stablehlo.reshape %701 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1273)
    %703 = stablehlo.broadcast_in_dim %702, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1274)
    %704 = stablehlo.add %699, %703 : tensor<1x257x1280xbf16> loc(#loc1275)
    %705 = stablehlo.reshape %704 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1276)
    %706 = stablehlo.transpose %705, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1277)
    %707 = stablehlo.convert %706 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1278)
    %708 = stablehlo.dot_general %693, %707, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1279)
    %709 = stablehlo.convert %708 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1280)
    %710 = stablehlo.transpose %709, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1281)
    %711 = stablehlo.reshape %710 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1282)
    %712 = stablehlo.reshape %arg320 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1283)
    %713 = stablehlo.custom_call @tt.mark_argument(%712) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1284)
    %714 = stablehlo.reshape %713 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1285)
    %715 = stablehlo.transpose %714, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1286)
    %716 = stablehlo.dot_general %711, %715, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1287)
    %717 = stablehlo.reshape %716 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1288)
    %718 = stablehlo.reshape %arg319 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1289)
    %719 = stablehlo.custom_call @tt.mark_argument(%718) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1290)
    %720 = stablehlo.reshape %719 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1291)
    %721 = stablehlo.broadcast_in_dim %720, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1292)
    %722 = stablehlo.add %717, %721 : tensor<1x257x1280xbf16> loc(#loc1293)
    %723 = stablehlo.add %637, %722 : tensor<1x257x1280xbf16> loc(#loc1294)
    %724 = stablehlo.reshape %arg318 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1295)
    %725 = stablehlo.custom_call @tt.mark_argument(%724) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1296)
    %726 = stablehlo.reshape %725 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1297)
    %727 = stablehlo.reshape %arg317 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1298)
    %728 = stablehlo.custom_call @tt.mark_argument(%727) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1299)
    %729 = stablehlo.reshape %728 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1300)
    %730 = stablehlo.composite "tenstorrent.layer_norm" %723, %726, %729 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_42} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1301)
    %731 = stablehlo.reshape %730 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1302)
    %732 = stablehlo.reshape %arg316 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1303)
    %733 = stablehlo.custom_call @tt.mark_argument(%732) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1304)
    %734 = stablehlo.reshape %733 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc1305)
    %735 = stablehlo.transpose %734, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1306)
    %736 = stablehlo.dot_general %731, %735, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1307)
    %737 = stablehlo.reshape %736 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1308)
    %738 = stablehlo.reshape %arg315 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1309)
    %739 = stablehlo.custom_call @tt.mark_argument(%738) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1310)
    %740 = stablehlo.reshape %739 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1311)
    %741 = stablehlo.broadcast_in_dim %740, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1312)
    %742 = stablehlo.add %737, %741 : tensor<1x257x5120xbf16> loc(#loc1313)
    %743 = stablehlo.composite "tenstorrent.gelu" %742 {decomposition = @tenstorrent.gelu.impl_23} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1314)
    %744 = stablehlo.reshape %743 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1315)
    %745 = stablehlo.reshape %arg314 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1316)
    %746 = stablehlo.custom_call @tt.mark_argument(%745) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1317)
    %747 = stablehlo.reshape %746 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc1318)
    %748 = stablehlo.transpose %747, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1319)
    %749 = stablehlo.dot_general %744, %748, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1320)
    %750 = stablehlo.reshape %749 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1321)
    %751 = stablehlo.reshape %arg313 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1322)
    %752 = stablehlo.custom_call @tt.mark_argument(%751) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1323)
    %753 = stablehlo.reshape %752 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1324)
    %754 = stablehlo.broadcast_in_dim %753, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1325)
    %755 = stablehlo.add %750, %754 : tensor<1x257x1280xbf16> loc(#loc1326)
    %756 = stablehlo.add %723, %755 : tensor<1x257x1280xbf16> loc(#loc1327)
    %757 = stablehlo.reshape %arg312 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1328)
    %758 = stablehlo.custom_call @tt.mark_argument(%757) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1329)
    %759 = stablehlo.reshape %758 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1330)
    %760 = stablehlo.reshape %arg311 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1331)
    %761 = stablehlo.custom_call @tt.mark_argument(%760) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1332)
    %762 = stablehlo.reshape %761 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1333)
    %763 = stablehlo.composite "tenstorrent.layer_norm" %756, %759, %762 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_29} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1334)
    %764 = stablehlo.reshape %763 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1335)
    %765 = stablehlo.reshape %arg419 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1336)
    %766 = stablehlo.custom_call @tt.mark_argument(%765) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1337)
    %767 = stablehlo.reshape %766 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1338)
    %768 = stablehlo.transpose %767, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1339)
    %769 = stablehlo.dot_general %764, %768, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1340)
    %770 = stablehlo.reshape %769 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1341)
    %771 = stablehlo.reshape %arg418 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1342)
    %772 = stablehlo.custom_call @tt.mark_argument(%771) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1343)
    %773 = stablehlo.reshape %772 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1344)
    %774 = stablehlo.broadcast_in_dim %773, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1345)
    %775 = stablehlo.add %770, %774 : tensor<1x257x1280xbf16> loc(#loc1346)
    %776 = stablehlo.reshape %775 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1347)
    %777 = stablehlo.transpose %776, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1348)
    %778 = stablehlo.convert %777 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1349)
    %779 = stablehlo.multiply %778, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1350)
    %780 = stablehlo.reshape %arg417 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1351)
    %781 = stablehlo.custom_call @tt.mark_argument(%780) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1352)
    %782 = stablehlo.reshape %781 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1353)
    %783 = stablehlo.transpose %782, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1354)
    %784 = stablehlo.dot_general %764, %783, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1355)
    %785 = stablehlo.reshape %784 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1356)
    %786 = stablehlo.reshape %arg416 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1357)
    %787 = stablehlo.custom_call @tt.mark_argument(%786) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1358)
    %788 = stablehlo.reshape %787 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1359)
    %789 = stablehlo.broadcast_in_dim %788, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1360)
    %790 = stablehlo.add %785, %789 : tensor<1x257x1280xbf16> loc(#loc1361)
    %791 = stablehlo.reshape %790 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1362)
    %792 = stablehlo.transpose %791, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1363)
    %793 = stablehlo.convert %792 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1364)
    %794 = stablehlo.transpose %793, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1365)
    %795 = stablehlo.multiply %794, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1366)
    %796 = stablehlo.dot_general %779, %795, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1367)
    %797 = stablehlo.convert %796 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1368)
    %798 = stablehlo.compare  EQ, %797, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1369)
    %799 = stablehlo.not %798 : tensor<1x16x257x257xi1> loc(#loc1370)
    %800 = stablehlo.reduce(%799 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.4426"), %arg559: tensor<i1> loc("reduce.4426"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1372)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc1373)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1371)
    %801 = stablehlo.reshape %800 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1374)
    %802 = stablehlo.not %801 : tensor<1x16x257x1xi1> loc(#loc1375)
    %803 = stablehlo.reshape %802 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1376)
    %804 = stablehlo.broadcast_in_dim %803, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1377)
    %805 = stablehlo.reduce(%796 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1378)
    %806 = stablehlo.broadcast_in_dim %805, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1379)
    %807 = stablehlo.subtract %796, %806 : tensor<1x16x257x257xf32> loc(#loc1380)
    %808 = stablehlo.exponential %807 : tensor<1x16x257x257xf32> loc(#loc1381)
    %809 = stablehlo.reduce(%808 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1382)
    %810 = stablehlo.broadcast_in_dim %809, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1383)
    %811 = stablehlo.divide %808, %810 : tensor<1x16x257x257xf32> loc(#loc1384)
    %812 = stablehlo.select %804, %cst_3, %811 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1385)
    %813 = stablehlo.reshape %arg310 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1386)
    %814 = stablehlo.custom_call @tt.mark_argument(%813) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1387)
    %815 = stablehlo.reshape %814 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1388)
    %816 = stablehlo.transpose %815, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1389)
    %817 = stablehlo.dot_general %764, %816, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1390)
    %818 = stablehlo.reshape %817 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1391)
    %819 = stablehlo.reshape %arg309 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1392)
    %820 = stablehlo.custom_call @tt.mark_argument(%819) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1393)
    %821 = stablehlo.reshape %820 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1394)
    %822 = stablehlo.broadcast_in_dim %821, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1395)
    %823 = stablehlo.add %818, %822 : tensor<1x257x1280xbf16> loc(#loc1396)
    %824 = stablehlo.reshape %823 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1397)
    %825 = stablehlo.transpose %824, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1398)
    %826 = stablehlo.convert %825 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1399)
    %827 = stablehlo.dot_general %812, %826, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1400)
    %828 = stablehlo.convert %827 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1401)
    %829 = stablehlo.transpose %828, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1402)
    %830 = stablehlo.reshape %829 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1403)
    %831 = stablehlo.reshape %arg308 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1404)
    %832 = stablehlo.custom_call @tt.mark_argument(%831) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1405)
    %833 = stablehlo.reshape %832 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1406)
    %834 = stablehlo.transpose %833, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1407)
    %835 = stablehlo.dot_general %830, %834, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1408)
    %836 = stablehlo.reshape %835 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1409)
    %837 = stablehlo.reshape %arg307 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1410)
    %838 = stablehlo.custom_call @tt.mark_argument(%837) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1411)
    %839 = stablehlo.reshape %838 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1412)
    %840 = stablehlo.broadcast_in_dim %839, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1413)
    %841 = stablehlo.add %836, %840 : tensor<1x257x1280xbf16> loc(#loc1414)
    %842 = stablehlo.add %756, %841 : tensor<1x257x1280xbf16> loc(#loc1415)
    %843 = stablehlo.reshape %arg306 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1416)
    %844 = stablehlo.custom_call @tt.mark_argument(%843) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1417)
    %845 = stablehlo.reshape %844 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1418)
    %846 = stablehlo.reshape %arg305 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1419)
    %847 = stablehlo.custom_call @tt.mark_argument(%846) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1420)
    %848 = stablehlo.reshape %847 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1421)
    %849 = stablehlo.composite "tenstorrent.layer_norm" %842, %845, %848 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_36} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1422)
    %850 = stablehlo.reshape %849 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1423)
    %851 = stablehlo.reshape %arg304 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1424)
    %852 = stablehlo.custom_call @tt.mark_argument(%851) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1425)
    %853 = stablehlo.reshape %852 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc1426)
    %854 = stablehlo.transpose %853, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1427)
    %855 = stablehlo.dot_general %850, %854, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1428)
    %856 = stablehlo.reshape %855 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1429)
    %857 = stablehlo.reshape %arg303 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1430)
    %858 = stablehlo.custom_call @tt.mark_argument(%857) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1431)
    %859 = stablehlo.reshape %858 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1432)
    %860 = stablehlo.broadcast_in_dim %859, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1433)
    %861 = stablehlo.add %856, %860 : tensor<1x257x5120xbf16> loc(#loc1434)
    %862 = stablehlo.composite "tenstorrent.gelu" %861 {decomposition = @tenstorrent.gelu.impl_8} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1435)
    %863 = stablehlo.reshape %862 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1436)
    %864 = stablehlo.reshape %arg302 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1437)
    %865 = stablehlo.custom_call @tt.mark_argument(%864) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1438)
    %866 = stablehlo.reshape %865 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc1439)
    %867 = stablehlo.transpose %866, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1440)
    %868 = stablehlo.dot_general %863, %867, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1441)
    %869 = stablehlo.reshape %868 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1442)
    %870 = stablehlo.reshape %arg301 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1443)
    %871 = stablehlo.custom_call @tt.mark_argument(%870) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1444)
    %872 = stablehlo.reshape %871 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1445)
    %873 = stablehlo.broadcast_in_dim %872, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1446)
    %874 = stablehlo.add %869, %873 : tensor<1x257x1280xbf16> loc(#loc1447)
    %875 = stablehlo.add %842, %874 : tensor<1x257x1280xbf16> loc(#loc1448)
    %876 = stablehlo.reshape %arg300 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1449)
    %877 = stablehlo.custom_call @tt.mark_argument(%876) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1450)
    %878 = stablehlo.reshape %877 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1451)
    %879 = stablehlo.reshape %arg299 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1452)
    %880 = stablehlo.custom_call @tt.mark_argument(%879) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1453)
    %881 = stablehlo.reshape %880 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1454)
    %882 = stablehlo.composite "tenstorrent.layer_norm" %875, %878, %881 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_72} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1455)
    %883 = stablehlo.reshape %882 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1456)
    %884 = stablehlo.reshape %arg423 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1457)
    %885 = stablehlo.custom_call @tt.mark_argument(%884) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1458)
    %886 = stablehlo.reshape %885 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1459)
    %887 = stablehlo.transpose %886, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1460)
    %888 = stablehlo.dot_general %883, %887, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1461)
    %889 = stablehlo.reshape %888 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1462)
    %890 = stablehlo.reshape %arg422 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1463)
    %891 = stablehlo.custom_call @tt.mark_argument(%890) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1464)
    %892 = stablehlo.reshape %891 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1465)
    %893 = stablehlo.broadcast_in_dim %892, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1466)
    %894 = stablehlo.add %889, %893 : tensor<1x257x1280xbf16> loc(#loc1467)
    %895 = stablehlo.reshape %894 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1468)
    %896 = stablehlo.transpose %895, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1469)
    %897 = stablehlo.convert %896 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1470)
    %898 = stablehlo.multiply %897, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1471)
    %899 = stablehlo.reshape %arg421 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1472)
    %900 = stablehlo.custom_call @tt.mark_argument(%899) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1473)
    %901 = stablehlo.reshape %900 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1474)
    %902 = stablehlo.transpose %901, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1475)
    %903 = stablehlo.dot_general %883, %902, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1476)
    %904 = stablehlo.reshape %903 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1477)
    %905 = stablehlo.reshape %arg420 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1478)
    %906 = stablehlo.custom_call @tt.mark_argument(%905) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1479)
    %907 = stablehlo.reshape %906 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1480)
    %908 = stablehlo.broadcast_in_dim %907, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1481)
    %909 = stablehlo.add %904, %908 : tensor<1x257x1280xbf16> loc(#loc1482)
    %910 = stablehlo.reshape %909 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1483)
    %911 = stablehlo.transpose %910, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1484)
    %912 = stablehlo.convert %911 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1485)
    %913 = stablehlo.transpose %912, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1486)
    %914 = stablehlo.multiply %913, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1487)
    %915 = stablehlo.dot_general %898, %914, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1488)
    %916 = stablehlo.convert %915 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1489)
    %917 = stablehlo.compare  EQ, %916, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1490)
    %918 = stablehlo.not %917 : tensor<1x16x257x257xi1> loc(#loc1491)
    %919 = stablehlo.reduce(%918 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.4736"), %arg559: tensor<i1> loc("reduce.4736"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1493)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc1494)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1492)
    %920 = stablehlo.reshape %919 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1495)
    %921 = stablehlo.not %920 : tensor<1x16x257x1xi1> loc(#loc1496)
    %922 = stablehlo.reshape %921 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1497)
    %923 = stablehlo.broadcast_in_dim %922, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1498)
    %924 = stablehlo.reduce(%915 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1499)
    %925 = stablehlo.broadcast_in_dim %924, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1500)
    %926 = stablehlo.subtract %915, %925 : tensor<1x16x257x257xf32> loc(#loc1501)
    %927 = stablehlo.exponential %926 : tensor<1x16x257x257xf32> loc(#loc1502)
    %928 = stablehlo.reduce(%927 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1503)
    %929 = stablehlo.broadcast_in_dim %928, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1504)
    %930 = stablehlo.divide %927, %929 : tensor<1x16x257x257xf32> loc(#loc1505)
    %931 = stablehlo.select %923, %cst_3, %930 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1506)
    %932 = stablehlo.reshape %arg298 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1507)
    %933 = stablehlo.custom_call @tt.mark_argument(%932) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1508)
    %934 = stablehlo.reshape %933 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1509)
    %935 = stablehlo.transpose %934, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1510)
    %936 = stablehlo.dot_general %883, %935, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1511)
    %937 = stablehlo.reshape %936 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1512)
    %938 = stablehlo.reshape %arg297 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1513)
    %939 = stablehlo.custom_call @tt.mark_argument(%938) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1514)
    %940 = stablehlo.reshape %939 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1515)
    %941 = stablehlo.broadcast_in_dim %940, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1516)
    %942 = stablehlo.add %937, %941 : tensor<1x257x1280xbf16> loc(#loc1517)
    %943 = stablehlo.reshape %942 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1518)
    %944 = stablehlo.transpose %943, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1519)
    %945 = stablehlo.convert %944 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1520)
    %946 = stablehlo.dot_general %931, %945, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1521)
    %947 = stablehlo.convert %946 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1522)
    %948 = stablehlo.transpose %947, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1523)
    %949 = stablehlo.reshape %948 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1524)
    %950 = stablehlo.reshape %arg296 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1525)
    %951 = stablehlo.custom_call @tt.mark_argument(%950) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1526)
    %952 = stablehlo.reshape %951 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1527)
    %953 = stablehlo.transpose %952, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1528)
    %954 = stablehlo.dot_general %949, %953, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1529)
    %955 = stablehlo.reshape %954 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1530)
    %956 = stablehlo.reshape %arg295 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1531)
    %957 = stablehlo.custom_call @tt.mark_argument(%956) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1532)
    %958 = stablehlo.reshape %957 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1533)
    %959 = stablehlo.broadcast_in_dim %958, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1534)
    %960 = stablehlo.add %955, %959 : tensor<1x257x1280xbf16> loc(#loc1535)
    %961 = stablehlo.add %875, %960 : tensor<1x257x1280xbf16> loc(#loc1536)
    %962 = stablehlo.reshape %arg294 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1537)
    %963 = stablehlo.custom_call @tt.mark_argument(%962) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1538)
    %964 = stablehlo.reshape %963 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1539)
    %965 = stablehlo.reshape %arg293 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1540)
    %966 = stablehlo.custom_call @tt.mark_argument(%965) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1541)
    %967 = stablehlo.reshape %966 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1542)
    %968 = stablehlo.composite "tenstorrent.layer_norm" %961, %964, %967 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_22} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1543)
    %969 = stablehlo.reshape %968 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1544)
    %970 = stablehlo.reshape %arg292 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1545)
    %971 = stablehlo.custom_call @tt.mark_argument(%970) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1546)
    %972 = stablehlo.reshape %971 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc1547)
    %973 = stablehlo.transpose %972, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1548)
    %974 = stablehlo.dot_general %969, %973, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1549)
    %975 = stablehlo.reshape %974 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1550)
    %976 = stablehlo.reshape %arg291 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1551)
    %977 = stablehlo.custom_call @tt.mark_argument(%976) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1552)
    %978 = stablehlo.reshape %977 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1553)
    %979 = stablehlo.broadcast_in_dim %978, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1554)
    %980 = stablehlo.add %975, %979 : tensor<1x257x5120xbf16> loc(#loc1555)
    %981 = stablehlo.composite "tenstorrent.gelu" %980 {decomposition = @tenstorrent.gelu.impl_18} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1556)
    %982 = stablehlo.reshape %981 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1557)
    %983 = stablehlo.reshape %arg290 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1558)
    %984 = stablehlo.custom_call @tt.mark_argument(%983) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1559)
    %985 = stablehlo.reshape %984 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc1560)
    %986 = stablehlo.transpose %985, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1561)
    %987 = stablehlo.dot_general %982, %986, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1562)
    %988 = stablehlo.reshape %987 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1563)
    %989 = stablehlo.reshape %arg289 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1564)
    %990 = stablehlo.custom_call @tt.mark_argument(%989) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1565)
    %991 = stablehlo.reshape %990 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1566)
    %992 = stablehlo.broadcast_in_dim %991, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1567)
    %993 = stablehlo.add %988, %992 : tensor<1x257x1280xbf16> loc(#loc1568)
    %994 = stablehlo.add %961, %993 : tensor<1x257x1280xbf16> loc(#loc1569)
    %995 = stablehlo.reshape %arg288 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1570)
    %996 = stablehlo.custom_call @tt.mark_argument(%995) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1571)
    %997 = stablehlo.reshape %996 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1572)
    %998 = stablehlo.reshape %arg287 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1573)
    %999 = stablehlo.custom_call @tt.mark_argument(%998) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1574)
    %1000 = stablehlo.reshape %999 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1575)
    %1001 = stablehlo.composite "tenstorrent.layer_norm" %994, %997, %1000 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_40} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1576)
    %1002 = stablehlo.reshape %1001 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1577)
    %1003 = stablehlo.reshape %arg427 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1578)
    %1004 = stablehlo.custom_call @tt.mark_argument(%1003) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1579)
    %1005 = stablehlo.reshape %1004 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1580)
    %1006 = stablehlo.transpose %1005, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1581)
    %1007 = stablehlo.dot_general %1002, %1006, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1582)
    %1008 = stablehlo.reshape %1007 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1583)
    %1009 = stablehlo.reshape %arg426 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1584)
    %1010 = stablehlo.custom_call @tt.mark_argument(%1009) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1585)
    %1011 = stablehlo.reshape %1010 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1586)
    %1012 = stablehlo.broadcast_in_dim %1011, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1587)
    %1013 = stablehlo.add %1008, %1012 : tensor<1x257x1280xbf16> loc(#loc1588)
    %1014 = stablehlo.reshape %1013 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1589)
    %1015 = stablehlo.transpose %1014, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1590)
    %1016 = stablehlo.convert %1015 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1591)
    %1017 = stablehlo.multiply %1016, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1592)
    %1018 = stablehlo.reshape %arg425 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1593)
    %1019 = stablehlo.custom_call @tt.mark_argument(%1018) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1594)
    %1020 = stablehlo.reshape %1019 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1595)
    %1021 = stablehlo.transpose %1020, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1596)
    %1022 = stablehlo.dot_general %1002, %1021, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1597)
    %1023 = stablehlo.reshape %1022 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1598)
    %1024 = stablehlo.reshape %arg424 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1599)
    %1025 = stablehlo.custom_call @tt.mark_argument(%1024) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1600)
    %1026 = stablehlo.reshape %1025 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1601)
    %1027 = stablehlo.broadcast_in_dim %1026, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1602)
    %1028 = stablehlo.add %1023, %1027 : tensor<1x257x1280xbf16> loc(#loc1603)
    %1029 = stablehlo.reshape %1028 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1604)
    %1030 = stablehlo.transpose %1029, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1605)
    %1031 = stablehlo.convert %1030 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1606)
    %1032 = stablehlo.transpose %1031, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1607)
    %1033 = stablehlo.multiply %1032, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1608)
    %1034 = stablehlo.dot_general %1017, %1033, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1609)
    %1035 = stablehlo.convert %1034 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1610)
    %1036 = stablehlo.compare  EQ, %1035, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1611)
    %1037 = stablehlo.not %1036 : tensor<1x16x257x257xi1> loc(#loc1612)
    %1038 = stablehlo.reduce(%1037 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.5046"), %arg559: tensor<i1> loc("reduce.5046"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1614)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc1615)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1613)
    %1039 = stablehlo.reshape %1038 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1616)
    %1040 = stablehlo.not %1039 : tensor<1x16x257x1xi1> loc(#loc1617)
    %1041 = stablehlo.reshape %1040 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1618)
    %1042 = stablehlo.broadcast_in_dim %1041, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1619)
    %1043 = stablehlo.reduce(%1034 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1620)
    %1044 = stablehlo.broadcast_in_dim %1043, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1621)
    %1045 = stablehlo.subtract %1034, %1044 : tensor<1x16x257x257xf32> loc(#loc1622)
    %1046 = stablehlo.exponential %1045 : tensor<1x16x257x257xf32> loc(#loc1623)
    %1047 = stablehlo.reduce(%1046 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1624)
    %1048 = stablehlo.broadcast_in_dim %1047, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1625)
    %1049 = stablehlo.divide %1046, %1048 : tensor<1x16x257x257xf32> loc(#loc1626)
    %1050 = stablehlo.select %1042, %cst_3, %1049 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1627)
    %1051 = stablehlo.reshape %arg286 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1628)
    %1052 = stablehlo.custom_call @tt.mark_argument(%1051) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1629)
    %1053 = stablehlo.reshape %1052 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1630)
    %1054 = stablehlo.transpose %1053, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1631)
    %1055 = stablehlo.dot_general %1002, %1054, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1632)
    %1056 = stablehlo.reshape %1055 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1633)
    %1057 = stablehlo.reshape %arg285 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1634)
    %1058 = stablehlo.custom_call @tt.mark_argument(%1057) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1635)
    %1059 = stablehlo.reshape %1058 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1636)
    %1060 = stablehlo.broadcast_in_dim %1059, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1637)
    %1061 = stablehlo.add %1056, %1060 : tensor<1x257x1280xbf16> loc(#loc1638)
    %1062 = stablehlo.reshape %1061 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1639)
    %1063 = stablehlo.transpose %1062, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1640)
    %1064 = stablehlo.convert %1063 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1641)
    %1065 = stablehlo.dot_general %1050, %1064, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1642)
    %1066 = stablehlo.convert %1065 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1643)
    %1067 = stablehlo.transpose %1066, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1644)
    %1068 = stablehlo.reshape %1067 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1645)
    %1069 = stablehlo.reshape %arg284 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1646)
    %1070 = stablehlo.custom_call @tt.mark_argument(%1069) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1647)
    %1071 = stablehlo.reshape %1070 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1648)
    %1072 = stablehlo.transpose %1071, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1649)
    %1073 = stablehlo.dot_general %1068, %1072, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1650)
    %1074 = stablehlo.reshape %1073 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1651)
    %1075 = stablehlo.reshape %arg283 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1652)
    %1076 = stablehlo.custom_call @tt.mark_argument(%1075) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1653)
    %1077 = stablehlo.reshape %1076 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1654)
    %1078 = stablehlo.broadcast_in_dim %1077, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1655)
    %1079 = stablehlo.add %1074, %1078 : tensor<1x257x1280xbf16> loc(#loc1656)
    %1080 = stablehlo.add %994, %1079 : tensor<1x257x1280xbf16> loc(#loc1657)
    %1081 = stablehlo.reshape %arg282 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1658)
    %1082 = stablehlo.custom_call @tt.mark_argument(%1081) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1659)
    %1083 = stablehlo.reshape %1082 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1660)
    %1084 = stablehlo.reshape %arg281 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1661)
    %1085 = stablehlo.custom_call @tt.mark_argument(%1084) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1662)
    %1086 = stablehlo.reshape %1085 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1663)
    %1087 = stablehlo.composite "tenstorrent.layer_norm" %1080, %1083, %1086 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_53} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1664)
    %1088 = stablehlo.reshape %1087 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1665)
    %1089 = stablehlo.reshape %arg280 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1666)
    %1090 = stablehlo.custom_call @tt.mark_argument(%1089) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1667)
    %1091 = stablehlo.reshape %1090 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc1668)
    %1092 = stablehlo.transpose %1091, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1669)
    %1093 = stablehlo.dot_general %1088, %1092, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1670)
    %1094 = stablehlo.reshape %1093 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1671)
    %1095 = stablehlo.reshape %arg279 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1672)
    %1096 = stablehlo.custom_call @tt.mark_argument(%1095) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1673)
    %1097 = stablehlo.reshape %1096 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1674)
    %1098 = stablehlo.broadcast_in_dim %1097, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1675)
    %1099 = stablehlo.add %1094, %1098 : tensor<1x257x5120xbf16> loc(#loc1676)
    %1100 = stablehlo.composite "tenstorrent.gelu" %1099 {decomposition = @tenstorrent.gelu.impl_13} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1677)
    %1101 = stablehlo.reshape %1100 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1678)
    %1102 = stablehlo.reshape %arg278 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1679)
    %1103 = stablehlo.custom_call @tt.mark_argument(%1102) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1680)
    %1104 = stablehlo.reshape %1103 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc1681)
    %1105 = stablehlo.transpose %1104, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1682)
    %1106 = stablehlo.dot_general %1101, %1105, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1683)
    %1107 = stablehlo.reshape %1106 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1684)
    %1108 = stablehlo.reshape %arg277 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1685)
    %1109 = stablehlo.custom_call @tt.mark_argument(%1108) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1686)
    %1110 = stablehlo.reshape %1109 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1687)
    %1111 = stablehlo.broadcast_in_dim %1110, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1688)
    %1112 = stablehlo.add %1107, %1111 : tensor<1x257x1280xbf16> loc(#loc1689)
    %1113 = stablehlo.add %1080, %1112 : tensor<1x257x1280xbf16> loc(#loc1690)
    %1114 = stablehlo.reshape %arg276 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1691)
    %1115 = stablehlo.custom_call @tt.mark_argument(%1114) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1692)
    %1116 = stablehlo.reshape %1115 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1693)
    %1117 = stablehlo.reshape %arg275 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1694)
    %1118 = stablehlo.custom_call @tt.mark_argument(%1117) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1695)
    %1119 = stablehlo.reshape %1118 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1696)
    %1120 = stablehlo.composite "tenstorrent.layer_norm" %1113, %1116, %1119 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_51} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1697)
    %1121 = stablehlo.reshape %1120 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1698)
    %1122 = stablehlo.reshape %arg431 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1699)
    %1123 = stablehlo.custom_call @tt.mark_argument(%1122) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1700)
    %1124 = stablehlo.reshape %1123 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1701)
    %1125 = stablehlo.transpose %1124, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1702)
    %1126 = stablehlo.dot_general %1121, %1125, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1703)
    %1127 = stablehlo.reshape %1126 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1704)
    %1128 = stablehlo.reshape %arg430 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1705)
    %1129 = stablehlo.custom_call @tt.mark_argument(%1128) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1706)
    %1130 = stablehlo.reshape %1129 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1707)
    %1131 = stablehlo.broadcast_in_dim %1130, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1708)
    %1132 = stablehlo.add %1127, %1131 : tensor<1x257x1280xbf16> loc(#loc1709)
    %1133 = stablehlo.reshape %1132 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1710)
    %1134 = stablehlo.transpose %1133, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1711)
    %1135 = stablehlo.convert %1134 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1712)
    %1136 = stablehlo.multiply %1135, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1713)
    %1137 = stablehlo.reshape %arg429 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1714)
    %1138 = stablehlo.custom_call @tt.mark_argument(%1137) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1715)
    %1139 = stablehlo.reshape %1138 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1716)
    %1140 = stablehlo.transpose %1139, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1717)
    %1141 = stablehlo.dot_general %1121, %1140, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1718)
    %1142 = stablehlo.reshape %1141 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1719)
    %1143 = stablehlo.reshape %arg428 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1720)
    %1144 = stablehlo.custom_call @tt.mark_argument(%1143) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1721)
    %1145 = stablehlo.reshape %1144 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1722)
    %1146 = stablehlo.broadcast_in_dim %1145, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1723)
    %1147 = stablehlo.add %1142, %1146 : tensor<1x257x1280xbf16> loc(#loc1724)
    %1148 = stablehlo.reshape %1147 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1725)
    %1149 = stablehlo.transpose %1148, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1726)
    %1150 = stablehlo.convert %1149 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1727)
    %1151 = stablehlo.transpose %1150, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1728)
    %1152 = stablehlo.multiply %1151, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1729)
    %1153 = stablehlo.dot_general %1136, %1152, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1730)
    %1154 = stablehlo.convert %1153 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1731)
    %1155 = stablehlo.compare  EQ, %1154, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1732)
    %1156 = stablehlo.not %1155 : tensor<1x16x257x257xi1> loc(#loc1733)
    %1157 = stablehlo.reduce(%1156 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.5356"), %arg559: tensor<i1> loc("reduce.5356"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1735)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc1736)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1734)
    %1158 = stablehlo.reshape %1157 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1737)
    %1159 = stablehlo.not %1158 : tensor<1x16x257x1xi1> loc(#loc1738)
    %1160 = stablehlo.reshape %1159 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1739)
    %1161 = stablehlo.broadcast_in_dim %1160, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1740)
    %1162 = stablehlo.reduce(%1153 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1741)
    %1163 = stablehlo.broadcast_in_dim %1162, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1742)
    %1164 = stablehlo.subtract %1153, %1163 : tensor<1x16x257x257xf32> loc(#loc1743)
    %1165 = stablehlo.exponential %1164 : tensor<1x16x257x257xf32> loc(#loc1744)
    %1166 = stablehlo.reduce(%1165 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1745)
    %1167 = stablehlo.broadcast_in_dim %1166, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1746)
    %1168 = stablehlo.divide %1165, %1167 : tensor<1x16x257x257xf32> loc(#loc1747)
    %1169 = stablehlo.select %1161, %cst_3, %1168 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1748)
    %1170 = stablehlo.reshape %arg274 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1749)
    %1171 = stablehlo.custom_call @tt.mark_argument(%1170) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1750)
    %1172 = stablehlo.reshape %1171 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1751)
    %1173 = stablehlo.transpose %1172, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1752)
    %1174 = stablehlo.dot_general %1121, %1173, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1753)
    %1175 = stablehlo.reshape %1174 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1754)
    %1176 = stablehlo.reshape %arg273 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1755)
    %1177 = stablehlo.custom_call @tt.mark_argument(%1176) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1756)
    %1178 = stablehlo.reshape %1177 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1757)
    %1179 = stablehlo.broadcast_in_dim %1178, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1758)
    %1180 = stablehlo.add %1175, %1179 : tensor<1x257x1280xbf16> loc(#loc1759)
    %1181 = stablehlo.reshape %1180 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1760)
    %1182 = stablehlo.transpose %1181, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1761)
    %1183 = stablehlo.convert %1182 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1762)
    %1184 = stablehlo.dot_general %1169, %1183, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1763)
    %1185 = stablehlo.convert %1184 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1764)
    %1186 = stablehlo.transpose %1185, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1765)
    %1187 = stablehlo.reshape %1186 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1766)
    %1188 = stablehlo.reshape %arg272 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1767)
    %1189 = stablehlo.custom_call @tt.mark_argument(%1188) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1768)
    %1190 = stablehlo.reshape %1189 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1769)
    %1191 = stablehlo.transpose %1190, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1770)
    %1192 = stablehlo.dot_general %1187, %1191, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1771)
    %1193 = stablehlo.reshape %1192 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1772)
    %1194 = stablehlo.reshape %arg271 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1773)
    %1195 = stablehlo.custom_call @tt.mark_argument(%1194) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1774)
    %1196 = stablehlo.reshape %1195 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1775)
    %1197 = stablehlo.broadcast_in_dim %1196, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1776)
    %1198 = stablehlo.add %1193, %1197 : tensor<1x257x1280xbf16> loc(#loc1777)
    %1199 = stablehlo.add %1113, %1198 : tensor<1x257x1280xbf16> loc(#loc1778)
    %1200 = stablehlo.reshape %arg270 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1779)
    %1201 = stablehlo.custom_call @tt.mark_argument(%1200) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1780)
    %1202 = stablehlo.reshape %1201 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1781)
    %1203 = stablehlo.reshape %arg269 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1782)
    %1204 = stablehlo.custom_call @tt.mark_argument(%1203) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1783)
    %1205 = stablehlo.reshape %1204 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1784)
    %1206 = stablehlo.composite "tenstorrent.layer_norm" %1199, %1202, %1205 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_52} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1785)
    %1207 = stablehlo.reshape %1206 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1786)
    %1208 = stablehlo.reshape %arg268 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1787)
    %1209 = stablehlo.custom_call @tt.mark_argument(%1208) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1788)
    %1210 = stablehlo.reshape %1209 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc1789)
    %1211 = stablehlo.transpose %1210, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1790)
    %1212 = stablehlo.dot_general %1207, %1211, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1791)
    %1213 = stablehlo.reshape %1212 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1792)
    %1214 = stablehlo.reshape %arg267 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1793)
    %1215 = stablehlo.custom_call @tt.mark_argument(%1214) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1794)
    %1216 = stablehlo.reshape %1215 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1795)
    %1217 = stablehlo.broadcast_in_dim %1216, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1796)
    %1218 = stablehlo.add %1213, %1217 : tensor<1x257x5120xbf16> loc(#loc1797)
    %1219 = stablehlo.composite "tenstorrent.gelu" %1218 {decomposition = @tenstorrent.gelu.impl_22} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1798)
    %1220 = stablehlo.reshape %1219 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1799)
    %1221 = stablehlo.reshape %arg266 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1800)
    %1222 = stablehlo.custom_call @tt.mark_argument(%1221) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1801)
    %1223 = stablehlo.reshape %1222 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc1802)
    %1224 = stablehlo.transpose %1223, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1803)
    %1225 = stablehlo.dot_general %1220, %1224, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1804)
    %1226 = stablehlo.reshape %1225 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1805)
    %1227 = stablehlo.reshape %arg265 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1806)
    %1228 = stablehlo.custom_call @tt.mark_argument(%1227) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1807)
    %1229 = stablehlo.reshape %1228 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1808)
    %1230 = stablehlo.broadcast_in_dim %1229, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1809)
    %1231 = stablehlo.add %1226, %1230 : tensor<1x257x1280xbf16> loc(#loc1810)
    %1232 = stablehlo.add %1199, %1231 : tensor<1x257x1280xbf16> loc(#loc1811)
    %1233 = stablehlo.reshape %arg264 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1812)
    %1234 = stablehlo.custom_call @tt.mark_argument(%1233) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1813)
    %1235 = stablehlo.reshape %1234 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1814)
    %1236 = stablehlo.reshape %arg263 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1815)
    %1237 = stablehlo.custom_call @tt.mark_argument(%1236) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1816)
    %1238 = stablehlo.reshape %1237 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1817)
    %1239 = stablehlo.composite "tenstorrent.layer_norm" %1232, %1235, %1238 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_33} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1818)
    %1240 = stablehlo.reshape %1239 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1819)
    %1241 = stablehlo.reshape %arg435 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1820)
    %1242 = stablehlo.custom_call @tt.mark_argument(%1241) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1821)
    %1243 = stablehlo.reshape %1242 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1822)
    %1244 = stablehlo.transpose %1243, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1823)
    %1245 = stablehlo.dot_general %1240, %1244, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1824)
    %1246 = stablehlo.reshape %1245 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1825)
    %1247 = stablehlo.reshape %arg434 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1826)
    %1248 = stablehlo.custom_call @tt.mark_argument(%1247) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1827)
    %1249 = stablehlo.reshape %1248 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1828)
    %1250 = stablehlo.broadcast_in_dim %1249, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1829)
    %1251 = stablehlo.add %1246, %1250 : tensor<1x257x1280xbf16> loc(#loc1830)
    %1252 = stablehlo.reshape %1251 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1831)
    %1253 = stablehlo.transpose %1252, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1832)
    %1254 = stablehlo.convert %1253 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1833)
    %1255 = stablehlo.multiply %1254, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1834)
    %1256 = stablehlo.reshape %arg433 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1835)
    %1257 = stablehlo.custom_call @tt.mark_argument(%1256) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1836)
    %1258 = stablehlo.reshape %1257 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1837)
    %1259 = stablehlo.transpose %1258, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1838)
    %1260 = stablehlo.dot_general %1240, %1259, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1839)
    %1261 = stablehlo.reshape %1260 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1840)
    %1262 = stablehlo.reshape %arg432 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1841)
    %1263 = stablehlo.custom_call @tt.mark_argument(%1262) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1842)
    %1264 = stablehlo.reshape %1263 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1843)
    %1265 = stablehlo.broadcast_in_dim %1264, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1844)
    %1266 = stablehlo.add %1261, %1265 : tensor<1x257x1280xbf16> loc(#loc1845)
    %1267 = stablehlo.reshape %1266 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1846)
    %1268 = stablehlo.transpose %1267, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1847)
    %1269 = stablehlo.convert %1268 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1848)
    %1270 = stablehlo.transpose %1269, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1849)
    %1271 = stablehlo.multiply %1270, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1850)
    %1272 = stablehlo.dot_general %1255, %1271, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1851)
    %1273 = stablehlo.convert %1272 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1852)
    %1274 = stablehlo.compare  EQ, %1273, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1853)
    %1275 = stablehlo.not %1274 : tensor<1x16x257x257xi1> loc(#loc1854)
    %1276 = stablehlo.reduce(%1275 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.5666"), %arg559: tensor<i1> loc("reduce.5666"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1856)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc1857)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1855)
    %1277 = stablehlo.reshape %1276 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1858)
    %1278 = stablehlo.not %1277 : tensor<1x16x257x1xi1> loc(#loc1859)
    %1279 = stablehlo.reshape %1278 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1860)
    %1280 = stablehlo.broadcast_in_dim %1279, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1861)
    %1281 = stablehlo.reduce(%1272 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1862)
    %1282 = stablehlo.broadcast_in_dim %1281, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1863)
    %1283 = stablehlo.subtract %1272, %1282 : tensor<1x16x257x257xf32> loc(#loc1864)
    %1284 = stablehlo.exponential %1283 : tensor<1x16x257x257xf32> loc(#loc1865)
    %1285 = stablehlo.reduce(%1284 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1866)
    %1286 = stablehlo.broadcast_in_dim %1285, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1867)
    %1287 = stablehlo.divide %1284, %1286 : tensor<1x16x257x257xf32> loc(#loc1868)
    %1288 = stablehlo.select %1280, %cst_3, %1287 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1869)
    %1289 = stablehlo.reshape %arg262 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1870)
    %1290 = stablehlo.custom_call @tt.mark_argument(%1289) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1871)
    %1291 = stablehlo.reshape %1290 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1872)
    %1292 = stablehlo.transpose %1291, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1873)
    %1293 = stablehlo.dot_general %1240, %1292, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1874)
    %1294 = stablehlo.reshape %1293 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1875)
    %1295 = stablehlo.reshape %arg261 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1876)
    %1296 = stablehlo.custom_call @tt.mark_argument(%1295) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1877)
    %1297 = stablehlo.reshape %1296 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1878)
    %1298 = stablehlo.broadcast_in_dim %1297, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1879)
    %1299 = stablehlo.add %1294, %1298 : tensor<1x257x1280xbf16> loc(#loc1880)
    %1300 = stablehlo.reshape %1299 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1881)
    %1301 = stablehlo.transpose %1300, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1882)
    %1302 = stablehlo.convert %1301 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1883)
    %1303 = stablehlo.dot_general %1288, %1302, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1884)
    %1304 = stablehlo.convert %1303 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1885)
    %1305 = stablehlo.transpose %1304, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1886)
    %1306 = stablehlo.reshape %1305 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1887)
    %1307 = stablehlo.reshape %arg260 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1888)
    %1308 = stablehlo.custom_call @tt.mark_argument(%1307) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1889)
    %1309 = stablehlo.reshape %1308 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1890)
    %1310 = stablehlo.transpose %1309, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1891)
    %1311 = stablehlo.dot_general %1306, %1310, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1892)
    %1312 = stablehlo.reshape %1311 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1893)
    %1313 = stablehlo.reshape %arg259 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1894)
    %1314 = stablehlo.custom_call @tt.mark_argument(%1313) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1895)
    %1315 = stablehlo.reshape %1314 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1896)
    %1316 = stablehlo.broadcast_in_dim %1315, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1897)
    %1317 = stablehlo.add %1312, %1316 : tensor<1x257x1280xbf16> loc(#loc1898)
    %1318 = stablehlo.add %1232, %1317 : tensor<1x257x1280xbf16> loc(#loc1899)
    %1319 = stablehlo.reshape %arg258 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1900)
    %1320 = stablehlo.custom_call @tt.mark_argument(%1319) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1901)
    %1321 = stablehlo.reshape %1320 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1902)
    %1322 = stablehlo.reshape %arg257 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1903)
    %1323 = stablehlo.custom_call @tt.mark_argument(%1322) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1904)
    %1324 = stablehlo.reshape %1323 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1905)
    %1325 = stablehlo.composite "tenstorrent.layer_norm" %1318, %1321, %1324 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_55} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1906)
    %1326 = stablehlo.reshape %1325 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1907)
    %1327 = stablehlo.reshape %arg256 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1908)
    %1328 = stablehlo.custom_call @tt.mark_argument(%1327) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc1909)
    %1329 = stablehlo.reshape %1328 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc1910)
    %1330 = stablehlo.transpose %1329, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1911)
    %1331 = stablehlo.dot_general %1326, %1330, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1912)
    %1332 = stablehlo.reshape %1331 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1913)
    %1333 = stablehlo.reshape %arg255 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1914)
    %1334 = stablehlo.custom_call @tt.mark_argument(%1333) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1915)
    %1335 = stablehlo.reshape %1334 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1916)
    %1336 = stablehlo.broadcast_in_dim %1335, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1917)
    %1337 = stablehlo.add %1332, %1336 : tensor<1x257x5120xbf16> loc(#loc1918)
    %1338 = stablehlo.composite "tenstorrent.gelu" %1337 {decomposition = @tenstorrent.gelu.impl_24} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1919)
    %1339 = stablehlo.reshape %1338 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1920)
    %1340 = stablehlo.reshape %arg254 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1921)
    %1341 = stablehlo.custom_call @tt.mark_argument(%1340) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc1922)
    %1342 = stablehlo.reshape %1341 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc1923)
    %1343 = stablehlo.transpose %1342, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1924)
    %1344 = stablehlo.dot_general %1339, %1343, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1925)
    %1345 = stablehlo.reshape %1344 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1926)
    %1346 = stablehlo.reshape %arg253 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1927)
    %1347 = stablehlo.custom_call @tt.mark_argument(%1346) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1928)
    %1348 = stablehlo.reshape %1347 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1929)
    %1349 = stablehlo.broadcast_in_dim %1348, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1930)
    %1350 = stablehlo.add %1345, %1349 : tensor<1x257x1280xbf16> loc(#loc1931)
    %1351 = stablehlo.add %1318, %1350 : tensor<1x257x1280xbf16> loc(#loc1932)
    %1352 = stablehlo.reshape %arg252 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1933)
    %1353 = stablehlo.custom_call @tt.mark_argument(%1352) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1934)
    %1354 = stablehlo.reshape %1353 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1935)
    %1355 = stablehlo.reshape %arg251 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1936)
    %1356 = stablehlo.custom_call @tt.mark_argument(%1355) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1937)
    %1357 = stablehlo.reshape %1356 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1938)
    %1358 = stablehlo.composite "tenstorrent.layer_norm" %1351, %1354, %1357 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_41} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1939)
    %1359 = stablehlo.reshape %1358 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1940)
    %1360 = stablehlo.reshape %arg439 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1941)
    %1361 = stablehlo.custom_call @tt.mark_argument(%1360) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1942)
    %1362 = stablehlo.reshape %1361 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1943)
    %1363 = stablehlo.transpose %1362, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1944)
    %1364 = stablehlo.dot_general %1359, %1363, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1945)
    %1365 = stablehlo.reshape %1364 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1946)
    %1366 = stablehlo.reshape %arg438 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1947)
    %1367 = stablehlo.custom_call @tt.mark_argument(%1366) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1948)
    %1368 = stablehlo.reshape %1367 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1949)
    %1369 = stablehlo.broadcast_in_dim %1368, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1950)
    %1370 = stablehlo.add %1365, %1369 : tensor<1x257x1280xbf16> loc(#loc1951)
    %1371 = stablehlo.reshape %1370 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1952)
    %1372 = stablehlo.transpose %1371, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1953)
    %1373 = stablehlo.convert %1372 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1954)
    %1374 = stablehlo.multiply %1373, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1955)
    %1375 = stablehlo.reshape %arg437 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1956)
    %1376 = stablehlo.custom_call @tt.mark_argument(%1375) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1957)
    %1377 = stablehlo.reshape %1376 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1958)
    %1378 = stablehlo.transpose %1377, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1959)
    %1379 = stablehlo.dot_general %1359, %1378, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1960)
    %1380 = stablehlo.reshape %1379 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1961)
    %1381 = stablehlo.reshape %arg436 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1962)
    %1382 = stablehlo.custom_call @tt.mark_argument(%1381) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1963)
    %1383 = stablehlo.reshape %1382 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1964)
    %1384 = stablehlo.broadcast_in_dim %1383, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1965)
    %1385 = stablehlo.add %1380, %1384 : tensor<1x257x1280xbf16> loc(#loc1966)
    %1386 = stablehlo.reshape %1385 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1967)
    %1387 = stablehlo.transpose %1386, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1968)
    %1388 = stablehlo.convert %1387 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1969)
    %1389 = stablehlo.transpose %1388, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1970)
    %1390 = stablehlo.multiply %1389, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1971)
    %1391 = stablehlo.dot_general %1374, %1390, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1972)
    %1392 = stablehlo.convert %1391 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1973)
    %1393 = stablehlo.compare  EQ, %1392, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1974)
    %1394 = stablehlo.not %1393 : tensor<1x16x257x257xi1> loc(#loc1975)
    %1395 = stablehlo.reduce(%1394 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.5976"), %arg559: tensor<i1> loc("reduce.5976"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1977)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc1978)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1976)
    %1396 = stablehlo.reshape %1395 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1979)
    %1397 = stablehlo.not %1396 : tensor<1x16x257x1xi1> loc(#loc1980)
    %1398 = stablehlo.reshape %1397 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1981)
    %1399 = stablehlo.broadcast_in_dim %1398, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1982)
    %1400 = stablehlo.reduce(%1391 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1983)
    %1401 = stablehlo.broadcast_in_dim %1400, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1984)
    %1402 = stablehlo.subtract %1391, %1401 : tensor<1x16x257x257xf32> loc(#loc1985)
    %1403 = stablehlo.exponential %1402 : tensor<1x16x257x257xf32> loc(#loc1986)
    %1404 = stablehlo.reduce(%1403 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1987)
    %1405 = stablehlo.broadcast_in_dim %1404, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1988)
    %1406 = stablehlo.divide %1403, %1405 : tensor<1x16x257x257xf32> loc(#loc1989)
    %1407 = stablehlo.select %1399, %cst_3, %1406 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1990)
    %1408 = stablehlo.reshape %arg250 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1991)
    %1409 = stablehlo.custom_call @tt.mark_argument(%1408) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc1992)
    %1410 = stablehlo.reshape %1409 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1993)
    %1411 = stablehlo.transpose %1410, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1994)
    %1412 = stablehlo.dot_general %1359, %1411, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1995)
    %1413 = stablehlo.reshape %1412 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1996)
    %1414 = stablehlo.reshape %arg249 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1997)
    %1415 = stablehlo.custom_call @tt.mark_argument(%1414) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1998)
    %1416 = stablehlo.reshape %1415 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc1999)
    %1417 = stablehlo.broadcast_in_dim %1416, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2000)
    %1418 = stablehlo.add %1413, %1417 : tensor<1x257x1280xbf16> loc(#loc2001)
    %1419 = stablehlo.reshape %1418 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2002)
    %1420 = stablehlo.transpose %1419, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2003)
    %1421 = stablehlo.convert %1420 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2004)
    %1422 = stablehlo.dot_general %1407, %1421, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc2005)
    %1423 = stablehlo.convert %1422 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc2006)
    %1424 = stablehlo.transpose %1423, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2007)
    %1425 = stablehlo.reshape %1424 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc2008)
    %1426 = stablehlo.reshape %arg248 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2009)
    %1427 = stablehlo.custom_call @tt.mark_argument(%1426) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2010)
    %1428 = stablehlo.reshape %1427 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2011)
    %1429 = stablehlo.transpose %1428, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2012)
    %1430 = stablehlo.dot_general %1425, %1429, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2013)
    %1431 = stablehlo.reshape %1430 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2014)
    %1432 = stablehlo.reshape %arg247 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2015)
    %1433 = stablehlo.custom_call @tt.mark_argument(%1432) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2016)
    %1434 = stablehlo.reshape %1433 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2017)
    %1435 = stablehlo.broadcast_in_dim %1434, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2018)
    %1436 = stablehlo.add %1431, %1435 : tensor<1x257x1280xbf16> loc(#loc2019)
    %1437 = stablehlo.add %1351, %1436 : tensor<1x257x1280xbf16> loc(#loc2020)
    %1438 = stablehlo.reshape %arg246 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2021)
    %1439 = stablehlo.custom_call @tt.mark_argument(%1438) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2022)
    %1440 = stablehlo.reshape %1439 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2023)
    %1441 = stablehlo.reshape %arg245 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2024)
    %1442 = stablehlo.custom_call @tt.mark_argument(%1441) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2025)
    %1443 = stablehlo.reshape %1442 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2026)
    %1444 = stablehlo.composite "tenstorrent.layer_norm" %1437, %1440, %1443 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_34} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2027)
    %1445 = stablehlo.reshape %1444 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2028)
    %1446 = stablehlo.reshape %arg244 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2029)
    %1447 = stablehlo.custom_call @tt.mark_argument(%1446) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2030)
    %1448 = stablehlo.reshape %1447 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2031)
    %1449 = stablehlo.transpose %1448, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc2032)
    %1450 = stablehlo.dot_general %1445, %1449, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2033)
    %1451 = stablehlo.reshape %1450 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2034)
    %1452 = stablehlo.reshape %arg243 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2035)
    %1453 = stablehlo.custom_call @tt.mark_argument(%1452) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2036)
    %1454 = stablehlo.reshape %1453 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2037)
    %1455 = stablehlo.broadcast_in_dim %1454, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2038)
    %1456 = stablehlo.add %1451, %1455 : tensor<1x257x5120xbf16> loc(#loc2039)
    %1457 = stablehlo.composite "tenstorrent.gelu" %1456 {decomposition = @tenstorrent.gelu.impl_25} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2040)
    %1458 = stablehlo.reshape %1457 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2041)
    %1459 = stablehlo.reshape %arg242 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2042)
    %1460 = stablehlo.custom_call @tt.mark_argument(%1459) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2043)
    %1461 = stablehlo.reshape %1460 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2044)
    %1462 = stablehlo.transpose %1461, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc2045)
    %1463 = stablehlo.dot_general %1458, %1462, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2046)
    %1464 = stablehlo.reshape %1463 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2047)
    %1465 = stablehlo.reshape %arg241 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2048)
    %1466 = stablehlo.custom_call @tt.mark_argument(%1465) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2049)
    %1467 = stablehlo.reshape %1466 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2050)
    %1468 = stablehlo.broadcast_in_dim %1467, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2051)
    %1469 = stablehlo.add %1464, %1468 : tensor<1x257x1280xbf16> loc(#loc2052)
    %1470 = stablehlo.add %1437, %1469 : tensor<1x257x1280xbf16> loc(#loc2053)
    %1471 = stablehlo.reshape %arg240 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2054)
    %1472 = stablehlo.custom_call @tt.mark_argument(%1471) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2055)
    %1473 = stablehlo.reshape %1472 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2056)
    %1474 = stablehlo.reshape %arg239 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2057)
    %1475 = stablehlo.custom_call @tt.mark_argument(%1474) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2058)
    %1476 = stablehlo.reshape %1475 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2059)
    %1477 = stablehlo.composite "tenstorrent.layer_norm" %1470, %1473, %1476 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_64} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2060)
    %1478 = stablehlo.reshape %1477 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2061)
    %1479 = stablehlo.reshape %arg443 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2062)
    %1480 = stablehlo.custom_call @tt.mark_argument(%1479) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2063)
    %1481 = stablehlo.reshape %1480 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2064)
    %1482 = stablehlo.transpose %1481, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2065)
    %1483 = stablehlo.dot_general %1478, %1482, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2066)
    %1484 = stablehlo.reshape %1483 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2067)
    %1485 = stablehlo.reshape %arg442 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2068)
    %1486 = stablehlo.custom_call @tt.mark_argument(%1485) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2069)
    %1487 = stablehlo.reshape %1486 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2070)
    %1488 = stablehlo.broadcast_in_dim %1487, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2071)
    %1489 = stablehlo.add %1484, %1488 : tensor<1x257x1280xbf16> loc(#loc2072)
    %1490 = stablehlo.reshape %1489 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2073)
    %1491 = stablehlo.transpose %1490, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2074)
    %1492 = stablehlo.convert %1491 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2075)
    %1493 = stablehlo.multiply %1492, %cst_6 : tensor<1x16x257x80xf32> loc(#loc2076)
    %1494 = stablehlo.reshape %arg441 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2077)
    %1495 = stablehlo.custom_call @tt.mark_argument(%1494) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2078)
    %1496 = stablehlo.reshape %1495 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2079)
    %1497 = stablehlo.transpose %1496, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2080)
    %1498 = stablehlo.dot_general %1478, %1497, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2081)
    %1499 = stablehlo.reshape %1498 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2082)
    %1500 = stablehlo.reshape %arg440 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2083)
    %1501 = stablehlo.custom_call @tt.mark_argument(%1500) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2084)
    %1502 = stablehlo.reshape %1501 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2085)
    %1503 = stablehlo.broadcast_in_dim %1502, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2086)
    %1504 = stablehlo.add %1499, %1503 : tensor<1x257x1280xbf16> loc(#loc2087)
    %1505 = stablehlo.reshape %1504 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2088)
    %1506 = stablehlo.transpose %1505, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2089)
    %1507 = stablehlo.convert %1506 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2090)
    %1508 = stablehlo.transpose %1507, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc2091)
    %1509 = stablehlo.multiply %1508, %cst_5 : tensor<1x16x80x257xf32> loc(#loc2092)
    %1510 = stablehlo.dot_general %1493, %1509, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2093)
    %1511 = stablehlo.convert %1510 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc2094)
    %1512 = stablehlo.compare  EQ, %1511, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc2095)
    %1513 = stablehlo.not %1512 : tensor<1x16x257x257xi1> loc(#loc2096)
    %1514 = stablehlo.reduce(%1513 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.6286"), %arg559: tensor<i1> loc("reduce.6286"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc2098)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc2099)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc2097)
    %1515 = stablehlo.reshape %1514 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc2100)
    %1516 = stablehlo.not %1515 : tensor<1x16x257x1xi1> loc(#loc2101)
    %1517 = stablehlo.reshape %1516 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc2102)
    %1518 = stablehlo.broadcast_in_dim %1517, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc2103)
    %1519 = stablehlo.reduce(%1510 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2104)
    %1520 = stablehlo.broadcast_in_dim %1519, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2105)
    %1521 = stablehlo.subtract %1510, %1520 : tensor<1x16x257x257xf32> loc(#loc2106)
    %1522 = stablehlo.exponential %1521 : tensor<1x16x257x257xf32> loc(#loc2107)
    %1523 = stablehlo.reduce(%1522 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2108)
    %1524 = stablehlo.broadcast_in_dim %1523, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2109)
    %1525 = stablehlo.divide %1522, %1524 : tensor<1x16x257x257xf32> loc(#loc2110)
    %1526 = stablehlo.select %1518, %cst_3, %1525 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc2111)
    %1527 = stablehlo.reshape %arg238 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2112)
    %1528 = stablehlo.custom_call @tt.mark_argument(%1527) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2113)
    %1529 = stablehlo.reshape %1528 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2114)
    %1530 = stablehlo.transpose %1529, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2115)
    %1531 = stablehlo.dot_general %1478, %1530, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2116)
    %1532 = stablehlo.reshape %1531 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2117)
    %1533 = stablehlo.reshape %arg237 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2118)
    %1534 = stablehlo.custom_call @tt.mark_argument(%1533) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2119)
    %1535 = stablehlo.reshape %1534 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2120)
    %1536 = stablehlo.broadcast_in_dim %1535, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2121)
    %1537 = stablehlo.add %1532, %1536 : tensor<1x257x1280xbf16> loc(#loc2122)
    %1538 = stablehlo.reshape %1537 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2123)
    %1539 = stablehlo.transpose %1538, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2124)
    %1540 = stablehlo.convert %1539 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2125)
    %1541 = stablehlo.dot_general %1526, %1540, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc2126)
    %1542 = stablehlo.convert %1541 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc2127)
    %1543 = stablehlo.transpose %1542, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2128)
    %1544 = stablehlo.reshape %1543 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc2129)
    %1545 = stablehlo.reshape %arg236 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2130)
    %1546 = stablehlo.custom_call @tt.mark_argument(%1545) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2131)
    %1547 = stablehlo.reshape %1546 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2132)
    %1548 = stablehlo.transpose %1547, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2133)
    %1549 = stablehlo.dot_general %1544, %1548, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2134)
    %1550 = stablehlo.reshape %1549 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2135)
    %1551 = stablehlo.reshape %arg235 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2136)
    %1552 = stablehlo.custom_call @tt.mark_argument(%1551) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2137)
    %1553 = stablehlo.reshape %1552 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2138)
    %1554 = stablehlo.broadcast_in_dim %1553, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2139)
    %1555 = stablehlo.add %1550, %1554 : tensor<1x257x1280xbf16> loc(#loc2140)
    %1556 = stablehlo.add %1470, %1555 : tensor<1x257x1280xbf16> loc(#loc2141)
    %1557 = stablehlo.reshape %arg234 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2142)
    %1558 = stablehlo.custom_call @tt.mark_argument(%1557) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2143)
    %1559 = stablehlo.reshape %1558 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2144)
    %1560 = stablehlo.reshape %arg233 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2145)
    %1561 = stablehlo.custom_call @tt.mark_argument(%1560) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2146)
    %1562 = stablehlo.reshape %1561 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2147)
    %1563 = stablehlo.composite "tenstorrent.layer_norm" %1556, %1559, %1562 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_35} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2148)
    %1564 = stablehlo.reshape %1563 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2149)
    %1565 = stablehlo.reshape %arg232 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2150)
    %1566 = stablehlo.custom_call @tt.mark_argument(%1565) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2151)
    %1567 = stablehlo.reshape %1566 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2152)
    %1568 = stablehlo.transpose %1567, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc2153)
    %1569 = stablehlo.dot_general %1564, %1568, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2154)
    %1570 = stablehlo.reshape %1569 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2155)
    %1571 = stablehlo.reshape %arg231 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2156)
    %1572 = stablehlo.custom_call @tt.mark_argument(%1571) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2157)
    %1573 = stablehlo.reshape %1572 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2158)
    %1574 = stablehlo.broadcast_in_dim %1573, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2159)
    %1575 = stablehlo.add %1570, %1574 : tensor<1x257x5120xbf16> loc(#loc2160)
    %1576 = stablehlo.composite "tenstorrent.gelu" %1575 {decomposition = @tenstorrent.gelu.impl_26} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2161)
    %1577 = stablehlo.reshape %1576 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2162)
    %1578 = stablehlo.reshape %arg230 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2163)
    %1579 = stablehlo.custom_call @tt.mark_argument(%1578) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2164)
    %1580 = stablehlo.reshape %1579 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2165)
    %1581 = stablehlo.transpose %1580, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc2166)
    %1582 = stablehlo.dot_general %1577, %1581, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2167)
    %1583 = stablehlo.reshape %1582 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2168)
    %1584 = stablehlo.reshape %arg229 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2169)
    %1585 = stablehlo.custom_call @tt.mark_argument(%1584) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2170)
    %1586 = stablehlo.reshape %1585 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2171)
    %1587 = stablehlo.broadcast_in_dim %1586, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2172)
    %1588 = stablehlo.add %1583, %1587 : tensor<1x257x1280xbf16> loc(#loc2173)
    %1589 = stablehlo.add %1556, %1588 : tensor<1x257x1280xbf16> loc(#loc2174)
    %1590 = stablehlo.reshape %arg228 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2175)
    %1591 = stablehlo.custom_call @tt.mark_argument(%1590) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2176)
    %1592 = stablehlo.reshape %1591 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2177)
    %1593 = stablehlo.reshape %arg227 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2178)
    %1594 = stablehlo.custom_call @tt.mark_argument(%1593) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2179)
    %1595 = stablehlo.reshape %1594 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2180)
    %1596 = stablehlo.composite "tenstorrent.layer_norm" %1589, %1592, %1595 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_30} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2181)
    %1597 = stablehlo.reshape %1596 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2182)
    %1598 = stablehlo.reshape %arg447 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2183)
    %1599 = stablehlo.custom_call @tt.mark_argument(%1598) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2184)
    %1600 = stablehlo.reshape %1599 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2185)
    %1601 = stablehlo.transpose %1600, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2186)
    %1602 = stablehlo.dot_general %1597, %1601, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2187)
    %1603 = stablehlo.reshape %1602 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2188)
    %1604 = stablehlo.reshape %arg446 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2189)
    %1605 = stablehlo.custom_call @tt.mark_argument(%1604) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2190)
    %1606 = stablehlo.reshape %1605 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2191)
    %1607 = stablehlo.broadcast_in_dim %1606, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2192)
    %1608 = stablehlo.add %1603, %1607 : tensor<1x257x1280xbf16> loc(#loc2193)
    %1609 = stablehlo.reshape %1608 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2194)
    %1610 = stablehlo.transpose %1609, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2195)
    %1611 = stablehlo.convert %1610 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2196)
    %1612 = stablehlo.multiply %1611, %cst_6 : tensor<1x16x257x80xf32> loc(#loc2197)
    %1613 = stablehlo.reshape %arg445 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2198)
    %1614 = stablehlo.custom_call @tt.mark_argument(%1613) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2199)
    %1615 = stablehlo.reshape %1614 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2200)
    %1616 = stablehlo.transpose %1615, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2201)
    %1617 = stablehlo.dot_general %1597, %1616, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2202)
    %1618 = stablehlo.reshape %1617 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2203)
    %1619 = stablehlo.reshape %arg444 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2204)
    %1620 = stablehlo.custom_call @tt.mark_argument(%1619) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2205)
    %1621 = stablehlo.reshape %1620 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2206)
    %1622 = stablehlo.broadcast_in_dim %1621, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2207)
    %1623 = stablehlo.add %1618, %1622 : tensor<1x257x1280xbf16> loc(#loc2208)
    %1624 = stablehlo.reshape %1623 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2209)
    %1625 = stablehlo.transpose %1624, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2210)
    %1626 = stablehlo.convert %1625 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2211)
    %1627 = stablehlo.transpose %1626, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc2212)
    %1628 = stablehlo.multiply %1627, %cst_5 : tensor<1x16x80x257xf32> loc(#loc2213)
    %1629 = stablehlo.dot_general %1612, %1628, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2214)
    %1630 = stablehlo.convert %1629 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc2215)
    %1631 = stablehlo.compare  EQ, %1630, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc2216)
    %1632 = stablehlo.not %1631 : tensor<1x16x257x257xi1> loc(#loc2217)
    %1633 = stablehlo.reduce(%1632 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.6596"), %arg559: tensor<i1> loc("reduce.6596"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc2219)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc2220)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc2218)
    %1634 = stablehlo.reshape %1633 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc2221)
    %1635 = stablehlo.not %1634 : tensor<1x16x257x1xi1> loc(#loc2222)
    %1636 = stablehlo.reshape %1635 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc2223)
    %1637 = stablehlo.broadcast_in_dim %1636, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc2224)
    %1638 = stablehlo.reduce(%1629 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2225)
    %1639 = stablehlo.broadcast_in_dim %1638, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2226)
    %1640 = stablehlo.subtract %1629, %1639 : tensor<1x16x257x257xf32> loc(#loc2227)
    %1641 = stablehlo.exponential %1640 : tensor<1x16x257x257xf32> loc(#loc2228)
    %1642 = stablehlo.reduce(%1641 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2229)
    %1643 = stablehlo.broadcast_in_dim %1642, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2230)
    %1644 = stablehlo.divide %1641, %1643 : tensor<1x16x257x257xf32> loc(#loc2231)
    %1645 = stablehlo.select %1637, %cst_3, %1644 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc2232)
    %1646 = stablehlo.reshape %arg226 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2233)
    %1647 = stablehlo.custom_call @tt.mark_argument(%1646) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2234)
    %1648 = stablehlo.reshape %1647 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2235)
    %1649 = stablehlo.transpose %1648, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2236)
    %1650 = stablehlo.dot_general %1597, %1649, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2237)
    %1651 = stablehlo.reshape %1650 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2238)
    %1652 = stablehlo.reshape %arg225 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2239)
    %1653 = stablehlo.custom_call @tt.mark_argument(%1652) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2240)
    %1654 = stablehlo.reshape %1653 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2241)
    %1655 = stablehlo.broadcast_in_dim %1654, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2242)
    %1656 = stablehlo.add %1651, %1655 : tensor<1x257x1280xbf16> loc(#loc2243)
    %1657 = stablehlo.reshape %1656 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2244)
    %1658 = stablehlo.transpose %1657, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2245)
    %1659 = stablehlo.convert %1658 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2246)
    %1660 = stablehlo.dot_general %1645, %1659, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc2247)
    %1661 = stablehlo.convert %1660 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc2248)
    %1662 = stablehlo.transpose %1661, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2249)
    %1663 = stablehlo.reshape %1662 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc2250)
    %1664 = stablehlo.reshape %arg224 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2251)
    %1665 = stablehlo.custom_call @tt.mark_argument(%1664) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2252)
    %1666 = stablehlo.reshape %1665 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2253)
    %1667 = stablehlo.transpose %1666, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2254)
    %1668 = stablehlo.dot_general %1663, %1667, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2255)
    %1669 = stablehlo.reshape %1668 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2256)
    %1670 = stablehlo.reshape %arg223 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2257)
    %1671 = stablehlo.custom_call @tt.mark_argument(%1670) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2258)
    %1672 = stablehlo.reshape %1671 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2259)
    %1673 = stablehlo.broadcast_in_dim %1672, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2260)
    %1674 = stablehlo.add %1669, %1673 : tensor<1x257x1280xbf16> loc(#loc2261)
    %1675 = stablehlo.add %1589, %1674 : tensor<1x257x1280xbf16> loc(#loc2262)
    %1676 = stablehlo.reshape %arg222 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2263)
    %1677 = stablehlo.custom_call @tt.mark_argument(%1676) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2264)
    %1678 = stablehlo.reshape %1677 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2265)
    %1679 = stablehlo.reshape %arg221 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2266)
    %1680 = stablehlo.custom_call @tt.mark_argument(%1679) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2267)
    %1681 = stablehlo.reshape %1680 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2268)
    %1682 = stablehlo.composite "tenstorrent.layer_norm" %1675, %1678, %1681 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_61} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2269)
    %1683 = stablehlo.reshape %1682 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2270)
    %1684 = stablehlo.reshape %arg220 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2271)
    %1685 = stablehlo.custom_call @tt.mark_argument(%1684) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2272)
    %1686 = stablehlo.reshape %1685 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2273)
    %1687 = stablehlo.transpose %1686, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc2274)
    %1688 = stablehlo.dot_general %1683, %1687, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2275)
    %1689 = stablehlo.reshape %1688 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2276)
    %1690 = stablehlo.reshape %arg219 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2277)
    %1691 = stablehlo.custom_call @tt.mark_argument(%1690) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2278)
    %1692 = stablehlo.reshape %1691 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2279)
    %1693 = stablehlo.broadcast_in_dim %1692, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2280)
    %1694 = stablehlo.add %1689, %1693 : tensor<1x257x5120xbf16> loc(#loc2281)
    %1695 = stablehlo.composite "tenstorrent.gelu" %1694 {decomposition = @tenstorrent.gelu.impl_28} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2282)
    %1696 = stablehlo.reshape %1695 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2283)
    %1697 = stablehlo.reshape %arg218 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2284)
    %1698 = stablehlo.custom_call @tt.mark_argument(%1697) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2285)
    %1699 = stablehlo.reshape %1698 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2286)
    %1700 = stablehlo.transpose %1699, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc2287)
    %1701 = stablehlo.dot_general %1696, %1700, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2288)
    %1702 = stablehlo.reshape %1701 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2289)
    %1703 = stablehlo.reshape %arg217 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2290)
    %1704 = stablehlo.custom_call @tt.mark_argument(%1703) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2291)
    %1705 = stablehlo.reshape %1704 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2292)
    %1706 = stablehlo.broadcast_in_dim %1705, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2293)
    %1707 = stablehlo.add %1702, %1706 : tensor<1x257x1280xbf16> loc(#loc2294)
    %1708 = stablehlo.add %1675, %1707 : tensor<1x257x1280xbf16> loc(#loc2295)
    %1709 = stablehlo.reshape %arg216 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2296)
    %1710 = stablehlo.custom_call @tt.mark_argument(%1709) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2297)
    %1711 = stablehlo.reshape %1710 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2298)
    %1712 = stablehlo.reshape %arg215 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2299)
    %1713 = stablehlo.custom_call @tt.mark_argument(%1712) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2300)
    %1714 = stablehlo.reshape %1713 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2301)
    %1715 = stablehlo.composite "tenstorrent.layer_norm" %1708, %1711, %1714 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_46} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2302)
    %1716 = stablehlo.reshape %1715 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2303)
    %1717 = stablehlo.reshape %arg451 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2304)
    %1718 = stablehlo.custom_call @tt.mark_argument(%1717) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2305)
    %1719 = stablehlo.reshape %1718 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2306)
    %1720 = stablehlo.transpose %1719, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2307)
    %1721 = stablehlo.dot_general %1716, %1720, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2308)
    %1722 = stablehlo.reshape %1721 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2309)
    %1723 = stablehlo.reshape %arg450 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2310)
    %1724 = stablehlo.custom_call @tt.mark_argument(%1723) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2311)
    %1725 = stablehlo.reshape %1724 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2312)
    %1726 = stablehlo.broadcast_in_dim %1725, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2313)
    %1727 = stablehlo.add %1722, %1726 : tensor<1x257x1280xbf16> loc(#loc2314)
    %1728 = stablehlo.reshape %1727 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2315)
    %1729 = stablehlo.transpose %1728, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2316)
    %1730 = stablehlo.convert %1729 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2317)
    %1731 = stablehlo.multiply %1730, %cst_6 : tensor<1x16x257x80xf32> loc(#loc2318)
    %1732 = stablehlo.reshape %arg449 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2319)
    %1733 = stablehlo.custom_call @tt.mark_argument(%1732) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2320)
    %1734 = stablehlo.reshape %1733 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2321)
    %1735 = stablehlo.transpose %1734, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2322)
    %1736 = stablehlo.dot_general %1716, %1735, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2323)
    %1737 = stablehlo.reshape %1736 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2324)
    %1738 = stablehlo.reshape %arg448 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2325)
    %1739 = stablehlo.custom_call @tt.mark_argument(%1738) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2326)
    %1740 = stablehlo.reshape %1739 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2327)
    %1741 = stablehlo.broadcast_in_dim %1740, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2328)
    %1742 = stablehlo.add %1737, %1741 : tensor<1x257x1280xbf16> loc(#loc2329)
    %1743 = stablehlo.reshape %1742 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2330)
    %1744 = stablehlo.transpose %1743, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2331)
    %1745 = stablehlo.convert %1744 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2332)
    %1746 = stablehlo.transpose %1745, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc2333)
    %1747 = stablehlo.multiply %1746, %cst_5 : tensor<1x16x80x257xf32> loc(#loc2334)
    %1748 = stablehlo.dot_general %1731, %1747, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2335)
    %1749 = stablehlo.convert %1748 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc2336)
    %1750 = stablehlo.compare  EQ, %1749, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc2337)
    %1751 = stablehlo.not %1750 : tensor<1x16x257x257xi1> loc(#loc2338)
    %1752 = stablehlo.reduce(%1751 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.6906"), %arg559: tensor<i1> loc("reduce.6906"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc2340)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc2341)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc2339)
    %1753 = stablehlo.reshape %1752 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc2342)
    %1754 = stablehlo.not %1753 : tensor<1x16x257x1xi1> loc(#loc2343)
    %1755 = stablehlo.reshape %1754 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc2344)
    %1756 = stablehlo.broadcast_in_dim %1755, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc2345)
    %1757 = stablehlo.reduce(%1748 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2346)
    %1758 = stablehlo.broadcast_in_dim %1757, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2347)
    %1759 = stablehlo.subtract %1748, %1758 : tensor<1x16x257x257xf32> loc(#loc2348)
    %1760 = stablehlo.exponential %1759 : tensor<1x16x257x257xf32> loc(#loc2349)
    %1761 = stablehlo.reduce(%1760 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2350)
    %1762 = stablehlo.broadcast_in_dim %1761, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2351)
    %1763 = stablehlo.divide %1760, %1762 : tensor<1x16x257x257xf32> loc(#loc2352)
    %1764 = stablehlo.select %1756, %cst_3, %1763 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc2353)
    %1765 = stablehlo.reshape %arg214 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2354)
    %1766 = stablehlo.custom_call @tt.mark_argument(%1765) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2355)
    %1767 = stablehlo.reshape %1766 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2356)
    %1768 = stablehlo.transpose %1767, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2357)
    %1769 = stablehlo.dot_general %1716, %1768, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2358)
    %1770 = stablehlo.reshape %1769 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2359)
    %1771 = stablehlo.reshape %arg213 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2360)
    %1772 = stablehlo.custom_call @tt.mark_argument(%1771) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2361)
    %1773 = stablehlo.reshape %1772 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2362)
    %1774 = stablehlo.broadcast_in_dim %1773, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2363)
    %1775 = stablehlo.add %1770, %1774 : tensor<1x257x1280xbf16> loc(#loc2364)
    %1776 = stablehlo.reshape %1775 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2365)
    %1777 = stablehlo.transpose %1776, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2366)
    %1778 = stablehlo.convert %1777 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2367)
    %1779 = stablehlo.dot_general %1764, %1778, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc2368)
    %1780 = stablehlo.convert %1779 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc2369)
    %1781 = stablehlo.transpose %1780, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2370)
    %1782 = stablehlo.reshape %1781 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc2371)
    %1783 = stablehlo.reshape %arg212 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2372)
    %1784 = stablehlo.custom_call @tt.mark_argument(%1783) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2373)
    %1785 = stablehlo.reshape %1784 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2374)
    %1786 = stablehlo.transpose %1785, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2375)
    %1787 = stablehlo.dot_general %1782, %1786, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2376)
    %1788 = stablehlo.reshape %1787 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2377)
    %1789 = stablehlo.reshape %arg211 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2378)
    %1790 = stablehlo.custom_call @tt.mark_argument(%1789) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2379)
    %1791 = stablehlo.reshape %1790 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2380)
    %1792 = stablehlo.broadcast_in_dim %1791, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2381)
    %1793 = stablehlo.add %1788, %1792 : tensor<1x257x1280xbf16> loc(#loc2382)
    %1794 = stablehlo.add %1708, %1793 : tensor<1x257x1280xbf16> loc(#loc2383)
    %1795 = stablehlo.reshape %arg210 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2384)
    %1796 = stablehlo.custom_call @tt.mark_argument(%1795) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2385)
    %1797 = stablehlo.reshape %1796 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2386)
    %1798 = stablehlo.reshape %arg209 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2387)
    %1799 = stablehlo.custom_call @tt.mark_argument(%1798) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2388)
    %1800 = stablehlo.reshape %1799 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2389)
    %1801 = stablehlo.composite "tenstorrent.layer_norm" %1794, %1797, %1800 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_65} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2390)
    %1802 = stablehlo.reshape %1801 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2391)
    %1803 = stablehlo.reshape %arg208 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2392)
    %1804 = stablehlo.custom_call @tt.mark_argument(%1803) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2393)
    %1805 = stablehlo.reshape %1804 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2394)
    %1806 = stablehlo.transpose %1805, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc2395)
    %1807 = stablehlo.dot_general %1802, %1806, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2396)
    %1808 = stablehlo.reshape %1807 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2397)
    %1809 = stablehlo.reshape %arg207 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2398)
    %1810 = stablehlo.custom_call @tt.mark_argument(%1809) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2399)
    %1811 = stablehlo.reshape %1810 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2400)
    %1812 = stablehlo.broadcast_in_dim %1811, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2401)
    %1813 = stablehlo.add %1808, %1812 : tensor<1x257x5120xbf16> loc(#loc2402)
    %1814 = stablehlo.composite "tenstorrent.gelu" %1813 {decomposition = @tenstorrent.gelu.impl_21} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2403)
    %1815 = stablehlo.reshape %1814 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2404)
    %1816 = stablehlo.reshape %arg206 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2405)
    %1817 = stablehlo.custom_call @tt.mark_argument(%1816) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2406)
    %1818 = stablehlo.reshape %1817 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2407)
    %1819 = stablehlo.transpose %1818, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc2408)
    %1820 = stablehlo.dot_general %1815, %1819, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2409)
    %1821 = stablehlo.reshape %1820 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2410)
    %1822 = stablehlo.reshape %arg205 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2411)
    %1823 = stablehlo.custom_call @tt.mark_argument(%1822) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2412)
    %1824 = stablehlo.reshape %1823 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2413)
    %1825 = stablehlo.broadcast_in_dim %1824, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2414)
    %1826 = stablehlo.add %1821, %1825 : tensor<1x257x1280xbf16> loc(#loc2415)
    %1827 = stablehlo.add %1794, %1826 : tensor<1x257x1280xbf16> loc(#loc2416)
    %1828 = stablehlo.reshape %arg204 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2417)
    %1829 = stablehlo.custom_call @tt.mark_argument(%1828) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2418)
    %1830 = stablehlo.reshape %1829 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2419)
    %1831 = stablehlo.reshape %arg203 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2420)
    %1832 = stablehlo.custom_call @tt.mark_argument(%1831) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2421)
    %1833 = stablehlo.reshape %1832 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2422)
    %1834 = stablehlo.composite "tenstorrent.layer_norm" %1827, %1830, %1833 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_60} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2423)
    %1835 = stablehlo.reshape %1834 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2424)
    %1836 = stablehlo.reshape %arg455 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2425)
    %1837 = stablehlo.custom_call @tt.mark_argument(%1836) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2426)
    %1838 = stablehlo.reshape %1837 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2427)
    %1839 = stablehlo.transpose %1838, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2428)
    %1840 = stablehlo.dot_general %1835, %1839, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2429)
    %1841 = stablehlo.reshape %1840 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2430)
    %1842 = stablehlo.reshape %arg454 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2431)
    %1843 = stablehlo.custom_call @tt.mark_argument(%1842) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2432)
    %1844 = stablehlo.reshape %1843 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2433)
    %1845 = stablehlo.broadcast_in_dim %1844, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2434)
    %1846 = stablehlo.add %1841, %1845 : tensor<1x257x1280xbf16> loc(#loc2435)
    %1847 = stablehlo.reshape %1846 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2436)
    %1848 = stablehlo.transpose %1847, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2437)
    %1849 = stablehlo.convert %1848 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2438)
    %1850 = stablehlo.multiply %1849, %cst_6 : tensor<1x16x257x80xf32> loc(#loc2439)
    %1851 = stablehlo.reshape %arg453 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2440)
    %1852 = stablehlo.custom_call @tt.mark_argument(%1851) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2441)
    %1853 = stablehlo.reshape %1852 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2442)
    %1854 = stablehlo.transpose %1853, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2443)
    %1855 = stablehlo.dot_general %1835, %1854, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2444)
    %1856 = stablehlo.reshape %1855 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2445)
    %1857 = stablehlo.reshape %arg452 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2446)
    %1858 = stablehlo.custom_call @tt.mark_argument(%1857) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2447)
    %1859 = stablehlo.reshape %1858 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2448)
    %1860 = stablehlo.broadcast_in_dim %1859, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2449)
    %1861 = stablehlo.add %1856, %1860 : tensor<1x257x1280xbf16> loc(#loc2450)
    %1862 = stablehlo.reshape %1861 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2451)
    %1863 = stablehlo.transpose %1862, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2452)
    %1864 = stablehlo.convert %1863 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2453)
    %1865 = stablehlo.transpose %1864, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc2454)
    %1866 = stablehlo.multiply %1865, %cst_5 : tensor<1x16x80x257xf32> loc(#loc2455)
    %1867 = stablehlo.dot_general %1850, %1866, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2456)
    %1868 = stablehlo.convert %1867 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc2457)
    %1869 = stablehlo.compare  EQ, %1868, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc2458)
    %1870 = stablehlo.not %1869 : tensor<1x16x257x257xi1> loc(#loc2459)
    %1871 = stablehlo.reduce(%1870 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.7216"), %arg559: tensor<i1> loc("reduce.7216"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc2461)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc2462)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc2460)
    %1872 = stablehlo.reshape %1871 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc2463)
    %1873 = stablehlo.not %1872 : tensor<1x16x257x1xi1> loc(#loc2464)
    %1874 = stablehlo.reshape %1873 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc2465)
    %1875 = stablehlo.broadcast_in_dim %1874, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc2466)
    %1876 = stablehlo.reduce(%1867 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2467)
    %1877 = stablehlo.broadcast_in_dim %1876, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2468)
    %1878 = stablehlo.subtract %1867, %1877 : tensor<1x16x257x257xf32> loc(#loc2469)
    %1879 = stablehlo.exponential %1878 : tensor<1x16x257x257xf32> loc(#loc2470)
    %1880 = stablehlo.reduce(%1879 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2471)
    %1881 = stablehlo.broadcast_in_dim %1880, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2472)
    %1882 = stablehlo.divide %1879, %1881 : tensor<1x16x257x257xf32> loc(#loc2473)
    %1883 = stablehlo.select %1875, %cst_3, %1882 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc2474)
    %1884 = stablehlo.reshape %arg202 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2475)
    %1885 = stablehlo.custom_call @tt.mark_argument(%1884) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2476)
    %1886 = stablehlo.reshape %1885 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2477)
    %1887 = stablehlo.transpose %1886, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2478)
    %1888 = stablehlo.dot_general %1835, %1887, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2479)
    %1889 = stablehlo.reshape %1888 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2480)
    %1890 = stablehlo.reshape %arg201 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2481)
    %1891 = stablehlo.custom_call @tt.mark_argument(%1890) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2482)
    %1892 = stablehlo.reshape %1891 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2483)
    %1893 = stablehlo.broadcast_in_dim %1892, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2484)
    %1894 = stablehlo.add %1889, %1893 : tensor<1x257x1280xbf16> loc(#loc2485)
    %1895 = stablehlo.reshape %1894 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2486)
    %1896 = stablehlo.transpose %1895, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2487)
    %1897 = stablehlo.convert %1896 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2488)
    %1898 = stablehlo.dot_general %1883, %1897, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc2489)
    %1899 = stablehlo.convert %1898 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc2490)
    %1900 = stablehlo.transpose %1899, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2491)
    %1901 = stablehlo.reshape %1900 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc2492)
    %1902 = stablehlo.reshape %arg200 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2493)
    %1903 = stablehlo.custom_call @tt.mark_argument(%1902) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2494)
    %1904 = stablehlo.reshape %1903 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2495)
    %1905 = stablehlo.transpose %1904, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2496)
    %1906 = stablehlo.dot_general %1901, %1905, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2497)
    %1907 = stablehlo.reshape %1906 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2498)
    %1908 = stablehlo.reshape %arg199 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2499)
    %1909 = stablehlo.custom_call @tt.mark_argument(%1908) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2500)
    %1910 = stablehlo.reshape %1909 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2501)
    %1911 = stablehlo.broadcast_in_dim %1910, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2502)
    %1912 = stablehlo.add %1907, %1911 : tensor<1x257x1280xbf16> loc(#loc2503)
    %1913 = stablehlo.add %1827, %1912 : tensor<1x257x1280xbf16> loc(#loc2504)
    %1914 = stablehlo.reshape %arg198 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2505)
    %1915 = stablehlo.custom_call @tt.mark_argument(%1914) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2506)
    %1916 = stablehlo.reshape %1915 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2507)
    %1917 = stablehlo.reshape %arg197 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2508)
    %1918 = stablehlo.custom_call @tt.mark_argument(%1917) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2509)
    %1919 = stablehlo.reshape %1918 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2510)
    %1920 = stablehlo.composite "tenstorrent.layer_norm" %1913, %1916, %1919 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_66} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2511)
    %1921 = stablehlo.reshape %1920 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2512)
    %1922 = stablehlo.reshape %arg196 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2513)
    %1923 = stablehlo.custom_call @tt.mark_argument(%1922) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2514)
    %1924 = stablehlo.reshape %1923 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2515)
    %1925 = stablehlo.transpose %1924, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc2516)
    %1926 = stablehlo.dot_general %1921, %1925, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2517)
    %1927 = stablehlo.reshape %1926 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2518)
    %1928 = stablehlo.reshape %arg195 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2519)
    %1929 = stablehlo.custom_call @tt.mark_argument(%1928) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2520)
    %1930 = stablehlo.reshape %1929 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2521)
    %1931 = stablehlo.broadcast_in_dim %1930, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2522)
    %1932 = stablehlo.add %1927, %1931 : tensor<1x257x5120xbf16> loc(#loc2523)
    %1933 = stablehlo.composite "tenstorrent.gelu" %1932 {decomposition = @tenstorrent.gelu.impl_12} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2524)
    %1934 = stablehlo.reshape %1933 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2525)
    %1935 = stablehlo.reshape %arg194 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2526)
    %1936 = stablehlo.custom_call @tt.mark_argument(%1935) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2527)
    %1937 = stablehlo.reshape %1936 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2528)
    %1938 = stablehlo.transpose %1937, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc2529)
    %1939 = stablehlo.dot_general %1934, %1938, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2530)
    %1940 = stablehlo.reshape %1939 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2531)
    %1941 = stablehlo.reshape %arg193 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2532)
    %1942 = stablehlo.custom_call @tt.mark_argument(%1941) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2533)
    %1943 = stablehlo.reshape %1942 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2534)
    %1944 = stablehlo.broadcast_in_dim %1943, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2535)
    %1945 = stablehlo.add %1940, %1944 : tensor<1x257x1280xbf16> loc(#loc2536)
    %1946 = stablehlo.add %1913, %1945 : tensor<1x257x1280xbf16> loc(#loc2537)
    %1947 = stablehlo.reshape %arg192 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2538)
    %1948 = stablehlo.custom_call @tt.mark_argument(%1947) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2539)
    %1949 = stablehlo.reshape %1948 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2540)
    %1950 = stablehlo.reshape %arg191 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2541)
    %1951 = stablehlo.custom_call @tt.mark_argument(%1950) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2542)
    %1952 = stablehlo.reshape %1951 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2543)
    %1953 = stablehlo.composite "tenstorrent.layer_norm" %1946, %1949, %1952 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_68} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2544)
    %1954 = stablehlo.reshape %1953 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2545)
    %1955 = stablehlo.reshape %arg459 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2546)
    %1956 = stablehlo.custom_call @tt.mark_argument(%1955) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2547)
    %1957 = stablehlo.reshape %1956 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2548)
    %1958 = stablehlo.transpose %1957, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2549)
    %1959 = stablehlo.dot_general %1954, %1958, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2550)
    %1960 = stablehlo.reshape %1959 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2551)
    %1961 = stablehlo.reshape %arg458 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2552)
    %1962 = stablehlo.custom_call @tt.mark_argument(%1961) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2553)
    %1963 = stablehlo.reshape %1962 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2554)
    %1964 = stablehlo.broadcast_in_dim %1963, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2555)
    %1965 = stablehlo.add %1960, %1964 : tensor<1x257x1280xbf16> loc(#loc2556)
    %1966 = stablehlo.reshape %1965 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2557)
    %1967 = stablehlo.transpose %1966, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2558)
    %1968 = stablehlo.convert %1967 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2559)
    %1969 = stablehlo.multiply %1968, %cst_6 : tensor<1x16x257x80xf32> loc(#loc2560)
    %1970 = stablehlo.reshape %arg457 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2561)
    %1971 = stablehlo.custom_call @tt.mark_argument(%1970) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2562)
    %1972 = stablehlo.reshape %1971 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2563)
    %1973 = stablehlo.transpose %1972, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2564)
    %1974 = stablehlo.dot_general %1954, %1973, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2565)
    %1975 = stablehlo.reshape %1974 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2566)
    %1976 = stablehlo.reshape %arg456 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2567)
    %1977 = stablehlo.custom_call @tt.mark_argument(%1976) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2568)
    %1978 = stablehlo.reshape %1977 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2569)
    %1979 = stablehlo.broadcast_in_dim %1978, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2570)
    %1980 = stablehlo.add %1975, %1979 : tensor<1x257x1280xbf16> loc(#loc2571)
    %1981 = stablehlo.reshape %1980 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2572)
    %1982 = stablehlo.transpose %1981, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2573)
    %1983 = stablehlo.convert %1982 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2574)
    %1984 = stablehlo.transpose %1983, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc2575)
    %1985 = stablehlo.multiply %1984, %cst_5 : tensor<1x16x80x257xf32> loc(#loc2576)
    %1986 = stablehlo.dot_general %1969, %1985, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2577)
    %1987 = stablehlo.convert %1986 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc2578)
    %1988 = stablehlo.compare  EQ, %1987, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc2579)
    %1989 = stablehlo.not %1988 : tensor<1x16x257x257xi1> loc(#loc2580)
    %1990 = stablehlo.reduce(%1989 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.7526"), %arg559: tensor<i1> loc("reduce.7526"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc2582)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc2583)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc2581)
    %1991 = stablehlo.reshape %1990 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc2584)
    %1992 = stablehlo.not %1991 : tensor<1x16x257x1xi1> loc(#loc2585)
    %1993 = stablehlo.reshape %1992 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc2586)
    %1994 = stablehlo.broadcast_in_dim %1993, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc2587)
    %1995 = stablehlo.reduce(%1986 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2588)
    %1996 = stablehlo.broadcast_in_dim %1995, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2589)
    %1997 = stablehlo.subtract %1986, %1996 : tensor<1x16x257x257xf32> loc(#loc2590)
    %1998 = stablehlo.exponential %1997 : tensor<1x16x257x257xf32> loc(#loc2591)
    %1999 = stablehlo.reduce(%1998 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2592)
    %2000 = stablehlo.broadcast_in_dim %1999, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2593)
    %2001 = stablehlo.divide %1998, %2000 : tensor<1x16x257x257xf32> loc(#loc2594)
    %2002 = stablehlo.select %1994, %cst_3, %2001 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc2595)
    %2003 = stablehlo.reshape %arg190 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2596)
    %2004 = stablehlo.custom_call @tt.mark_argument(%2003) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2597)
    %2005 = stablehlo.reshape %2004 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2598)
    %2006 = stablehlo.transpose %2005, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2599)
    %2007 = stablehlo.dot_general %1954, %2006, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2600)
    %2008 = stablehlo.reshape %2007 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2601)
    %2009 = stablehlo.reshape %arg189 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2602)
    %2010 = stablehlo.custom_call @tt.mark_argument(%2009) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2603)
    %2011 = stablehlo.reshape %2010 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2604)
    %2012 = stablehlo.broadcast_in_dim %2011, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2605)
    %2013 = stablehlo.add %2008, %2012 : tensor<1x257x1280xbf16> loc(#loc2606)
    %2014 = stablehlo.reshape %2013 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2607)
    %2015 = stablehlo.transpose %2014, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2608)
    %2016 = stablehlo.convert %2015 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2609)
    %2017 = stablehlo.dot_general %2002, %2016, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc2610)
    %2018 = stablehlo.convert %2017 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc2611)
    %2019 = stablehlo.transpose %2018, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2612)
    %2020 = stablehlo.reshape %2019 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc2613)
    %2021 = stablehlo.reshape %arg188 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2614)
    %2022 = stablehlo.custom_call @tt.mark_argument(%2021) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2615)
    %2023 = stablehlo.reshape %2022 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2616)
    %2024 = stablehlo.transpose %2023, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2617)
    %2025 = stablehlo.dot_general %2020, %2024, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2618)
    %2026 = stablehlo.reshape %2025 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2619)
    %2027 = stablehlo.reshape %arg187 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2620)
    %2028 = stablehlo.custom_call @tt.mark_argument(%2027) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2621)
    %2029 = stablehlo.reshape %2028 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2622)
    %2030 = stablehlo.broadcast_in_dim %2029, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2623)
    %2031 = stablehlo.add %2026, %2030 : tensor<1x257x1280xbf16> loc(#loc2624)
    %2032 = stablehlo.add %1946, %2031 : tensor<1x257x1280xbf16> loc(#loc2625)
    %2033 = stablehlo.reshape %arg186 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2626)
    %2034 = stablehlo.custom_call @tt.mark_argument(%2033) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2627)
    %2035 = stablehlo.reshape %2034 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2628)
    %2036 = stablehlo.reshape %arg185 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2629)
    %2037 = stablehlo.custom_call @tt.mark_argument(%2036) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2630)
    %2038 = stablehlo.reshape %2037 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2631)
    %2039 = stablehlo.composite "tenstorrent.layer_norm" %2032, %2035, %2038 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_69} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2632)
    %2040 = stablehlo.reshape %2039 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2633)
    %2041 = stablehlo.reshape %arg184 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2634)
    %2042 = stablehlo.custom_call @tt.mark_argument(%2041) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2635)
    %2043 = stablehlo.reshape %2042 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2636)
    %2044 = stablehlo.transpose %2043, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc2637)
    %2045 = stablehlo.dot_general %2040, %2044, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2638)
    %2046 = stablehlo.reshape %2045 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2639)
    %2047 = stablehlo.reshape %arg183 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2640)
    %2048 = stablehlo.custom_call @tt.mark_argument(%2047) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2641)
    %2049 = stablehlo.reshape %2048 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2642)
    %2050 = stablehlo.broadcast_in_dim %2049, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2643)
    %2051 = stablehlo.add %2046, %2050 : tensor<1x257x5120xbf16> loc(#loc2644)
    %2052 = stablehlo.composite "tenstorrent.gelu" %2051 {decomposition = @tenstorrent.gelu.impl_31} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2645)
    %2053 = stablehlo.reshape %2052 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2646)
    %2054 = stablehlo.reshape %arg182 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2647)
    %2055 = stablehlo.custom_call @tt.mark_argument(%2054) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2648)
    %2056 = stablehlo.reshape %2055 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2649)
    %2057 = stablehlo.transpose %2056, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc2650)
    %2058 = stablehlo.dot_general %2053, %2057, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2651)
    %2059 = stablehlo.reshape %2058 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2652)
    %2060 = stablehlo.reshape %arg181 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2653)
    %2061 = stablehlo.custom_call @tt.mark_argument(%2060) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2654)
    %2062 = stablehlo.reshape %2061 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2655)
    %2063 = stablehlo.broadcast_in_dim %2062, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2656)
    %2064 = stablehlo.add %2059, %2063 : tensor<1x257x1280xbf16> loc(#loc2657)
    %2065 = stablehlo.add %2032, %2064 : tensor<1x257x1280xbf16> loc(#loc2658)
    %2066 = stablehlo.reshape %arg180 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2659)
    %2067 = stablehlo.custom_call @tt.mark_argument(%2066) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2660)
    %2068 = stablehlo.reshape %2067 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2661)
    %2069 = stablehlo.reshape %arg179 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2662)
    %2070 = stablehlo.custom_call @tt.mark_argument(%2069) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2663)
    %2071 = stablehlo.reshape %2070 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2664)
    %2072 = stablehlo.composite "tenstorrent.layer_norm" %2065, %2068, %2071 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_71} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2665)
    %2073 = stablehlo.reshape %2072 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2666)
    %2074 = stablehlo.reshape %arg463 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2667)
    %2075 = stablehlo.custom_call @tt.mark_argument(%2074) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2668)
    %2076 = stablehlo.reshape %2075 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2669)
    %2077 = stablehlo.transpose %2076, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2670)
    %2078 = stablehlo.dot_general %2073, %2077, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2671)
    %2079 = stablehlo.reshape %2078 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2672)
    %2080 = stablehlo.reshape %arg462 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2673)
    %2081 = stablehlo.custom_call @tt.mark_argument(%2080) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2674)
    %2082 = stablehlo.reshape %2081 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2675)
    %2083 = stablehlo.broadcast_in_dim %2082, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2676)
    %2084 = stablehlo.add %2079, %2083 : tensor<1x257x1280xbf16> loc(#loc2677)
    %2085 = stablehlo.reshape %2084 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2678)
    %2086 = stablehlo.transpose %2085, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2679)
    %2087 = stablehlo.convert %2086 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2680)
    %2088 = stablehlo.multiply %2087, %cst_6 : tensor<1x16x257x80xf32> loc(#loc2681)
    %2089 = stablehlo.reshape %arg461 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2682)
    %2090 = stablehlo.custom_call @tt.mark_argument(%2089) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2683)
    %2091 = stablehlo.reshape %2090 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2684)
    %2092 = stablehlo.transpose %2091, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2685)
    %2093 = stablehlo.dot_general %2073, %2092, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2686)
    %2094 = stablehlo.reshape %2093 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2687)
    %2095 = stablehlo.reshape %arg460 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2688)
    %2096 = stablehlo.custom_call @tt.mark_argument(%2095) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2689)
    %2097 = stablehlo.reshape %2096 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2690)
    %2098 = stablehlo.broadcast_in_dim %2097, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2691)
    %2099 = stablehlo.add %2094, %2098 : tensor<1x257x1280xbf16> loc(#loc2692)
    %2100 = stablehlo.reshape %2099 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2693)
    %2101 = stablehlo.transpose %2100, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2694)
    %2102 = stablehlo.convert %2101 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2695)
    %2103 = stablehlo.transpose %2102, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc2696)
    %2104 = stablehlo.multiply %2103, %cst_5 : tensor<1x16x80x257xf32> loc(#loc2697)
    %2105 = stablehlo.dot_general %2088, %2104, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2698)
    %2106 = stablehlo.convert %2105 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc2699)
    %2107 = stablehlo.compare  EQ, %2106, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc2700)
    %2108 = stablehlo.not %2107 : tensor<1x16x257x257xi1> loc(#loc2701)
    %2109 = stablehlo.reduce(%2108 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.7836"), %arg559: tensor<i1> loc("reduce.7836"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc2703)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc2704)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc2702)
    %2110 = stablehlo.reshape %2109 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc2705)
    %2111 = stablehlo.not %2110 : tensor<1x16x257x1xi1> loc(#loc2706)
    %2112 = stablehlo.reshape %2111 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc2707)
    %2113 = stablehlo.broadcast_in_dim %2112, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc2708)
    %2114 = stablehlo.reduce(%2105 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2709)
    %2115 = stablehlo.broadcast_in_dim %2114, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2710)
    %2116 = stablehlo.subtract %2105, %2115 : tensor<1x16x257x257xf32> loc(#loc2711)
    %2117 = stablehlo.exponential %2116 : tensor<1x16x257x257xf32> loc(#loc2712)
    %2118 = stablehlo.reduce(%2117 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2713)
    %2119 = stablehlo.broadcast_in_dim %2118, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2714)
    %2120 = stablehlo.divide %2117, %2119 : tensor<1x16x257x257xf32> loc(#loc2715)
    %2121 = stablehlo.select %2113, %cst_3, %2120 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc2716)
    %2122 = stablehlo.reshape %arg178 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2717)
    %2123 = stablehlo.custom_call @tt.mark_argument(%2122) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2718)
    %2124 = stablehlo.reshape %2123 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2719)
    %2125 = stablehlo.transpose %2124, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2720)
    %2126 = stablehlo.dot_general %2073, %2125, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2721)
    %2127 = stablehlo.reshape %2126 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2722)
    %2128 = stablehlo.reshape %arg177 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2723)
    %2129 = stablehlo.custom_call @tt.mark_argument(%2128) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2724)
    %2130 = stablehlo.reshape %2129 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2725)
    %2131 = stablehlo.broadcast_in_dim %2130, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2726)
    %2132 = stablehlo.add %2127, %2131 : tensor<1x257x1280xbf16> loc(#loc2727)
    %2133 = stablehlo.reshape %2132 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2728)
    %2134 = stablehlo.transpose %2133, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2729)
    %2135 = stablehlo.convert %2134 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2730)
    %2136 = stablehlo.dot_general %2121, %2135, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc2731)
    %2137 = stablehlo.convert %2136 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc2732)
    %2138 = stablehlo.transpose %2137, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2733)
    %2139 = stablehlo.reshape %2138 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc2734)
    %2140 = stablehlo.reshape %arg176 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2735)
    %2141 = stablehlo.custom_call @tt.mark_argument(%2140) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2736)
    %2142 = stablehlo.reshape %2141 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2737)
    %2143 = stablehlo.transpose %2142, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2738)
    %2144 = stablehlo.dot_general %2139, %2143, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2739)
    %2145 = stablehlo.reshape %2144 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2740)
    %2146 = stablehlo.reshape %arg175 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2741)
    %2147 = stablehlo.custom_call @tt.mark_argument(%2146) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2742)
    %2148 = stablehlo.reshape %2147 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2743)
    %2149 = stablehlo.broadcast_in_dim %2148, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2744)
    %2150 = stablehlo.add %2145, %2149 : tensor<1x257x1280xbf16> loc(#loc2745)
    %2151 = stablehlo.add %2065, %2150 : tensor<1x257x1280xbf16> loc(#loc2746)
    %2152 = stablehlo.reshape %arg174 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2747)
    %2153 = stablehlo.custom_call @tt.mark_argument(%2152) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2748)
    %2154 = stablehlo.reshape %2153 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2749)
    %2155 = stablehlo.reshape %arg173 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2750)
    %2156 = stablehlo.custom_call @tt.mark_argument(%2155) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2751)
    %2157 = stablehlo.reshape %2156 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2752)
    %2158 = stablehlo.composite "tenstorrent.layer_norm" %2151, %2154, %2157 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_50} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2753)
    %2159 = stablehlo.reshape %2158 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2754)
    %2160 = stablehlo.reshape %arg172 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2755)
    %2161 = stablehlo.custom_call @tt.mark_argument(%2160) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2756)
    %2162 = stablehlo.reshape %2161 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2757)
    %2163 = stablehlo.transpose %2162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc2758)
    %2164 = stablehlo.dot_general %2159, %2163, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2759)
    %2165 = stablehlo.reshape %2164 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2760)
    %2166 = stablehlo.reshape %arg171 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2761)
    %2167 = stablehlo.custom_call @tt.mark_argument(%2166) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2762)
    %2168 = stablehlo.reshape %2167 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2763)
    %2169 = stablehlo.broadcast_in_dim %2168, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2764)
    %2170 = stablehlo.add %2165, %2169 : tensor<1x257x5120xbf16> loc(#loc2765)
    %2171 = stablehlo.composite "tenstorrent.gelu" %2170 {decomposition = @tenstorrent.gelu.impl_32} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2766)
    %2172 = stablehlo.reshape %2171 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2767)
    %2173 = stablehlo.reshape %arg170 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2768)
    %2174 = stablehlo.custom_call @tt.mark_argument(%2173) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2769)
    %2175 = stablehlo.reshape %2174 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2770)
    %2176 = stablehlo.transpose %2175, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc2771)
    %2177 = stablehlo.dot_general %2172, %2176, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2772)
    %2178 = stablehlo.reshape %2177 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2773)
    %2179 = stablehlo.reshape %arg169 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2774)
    %2180 = stablehlo.custom_call @tt.mark_argument(%2179) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2775)
    %2181 = stablehlo.reshape %2180 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2776)
    %2182 = stablehlo.broadcast_in_dim %2181, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2777)
    %2183 = stablehlo.add %2178, %2182 : tensor<1x257x1280xbf16> loc(#loc2778)
    %2184 = stablehlo.add %2151, %2183 : tensor<1x257x1280xbf16> loc(#loc2779)
    %2185 = stablehlo.reshape %arg168 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2780)
    %2186 = stablehlo.custom_call @tt.mark_argument(%2185) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2781)
    %2187 = stablehlo.reshape %2186 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2782)
    %2188 = stablehlo.reshape %arg167 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2783)
    %2189 = stablehlo.custom_call @tt.mark_argument(%2188) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2784)
    %2190 = stablehlo.reshape %2189 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2785)
    %2191 = stablehlo.composite "tenstorrent.layer_norm" %2184, %2187, %2190 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_67} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2786)
    %2192 = stablehlo.reshape %2191 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2787)
    %2193 = stablehlo.reshape %arg467 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2788)
    %2194 = stablehlo.custom_call @tt.mark_argument(%2193) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2789)
    %2195 = stablehlo.reshape %2194 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2790)
    %2196 = stablehlo.transpose %2195, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2791)
    %2197 = stablehlo.dot_general %2192, %2196, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2792)
    %2198 = stablehlo.reshape %2197 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2793)
    %2199 = stablehlo.reshape %arg466 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2794)
    %2200 = stablehlo.custom_call @tt.mark_argument(%2199) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2795)
    %2201 = stablehlo.reshape %2200 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2796)
    %2202 = stablehlo.broadcast_in_dim %2201, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2797)
    %2203 = stablehlo.add %2198, %2202 : tensor<1x257x1280xbf16> loc(#loc2798)
    %2204 = stablehlo.reshape %2203 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2799)
    %2205 = stablehlo.transpose %2204, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2800)
    %2206 = stablehlo.convert %2205 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2801)
    %2207 = stablehlo.multiply %2206, %cst_6 : tensor<1x16x257x80xf32> loc(#loc2802)
    %2208 = stablehlo.reshape %arg465 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2803)
    %2209 = stablehlo.custom_call @tt.mark_argument(%2208) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2804)
    %2210 = stablehlo.reshape %2209 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2805)
    %2211 = stablehlo.transpose %2210, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2806)
    %2212 = stablehlo.dot_general %2192, %2211, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2807)
    %2213 = stablehlo.reshape %2212 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2808)
    %2214 = stablehlo.reshape %arg464 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2809)
    %2215 = stablehlo.custom_call @tt.mark_argument(%2214) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2810)
    %2216 = stablehlo.reshape %2215 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2811)
    %2217 = stablehlo.broadcast_in_dim %2216, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2812)
    %2218 = stablehlo.add %2213, %2217 : tensor<1x257x1280xbf16> loc(#loc2813)
    %2219 = stablehlo.reshape %2218 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2814)
    %2220 = stablehlo.transpose %2219, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2815)
    %2221 = stablehlo.convert %2220 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2816)
    %2222 = stablehlo.transpose %2221, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc2817)
    %2223 = stablehlo.multiply %2222, %cst_5 : tensor<1x16x80x257xf32> loc(#loc2818)
    %2224 = stablehlo.dot_general %2207, %2223, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2819)
    %2225 = stablehlo.convert %2224 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc2820)
    %2226 = stablehlo.compare  EQ, %2225, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc2821)
    %2227 = stablehlo.not %2226 : tensor<1x16x257x257xi1> loc(#loc2822)
    %2228 = stablehlo.reduce(%2227 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.8146"), %arg559: tensor<i1> loc("reduce.8146"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc2824)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc2825)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc2823)
    %2229 = stablehlo.reshape %2228 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc2826)
    %2230 = stablehlo.not %2229 : tensor<1x16x257x1xi1> loc(#loc2827)
    %2231 = stablehlo.reshape %2230 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc2828)
    %2232 = stablehlo.broadcast_in_dim %2231, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc2829)
    %2233 = stablehlo.reduce(%2224 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2830)
    %2234 = stablehlo.broadcast_in_dim %2233, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2831)
    %2235 = stablehlo.subtract %2224, %2234 : tensor<1x16x257x257xf32> loc(#loc2832)
    %2236 = stablehlo.exponential %2235 : tensor<1x16x257x257xf32> loc(#loc2833)
    %2237 = stablehlo.reduce(%2236 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2834)
    %2238 = stablehlo.broadcast_in_dim %2237, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2835)
    %2239 = stablehlo.divide %2236, %2238 : tensor<1x16x257x257xf32> loc(#loc2836)
    %2240 = stablehlo.select %2232, %cst_3, %2239 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc2837)
    %2241 = stablehlo.reshape %arg166 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2838)
    %2242 = stablehlo.custom_call @tt.mark_argument(%2241) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2839)
    %2243 = stablehlo.reshape %2242 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2840)
    %2244 = stablehlo.transpose %2243, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2841)
    %2245 = stablehlo.dot_general %2192, %2244, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2842)
    %2246 = stablehlo.reshape %2245 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2843)
    %2247 = stablehlo.reshape %arg165 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2844)
    %2248 = stablehlo.custom_call @tt.mark_argument(%2247) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2845)
    %2249 = stablehlo.reshape %2248 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2846)
    %2250 = stablehlo.broadcast_in_dim %2249, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2847)
    %2251 = stablehlo.add %2246, %2250 : tensor<1x257x1280xbf16> loc(#loc2848)
    %2252 = stablehlo.reshape %2251 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2849)
    %2253 = stablehlo.transpose %2252, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2850)
    %2254 = stablehlo.convert %2253 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2851)
    %2255 = stablehlo.dot_general %2240, %2254, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc2852)
    %2256 = stablehlo.convert %2255 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc2853)
    %2257 = stablehlo.transpose %2256, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2854)
    %2258 = stablehlo.reshape %2257 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc2855)
    %2259 = stablehlo.reshape %arg164 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2856)
    %2260 = stablehlo.custom_call @tt.mark_argument(%2259) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2857)
    %2261 = stablehlo.reshape %2260 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2858)
    %2262 = stablehlo.transpose %2261, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2859)
    %2263 = stablehlo.dot_general %2258, %2262, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2860)
    %2264 = stablehlo.reshape %2263 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2861)
    %2265 = stablehlo.reshape %arg163 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2862)
    %2266 = stablehlo.custom_call @tt.mark_argument(%2265) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2863)
    %2267 = stablehlo.reshape %2266 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2864)
    %2268 = stablehlo.broadcast_in_dim %2267, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2865)
    %2269 = stablehlo.add %2264, %2268 : tensor<1x257x1280xbf16> loc(#loc2866)
    %2270 = stablehlo.add %2184, %2269 : tensor<1x257x1280xbf16> loc(#loc2867)
    %2271 = stablehlo.reshape %arg162 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2868)
    %2272 = stablehlo.custom_call @tt.mark_argument(%2271) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2869)
    %2273 = stablehlo.reshape %2272 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2870)
    %2274 = stablehlo.reshape %arg161 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2871)
    %2275 = stablehlo.custom_call @tt.mark_argument(%2274) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2872)
    %2276 = stablehlo.reshape %2275 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2873)
    %2277 = stablehlo.composite "tenstorrent.layer_norm" %2270, %2273, %2276 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_74} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2874)
    %2278 = stablehlo.reshape %2277 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2875)
    %2279 = stablehlo.reshape %arg160 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2876)
    %2280 = stablehlo.custom_call @tt.mark_argument(%2279) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2877)
    %2281 = stablehlo.reshape %2280 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2878)
    %2282 = stablehlo.transpose %2281, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc2879)
    %2283 = stablehlo.dot_general %2278, %2282, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2880)
    %2284 = stablehlo.reshape %2283 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2881)
    %2285 = stablehlo.reshape %arg159 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2882)
    %2286 = stablehlo.custom_call @tt.mark_argument(%2285) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2883)
    %2287 = stablehlo.reshape %2286 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2884)
    %2288 = stablehlo.broadcast_in_dim %2287, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2885)
    %2289 = stablehlo.add %2284, %2288 : tensor<1x257x5120xbf16> loc(#loc2886)
    %2290 = stablehlo.composite "tenstorrent.gelu" %2289 {decomposition = @tenstorrent.gelu.impl_33} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2887)
    %2291 = stablehlo.reshape %2290 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc2888)
    %2292 = stablehlo.reshape %arg158 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2889)
    %2293 = stablehlo.custom_call @tt.mark_argument(%2292) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2890)
    %2294 = stablehlo.reshape %2293 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2891)
    %2295 = stablehlo.transpose %2294, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc2892)
    %2296 = stablehlo.dot_general %2291, %2295, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2893)
    %2297 = stablehlo.reshape %2296 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2894)
    %2298 = stablehlo.reshape %arg157 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2895)
    %2299 = stablehlo.custom_call @tt.mark_argument(%2298) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2896)
    %2300 = stablehlo.reshape %2299 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2897)
    %2301 = stablehlo.broadcast_in_dim %2300, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2898)
    %2302 = stablehlo.add %2297, %2301 : tensor<1x257x1280xbf16> loc(#loc2899)
    %2303 = stablehlo.add %2270, %2302 : tensor<1x257x1280xbf16> loc(#loc2900)
    %2304 = stablehlo.reshape %arg156 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2901)
    %2305 = stablehlo.custom_call @tt.mark_argument(%2304) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2902)
    %2306 = stablehlo.reshape %2305 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2903)
    %2307 = stablehlo.reshape %arg155 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2904)
    %2308 = stablehlo.custom_call @tt.mark_argument(%2307) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2905)
    %2309 = stablehlo.reshape %2308 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2906)
    %2310 = stablehlo.composite "tenstorrent.layer_norm" %2303, %2306, %2309 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_59} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2907)
    %2311 = stablehlo.reshape %2310 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2908)
    %2312 = stablehlo.reshape %arg471 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2909)
    %2313 = stablehlo.custom_call @tt.mark_argument(%2312) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2910)
    %2314 = stablehlo.reshape %2313 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2911)
    %2315 = stablehlo.transpose %2314, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2912)
    %2316 = stablehlo.dot_general %2311, %2315, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2913)
    %2317 = stablehlo.reshape %2316 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2914)
    %2318 = stablehlo.reshape %arg470 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2915)
    %2319 = stablehlo.custom_call @tt.mark_argument(%2318) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2916)
    %2320 = stablehlo.reshape %2319 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2917)
    %2321 = stablehlo.broadcast_in_dim %2320, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2918)
    %2322 = stablehlo.add %2317, %2321 : tensor<1x257x1280xbf16> loc(#loc2919)
    %2323 = stablehlo.reshape %2322 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2920)
    %2324 = stablehlo.transpose %2323, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2921)
    %2325 = stablehlo.convert %2324 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2922)
    %2326 = stablehlo.multiply %2325, %cst_6 : tensor<1x16x257x80xf32> loc(#loc2923)
    %2327 = stablehlo.reshape %arg469 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2924)
    %2328 = stablehlo.custom_call @tt.mark_argument(%2327) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2925)
    %2329 = stablehlo.reshape %2328 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2926)
    %2330 = stablehlo.transpose %2329, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2927)
    %2331 = stablehlo.dot_general %2311, %2330, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2928)
    %2332 = stablehlo.reshape %2331 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2929)
    %2333 = stablehlo.reshape %arg468 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2930)
    %2334 = stablehlo.custom_call @tt.mark_argument(%2333) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2931)
    %2335 = stablehlo.reshape %2334 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2932)
    %2336 = stablehlo.broadcast_in_dim %2335, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2933)
    %2337 = stablehlo.add %2332, %2336 : tensor<1x257x1280xbf16> loc(#loc2934)
    %2338 = stablehlo.reshape %2337 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2935)
    %2339 = stablehlo.transpose %2338, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2936)
    %2340 = stablehlo.convert %2339 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2937)
    %2341 = stablehlo.transpose %2340, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc2938)
    %2342 = stablehlo.multiply %2341, %cst_5 : tensor<1x16x80x257xf32> loc(#loc2939)
    %2343 = stablehlo.dot_general %2326, %2342, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2940)
    %2344 = stablehlo.convert %2343 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc2941)
    %2345 = stablehlo.compare  EQ, %2344, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc2942)
    %2346 = stablehlo.not %2345 : tensor<1x16x257x257xi1> loc(#loc2943)
    %2347 = stablehlo.reduce(%2346 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.8456"), %arg559: tensor<i1> loc("reduce.8456"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc2945)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc2946)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc2944)
    %2348 = stablehlo.reshape %2347 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc2947)
    %2349 = stablehlo.not %2348 : tensor<1x16x257x1xi1> loc(#loc2948)
    %2350 = stablehlo.reshape %2349 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc2949)
    %2351 = stablehlo.broadcast_in_dim %2350, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc2950)
    %2352 = stablehlo.reduce(%2343 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2951)
    %2353 = stablehlo.broadcast_in_dim %2352, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2952)
    %2354 = stablehlo.subtract %2343, %2353 : tensor<1x16x257x257xf32> loc(#loc2953)
    %2355 = stablehlo.exponential %2354 : tensor<1x16x257x257xf32> loc(#loc2954)
    %2356 = stablehlo.reduce(%2355 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc2955)
    %2357 = stablehlo.broadcast_in_dim %2356, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc2956)
    %2358 = stablehlo.divide %2355, %2357 : tensor<1x16x257x257xf32> loc(#loc2957)
    %2359 = stablehlo.select %2351, %cst_3, %2358 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc2958)
    %2360 = stablehlo.reshape %arg154 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2959)
    %2361 = stablehlo.custom_call @tt.mark_argument(%2360) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2960)
    %2362 = stablehlo.reshape %2361 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2961)
    %2363 = stablehlo.transpose %2362, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2962)
    %2364 = stablehlo.dot_general %2311, %2363, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2963)
    %2365 = stablehlo.reshape %2364 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2964)
    %2366 = stablehlo.reshape %arg153 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2965)
    %2367 = stablehlo.custom_call @tt.mark_argument(%2366) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2966)
    %2368 = stablehlo.reshape %2367 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2967)
    %2369 = stablehlo.broadcast_in_dim %2368, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2968)
    %2370 = stablehlo.add %2365, %2369 : tensor<1x257x1280xbf16> loc(#loc2969)
    %2371 = stablehlo.reshape %2370 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2970)
    %2372 = stablehlo.transpose %2371, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc2971)
    %2373 = stablehlo.convert %2372 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc2972)
    %2374 = stablehlo.dot_general %2359, %2373, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc2973)
    %2375 = stablehlo.convert %2374 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc2974)
    %2376 = stablehlo.transpose %2375, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc2975)
    %2377 = stablehlo.reshape %2376 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc2976)
    %2378 = stablehlo.reshape %arg152 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2977)
    %2379 = stablehlo.custom_call @tt.mark_argument(%2378) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2978)
    %2380 = stablehlo.reshape %2379 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2979)
    %2381 = stablehlo.transpose %2380, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2980)
    %2382 = stablehlo.dot_general %2377, %2381, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2981)
    %2383 = stablehlo.reshape %2382 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2982)
    %2384 = stablehlo.reshape %arg151 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2983)
    %2385 = stablehlo.custom_call @tt.mark_argument(%2384) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2984)
    %2386 = stablehlo.reshape %2385 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2985)
    %2387 = stablehlo.broadcast_in_dim %2386, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2986)
    %2388 = stablehlo.add %2383, %2387 : tensor<1x257x1280xbf16> loc(#loc2987)
    %2389 = stablehlo.add %2303, %2388 : tensor<1x257x1280xbf16> loc(#loc2988)
    %2390 = stablehlo.reshape %arg150 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2989)
    %2391 = stablehlo.custom_call @tt.mark_argument(%2390) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2990)
    %2392 = stablehlo.reshape %2391 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2991)
    %2393 = stablehlo.reshape %arg149 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2992)
    %2394 = stablehlo.custom_call @tt.mark_argument(%2393) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2993)
    %2395 = stablehlo.reshape %2394 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2994)
    %2396 = stablehlo.composite "tenstorrent.layer_norm" %2389, %2392, %2395 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_21} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2995)
    %2397 = stablehlo.reshape %2396 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2996)
    %2398 = stablehlo.reshape %arg148 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2997)
    %2399 = stablehlo.custom_call @tt.mark_argument(%2398) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2998)
    %2400 = stablehlo.reshape %2399 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2999)
    %2401 = stablehlo.transpose %2400, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc3000)
    %2402 = stablehlo.dot_general %2397, %2401, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3001)
    %2403 = stablehlo.reshape %2402 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3002)
    %2404 = stablehlo.reshape %arg147 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3003)
    %2405 = stablehlo.custom_call @tt.mark_argument(%2404) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3004)
    %2406 = stablehlo.reshape %2405 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3005)
    %2407 = stablehlo.broadcast_in_dim %2406, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3006)
    %2408 = stablehlo.add %2403, %2407 : tensor<1x257x5120xbf16> loc(#loc3007)
    %2409 = stablehlo.composite "tenstorrent.gelu" %2408 {decomposition = @tenstorrent.gelu.impl_15} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3008)
    %2410 = stablehlo.reshape %2409 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3009)
    %2411 = stablehlo.reshape %arg146 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3010)
    %2412 = stablehlo.custom_call @tt.mark_argument(%2411) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3011)
    %2413 = stablehlo.reshape %2412 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3012)
    %2414 = stablehlo.transpose %2413, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc3013)
    %2415 = stablehlo.dot_general %2410, %2414, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3014)
    %2416 = stablehlo.reshape %2415 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3015)
    %2417 = stablehlo.reshape %arg145 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3016)
    %2418 = stablehlo.custom_call @tt.mark_argument(%2417) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3017)
    %2419 = stablehlo.reshape %2418 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3018)
    %2420 = stablehlo.broadcast_in_dim %2419, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3019)
    %2421 = stablehlo.add %2416, %2420 : tensor<1x257x1280xbf16> loc(#loc3020)
    %2422 = stablehlo.add %2389, %2421 : tensor<1x257x1280xbf16> loc(#loc3021)
    %2423 = stablehlo.reshape %arg144 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3022)
    %2424 = stablehlo.custom_call @tt.mark_argument(%2423) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3023)
    %2425 = stablehlo.reshape %2424 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3024)
    %2426 = stablehlo.reshape %arg143 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3025)
    %2427 = stablehlo.custom_call @tt.mark_argument(%2426) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3026)
    %2428 = stablehlo.reshape %2427 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3027)
    %2429 = stablehlo.composite "tenstorrent.layer_norm" %2422, %2425, %2428 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_20} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3028)
    %2430 = stablehlo.reshape %2429 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3029)
    %2431 = stablehlo.reshape %arg475 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3030)
    %2432 = stablehlo.custom_call @tt.mark_argument(%2431) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3031)
    %2433 = stablehlo.reshape %2432 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3032)
    %2434 = stablehlo.transpose %2433, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3033)
    %2435 = stablehlo.dot_general %2430, %2434, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3034)
    %2436 = stablehlo.reshape %2435 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3035)
    %2437 = stablehlo.reshape %arg474 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3036)
    %2438 = stablehlo.custom_call @tt.mark_argument(%2437) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3037)
    %2439 = stablehlo.reshape %2438 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3038)
    %2440 = stablehlo.broadcast_in_dim %2439, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3039)
    %2441 = stablehlo.add %2436, %2440 : tensor<1x257x1280xbf16> loc(#loc3040)
    %2442 = stablehlo.reshape %2441 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3041)
    %2443 = stablehlo.transpose %2442, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3042)
    %2444 = stablehlo.convert %2443 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3043)
    %2445 = stablehlo.multiply %2444, %cst_6 : tensor<1x16x257x80xf32> loc(#loc3044)
    %2446 = stablehlo.reshape %arg473 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3045)
    %2447 = stablehlo.custom_call @tt.mark_argument(%2446) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3046)
    %2448 = stablehlo.reshape %2447 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3047)
    %2449 = stablehlo.transpose %2448, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3048)
    %2450 = stablehlo.dot_general %2430, %2449, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3049)
    %2451 = stablehlo.reshape %2450 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3050)
    %2452 = stablehlo.reshape %arg472 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3051)
    %2453 = stablehlo.custom_call @tt.mark_argument(%2452) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3052)
    %2454 = stablehlo.reshape %2453 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3053)
    %2455 = stablehlo.broadcast_in_dim %2454, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3054)
    %2456 = stablehlo.add %2451, %2455 : tensor<1x257x1280xbf16> loc(#loc3055)
    %2457 = stablehlo.reshape %2456 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3056)
    %2458 = stablehlo.transpose %2457, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3057)
    %2459 = stablehlo.convert %2458 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3058)
    %2460 = stablehlo.transpose %2459, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc3059)
    %2461 = stablehlo.multiply %2460, %cst_5 : tensor<1x16x80x257xf32> loc(#loc3060)
    %2462 = stablehlo.dot_general %2445, %2461, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3061)
    %2463 = stablehlo.convert %2462 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc3062)
    %2464 = stablehlo.compare  EQ, %2463, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc3063)
    %2465 = stablehlo.not %2464 : tensor<1x16x257x257xi1> loc(#loc3064)
    %2466 = stablehlo.reduce(%2465 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.8766"), %arg559: tensor<i1> loc("reduce.8766"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc3066)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc3067)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc3065)
    %2467 = stablehlo.reshape %2466 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc3068)
    %2468 = stablehlo.not %2467 : tensor<1x16x257x1xi1> loc(#loc3069)
    %2469 = stablehlo.reshape %2468 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc3070)
    %2470 = stablehlo.broadcast_in_dim %2469, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc3071)
    %2471 = stablehlo.reduce(%2462 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3072)
    %2472 = stablehlo.broadcast_in_dim %2471, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3073)
    %2473 = stablehlo.subtract %2462, %2472 : tensor<1x16x257x257xf32> loc(#loc3074)
    %2474 = stablehlo.exponential %2473 : tensor<1x16x257x257xf32> loc(#loc3075)
    %2475 = stablehlo.reduce(%2474 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3076)
    %2476 = stablehlo.broadcast_in_dim %2475, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3077)
    %2477 = stablehlo.divide %2474, %2476 : tensor<1x16x257x257xf32> loc(#loc3078)
    %2478 = stablehlo.select %2470, %cst_3, %2477 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc3079)
    %2479 = stablehlo.reshape %arg142 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3080)
    %2480 = stablehlo.custom_call @tt.mark_argument(%2479) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3081)
    %2481 = stablehlo.reshape %2480 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3082)
    %2482 = stablehlo.transpose %2481, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3083)
    %2483 = stablehlo.dot_general %2430, %2482, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3084)
    %2484 = stablehlo.reshape %2483 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3085)
    %2485 = stablehlo.reshape %arg141 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3086)
    %2486 = stablehlo.custom_call @tt.mark_argument(%2485) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3087)
    %2487 = stablehlo.reshape %2486 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3088)
    %2488 = stablehlo.broadcast_in_dim %2487, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3089)
    %2489 = stablehlo.add %2484, %2488 : tensor<1x257x1280xbf16> loc(#loc3090)
    %2490 = stablehlo.reshape %2489 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3091)
    %2491 = stablehlo.transpose %2490, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3092)
    %2492 = stablehlo.convert %2491 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3093)
    %2493 = stablehlo.dot_general %2478, %2492, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc3094)
    %2494 = stablehlo.convert %2493 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc3095)
    %2495 = stablehlo.transpose %2494, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3096)
    %2496 = stablehlo.reshape %2495 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc3097)
    %2497 = stablehlo.reshape %arg140 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3098)
    %2498 = stablehlo.custom_call @tt.mark_argument(%2497) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3099)
    %2499 = stablehlo.reshape %2498 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3100)
    %2500 = stablehlo.transpose %2499, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3101)
    %2501 = stablehlo.dot_general %2496, %2500, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3102)
    %2502 = stablehlo.reshape %2501 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3103)
    %2503 = stablehlo.reshape %arg139 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3104)
    %2504 = stablehlo.custom_call @tt.mark_argument(%2503) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3105)
    %2505 = stablehlo.reshape %2504 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3106)
    %2506 = stablehlo.broadcast_in_dim %2505, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3107)
    %2507 = stablehlo.add %2502, %2506 : tensor<1x257x1280xbf16> loc(#loc3108)
    %2508 = stablehlo.add %2422, %2507 : tensor<1x257x1280xbf16> loc(#loc3109)
    %2509 = stablehlo.reshape %arg138 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3110)
    %2510 = stablehlo.custom_call @tt.mark_argument(%2509) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3111)
    %2511 = stablehlo.reshape %2510 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3112)
    %2512 = stablehlo.reshape %arg137 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3113)
    %2513 = stablehlo.custom_call @tt.mark_argument(%2512) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3114)
    %2514 = stablehlo.reshape %2513 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3115)
    %2515 = stablehlo.composite "tenstorrent.layer_norm" %2508, %2511, %2514 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_63} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3116)
    %2516 = stablehlo.reshape %2515 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3117)
    %2517 = stablehlo.reshape %arg136 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3118)
    %2518 = stablehlo.custom_call @tt.mark_argument(%2517) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3119)
    %2519 = stablehlo.reshape %2518 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3120)
    %2520 = stablehlo.transpose %2519, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc3121)
    %2521 = stablehlo.dot_general %2516, %2520, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3122)
    %2522 = stablehlo.reshape %2521 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3123)
    %2523 = stablehlo.reshape %arg135 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3124)
    %2524 = stablehlo.custom_call @tt.mark_argument(%2523) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3125)
    %2525 = stablehlo.reshape %2524 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3126)
    %2526 = stablehlo.broadcast_in_dim %2525, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3127)
    %2527 = stablehlo.add %2522, %2526 : tensor<1x257x5120xbf16> loc(#loc3128)
    %2528 = stablehlo.composite "tenstorrent.gelu" %2527 {decomposition = @tenstorrent.gelu.impl_7} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3129)
    %2529 = stablehlo.reshape %2528 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3130)
    %2530 = stablehlo.reshape %arg134 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3131)
    %2531 = stablehlo.custom_call @tt.mark_argument(%2530) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3132)
    %2532 = stablehlo.reshape %2531 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3133)
    %2533 = stablehlo.transpose %2532, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc3134)
    %2534 = stablehlo.dot_general %2529, %2533, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3135)
    %2535 = stablehlo.reshape %2534 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3136)
    %2536 = stablehlo.reshape %arg133 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3137)
    %2537 = stablehlo.custom_call @tt.mark_argument(%2536) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3138)
    %2538 = stablehlo.reshape %2537 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3139)
    %2539 = stablehlo.broadcast_in_dim %2538, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3140)
    %2540 = stablehlo.add %2535, %2539 : tensor<1x257x1280xbf16> loc(#loc3141)
    %2541 = stablehlo.add %2508, %2540 : tensor<1x257x1280xbf16> loc(#loc3142)
    %2542 = stablehlo.reshape %arg132 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3143)
    %2543 = stablehlo.custom_call @tt.mark_argument(%2542) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3144)
    %2544 = stablehlo.reshape %2543 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3145)
    %2545 = stablehlo.reshape %arg131 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3146)
    %2546 = stablehlo.custom_call @tt.mark_argument(%2545) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3147)
    %2547 = stablehlo.reshape %2546 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3148)
    %2548 = stablehlo.composite "tenstorrent.layer_norm" %2541, %2544, %2547 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_18} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3149)
    %2549 = stablehlo.reshape %2548 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3150)
    %2550 = stablehlo.reshape %arg479 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3151)
    %2551 = stablehlo.custom_call @tt.mark_argument(%2550) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3152)
    %2552 = stablehlo.reshape %2551 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3153)
    %2553 = stablehlo.transpose %2552, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3154)
    %2554 = stablehlo.dot_general %2549, %2553, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3155)
    %2555 = stablehlo.reshape %2554 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3156)
    %2556 = stablehlo.reshape %arg478 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3157)
    %2557 = stablehlo.custom_call @tt.mark_argument(%2556) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3158)
    %2558 = stablehlo.reshape %2557 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3159)
    %2559 = stablehlo.broadcast_in_dim %2558, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3160)
    %2560 = stablehlo.add %2555, %2559 : tensor<1x257x1280xbf16> loc(#loc3161)
    %2561 = stablehlo.reshape %2560 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3162)
    %2562 = stablehlo.transpose %2561, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3163)
    %2563 = stablehlo.convert %2562 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3164)
    %2564 = stablehlo.multiply %2563, %cst_6 : tensor<1x16x257x80xf32> loc(#loc3165)
    %2565 = stablehlo.reshape %arg477 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3166)
    %2566 = stablehlo.custom_call @tt.mark_argument(%2565) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3167)
    %2567 = stablehlo.reshape %2566 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3168)
    %2568 = stablehlo.transpose %2567, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3169)
    %2569 = stablehlo.dot_general %2549, %2568, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3170)
    %2570 = stablehlo.reshape %2569 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3171)
    %2571 = stablehlo.reshape %arg476 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3172)
    %2572 = stablehlo.custom_call @tt.mark_argument(%2571) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3173)
    %2573 = stablehlo.reshape %2572 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3174)
    %2574 = stablehlo.broadcast_in_dim %2573, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3175)
    %2575 = stablehlo.add %2570, %2574 : tensor<1x257x1280xbf16> loc(#loc3176)
    %2576 = stablehlo.reshape %2575 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3177)
    %2577 = stablehlo.transpose %2576, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3178)
    %2578 = stablehlo.convert %2577 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3179)
    %2579 = stablehlo.transpose %2578, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc3180)
    %2580 = stablehlo.multiply %2579, %cst_5 : tensor<1x16x80x257xf32> loc(#loc3181)
    %2581 = stablehlo.dot_general %2564, %2580, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3182)
    %2582 = stablehlo.convert %2581 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc3183)
    %2583 = stablehlo.compare  EQ, %2582, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc3184)
    %2584 = stablehlo.not %2583 : tensor<1x16x257x257xi1> loc(#loc3185)
    %2585 = stablehlo.reduce(%2584 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.9076"), %arg559: tensor<i1> loc("reduce.9076"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc3187)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc3188)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc3186)
    %2586 = stablehlo.reshape %2585 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc3189)
    %2587 = stablehlo.not %2586 : tensor<1x16x257x1xi1> loc(#loc3190)
    %2588 = stablehlo.reshape %2587 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc3191)
    %2589 = stablehlo.broadcast_in_dim %2588, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc3192)
    %2590 = stablehlo.reduce(%2581 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3193)
    %2591 = stablehlo.broadcast_in_dim %2590, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3194)
    %2592 = stablehlo.subtract %2581, %2591 : tensor<1x16x257x257xf32> loc(#loc3195)
    %2593 = stablehlo.exponential %2592 : tensor<1x16x257x257xf32> loc(#loc3196)
    %2594 = stablehlo.reduce(%2593 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3197)
    %2595 = stablehlo.broadcast_in_dim %2594, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3198)
    %2596 = stablehlo.divide %2593, %2595 : tensor<1x16x257x257xf32> loc(#loc3199)
    %2597 = stablehlo.select %2589, %cst_3, %2596 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc3200)
    %2598 = stablehlo.reshape %arg130 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3201)
    %2599 = stablehlo.custom_call @tt.mark_argument(%2598) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3202)
    %2600 = stablehlo.reshape %2599 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3203)
    %2601 = stablehlo.transpose %2600, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3204)
    %2602 = stablehlo.dot_general %2549, %2601, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3205)
    %2603 = stablehlo.reshape %2602 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3206)
    %2604 = stablehlo.reshape %arg129 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3207)
    %2605 = stablehlo.custom_call @tt.mark_argument(%2604) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3208)
    %2606 = stablehlo.reshape %2605 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3209)
    %2607 = stablehlo.broadcast_in_dim %2606, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3210)
    %2608 = stablehlo.add %2603, %2607 : tensor<1x257x1280xbf16> loc(#loc3211)
    %2609 = stablehlo.reshape %2608 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3212)
    %2610 = stablehlo.transpose %2609, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3213)
    %2611 = stablehlo.convert %2610 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3214)
    %2612 = stablehlo.dot_general %2597, %2611, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc3215)
    %2613 = stablehlo.convert %2612 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc3216)
    %2614 = stablehlo.transpose %2613, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3217)
    %2615 = stablehlo.reshape %2614 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc3218)
    %2616 = stablehlo.reshape %arg128 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3219)
    %2617 = stablehlo.custom_call @tt.mark_argument(%2616) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3220)
    %2618 = stablehlo.reshape %2617 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3221)
    %2619 = stablehlo.transpose %2618, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3222)
    %2620 = stablehlo.dot_general %2615, %2619, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3223)
    %2621 = stablehlo.reshape %2620 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3224)
    %2622 = stablehlo.reshape %arg127 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3225)
    %2623 = stablehlo.custom_call @tt.mark_argument(%2622) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3226)
    %2624 = stablehlo.reshape %2623 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3227)
    %2625 = stablehlo.broadcast_in_dim %2624, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3228)
    %2626 = stablehlo.add %2621, %2625 : tensor<1x257x1280xbf16> loc(#loc3229)
    %2627 = stablehlo.add %2541, %2626 : tensor<1x257x1280xbf16> loc(#loc3230)
    %2628 = stablehlo.reshape %arg126 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3231)
    %2629 = stablehlo.custom_call @tt.mark_argument(%2628) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3232)
    %2630 = stablehlo.reshape %2629 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3233)
    %2631 = stablehlo.reshape %arg125 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3234)
    %2632 = stablehlo.custom_call @tt.mark_argument(%2631) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3235)
    %2633 = stablehlo.reshape %2632 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3236)
    %2634 = stablehlo.composite "tenstorrent.layer_norm" %2627, %2630, %2633 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_32} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3237)
    %2635 = stablehlo.reshape %2634 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3238)
    %2636 = stablehlo.reshape %arg124 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3239)
    %2637 = stablehlo.custom_call @tt.mark_argument(%2636) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3240)
    %2638 = stablehlo.reshape %2637 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3241)
    %2639 = stablehlo.transpose %2638, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc3242)
    %2640 = stablehlo.dot_general %2635, %2639, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3243)
    %2641 = stablehlo.reshape %2640 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3244)
    %2642 = stablehlo.reshape %arg123 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3245)
    %2643 = stablehlo.custom_call @tt.mark_argument(%2642) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3246)
    %2644 = stablehlo.reshape %2643 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3247)
    %2645 = stablehlo.broadcast_in_dim %2644, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3248)
    %2646 = stablehlo.add %2641, %2645 : tensor<1x257x5120xbf16> loc(#loc3249)
    %2647 = stablehlo.composite "tenstorrent.gelu" %2646 {decomposition = @tenstorrent.gelu.impl_30} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3250)
    %2648 = stablehlo.reshape %2647 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3251)
    %2649 = stablehlo.reshape %arg122 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3252)
    %2650 = stablehlo.custom_call @tt.mark_argument(%2649) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3253)
    %2651 = stablehlo.reshape %2650 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3254)
    %2652 = stablehlo.transpose %2651, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc3255)
    %2653 = stablehlo.dot_general %2648, %2652, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3256)
    %2654 = stablehlo.reshape %2653 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3257)
    %2655 = stablehlo.reshape %arg121 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3258)
    %2656 = stablehlo.custom_call @tt.mark_argument(%2655) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3259)
    %2657 = stablehlo.reshape %2656 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3260)
    %2658 = stablehlo.broadcast_in_dim %2657, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3261)
    %2659 = stablehlo.add %2654, %2658 : tensor<1x257x1280xbf16> loc(#loc3262)
    %2660 = stablehlo.add %2627, %2659 : tensor<1x257x1280xbf16> loc(#loc3263)
    %2661 = stablehlo.reshape %arg120 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3264)
    %2662 = stablehlo.custom_call @tt.mark_argument(%2661) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3265)
    %2663 = stablehlo.reshape %2662 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3266)
    %2664 = stablehlo.reshape %arg119 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3267)
    %2665 = stablehlo.custom_call @tt.mark_argument(%2664) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3268)
    %2666 = stablehlo.reshape %2665 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3269)
    %2667 = stablehlo.composite "tenstorrent.layer_norm" %2660, %2663, %2666 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_28} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3270)
    %2668 = stablehlo.reshape %2667 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3271)
    %2669 = stablehlo.reshape %arg483 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3272)
    %2670 = stablehlo.custom_call @tt.mark_argument(%2669) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3273)
    %2671 = stablehlo.reshape %2670 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3274)
    %2672 = stablehlo.transpose %2671, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3275)
    %2673 = stablehlo.dot_general %2668, %2672, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3276)
    %2674 = stablehlo.reshape %2673 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3277)
    %2675 = stablehlo.reshape %arg482 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3278)
    %2676 = stablehlo.custom_call @tt.mark_argument(%2675) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3279)
    %2677 = stablehlo.reshape %2676 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3280)
    %2678 = stablehlo.broadcast_in_dim %2677, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3281)
    %2679 = stablehlo.add %2674, %2678 : tensor<1x257x1280xbf16> loc(#loc3282)
    %2680 = stablehlo.reshape %2679 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3283)
    %2681 = stablehlo.transpose %2680, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3284)
    %2682 = stablehlo.convert %2681 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3285)
    %2683 = stablehlo.multiply %2682, %cst_6 : tensor<1x16x257x80xf32> loc(#loc3286)
    %2684 = stablehlo.reshape %arg481 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3287)
    %2685 = stablehlo.custom_call @tt.mark_argument(%2684) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3288)
    %2686 = stablehlo.reshape %2685 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3289)
    %2687 = stablehlo.transpose %2686, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3290)
    %2688 = stablehlo.dot_general %2668, %2687, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3291)
    %2689 = stablehlo.reshape %2688 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3292)
    %2690 = stablehlo.reshape %arg480 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3293)
    %2691 = stablehlo.custom_call @tt.mark_argument(%2690) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3294)
    %2692 = stablehlo.reshape %2691 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3295)
    %2693 = stablehlo.broadcast_in_dim %2692, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3296)
    %2694 = stablehlo.add %2689, %2693 : tensor<1x257x1280xbf16> loc(#loc3297)
    %2695 = stablehlo.reshape %2694 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3298)
    %2696 = stablehlo.transpose %2695, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3299)
    %2697 = stablehlo.convert %2696 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3300)
    %2698 = stablehlo.transpose %2697, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc3301)
    %2699 = stablehlo.multiply %2698, %cst_5 : tensor<1x16x80x257xf32> loc(#loc3302)
    %2700 = stablehlo.dot_general %2683, %2699, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3303)
    %2701 = stablehlo.convert %2700 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc3304)
    %2702 = stablehlo.compare  EQ, %2701, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc3305)
    %2703 = stablehlo.not %2702 : tensor<1x16x257x257xi1> loc(#loc3306)
    %2704 = stablehlo.reduce(%2703 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.9386"), %arg559: tensor<i1> loc("reduce.9386"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc3308)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc3309)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc3307)
    %2705 = stablehlo.reshape %2704 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc3310)
    %2706 = stablehlo.not %2705 : tensor<1x16x257x1xi1> loc(#loc3311)
    %2707 = stablehlo.reshape %2706 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc3312)
    %2708 = stablehlo.broadcast_in_dim %2707, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc3313)
    %2709 = stablehlo.reduce(%2700 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3314)
    %2710 = stablehlo.broadcast_in_dim %2709, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3315)
    %2711 = stablehlo.subtract %2700, %2710 : tensor<1x16x257x257xf32> loc(#loc3316)
    %2712 = stablehlo.exponential %2711 : tensor<1x16x257x257xf32> loc(#loc3317)
    %2713 = stablehlo.reduce(%2712 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3318)
    %2714 = stablehlo.broadcast_in_dim %2713, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3319)
    %2715 = stablehlo.divide %2712, %2714 : tensor<1x16x257x257xf32> loc(#loc3320)
    %2716 = stablehlo.select %2708, %cst_3, %2715 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc3321)
    %2717 = stablehlo.reshape %arg118 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3322)
    %2718 = stablehlo.custom_call @tt.mark_argument(%2717) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3323)
    %2719 = stablehlo.reshape %2718 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3324)
    %2720 = stablehlo.transpose %2719, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3325)
    %2721 = stablehlo.dot_general %2668, %2720, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3326)
    %2722 = stablehlo.reshape %2721 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3327)
    %2723 = stablehlo.reshape %arg117 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3328)
    %2724 = stablehlo.custom_call @tt.mark_argument(%2723) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3329)
    %2725 = stablehlo.reshape %2724 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3330)
    %2726 = stablehlo.broadcast_in_dim %2725, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3331)
    %2727 = stablehlo.add %2722, %2726 : tensor<1x257x1280xbf16> loc(#loc3332)
    %2728 = stablehlo.reshape %2727 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3333)
    %2729 = stablehlo.transpose %2728, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3334)
    %2730 = stablehlo.convert %2729 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3335)
    %2731 = stablehlo.dot_general %2716, %2730, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc3336)
    %2732 = stablehlo.convert %2731 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc3337)
    %2733 = stablehlo.transpose %2732, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3338)
    %2734 = stablehlo.reshape %2733 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc3339)
    %2735 = stablehlo.reshape %arg116 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3340)
    %2736 = stablehlo.custom_call @tt.mark_argument(%2735) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3341)
    %2737 = stablehlo.reshape %2736 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3342)
    %2738 = stablehlo.transpose %2737, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3343)
    %2739 = stablehlo.dot_general %2734, %2738, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3344)
    %2740 = stablehlo.reshape %2739 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3345)
    %2741 = stablehlo.reshape %arg115 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3346)
    %2742 = stablehlo.custom_call @tt.mark_argument(%2741) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3347)
    %2743 = stablehlo.reshape %2742 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3348)
    %2744 = stablehlo.broadcast_in_dim %2743, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3349)
    %2745 = stablehlo.add %2740, %2744 : tensor<1x257x1280xbf16> loc(#loc3350)
    %2746 = stablehlo.add %2660, %2745 : tensor<1x257x1280xbf16> loc(#loc3351)
    %2747 = stablehlo.reshape %arg114 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3352)
    %2748 = stablehlo.custom_call @tt.mark_argument(%2747) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3353)
    %2749 = stablehlo.reshape %2748 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3354)
    %2750 = stablehlo.reshape %arg113 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3355)
    %2751 = stablehlo.custom_call @tt.mark_argument(%2750) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3356)
    %2752 = stablehlo.reshape %2751 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3357)
    %2753 = stablehlo.composite "tenstorrent.layer_norm" %2746, %2749, %2752 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_62} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3358)
    %2754 = stablehlo.reshape %2753 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3359)
    %2755 = stablehlo.reshape %arg112 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3360)
    %2756 = stablehlo.custom_call @tt.mark_argument(%2755) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3361)
    %2757 = stablehlo.reshape %2756 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3362)
    %2758 = stablehlo.transpose %2757, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc3363)
    %2759 = stablehlo.dot_general %2754, %2758, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3364)
    %2760 = stablehlo.reshape %2759 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3365)
    %2761 = stablehlo.reshape %arg111 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3366)
    %2762 = stablehlo.custom_call @tt.mark_argument(%2761) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3367)
    %2763 = stablehlo.reshape %2762 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3368)
    %2764 = stablehlo.broadcast_in_dim %2763, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3369)
    %2765 = stablehlo.add %2760, %2764 : tensor<1x257x5120xbf16> loc(#loc3370)
    %2766 = stablehlo.composite "tenstorrent.gelu" %2765 {decomposition = @tenstorrent.gelu.impl_6} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3371)
    %2767 = stablehlo.reshape %2766 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3372)
    %2768 = stablehlo.reshape %arg110 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3373)
    %2769 = stablehlo.custom_call @tt.mark_argument(%2768) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3374)
    %2770 = stablehlo.reshape %2769 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3375)
    %2771 = stablehlo.transpose %2770, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc3376)
    %2772 = stablehlo.dot_general %2767, %2771, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3377)
    %2773 = stablehlo.reshape %2772 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3378)
    %2774 = stablehlo.reshape %arg109 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3379)
    %2775 = stablehlo.custom_call @tt.mark_argument(%2774) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3380)
    %2776 = stablehlo.reshape %2775 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3381)
    %2777 = stablehlo.broadcast_in_dim %2776, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3382)
    %2778 = stablehlo.add %2773, %2777 : tensor<1x257x1280xbf16> loc(#loc3383)
    %2779 = stablehlo.add %2746, %2778 : tensor<1x257x1280xbf16> loc(#loc3384)
    %2780 = stablehlo.reshape %arg108 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3385)
    %2781 = stablehlo.custom_call @tt.mark_argument(%2780) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3386)
    %2782 = stablehlo.reshape %2781 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3387)
    %2783 = stablehlo.reshape %arg107 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3388)
    %2784 = stablehlo.custom_call @tt.mark_argument(%2783) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3389)
    %2785 = stablehlo.reshape %2784 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3390)
    %2786 = stablehlo.composite "tenstorrent.layer_norm" %2779, %2782, %2785 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_57} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3391)
    %2787 = stablehlo.reshape %2786 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3392)
    %2788 = stablehlo.reshape %arg487 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3393)
    %2789 = stablehlo.custom_call @tt.mark_argument(%2788) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3394)
    %2790 = stablehlo.reshape %2789 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3395)
    %2791 = stablehlo.transpose %2790, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3396)
    %2792 = stablehlo.dot_general %2787, %2791, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3397)
    %2793 = stablehlo.reshape %2792 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3398)
    %2794 = stablehlo.reshape %arg486 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3399)
    %2795 = stablehlo.custom_call @tt.mark_argument(%2794) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3400)
    %2796 = stablehlo.reshape %2795 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3401)
    %2797 = stablehlo.broadcast_in_dim %2796, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3402)
    %2798 = stablehlo.add %2793, %2797 : tensor<1x257x1280xbf16> loc(#loc3403)
    %2799 = stablehlo.reshape %2798 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3404)
    %2800 = stablehlo.transpose %2799, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3405)
    %2801 = stablehlo.convert %2800 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3406)
    %2802 = stablehlo.multiply %2801, %cst_6 : tensor<1x16x257x80xf32> loc(#loc3407)
    %2803 = stablehlo.reshape %arg485 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3408)
    %2804 = stablehlo.custom_call @tt.mark_argument(%2803) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3409)
    %2805 = stablehlo.reshape %2804 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3410)
    %2806 = stablehlo.transpose %2805, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3411)
    %2807 = stablehlo.dot_general %2787, %2806, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3412)
    %2808 = stablehlo.reshape %2807 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3413)
    %2809 = stablehlo.reshape %arg484 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3414)
    %2810 = stablehlo.custom_call @tt.mark_argument(%2809) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3415)
    %2811 = stablehlo.reshape %2810 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3416)
    %2812 = stablehlo.broadcast_in_dim %2811, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3417)
    %2813 = stablehlo.add %2808, %2812 : tensor<1x257x1280xbf16> loc(#loc3418)
    %2814 = stablehlo.reshape %2813 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3419)
    %2815 = stablehlo.transpose %2814, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3420)
    %2816 = stablehlo.convert %2815 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3421)
    %2817 = stablehlo.transpose %2816, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc3422)
    %2818 = stablehlo.multiply %2817, %cst_5 : tensor<1x16x80x257xf32> loc(#loc3423)
    %2819 = stablehlo.dot_general %2802, %2818, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3424)
    %2820 = stablehlo.convert %2819 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc3425)
    %2821 = stablehlo.compare  EQ, %2820, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc3426)
    %2822 = stablehlo.not %2821 : tensor<1x16x257x257xi1> loc(#loc3427)
    %2823 = stablehlo.reduce(%2822 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.9696"), %arg559: tensor<i1> loc("reduce.9696"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc3429)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc3430)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc3428)
    %2824 = stablehlo.reshape %2823 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc3431)
    %2825 = stablehlo.not %2824 : tensor<1x16x257x1xi1> loc(#loc3432)
    %2826 = stablehlo.reshape %2825 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc3433)
    %2827 = stablehlo.broadcast_in_dim %2826, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc3434)
    %2828 = stablehlo.reduce(%2819 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3435)
    %2829 = stablehlo.broadcast_in_dim %2828, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3436)
    %2830 = stablehlo.subtract %2819, %2829 : tensor<1x16x257x257xf32> loc(#loc3437)
    %2831 = stablehlo.exponential %2830 : tensor<1x16x257x257xf32> loc(#loc3438)
    %2832 = stablehlo.reduce(%2831 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3439)
    %2833 = stablehlo.broadcast_in_dim %2832, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3440)
    %2834 = stablehlo.divide %2831, %2833 : tensor<1x16x257x257xf32> loc(#loc3441)
    %2835 = stablehlo.select %2827, %cst_3, %2834 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc3442)
    %2836 = stablehlo.reshape %arg106 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3443)
    %2837 = stablehlo.custom_call @tt.mark_argument(%2836) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3444)
    %2838 = stablehlo.reshape %2837 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3445)
    %2839 = stablehlo.transpose %2838, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3446)
    %2840 = stablehlo.dot_general %2787, %2839, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3447)
    %2841 = stablehlo.reshape %2840 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3448)
    %2842 = stablehlo.reshape %arg105 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3449)
    %2843 = stablehlo.custom_call @tt.mark_argument(%2842) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3450)
    %2844 = stablehlo.reshape %2843 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3451)
    %2845 = stablehlo.broadcast_in_dim %2844, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3452)
    %2846 = stablehlo.add %2841, %2845 : tensor<1x257x1280xbf16> loc(#loc3453)
    %2847 = stablehlo.reshape %2846 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3454)
    %2848 = stablehlo.transpose %2847, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3455)
    %2849 = stablehlo.convert %2848 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3456)
    %2850 = stablehlo.dot_general %2835, %2849, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc3457)
    %2851 = stablehlo.convert %2850 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc3458)
    %2852 = stablehlo.transpose %2851, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3459)
    %2853 = stablehlo.reshape %2852 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc3460)
    %2854 = stablehlo.reshape %arg104 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3461)
    %2855 = stablehlo.custom_call @tt.mark_argument(%2854) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3462)
    %2856 = stablehlo.reshape %2855 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3463)
    %2857 = stablehlo.transpose %2856, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3464)
    %2858 = stablehlo.dot_general %2853, %2857, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3465)
    %2859 = stablehlo.reshape %2858 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3466)
    %2860 = stablehlo.reshape %arg103 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3467)
    %2861 = stablehlo.custom_call @tt.mark_argument(%2860) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3468)
    %2862 = stablehlo.reshape %2861 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3469)
    %2863 = stablehlo.broadcast_in_dim %2862, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3470)
    %2864 = stablehlo.add %2859, %2863 : tensor<1x257x1280xbf16> loc(#loc3471)
    %2865 = stablehlo.add %2779, %2864 : tensor<1x257x1280xbf16> loc(#loc3472)
    %2866 = stablehlo.reshape %arg102 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3473)
    %2867 = stablehlo.custom_call @tt.mark_argument(%2866) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3474)
    %2868 = stablehlo.reshape %2867 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3475)
    %2869 = stablehlo.reshape %arg101 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3476)
    %2870 = stablehlo.custom_call @tt.mark_argument(%2869) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3477)
    %2871 = stablehlo.reshape %2870 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3478)
    %2872 = stablehlo.composite "tenstorrent.layer_norm" %2865, %2868, %2871 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_17} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3479)
    %2873 = stablehlo.reshape %2872 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3480)
    %2874 = stablehlo.reshape %arg100 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3481)
    %2875 = stablehlo.custom_call @tt.mark_argument(%2874) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3482)
    %2876 = stablehlo.reshape %2875 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3483)
    %2877 = stablehlo.transpose %2876, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc3484)
    %2878 = stablehlo.dot_general %2873, %2877, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3485)
    %2879 = stablehlo.reshape %2878 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3486)
    %2880 = stablehlo.reshape %arg99 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3487)
    %2881 = stablehlo.custom_call @tt.mark_argument(%2880) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3488)
    %2882 = stablehlo.reshape %2881 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3489)
    %2883 = stablehlo.broadcast_in_dim %2882, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3490)
    %2884 = stablehlo.add %2879, %2883 : tensor<1x257x5120xbf16> loc(#loc3491)
    %2885 = stablehlo.composite "tenstorrent.gelu" %2884 {decomposition = @tenstorrent.gelu.impl_19} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3492)
    %2886 = stablehlo.reshape %2885 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3493)
    %2887 = stablehlo.reshape %arg98 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3494)
    %2888 = stablehlo.custom_call @tt.mark_argument(%2887) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3495)
    %2889 = stablehlo.reshape %2888 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3496)
    %2890 = stablehlo.transpose %2889, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc3497)
    %2891 = stablehlo.dot_general %2886, %2890, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3498)
    %2892 = stablehlo.reshape %2891 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3499)
    %2893 = stablehlo.reshape %arg97 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3500)
    %2894 = stablehlo.custom_call @tt.mark_argument(%2893) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3501)
    %2895 = stablehlo.reshape %2894 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3502)
    %2896 = stablehlo.broadcast_in_dim %2895, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3503)
    %2897 = stablehlo.add %2892, %2896 : tensor<1x257x1280xbf16> loc(#loc3504)
    %2898 = stablehlo.add %2865, %2897 : tensor<1x257x1280xbf16> loc(#loc3505)
    %2899 = stablehlo.reshape %arg96 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3506)
    %2900 = stablehlo.custom_call @tt.mark_argument(%2899) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3507)
    %2901 = stablehlo.reshape %2900 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3508)
    %2902 = stablehlo.reshape %arg95 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3509)
    %2903 = stablehlo.custom_call @tt.mark_argument(%2902) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3510)
    %2904 = stablehlo.reshape %2903 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3511)
    %2905 = stablehlo.composite "tenstorrent.layer_norm" %2898, %2901, %2904 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_44} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3512)
    %2906 = stablehlo.reshape %2905 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3513)
    %2907 = stablehlo.reshape %arg491 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3514)
    %2908 = stablehlo.custom_call @tt.mark_argument(%2907) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3515)
    %2909 = stablehlo.reshape %2908 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3516)
    %2910 = stablehlo.transpose %2909, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3517)
    %2911 = stablehlo.dot_general %2906, %2910, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3518)
    %2912 = stablehlo.reshape %2911 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3519)
    %2913 = stablehlo.reshape %arg490 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3520)
    %2914 = stablehlo.custom_call @tt.mark_argument(%2913) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3521)
    %2915 = stablehlo.reshape %2914 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3522)
    %2916 = stablehlo.broadcast_in_dim %2915, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3523)
    %2917 = stablehlo.add %2912, %2916 : tensor<1x257x1280xbf16> loc(#loc3524)
    %2918 = stablehlo.reshape %2917 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3525)
    %2919 = stablehlo.transpose %2918, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3526)
    %2920 = stablehlo.convert %2919 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3527)
    %2921 = stablehlo.multiply %2920, %cst_6 : tensor<1x16x257x80xf32> loc(#loc3528)
    %2922 = stablehlo.reshape %arg489 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3529)
    %2923 = stablehlo.custom_call @tt.mark_argument(%2922) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3530)
    %2924 = stablehlo.reshape %2923 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3531)
    %2925 = stablehlo.transpose %2924, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3532)
    %2926 = stablehlo.dot_general %2906, %2925, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3533)
    %2927 = stablehlo.reshape %2926 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3534)
    %2928 = stablehlo.reshape %arg488 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3535)
    %2929 = stablehlo.custom_call @tt.mark_argument(%2928) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3536)
    %2930 = stablehlo.reshape %2929 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3537)
    %2931 = stablehlo.broadcast_in_dim %2930, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3538)
    %2932 = stablehlo.add %2927, %2931 : tensor<1x257x1280xbf16> loc(#loc3539)
    %2933 = stablehlo.reshape %2932 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3540)
    %2934 = stablehlo.transpose %2933, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3541)
    %2935 = stablehlo.convert %2934 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3542)
    %2936 = stablehlo.transpose %2935, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc3543)
    %2937 = stablehlo.multiply %2936, %cst_5 : tensor<1x16x80x257xf32> loc(#loc3544)
    %2938 = stablehlo.dot_general %2921, %2937, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3545)
    %2939 = stablehlo.convert %2938 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc3546)
    %2940 = stablehlo.compare  EQ, %2939, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc3547)
    %2941 = stablehlo.not %2940 : tensor<1x16x257x257xi1> loc(#loc3548)
    %2942 = stablehlo.reduce(%2941 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.10006"), %arg559: tensor<i1> loc("reduce.10006"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc3550)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc3551)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc3549)
    %2943 = stablehlo.reshape %2942 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc3552)
    %2944 = stablehlo.not %2943 : tensor<1x16x257x1xi1> loc(#loc3553)
    %2945 = stablehlo.reshape %2944 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc3554)
    %2946 = stablehlo.broadcast_in_dim %2945, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc3555)
    %2947 = stablehlo.reduce(%2938 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3556)
    %2948 = stablehlo.broadcast_in_dim %2947, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3557)
    %2949 = stablehlo.subtract %2938, %2948 : tensor<1x16x257x257xf32> loc(#loc3558)
    %2950 = stablehlo.exponential %2949 : tensor<1x16x257x257xf32> loc(#loc3559)
    %2951 = stablehlo.reduce(%2950 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3560)
    %2952 = stablehlo.broadcast_in_dim %2951, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3561)
    %2953 = stablehlo.divide %2950, %2952 : tensor<1x16x257x257xf32> loc(#loc3562)
    %2954 = stablehlo.select %2946, %cst_3, %2953 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc3563)
    %2955 = stablehlo.reshape %arg94 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3564)
    %2956 = stablehlo.custom_call @tt.mark_argument(%2955) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3565)
    %2957 = stablehlo.reshape %2956 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3566)
    %2958 = stablehlo.transpose %2957, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3567)
    %2959 = stablehlo.dot_general %2906, %2958, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3568)
    %2960 = stablehlo.reshape %2959 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3569)
    %2961 = stablehlo.reshape %arg93 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3570)
    %2962 = stablehlo.custom_call @tt.mark_argument(%2961) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3571)
    %2963 = stablehlo.reshape %2962 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3572)
    %2964 = stablehlo.broadcast_in_dim %2963, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3573)
    %2965 = stablehlo.add %2960, %2964 : tensor<1x257x1280xbf16> loc(#loc3574)
    %2966 = stablehlo.reshape %2965 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3575)
    %2967 = stablehlo.transpose %2966, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3576)
    %2968 = stablehlo.convert %2967 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3577)
    %2969 = stablehlo.dot_general %2954, %2968, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc3578)
    %2970 = stablehlo.convert %2969 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc3579)
    %2971 = stablehlo.transpose %2970, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3580)
    %2972 = stablehlo.reshape %2971 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc3581)
    %2973 = stablehlo.reshape %arg92 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3582)
    %2974 = stablehlo.custom_call @tt.mark_argument(%2973) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3583)
    %2975 = stablehlo.reshape %2974 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3584)
    %2976 = stablehlo.transpose %2975, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3585)
    %2977 = stablehlo.dot_general %2972, %2976, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3586)
    %2978 = stablehlo.reshape %2977 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3587)
    %2979 = stablehlo.reshape %arg91 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3588)
    %2980 = stablehlo.custom_call @tt.mark_argument(%2979) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3589)
    %2981 = stablehlo.reshape %2980 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3590)
    %2982 = stablehlo.broadcast_in_dim %2981, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3591)
    %2983 = stablehlo.add %2978, %2982 : tensor<1x257x1280xbf16> loc(#loc3592)
    %2984 = stablehlo.add %2898, %2983 : tensor<1x257x1280xbf16> loc(#loc3593)
    %2985 = stablehlo.reshape %arg90 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3594)
    %2986 = stablehlo.custom_call @tt.mark_argument(%2985) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3595)
    %2987 = stablehlo.reshape %2986 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3596)
    %2988 = stablehlo.reshape %arg89 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3597)
    %2989 = stablehlo.custom_call @tt.mark_argument(%2988) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3598)
    %2990 = stablehlo.reshape %2989 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3599)
    %2991 = stablehlo.composite "tenstorrent.layer_norm" %2984, %2987, %2990 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_16} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3600)
    %2992 = stablehlo.reshape %2991 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3601)
    %2993 = stablehlo.reshape %arg88 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3602)
    %2994 = stablehlo.custom_call @tt.mark_argument(%2993) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3603)
    %2995 = stablehlo.reshape %2994 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3604)
    %2996 = stablehlo.transpose %2995, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc3605)
    %2997 = stablehlo.dot_general %2992, %2996, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3606)
    %2998 = stablehlo.reshape %2997 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3607)
    %2999 = stablehlo.reshape %arg87 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3608)
    %3000 = stablehlo.custom_call @tt.mark_argument(%2999) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3609)
    %3001 = stablehlo.reshape %3000 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3610)
    %3002 = stablehlo.broadcast_in_dim %3001, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3611)
    %3003 = stablehlo.add %2998, %3002 : tensor<1x257x5120xbf16> loc(#loc3612)
    %3004 = stablehlo.composite "tenstorrent.gelu" %3003 {decomposition = @tenstorrent.gelu.impl_29} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3613)
    %3005 = stablehlo.reshape %3004 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3614)
    %3006 = stablehlo.reshape %arg86 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3615)
    %3007 = stablehlo.custom_call @tt.mark_argument(%3006) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3616)
    %3008 = stablehlo.reshape %3007 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3617)
    %3009 = stablehlo.transpose %3008, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc3618)
    %3010 = stablehlo.dot_general %3005, %3009, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3619)
    %3011 = stablehlo.reshape %3010 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3620)
    %3012 = stablehlo.reshape %arg85 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3621)
    %3013 = stablehlo.custom_call @tt.mark_argument(%3012) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3622)
    %3014 = stablehlo.reshape %3013 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3623)
    %3015 = stablehlo.broadcast_in_dim %3014, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3624)
    %3016 = stablehlo.add %3011, %3015 : tensor<1x257x1280xbf16> loc(#loc3625)
    %3017 = stablehlo.add %2984, %3016 : tensor<1x257x1280xbf16> loc(#loc3626)
    %3018 = stablehlo.reshape %arg84 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3627)
    %3019 = stablehlo.custom_call @tt.mark_argument(%3018) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3628)
    %3020 = stablehlo.reshape %3019 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3629)
    %3021 = stablehlo.reshape %arg83 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3630)
    %3022 = stablehlo.custom_call @tt.mark_argument(%3021) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3631)
    %3023 = stablehlo.reshape %3022 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3632)
    %3024 = stablehlo.composite "tenstorrent.layer_norm" %3017, %3020, %3023 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_15} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3633)
    %3025 = stablehlo.reshape %3024 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3634)
    %3026 = stablehlo.reshape %arg495 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3635)
    %3027 = stablehlo.custom_call @tt.mark_argument(%3026) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3636)
    %3028 = stablehlo.reshape %3027 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3637)
    %3029 = stablehlo.transpose %3028, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3638)
    %3030 = stablehlo.dot_general %3025, %3029, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3639)
    %3031 = stablehlo.reshape %3030 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3640)
    %3032 = stablehlo.reshape %arg494 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3641)
    %3033 = stablehlo.custom_call @tt.mark_argument(%3032) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3642)
    %3034 = stablehlo.reshape %3033 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3643)
    %3035 = stablehlo.broadcast_in_dim %3034, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3644)
    %3036 = stablehlo.add %3031, %3035 : tensor<1x257x1280xbf16> loc(#loc3645)
    %3037 = stablehlo.reshape %3036 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3646)
    %3038 = stablehlo.transpose %3037, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3647)
    %3039 = stablehlo.convert %3038 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3648)
    %3040 = stablehlo.multiply %3039, %cst_6 : tensor<1x16x257x80xf32> loc(#loc3649)
    %3041 = stablehlo.reshape %arg493 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3650)
    %3042 = stablehlo.custom_call @tt.mark_argument(%3041) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3651)
    %3043 = stablehlo.reshape %3042 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3652)
    %3044 = stablehlo.transpose %3043, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3653)
    %3045 = stablehlo.dot_general %3025, %3044, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3654)
    %3046 = stablehlo.reshape %3045 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3655)
    %3047 = stablehlo.reshape %arg492 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3656)
    %3048 = stablehlo.custom_call @tt.mark_argument(%3047) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3657)
    %3049 = stablehlo.reshape %3048 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3658)
    %3050 = stablehlo.broadcast_in_dim %3049, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3659)
    %3051 = stablehlo.add %3046, %3050 : tensor<1x257x1280xbf16> loc(#loc3660)
    %3052 = stablehlo.reshape %3051 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3661)
    %3053 = stablehlo.transpose %3052, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3662)
    %3054 = stablehlo.convert %3053 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3663)
    %3055 = stablehlo.transpose %3054, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc3664)
    %3056 = stablehlo.multiply %3055, %cst_5 : tensor<1x16x80x257xf32> loc(#loc3665)
    %3057 = stablehlo.dot_general %3040, %3056, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3666)
    %3058 = stablehlo.convert %3057 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc3667)
    %3059 = stablehlo.compare  EQ, %3058, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc3668)
    %3060 = stablehlo.not %3059 : tensor<1x16x257x257xi1> loc(#loc3669)
    %3061 = stablehlo.reduce(%3060 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.10316"), %arg559: tensor<i1> loc("reduce.10316"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc3671)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc3672)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc3670)
    %3062 = stablehlo.reshape %3061 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc3673)
    %3063 = stablehlo.not %3062 : tensor<1x16x257x1xi1> loc(#loc3674)
    %3064 = stablehlo.reshape %3063 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc3675)
    %3065 = stablehlo.broadcast_in_dim %3064, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc3676)
    %3066 = stablehlo.reduce(%3057 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3677)
    %3067 = stablehlo.broadcast_in_dim %3066, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3678)
    %3068 = stablehlo.subtract %3057, %3067 : tensor<1x16x257x257xf32> loc(#loc3679)
    %3069 = stablehlo.exponential %3068 : tensor<1x16x257x257xf32> loc(#loc3680)
    %3070 = stablehlo.reduce(%3069 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3681)
    %3071 = stablehlo.broadcast_in_dim %3070, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3682)
    %3072 = stablehlo.divide %3069, %3071 : tensor<1x16x257x257xf32> loc(#loc3683)
    %3073 = stablehlo.select %3065, %cst_3, %3072 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc3684)
    %3074 = stablehlo.reshape %arg82 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3685)
    %3075 = stablehlo.custom_call @tt.mark_argument(%3074) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3686)
    %3076 = stablehlo.reshape %3075 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3687)
    %3077 = stablehlo.transpose %3076, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3688)
    %3078 = stablehlo.dot_general %3025, %3077, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3689)
    %3079 = stablehlo.reshape %3078 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3690)
    %3080 = stablehlo.reshape %arg81 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3691)
    %3081 = stablehlo.custom_call @tt.mark_argument(%3080) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3692)
    %3082 = stablehlo.reshape %3081 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3693)
    %3083 = stablehlo.broadcast_in_dim %3082, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3694)
    %3084 = stablehlo.add %3079, %3083 : tensor<1x257x1280xbf16> loc(#loc3695)
    %3085 = stablehlo.reshape %3084 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3696)
    %3086 = stablehlo.transpose %3085, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3697)
    %3087 = stablehlo.convert %3086 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3698)
    %3088 = stablehlo.dot_general %3073, %3087, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc3699)
    %3089 = stablehlo.convert %3088 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc3700)
    %3090 = stablehlo.transpose %3089, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3701)
    %3091 = stablehlo.reshape %3090 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc3702)
    %3092 = stablehlo.reshape %arg80 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3703)
    %3093 = stablehlo.custom_call @tt.mark_argument(%3092) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3704)
    %3094 = stablehlo.reshape %3093 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3705)
    %3095 = stablehlo.transpose %3094, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3706)
    %3096 = stablehlo.dot_general %3091, %3095, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3707)
    %3097 = stablehlo.reshape %3096 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3708)
    %3098 = stablehlo.reshape %arg79 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3709)
    %3099 = stablehlo.custom_call @tt.mark_argument(%3098) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3710)
    %3100 = stablehlo.reshape %3099 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3711)
    %3101 = stablehlo.broadcast_in_dim %3100, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3712)
    %3102 = stablehlo.add %3097, %3101 : tensor<1x257x1280xbf16> loc(#loc3713)
    %3103 = stablehlo.add %3017, %3102 : tensor<1x257x1280xbf16> loc(#loc3714)
    %3104 = stablehlo.reshape %arg78 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3715)
    %3105 = stablehlo.custom_call @tt.mark_argument(%3104) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3716)
    %3106 = stablehlo.reshape %3105 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3717)
    %3107 = stablehlo.reshape %arg77 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3718)
    %3108 = stablehlo.custom_call @tt.mark_argument(%3107) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3719)
    %3109 = stablehlo.reshape %3108 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3720)
    %3110 = stablehlo.composite "tenstorrent.layer_norm" %3103, %3106, %3109 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_13} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3721)
    %3111 = stablehlo.reshape %3110 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3722)
    %3112 = stablehlo.reshape %arg76 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3723)
    %3113 = stablehlo.custom_call @tt.mark_argument(%3112) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3724)
    %3114 = stablehlo.reshape %3113 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3725)
    %3115 = stablehlo.transpose %3114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc3726)
    %3116 = stablehlo.dot_general %3111, %3115, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3727)
    %3117 = stablehlo.reshape %3116 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3728)
    %3118 = stablehlo.reshape %arg75 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3729)
    %3119 = stablehlo.custom_call @tt.mark_argument(%3118) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3730)
    %3120 = stablehlo.reshape %3119 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3731)
    %3121 = stablehlo.broadcast_in_dim %3120, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3732)
    %3122 = stablehlo.add %3117, %3121 : tensor<1x257x5120xbf16> loc(#loc3733)
    %3123 = stablehlo.composite "tenstorrent.gelu" %3122 {decomposition = @tenstorrent.gelu.impl_5} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3734)
    %3124 = stablehlo.reshape %3123 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3735)
    %3125 = stablehlo.reshape %arg74 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3736)
    %3126 = stablehlo.custom_call @tt.mark_argument(%3125) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3737)
    %3127 = stablehlo.reshape %3126 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3738)
    %3128 = stablehlo.transpose %3127, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc3739)
    %3129 = stablehlo.dot_general %3124, %3128, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3740)
    %3130 = stablehlo.reshape %3129 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3741)
    %3131 = stablehlo.reshape %arg73 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3742)
    %3132 = stablehlo.custom_call @tt.mark_argument(%3131) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3743)
    %3133 = stablehlo.reshape %3132 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3744)
    %3134 = stablehlo.broadcast_in_dim %3133, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3745)
    %3135 = stablehlo.add %3130, %3134 : tensor<1x257x1280xbf16> loc(#loc3746)
    %3136 = stablehlo.add %3103, %3135 : tensor<1x257x1280xbf16> loc(#loc3747)
    %3137 = stablehlo.reshape %arg72 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3748)
    %3138 = stablehlo.custom_call @tt.mark_argument(%3137) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3749)
    %3139 = stablehlo.reshape %3138 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3750)
    %3140 = stablehlo.reshape %arg71 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3751)
    %3141 = stablehlo.custom_call @tt.mark_argument(%3140) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3752)
    %3142 = stablehlo.reshape %3141 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3753)
    %3143 = stablehlo.composite "tenstorrent.layer_norm" %3136, %3139, %3142 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_12} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3754)
    %3144 = stablehlo.reshape %3143 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3755)
    %3145 = stablehlo.reshape %arg499 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3756)
    %3146 = stablehlo.custom_call @tt.mark_argument(%3145) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3757)
    %3147 = stablehlo.reshape %3146 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3758)
    %3148 = stablehlo.transpose %3147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3759)
    %3149 = stablehlo.dot_general %3144, %3148, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3760)
    %3150 = stablehlo.reshape %3149 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3761)
    %3151 = stablehlo.reshape %arg498 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3762)
    %3152 = stablehlo.custom_call @tt.mark_argument(%3151) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3763)
    %3153 = stablehlo.reshape %3152 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3764)
    %3154 = stablehlo.broadcast_in_dim %3153, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3765)
    %3155 = stablehlo.add %3150, %3154 : tensor<1x257x1280xbf16> loc(#loc3766)
    %3156 = stablehlo.reshape %3155 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3767)
    %3157 = stablehlo.transpose %3156, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3768)
    %3158 = stablehlo.convert %3157 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3769)
    %3159 = stablehlo.multiply %3158, %cst_6 : tensor<1x16x257x80xf32> loc(#loc3770)
    %3160 = stablehlo.reshape %arg497 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3771)
    %3161 = stablehlo.custom_call @tt.mark_argument(%3160) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3772)
    %3162 = stablehlo.reshape %3161 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3773)
    %3163 = stablehlo.transpose %3162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3774)
    %3164 = stablehlo.dot_general %3144, %3163, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3775)
    %3165 = stablehlo.reshape %3164 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3776)
    %3166 = stablehlo.reshape %arg496 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3777)
    %3167 = stablehlo.custom_call @tt.mark_argument(%3166) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3778)
    %3168 = stablehlo.reshape %3167 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3779)
    %3169 = stablehlo.broadcast_in_dim %3168, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3780)
    %3170 = stablehlo.add %3165, %3169 : tensor<1x257x1280xbf16> loc(#loc3781)
    %3171 = stablehlo.reshape %3170 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3782)
    %3172 = stablehlo.transpose %3171, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3783)
    %3173 = stablehlo.convert %3172 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3784)
    %3174 = stablehlo.transpose %3173, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc3785)
    %3175 = stablehlo.multiply %3174, %cst_5 : tensor<1x16x80x257xf32> loc(#loc3786)
    %3176 = stablehlo.dot_general %3159, %3175, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3787)
    %3177 = stablehlo.convert %3176 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc3788)
    %3178 = stablehlo.compare  EQ, %3177, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc3789)
    %3179 = stablehlo.not %3178 : tensor<1x16x257x257xi1> loc(#loc3790)
    %3180 = stablehlo.reduce(%3179 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.10626"), %arg559: tensor<i1> loc("reduce.10626"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc3792)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc3793)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc3791)
    %3181 = stablehlo.reshape %3180 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc3794)
    %3182 = stablehlo.not %3181 : tensor<1x16x257x1xi1> loc(#loc3795)
    %3183 = stablehlo.reshape %3182 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc3796)
    %3184 = stablehlo.broadcast_in_dim %3183, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc3797)
    %3185 = stablehlo.reduce(%3176 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3798)
    %3186 = stablehlo.broadcast_in_dim %3185, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3799)
    %3187 = stablehlo.subtract %3176, %3186 : tensor<1x16x257x257xf32> loc(#loc3800)
    %3188 = stablehlo.exponential %3187 : tensor<1x16x257x257xf32> loc(#loc3801)
    %3189 = stablehlo.reduce(%3188 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3802)
    %3190 = stablehlo.broadcast_in_dim %3189, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3803)
    %3191 = stablehlo.divide %3188, %3190 : tensor<1x16x257x257xf32> loc(#loc3804)
    %3192 = stablehlo.select %3184, %cst_3, %3191 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc3805)
    %3193 = stablehlo.reshape %arg70 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3806)
    %3194 = stablehlo.custom_call @tt.mark_argument(%3193) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3807)
    %3195 = stablehlo.reshape %3194 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3808)
    %3196 = stablehlo.transpose %3195, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3809)
    %3197 = stablehlo.dot_general %3144, %3196, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3810)
    %3198 = stablehlo.reshape %3197 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3811)
    %3199 = stablehlo.reshape %arg69 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3812)
    %3200 = stablehlo.custom_call @tt.mark_argument(%3199) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3813)
    %3201 = stablehlo.reshape %3200 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3814)
    %3202 = stablehlo.broadcast_in_dim %3201, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3815)
    %3203 = stablehlo.add %3198, %3202 : tensor<1x257x1280xbf16> loc(#loc3816)
    %3204 = stablehlo.reshape %3203 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3817)
    %3205 = stablehlo.transpose %3204, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3818)
    %3206 = stablehlo.convert %3205 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3819)
    %3207 = stablehlo.dot_general %3192, %3206, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc3820)
    %3208 = stablehlo.convert %3207 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc3821)
    %3209 = stablehlo.transpose %3208, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3822)
    %3210 = stablehlo.reshape %3209 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc3823)
    %3211 = stablehlo.reshape %arg68 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3824)
    %3212 = stablehlo.custom_call @tt.mark_argument(%3211) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3825)
    %3213 = stablehlo.reshape %3212 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3826)
    %3214 = stablehlo.transpose %3213, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3827)
    %3215 = stablehlo.dot_general %3210, %3214, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3828)
    %3216 = stablehlo.reshape %3215 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3829)
    %3217 = stablehlo.reshape %arg67 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3830)
    %3218 = stablehlo.custom_call @tt.mark_argument(%3217) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3831)
    %3219 = stablehlo.reshape %3218 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3832)
    %3220 = stablehlo.broadcast_in_dim %3219, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3833)
    %3221 = stablehlo.add %3216, %3220 : tensor<1x257x1280xbf16> loc(#loc3834)
    %3222 = stablehlo.add %3136, %3221 : tensor<1x257x1280xbf16> loc(#loc3835)
    %3223 = stablehlo.reshape %arg66 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3836)
    %3224 = stablehlo.custom_call @tt.mark_argument(%3223) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3837)
    %3225 = stablehlo.reshape %3224 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3838)
    %3226 = stablehlo.reshape %arg65 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3839)
    %3227 = stablehlo.custom_call @tt.mark_argument(%3226) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3840)
    %3228 = stablehlo.reshape %3227 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3841)
    %3229 = stablehlo.composite "tenstorrent.layer_norm" %3222, %3225, %3228 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_10} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3842)
    %3230 = stablehlo.reshape %3229 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3843)
    %3231 = stablehlo.reshape %arg64 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3844)
    %3232 = stablehlo.custom_call @tt.mark_argument(%3231) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3845)
    %3233 = stablehlo.reshape %3232 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3846)
    %3234 = stablehlo.transpose %3233, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc3847)
    %3235 = stablehlo.dot_general %3230, %3234, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3848)
    %3236 = stablehlo.reshape %3235 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3849)
    %3237 = stablehlo.reshape %arg63 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3850)
    %3238 = stablehlo.custom_call @tt.mark_argument(%3237) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3851)
    %3239 = stablehlo.reshape %3238 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3852)
    %3240 = stablehlo.broadcast_in_dim %3239, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3853)
    %3241 = stablehlo.add %3236, %3240 : tensor<1x257x5120xbf16> loc(#loc3854)
    %3242 = stablehlo.composite "tenstorrent.gelu" %3241 {decomposition = @tenstorrent.gelu.impl_4} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3855)
    %3243 = stablehlo.reshape %3242 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3856)
    %3244 = stablehlo.reshape %arg62 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3857)
    %3245 = stablehlo.custom_call @tt.mark_argument(%3244) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3858)
    %3246 = stablehlo.reshape %3245 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3859)
    %3247 = stablehlo.transpose %3246, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc3860)
    %3248 = stablehlo.dot_general %3243, %3247, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3861)
    %3249 = stablehlo.reshape %3248 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3862)
    %3250 = stablehlo.reshape %arg61 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3863)
    %3251 = stablehlo.custom_call @tt.mark_argument(%3250) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3864)
    %3252 = stablehlo.reshape %3251 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3865)
    %3253 = stablehlo.broadcast_in_dim %3252, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3866)
    %3254 = stablehlo.add %3249, %3253 : tensor<1x257x1280xbf16> loc(#loc3867)
    %3255 = stablehlo.add %3222, %3254 : tensor<1x257x1280xbf16> loc(#loc3868)
    %3256 = stablehlo.reshape %arg60 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3869)
    %3257 = stablehlo.custom_call @tt.mark_argument(%3256) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3870)
    %3258 = stablehlo.reshape %3257 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3871)
    %3259 = stablehlo.reshape %arg59 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3872)
    %3260 = stablehlo.custom_call @tt.mark_argument(%3259) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3873)
    %3261 = stablehlo.reshape %3260 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3874)
    %3262 = stablehlo.composite "tenstorrent.layer_norm" %3255, %3258, %3261 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_26} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3875)
    %3263 = stablehlo.reshape %3262 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3876)
    %3264 = stablehlo.reshape %arg503 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3877)
    %3265 = stablehlo.custom_call @tt.mark_argument(%3264) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3878)
    %3266 = stablehlo.reshape %3265 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3879)
    %3267 = stablehlo.transpose %3266, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3880)
    %3268 = stablehlo.dot_general %3263, %3267, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3881)
    %3269 = stablehlo.reshape %3268 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3882)
    %3270 = stablehlo.reshape %arg502 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3883)
    %3271 = stablehlo.custom_call @tt.mark_argument(%3270) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3884)
    %3272 = stablehlo.reshape %3271 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3885)
    %3273 = stablehlo.broadcast_in_dim %3272, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3886)
    %3274 = stablehlo.add %3269, %3273 : tensor<1x257x1280xbf16> loc(#loc3887)
    %3275 = stablehlo.reshape %3274 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3888)
    %3276 = stablehlo.transpose %3275, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3889)
    %3277 = stablehlo.convert %3276 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3890)
    %3278 = stablehlo.multiply %3277, %cst_6 : tensor<1x16x257x80xf32> loc(#loc3891)
    %3279 = stablehlo.reshape %arg501 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3892)
    %3280 = stablehlo.custom_call @tt.mark_argument(%3279) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3893)
    %3281 = stablehlo.reshape %3280 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3894)
    %3282 = stablehlo.transpose %3281, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3895)
    %3283 = stablehlo.dot_general %3263, %3282, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3896)
    %3284 = stablehlo.reshape %3283 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3897)
    %3285 = stablehlo.reshape %arg500 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3898)
    %3286 = stablehlo.custom_call @tt.mark_argument(%3285) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3899)
    %3287 = stablehlo.reshape %3286 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3900)
    %3288 = stablehlo.broadcast_in_dim %3287, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3901)
    %3289 = stablehlo.add %3284, %3288 : tensor<1x257x1280xbf16> loc(#loc3902)
    %3290 = stablehlo.reshape %3289 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3903)
    %3291 = stablehlo.transpose %3290, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3904)
    %3292 = stablehlo.convert %3291 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3905)
    %3293 = stablehlo.transpose %3292, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc3906)
    %3294 = stablehlo.multiply %3293, %cst_5 : tensor<1x16x80x257xf32> loc(#loc3907)
    %3295 = stablehlo.dot_general %3278, %3294, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3908)
    %3296 = stablehlo.convert %3295 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc3909)
    %3297 = stablehlo.compare  EQ, %3296, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc3910)
    %3298 = stablehlo.not %3297 : tensor<1x16x257x257xi1> loc(#loc3911)
    %3299 = stablehlo.reduce(%3298 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.10936"), %arg559: tensor<i1> loc("reduce.10936"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc3913)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc3914)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc3912)
    %3300 = stablehlo.reshape %3299 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc3915)
    %3301 = stablehlo.not %3300 : tensor<1x16x257x1xi1> loc(#loc3916)
    %3302 = stablehlo.reshape %3301 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc3917)
    %3303 = stablehlo.broadcast_in_dim %3302, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc3918)
    %3304 = stablehlo.reduce(%3295 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3919)
    %3305 = stablehlo.broadcast_in_dim %3304, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3920)
    %3306 = stablehlo.subtract %3295, %3305 : tensor<1x16x257x257xf32> loc(#loc3921)
    %3307 = stablehlo.exponential %3306 : tensor<1x16x257x257xf32> loc(#loc3922)
    %3308 = stablehlo.reduce(%3307 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc3923)
    %3309 = stablehlo.broadcast_in_dim %3308, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc3924)
    %3310 = stablehlo.divide %3307, %3309 : tensor<1x16x257x257xf32> loc(#loc3925)
    %3311 = stablehlo.select %3303, %cst_3, %3310 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc3926)
    %3312 = stablehlo.reshape %arg58 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3927)
    %3313 = stablehlo.custom_call @tt.mark_argument(%3312) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3928)
    %3314 = stablehlo.reshape %3313 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3929)
    %3315 = stablehlo.transpose %3314, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3930)
    %3316 = stablehlo.dot_general %3263, %3315, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3931)
    %3317 = stablehlo.reshape %3316 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3932)
    %3318 = stablehlo.reshape %arg57 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3933)
    %3319 = stablehlo.custom_call @tt.mark_argument(%3318) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3934)
    %3320 = stablehlo.reshape %3319 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3935)
    %3321 = stablehlo.broadcast_in_dim %3320, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3936)
    %3322 = stablehlo.add %3317, %3321 : tensor<1x257x1280xbf16> loc(#loc3937)
    %3323 = stablehlo.reshape %3322 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3938)
    %3324 = stablehlo.transpose %3323, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc3939)
    %3325 = stablehlo.convert %3324 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc3940)
    %3326 = stablehlo.dot_general %3311, %3325, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc3941)
    %3327 = stablehlo.convert %3326 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc3942)
    %3328 = stablehlo.transpose %3327, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc3943)
    %3329 = stablehlo.reshape %3328 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc3944)
    %3330 = stablehlo.reshape %arg56 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3945)
    %3331 = stablehlo.custom_call @tt.mark_argument(%3330) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3946)
    %3332 = stablehlo.reshape %3331 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3947)
    %3333 = stablehlo.transpose %3332, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3948)
    %3334 = stablehlo.dot_general %3329, %3333, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3949)
    %3335 = stablehlo.reshape %3334 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3950)
    %3336 = stablehlo.reshape %arg55 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3951)
    %3337 = stablehlo.custom_call @tt.mark_argument(%3336) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3952)
    %3338 = stablehlo.reshape %3337 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3953)
    %3339 = stablehlo.broadcast_in_dim %3338, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3954)
    %3340 = stablehlo.add %3335, %3339 : tensor<1x257x1280xbf16> loc(#loc3955)
    %3341 = stablehlo.add %3255, %3340 : tensor<1x257x1280xbf16> loc(#loc3956)
    %3342 = stablehlo.reshape %arg54 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3957)
    %3343 = stablehlo.custom_call @tt.mark_argument(%3342) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3958)
    %3344 = stablehlo.reshape %3343 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3959)
    %3345 = stablehlo.reshape %arg53 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3960)
    %3346 = stablehlo.custom_call @tt.mark_argument(%3345) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3961)
    %3347 = stablehlo.reshape %3346 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3962)
    %3348 = stablehlo.composite "tenstorrent.layer_norm" %3341, %3344, %3347 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_9} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3963)
    %3349 = stablehlo.reshape %3348 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3964)
    %3350 = stablehlo.reshape %arg52 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3965)
    %3351 = stablehlo.custom_call @tt.mark_argument(%3350) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3966)
    %3352 = stablehlo.reshape %3351 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3967)
    %3353 = stablehlo.transpose %3352, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc3968)
    %3354 = stablehlo.dot_general %3349, %3353, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3969)
    %3355 = stablehlo.reshape %3354 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3970)
    %3356 = stablehlo.reshape %arg51 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3971)
    %3357 = stablehlo.custom_call @tt.mark_argument(%3356) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3972)
    %3358 = stablehlo.reshape %3357 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3973)
    %3359 = stablehlo.broadcast_in_dim %3358, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3974)
    %3360 = stablehlo.add %3355, %3359 : tensor<1x257x5120xbf16> loc(#loc3975)
    %3361 = stablehlo.composite "tenstorrent.gelu" %3360 {decomposition = @tenstorrent.gelu.impl_3} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc3976)
    %3362 = stablehlo.reshape %3361 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc3977)
    %3363 = stablehlo.reshape %arg50 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3978)
    %3364 = stablehlo.custom_call @tt.mark_argument(%3363) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3979)
    %3365 = stablehlo.reshape %3364 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3980)
    %3366 = stablehlo.transpose %3365, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc3981)
    %3367 = stablehlo.dot_general %3362, %3366, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3982)
    %3368 = stablehlo.reshape %3367 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3983)
    %3369 = stablehlo.reshape %arg49 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3984)
    %3370 = stablehlo.custom_call @tt.mark_argument(%3369) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3985)
    %3371 = stablehlo.reshape %3370 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3986)
    %3372 = stablehlo.broadcast_in_dim %3371, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3987)
    %3373 = stablehlo.add %3368, %3372 : tensor<1x257x1280xbf16> loc(#loc3988)
    %3374 = stablehlo.add %3341, %3373 : tensor<1x257x1280xbf16> loc(#loc3989)
    %3375 = stablehlo.reshape %arg48 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3990)
    %3376 = stablehlo.custom_call @tt.mark_argument(%3375) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3991)
    %3377 = stablehlo.reshape %3376 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3992)
    %3378 = stablehlo.reshape %arg47 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3993)
    %3379 = stablehlo.custom_call @tt.mark_argument(%3378) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3994)
    %3380 = stablehlo.reshape %3379 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3995)
    %3381 = stablehlo.composite "tenstorrent.layer_norm" %3374, %3377, %3380 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_47} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3996)
    %3382 = stablehlo.reshape %3381 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3997)
    %3383 = stablehlo.reshape %arg507 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3998)
    %3384 = stablehlo.custom_call @tt.mark_argument(%3383) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3999)
    %3385 = stablehlo.reshape %3384 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4000)
    %3386 = stablehlo.transpose %3385, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4001)
    %3387 = stablehlo.dot_general %3382, %3386, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4002)
    %3388 = stablehlo.reshape %3387 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4003)
    %3389 = stablehlo.reshape %arg506 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4004)
    %3390 = stablehlo.custom_call @tt.mark_argument(%3389) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4005)
    %3391 = stablehlo.reshape %3390 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4006)
    %3392 = stablehlo.broadcast_in_dim %3391, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4007)
    %3393 = stablehlo.add %3388, %3392 : tensor<1x257x1280xbf16> loc(#loc4008)
    %3394 = stablehlo.reshape %3393 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4009)
    %3395 = stablehlo.transpose %3394, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc4010)
    %3396 = stablehlo.convert %3395 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc4011)
    %3397 = stablehlo.multiply %3396, %cst_6 : tensor<1x16x257x80xf32> loc(#loc4012)
    %3398 = stablehlo.reshape %arg505 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4013)
    %3399 = stablehlo.custom_call @tt.mark_argument(%3398) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4014)
    %3400 = stablehlo.reshape %3399 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4015)
    %3401 = stablehlo.transpose %3400, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4016)
    %3402 = stablehlo.dot_general %3382, %3401, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4017)
    %3403 = stablehlo.reshape %3402 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4018)
    %3404 = stablehlo.reshape %arg504 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4019)
    %3405 = stablehlo.custom_call @tt.mark_argument(%3404) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4020)
    %3406 = stablehlo.reshape %3405 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4021)
    %3407 = stablehlo.broadcast_in_dim %3406, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4022)
    %3408 = stablehlo.add %3403, %3407 : tensor<1x257x1280xbf16> loc(#loc4023)
    %3409 = stablehlo.reshape %3408 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4024)
    %3410 = stablehlo.transpose %3409, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc4025)
    %3411 = stablehlo.convert %3410 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc4026)
    %3412 = stablehlo.transpose %3411, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc4027)
    %3413 = stablehlo.multiply %3412, %cst_5 : tensor<1x16x80x257xf32> loc(#loc4028)
    %3414 = stablehlo.dot_general %3397, %3413, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc4029)
    %3415 = stablehlo.convert %3414 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc4030)
    %3416 = stablehlo.compare  EQ, %3415, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc4031)
    %3417 = stablehlo.not %3416 : tensor<1x16x257x257xi1> loc(#loc4032)
    %3418 = stablehlo.reduce(%3417 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.11246"), %arg559: tensor<i1> loc("reduce.11246"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc4034)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc4035)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc4033)
    %3419 = stablehlo.reshape %3418 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc4036)
    %3420 = stablehlo.not %3419 : tensor<1x16x257x1xi1> loc(#loc4037)
    %3421 = stablehlo.reshape %3420 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc4038)
    %3422 = stablehlo.broadcast_in_dim %3421, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc4039)
    %3423 = stablehlo.reduce(%3414 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc4040)
    %3424 = stablehlo.broadcast_in_dim %3423, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc4041)
    %3425 = stablehlo.subtract %3414, %3424 : tensor<1x16x257x257xf32> loc(#loc4042)
    %3426 = stablehlo.exponential %3425 : tensor<1x16x257x257xf32> loc(#loc4043)
    %3427 = stablehlo.reduce(%3426 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc4044)
    %3428 = stablehlo.broadcast_in_dim %3427, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc4045)
    %3429 = stablehlo.divide %3426, %3428 : tensor<1x16x257x257xf32> loc(#loc4046)
    %3430 = stablehlo.select %3422, %cst_3, %3429 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc4047)
    %3431 = stablehlo.reshape %arg46 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4048)
    %3432 = stablehlo.custom_call @tt.mark_argument(%3431) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4049)
    %3433 = stablehlo.reshape %3432 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4050)
    %3434 = stablehlo.transpose %3433, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4051)
    %3435 = stablehlo.dot_general %3382, %3434, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4052)
    %3436 = stablehlo.reshape %3435 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4053)
    %3437 = stablehlo.reshape %arg45 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4054)
    %3438 = stablehlo.custom_call @tt.mark_argument(%3437) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4055)
    %3439 = stablehlo.reshape %3438 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4056)
    %3440 = stablehlo.broadcast_in_dim %3439, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4057)
    %3441 = stablehlo.add %3436, %3440 : tensor<1x257x1280xbf16> loc(#loc4058)
    %3442 = stablehlo.reshape %3441 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4059)
    %3443 = stablehlo.transpose %3442, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc4060)
    %3444 = stablehlo.convert %3443 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc4061)
    %3445 = stablehlo.dot_general %3430, %3444, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc4062)
    %3446 = stablehlo.convert %3445 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc4063)
    %3447 = stablehlo.transpose %3446, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4064)
    %3448 = stablehlo.reshape %3447 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc4065)
    %3449 = stablehlo.reshape %arg44 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4066)
    %3450 = stablehlo.custom_call @tt.mark_argument(%3449) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4067)
    %3451 = stablehlo.reshape %3450 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4068)
    %3452 = stablehlo.transpose %3451, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4069)
    %3453 = stablehlo.dot_general %3448, %3452, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4070)
    %3454 = stablehlo.reshape %3453 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4071)
    %3455 = stablehlo.reshape %arg43 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4072)
    %3456 = stablehlo.custom_call @tt.mark_argument(%3455) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4073)
    %3457 = stablehlo.reshape %3456 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4074)
    %3458 = stablehlo.broadcast_in_dim %3457, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4075)
    %3459 = stablehlo.add %3454, %3458 : tensor<1x257x1280xbf16> loc(#loc4076)
    %3460 = stablehlo.add %3374, %3459 : tensor<1x257x1280xbf16> loc(#loc4077)
    %3461 = stablehlo.reshape %arg42 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4078)
    %3462 = stablehlo.custom_call @tt.mark_argument(%3461) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4079)
    %3463 = stablehlo.reshape %3462 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4080)
    %3464 = stablehlo.reshape %arg41 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4081)
    %3465 = stablehlo.custom_call @tt.mark_argument(%3464) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4082)
    %3466 = stablehlo.reshape %3465 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4083)
    %3467 = stablehlo.composite "tenstorrent.layer_norm" %3460, %3463, %3466 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_39} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4084)
    %3468 = stablehlo.reshape %3467 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4085)
    %3469 = stablehlo.reshape %arg40 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4086)
    %3470 = stablehlo.custom_call @tt.mark_argument(%3469) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4087)
    %3471 = stablehlo.reshape %3470 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc4088)
    %3472 = stablehlo.transpose %3471, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc4089)
    %3473 = stablehlo.dot_general %3468, %3472, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc4090)
    %3474 = stablehlo.reshape %3473 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4091)
    %3475 = stablehlo.reshape %arg39 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4092)
    %3476 = stablehlo.custom_call @tt.mark_argument(%3475) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4093)
    %3477 = stablehlo.reshape %3476 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4094)
    %3478 = stablehlo.broadcast_in_dim %3477, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4095)
    %3479 = stablehlo.add %3474, %3478 : tensor<1x257x5120xbf16> loc(#loc4096)
    %3480 = stablehlo.composite "tenstorrent.gelu" %3479 {decomposition = @tenstorrent.gelu.impl_2} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4097)
    %3481 = stablehlo.reshape %3480 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc4098)
    %3482 = stablehlo.reshape %arg38 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4099)
    %3483 = stablehlo.custom_call @tt.mark_argument(%3482) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4100)
    %3484 = stablehlo.reshape %3483 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc4101)
    %3485 = stablehlo.transpose %3484, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc4102)
    %3486 = stablehlo.dot_general %3481, %3485, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4103)
    %3487 = stablehlo.reshape %3486 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4104)
    %3488 = stablehlo.reshape %arg37 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4105)
    %3489 = stablehlo.custom_call @tt.mark_argument(%3488) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4106)
    %3490 = stablehlo.reshape %3489 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4107)
    %3491 = stablehlo.broadcast_in_dim %3490, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4108)
    %3492 = stablehlo.add %3487, %3491 : tensor<1x257x1280xbf16> loc(#loc4109)
    %3493 = stablehlo.add %3460, %3492 : tensor<1x257x1280xbf16> loc(#loc4110)
    %3494 = stablehlo.reshape %arg36 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4111)
    %3495 = stablehlo.custom_call @tt.mark_argument(%3494) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4112)
    %3496 = stablehlo.reshape %3495 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4113)
    %3497 = stablehlo.reshape %arg35 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4114)
    %3498 = stablehlo.custom_call @tt.mark_argument(%3497) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4115)
    %3499 = stablehlo.reshape %3498 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4116)
    %3500 = stablehlo.composite "tenstorrent.layer_norm" %3493, %3496, %3499 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_8} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4117)
    %3501 = stablehlo.reshape %3500 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4118)
    %3502 = stablehlo.reshape %arg511 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4119)
    %3503 = stablehlo.custom_call @tt.mark_argument(%3502) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4120)
    %3504 = stablehlo.reshape %3503 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4121)
    %3505 = stablehlo.transpose %3504, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4122)
    %3506 = stablehlo.dot_general %3501, %3505, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4123)
    %3507 = stablehlo.reshape %3506 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4124)
    %3508 = stablehlo.reshape %arg510 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4125)
    %3509 = stablehlo.custom_call @tt.mark_argument(%3508) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4126)
    %3510 = stablehlo.reshape %3509 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4127)
    %3511 = stablehlo.broadcast_in_dim %3510, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4128)
    %3512 = stablehlo.add %3507, %3511 : tensor<1x257x1280xbf16> loc(#loc4129)
    %3513 = stablehlo.reshape %3512 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4130)
    %3514 = stablehlo.transpose %3513, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc4131)
    %3515 = stablehlo.convert %3514 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc4132)
    %3516 = stablehlo.multiply %3515, %cst_6 : tensor<1x16x257x80xf32> loc(#loc4133)
    %3517 = stablehlo.reshape %arg509 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4134)
    %3518 = stablehlo.custom_call @tt.mark_argument(%3517) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4135)
    %3519 = stablehlo.reshape %3518 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4136)
    %3520 = stablehlo.transpose %3519, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4137)
    %3521 = stablehlo.dot_general %3501, %3520, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4138)
    %3522 = stablehlo.reshape %3521 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4139)
    %3523 = stablehlo.reshape %arg508 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4140)
    %3524 = stablehlo.custom_call @tt.mark_argument(%3523) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4141)
    %3525 = stablehlo.reshape %3524 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4142)
    %3526 = stablehlo.broadcast_in_dim %3525, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4143)
    %3527 = stablehlo.add %3522, %3526 : tensor<1x257x1280xbf16> loc(#loc4144)
    %3528 = stablehlo.reshape %3527 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4145)
    %3529 = stablehlo.transpose %3528, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc4146)
    %3530 = stablehlo.convert %3529 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc4147)
    %3531 = stablehlo.transpose %3530, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc4148)
    %3532 = stablehlo.multiply %3531, %cst_5 : tensor<1x16x80x257xf32> loc(#loc4149)
    %3533 = stablehlo.dot_general %3516, %3532, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc4150)
    %3534 = stablehlo.convert %3533 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc4151)
    %3535 = stablehlo.compare  EQ, %3534, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc4152)
    %3536 = stablehlo.not %3535 : tensor<1x16x257x257xi1> loc(#loc4153)
    %3537 = stablehlo.reduce(%3536 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.11556"), %arg559: tensor<i1> loc("reduce.11556"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc4155)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc4156)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc4154)
    %3538 = stablehlo.reshape %3537 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc4157)
    %3539 = stablehlo.not %3538 : tensor<1x16x257x1xi1> loc(#loc4158)
    %3540 = stablehlo.reshape %3539 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc4159)
    %3541 = stablehlo.broadcast_in_dim %3540, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc4160)
    %3542 = stablehlo.reduce(%3533 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc4161)
    %3543 = stablehlo.broadcast_in_dim %3542, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc4162)
    %3544 = stablehlo.subtract %3533, %3543 : tensor<1x16x257x257xf32> loc(#loc4163)
    %3545 = stablehlo.exponential %3544 : tensor<1x16x257x257xf32> loc(#loc4164)
    %3546 = stablehlo.reduce(%3545 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc4165)
    %3547 = stablehlo.broadcast_in_dim %3546, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc4166)
    %3548 = stablehlo.divide %3545, %3547 : tensor<1x16x257x257xf32> loc(#loc4167)
    %3549 = stablehlo.select %3541, %cst_3, %3548 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc4168)
    %3550 = stablehlo.reshape %arg34 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4169)
    %3551 = stablehlo.custom_call @tt.mark_argument(%3550) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4170)
    %3552 = stablehlo.reshape %3551 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4171)
    %3553 = stablehlo.transpose %3552, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4172)
    %3554 = stablehlo.dot_general %3501, %3553, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4173)
    %3555 = stablehlo.reshape %3554 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4174)
    %3556 = stablehlo.reshape %arg33 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4175)
    %3557 = stablehlo.custom_call @tt.mark_argument(%3556) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4176)
    %3558 = stablehlo.reshape %3557 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4177)
    %3559 = stablehlo.broadcast_in_dim %3558, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4178)
    %3560 = stablehlo.add %3555, %3559 : tensor<1x257x1280xbf16> loc(#loc4179)
    %3561 = stablehlo.reshape %3560 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4180)
    %3562 = stablehlo.transpose %3561, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc4181)
    %3563 = stablehlo.convert %3562 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc4182)
    %3564 = stablehlo.dot_general %3549, %3563, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc4183)
    %3565 = stablehlo.convert %3564 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc4184)
    %3566 = stablehlo.transpose %3565, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4185)
    %3567 = stablehlo.reshape %3566 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc4186)
    %3568 = stablehlo.reshape %arg32 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4187)
    %3569 = stablehlo.custom_call @tt.mark_argument(%3568) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4188)
    %3570 = stablehlo.reshape %3569 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4189)
    %3571 = stablehlo.transpose %3570, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4190)
    %3572 = stablehlo.dot_general %3567, %3571, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4191)
    %3573 = stablehlo.reshape %3572 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4192)
    %3574 = stablehlo.reshape %arg31 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4193)
    %3575 = stablehlo.custom_call @tt.mark_argument(%3574) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4194)
    %3576 = stablehlo.reshape %3575 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4195)
    %3577 = stablehlo.broadcast_in_dim %3576, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4196)
    %3578 = stablehlo.add %3573, %3577 : tensor<1x257x1280xbf16> loc(#loc4197)
    %3579 = stablehlo.add %3493, %3578 : tensor<1x257x1280xbf16> loc(#loc4198)
    %3580 = stablehlo.reshape %arg30 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4199)
    %3581 = stablehlo.custom_call @tt.mark_argument(%3580) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4200)
    %3582 = stablehlo.reshape %3581 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4201)
    %3583 = stablehlo.reshape %arg29 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4202)
    %3584 = stablehlo.custom_call @tt.mark_argument(%3583) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4203)
    %3585 = stablehlo.reshape %3584 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4204)
    %3586 = stablehlo.composite "tenstorrent.layer_norm" %3579, %3582, %3585 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_7} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4205)
    %3587 = stablehlo.reshape %3586 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4206)
    %3588 = stablehlo.reshape %arg28 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4207)
    %3589 = stablehlo.custom_call @tt.mark_argument(%3588) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4208)
    %3590 = stablehlo.reshape %3589 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc4209)
    %3591 = stablehlo.transpose %3590, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc4210)
    %3592 = stablehlo.dot_general %3587, %3591, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc4211)
    %3593 = stablehlo.reshape %3592 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4212)
    %3594 = stablehlo.reshape %arg27 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4213)
    %3595 = stablehlo.custom_call @tt.mark_argument(%3594) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4214)
    %3596 = stablehlo.reshape %3595 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4215)
    %3597 = stablehlo.broadcast_in_dim %3596, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4216)
    %3598 = stablehlo.add %3593, %3597 : tensor<1x257x5120xbf16> loc(#loc4217)
    %3599 = stablehlo.composite "tenstorrent.gelu" %3598 {decomposition = @tenstorrent.gelu.impl_1} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4218)
    %3600 = stablehlo.reshape %3599 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc4219)
    %3601 = stablehlo.reshape %arg26 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4220)
    %3602 = stablehlo.custom_call @tt.mark_argument(%3601) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4221)
    %3603 = stablehlo.reshape %3602 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc4222)
    %3604 = stablehlo.transpose %3603, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc4223)
    %3605 = stablehlo.dot_general %3600, %3604, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4224)
    %3606 = stablehlo.reshape %3605 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4225)
    %3607 = stablehlo.reshape %arg25 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4226)
    %3608 = stablehlo.custom_call @tt.mark_argument(%3607) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4227)
    %3609 = stablehlo.reshape %3608 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4228)
    %3610 = stablehlo.broadcast_in_dim %3609, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4229)
    %3611 = stablehlo.add %3606, %3610 : tensor<1x257x1280xbf16> loc(#loc4230)
    %3612 = stablehlo.add %3579, %3611 : tensor<1x257x1280xbf16> loc(#loc4231)
    %3613 = stablehlo.reshape %arg24 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4232)
    %3614 = stablehlo.custom_call @tt.mark_argument(%3613) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4233)
    %3615 = stablehlo.reshape %3614 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4234)
    %3616 = stablehlo.reshape %arg23 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4235)
    %3617 = stablehlo.custom_call @tt.mark_argument(%3616) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4236)
    %3618 = stablehlo.reshape %3617 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4237)
    %3619 = stablehlo.composite "tenstorrent.layer_norm" %3612, %3615, %3618 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_73} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4238)
    %3620 = stablehlo.reshape %3619 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4239)
    %3621 = stablehlo.reshape %arg515 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4240)
    %3622 = stablehlo.custom_call @tt.mark_argument(%3621) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4241)
    %3623 = stablehlo.reshape %3622 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4242)
    %3624 = stablehlo.transpose %3623, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4243)
    %3625 = stablehlo.dot_general %3620, %3624, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4244)
    %3626 = stablehlo.reshape %3625 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4245)
    %3627 = stablehlo.reshape %arg514 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4246)
    %3628 = stablehlo.custom_call @tt.mark_argument(%3627) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4247)
    %3629 = stablehlo.reshape %3628 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4248)
    %3630 = stablehlo.broadcast_in_dim %3629, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4249)
    %3631 = stablehlo.add %3626, %3630 : tensor<1x257x1280xbf16> loc(#loc4250)
    %3632 = stablehlo.reshape %3631 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4251)
    %3633 = stablehlo.transpose %3632, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc4252)
    %3634 = stablehlo.convert %3633 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc4253)
    %3635 = stablehlo.multiply %3634, %cst_6 : tensor<1x16x257x80xf32> loc(#loc4254)
    %3636 = stablehlo.reshape %arg513 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4255)
    %3637 = stablehlo.custom_call @tt.mark_argument(%3636) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4256)
    %3638 = stablehlo.reshape %3637 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4257)
    %3639 = stablehlo.transpose %3638, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4258)
    %3640 = stablehlo.dot_general %3620, %3639, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4259)
    %3641 = stablehlo.reshape %3640 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4260)
    %3642 = stablehlo.reshape %arg512 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4261)
    %3643 = stablehlo.custom_call @tt.mark_argument(%3642) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4262)
    %3644 = stablehlo.reshape %3643 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4263)
    %3645 = stablehlo.broadcast_in_dim %3644, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4264)
    %3646 = stablehlo.add %3641, %3645 : tensor<1x257x1280xbf16> loc(#loc4265)
    %3647 = stablehlo.reshape %3646 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4266)
    %3648 = stablehlo.transpose %3647, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc4267)
    %3649 = stablehlo.convert %3648 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc4268)
    %3650 = stablehlo.transpose %3649, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc4269)
    %3651 = stablehlo.multiply %3650, %cst_5 : tensor<1x16x80x257xf32> loc(#loc4270)
    %3652 = stablehlo.dot_general %3635, %3651, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc4271)
    %3653 = stablehlo.convert %3652 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc4272)
    %3654 = stablehlo.compare  EQ, %3653, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc4273)
    %3655 = stablehlo.not %3654 : tensor<1x16x257x257xi1> loc(#loc4274)
    %3656 = stablehlo.reduce(%3655 init: %c_10) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("reduce.11866"), %arg559: tensor<i1> loc("reduce.11866"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc4276)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc4277)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc4275)
    %3657 = stablehlo.reshape %3656 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc4278)
    %3658 = stablehlo.not %3657 : tensor<1x16x257x1xi1> loc(#loc4279)
    %3659 = stablehlo.reshape %3658 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc4280)
    %3660 = stablehlo.broadcast_in_dim %3659, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc4281)
    %3661 = stablehlo.reduce(%3652 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc4282)
    %3662 = stablehlo.broadcast_in_dim %3661, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc4283)
    %3663 = stablehlo.subtract %3652, %3662 : tensor<1x16x257x257xf32> loc(#loc4284)
    %3664 = stablehlo.exponential %3663 : tensor<1x16x257x257xf32> loc(#loc4285)
    %3665 = stablehlo.reduce(%3664 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc4286)
    %3666 = stablehlo.broadcast_in_dim %3665, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc4287)
    %3667 = stablehlo.divide %3664, %3666 : tensor<1x16x257x257xf32> loc(#loc4288)
    %3668 = stablehlo.select %3660, %cst_3, %3667 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc4289)
    %3669 = stablehlo.reshape %arg22 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4290)
    %3670 = stablehlo.custom_call @tt.mark_argument(%3669) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4291)
    %3671 = stablehlo.reshape %3670 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4292)
    %3672 = stablehlo.transpose %3671, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4293)
    %3673 = stablehlo.dot_general %3620, %3672, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4294)
    %3674 = stablehlo.reshape %3673 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4295)
    %3675 = stablehlo.reshape %arg21 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4296)
    %3676 = stablehlo.custom_call @tt.mark_argument(%3675) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4297)
    %3677 = stablehlo.reshape %3676 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4298)
    %3678 = stablehlo.broadcast_in_dim %3677, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4299)
    %3679 = stablehlo.add %3674, %3678 : tensor<1x257x1280xbf16> loc(#loc4300)
    %3680 = stablehlo.reshape %3679 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4301)
    %3681 = stablehlo.transpose %3680, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc4302)
    %3682 = stablehlo.convert %3681 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc4303)
    %3683 = stablehlo.dot_general %3668, %3682, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc4304)
    %3684 = stablehlo.convert %3683 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc4305)
    %3685 = stablehlo.transpose %3684, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc4306)
    %3686 = stablehlo.reshape %3685 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc4307)
    %3687 = stablehlo.reshape %arg20 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4308)
    %3688 = stablehlo.custom_call @tt.mark_argument(%3687) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4309)
    %3689 = stablehlo.reshape %3688 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4310)
    %3690 = stablehlo.transpose %3689, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4311)
    %3691 = stablehlo.dot_general %3686, %3690, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4312)
    %3692 = stablehlo.reshape %3691 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4313)
    %3693 = stablehlo.reshape %arg19 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4314)
    %3694 = stablehlo.custom_call @tt.mark_argument(%3693) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4315)
    %3695 = stablehlo.reshape %3694 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4316)
    %3696 = stablehlo.broadcast_in_dim %3695, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4317)
    %3697 = stablehlo.add %3692, %3696 : tensor<1x257x1280xbf16> loc(#loc4318)
    %3698 = stablehlo.add %3612, %3697 : tensor<1x257x1280xbf16> loc(#loc4319)
    %3699 = stablehlo.reshape %arg18 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4320)
    %3700 = stablehlo.custom_call @tt.mark_argument(%3699) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4321)
    %3701 = stablehlo.reshape %3700 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4322)
    %3702 = stablehlo.reshape %arg17 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4323)
    %3703 = stablehlo.custom_call @tt.mark_argument(%3702) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4324)
    %3704 = stablehlo.reshape %3703 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4325)
    %3705 = stablehlo.composite "tenstorrent.layer_norm" %3698, %3701, %3704 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_56} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4326)
    %3706 = stablehlo.reshape %3705 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4327)
    %3707 = stablehlo.reshape %arg16 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4328)
    %3708 = stablehlo.custom_call @tt.mark_argument(%3707) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4329)
    %3709 = stablehlo.reshape %3708 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc4330)
    %3710 = stablehlo.transpose %3709, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc4331)
    %3711 = stablehlo.dot_general %3706, %3710, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc4332)
    %3712 = stablehlo.reshape %3711 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4333)
    %3713 = stablehlo.reshape %arg15 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4334)
    %3714 = stablehlo.custom_call @tt.mark_argument(%3713) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4335)
    %3715 = stablehlo.reshape %3714 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4336)
    %3716 = stablehlo.broadcast_in_dim %3715, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4337)
    %3717 = stablehlo.add %3712, %3716 : tensor<1x257x5120xbf16> loc(#loc4338)
    %3718 = stablehlo.composite "tenstorrent.gelu" %3717 {decomposition = @tenstorrent.gelu.impl_0} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4339)
    %3719 = stablehlo.reshape %3718 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc4340)
    %3720 = stablehlo.reshape %arg14 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4341)
    %3721 = stablehlo.custom_call @tt.mark_argument(%3720) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4342)
    %3722 = stablehlo.reshape %3721 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc4343)
    %3723 = stablehlo.transpose %3722, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc4344)
    %3724 = stablehlo.dot_general %3719, %3723, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4345)
    %3725 = stablehlo.reshape %3724 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4346)
    %3726 = stablehlo.reshape %arg13 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4347)
    %3727 = stablehlo.custom_call @tt.mark_argument(%3726) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4348)
    %3728 = stablehlo.reshape %3727 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4349)
    %3729 = stablehlo.broadcast_in_dim %3728, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4350)
    %3730 = stablehlo.add %3725, %3729 : tensor<1x257x1280xbf16> loc(#loc4351)
    %3731 = stablehlo.add %3698, %3730 : tensor<1x257x1280xbf16> loc(#loc4352)
    %3732 = stablehlo.reshape %3731 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4353)
    %3733 = stablehlo.reshape %arg12 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4354)
    %3734 = stablehlo.custom_call @tt.mark_argument(%3733) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_proj_in_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4355)
    %3735 = stablehlo.reshape %3734 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4356)
    %3736 = stablehlo.transpose %3735, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4357)
    %3737 = stablehlo.dot_general %3732, %3736, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc4358)
    %3738 = stablehlo.reshape %3737 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4359)
    %3739 = stablehlo.reshape %arg11 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4360)
    %3740 = stablehlo.custom_call @tt.mark_argument(%3739) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_proj_in_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4361)
    %3741 = stablehlo.reshape %3740 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4362)
    %3742 = stablehlo.broadcast_in_dim %3741, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4363)
    %3743 = stablehlo.add %3738, %3742 : tensor<1x257x1280xbf16> loc(#loc4364)
    %3744 = stablehlo.reshape %arg10 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4365)
    %3745 = stablehlo.custom_call @tt.mark_argument(%3744) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ln0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4366)
    %3746 = stablehlo.reshape %3745 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4367)
    %3747 = stablehlo.reshape %arg9 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4368)
    %3748 = stablehlo.custom_call @tt.mark_argument(%3747) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ln0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4369)
    %3749 = stablehlo.reshape %3748 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4370)
    %3750 = stablehlo.composite "tenstorrent.layer_norm" %3743, %3746, %3749 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_6} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4371)
    %3751 = stablehlo.concatenate %3750, %7, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc4372)
    %3752 = stablehlo.reshape %3751 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4373)
    %3753 = stablehlo.reshape %arg516 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4374)
    %3754 = stablehlo.custom_call @tt.mark_argument(%3753) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_attn_to_k_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4375)
    %3755 = stablehlo.reshape %3754 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4376)
    %3756 = stablehlo.transpose %3755, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4377)
    %3757 = stablehlo.dot_general %3752, %3756, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4378)
    %3758 = stablehlo.reshape %3757 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc4379)
    %3759 = stablehlo.transpose %3758, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc4380)
    %3760 = stablehlo.convert %3759 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc4381)
    %3761 = stablehlo.transpose %3760, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc4382)
    %3762 = stablehlo.multiply %3761, %cst_2 : tensor<1x20x64x273xf32> loc(#loc4383)
    %3763 = stablehlo.dot_general %17, %3762, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc4384)
    %3764 = stablehlo.convert %3763 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc4385)
    %3765 = stablehlo.compare  EQ, %3764, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc4386)
    %3766 = stablehlo.not %3765 : tensor<1x20x16x273xi1> loc(#loc4387)
    %3767 = stablehlo.reduce(%3766 init: %c_10) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("reduce.12162"), %arg559: tensor<i1> loc("reduce.12162"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc4389)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc4390)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc4388)
    %3768 = stablehlo.reshape %3767 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc4391)
    %3769 = stablehlo.not %3768 : tensor<1x20x16x1xi1> loc(#loc4392)
    %3770 = stablehlo.reshape %3769 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc4393)
    %3771 = stablehlo.broadcast_in_dim %3770, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc4394)
    %3772 = stablehlo.reduce(%3763 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc4395)
    %3773 = stablehlo.broadcast_in_dim %3772, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc4396)
    %3774 = stablehlo.subtract %3763, %3773 : tensor<1x20x16x273xf32> loc(#loc4397)
    %3775 = stablehlo.exponential %3774 : tensor<1x20x16x273xf32> loc(#loc4398)
    %3776 = stablehlo.reduce(%3775 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc4399)
    %3777 = stablehlo.broadcast_in_dim %3776, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc4400)
    %3778 = stablehlo.divide %3775, %3777 : tensor<1x20x16x273xf32> loc(#loc4401)
    %3779 = stablehlo.select %3771, %cst_0, %3778 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc4402)
    %3780 = stablehlo.reshape %arg6 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4403)
    %3781 = stablehlo.custom_call @tt.mark_argument(%3780) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_attn_to_v_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4404)
    %3782 = stablehlo.reshape %3781 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4405)
    %3783 = stablehlo.transpose %3782, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4406)
    %3784 = stablehlo.dot_general %3752, %3783, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4407)
    %3785 = stablehlo.reshape %3784 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc4408)
    %3786 = stablehlo.transpose %3785, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc4409)
    %3787 = stablehlo.convert %3786 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc4410)
    %3788 = stablehlo.dot_general %3779, %3787, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc4411)
    %3789 = stablehlo.convert %3788 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc4412)
    %3790 = stablehlo.transpose %3789, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc4413)
    %3791 = stablehlo.reshape %3790 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc4414)
    %3792 = stablehlo.reshape %arg5 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4415)
    %3793 = stablehlo.custom_call @tt.mark_argument(%3792) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_attn_to_out_0_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4416)
    %3794 = stablehlo.reshape %3793 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4417)
    %3795 = stablehlo.transpose %3794, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4418)
    %3796 = stablehlo.dot_general %3791, %3795, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4419)
    %3797 = stablehlo.reshape %3796 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4420)
    %3798 = stablehlo.divide %3797, %cst : tensor<1x16x1280xbf16> loc(#loc4421)
    %3799 = stablehlo.add %3798, %0 : tensor<1x16x1280xbf16> loc(#loc4422)
    %3800 = stablehlo.reshape %arg521 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4423)
    %3801 = stablehlo.custom_call @tt.mark_argument(%3800) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ff_0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4424)
    %3802 = stablehlo.reshape %3801 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4425)
    %3803 = stablehlo.reshape %arg520 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4426)
    %3804 = stablehlo.custom_call @tt.mark_argument(%3803) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ff_0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4427)
    %3805 = stablehlo.reshape %3804 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4428)
    %3806 = stablehlo.composite "tenstorrent.layer_norm" %3799, %3802, %3805 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_48} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4429)
    %3807 = stablehlo.reshape %3806 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4430)
    %3808 = stablehlo.reshape %arg519 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4431)
    %3809 = stablehlo.custom_call @tt.mark_argument(%3808) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_0_proj_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4432)
    %3810 = stablehlo.reshape %3809 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc4433)
    %3811 = stablehlo.transpose %3810, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc4434)
    %3812 = stablehlo.dot_general %3807, %3811, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc4435)
    %3813 = stablehlo.reshape %3812 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc4436)
    %3814 = stablehlo.composite "tenstorrent.gelu" %3813 {decomposition = @tenstorrent.gelu.impl} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc4437)
    %3815 = stablehlo.reshape %3814 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc4438)
    %3816 = stablehlo.reshape %arg518 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4439)
    %3817 = stablehlo.custom_call @tt.mark_argument(%3816) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4440)
    %3818 = stablehlo.reshape %3817 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc4441)
    %3819 = stablehlo.transpose %3818, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc4442)
    %3820 = stablehlo.dot_general %3815, %3819, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4443)
    %3821 = stablehlo.reshape %3820 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4444)
    %3822 = stablehlo.add %3821, %3799 : tensor<1x16x1280xbf16> loc(#loc4445)
    %3823 = stablehlo.reshape %arg525 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4446)
    %3824 = stablehlo.custom_call @tt.mark_argument(%3823) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ln1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4447)
    %3825 = stablehlo.reshape %3824 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4448)
    %3826 = stablehlo.reshape %arg524 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4449)
    %3827 = stablehlo.custom_call @tt.mark_argument(%3826) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ln1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4450)
    %3828 = stablehlo.reshape %3827 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4451)
    %3829 = stablehlo.composite "tenstorrent.layer_norm" %3822, %3825, %3828 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_5} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4452)
    %3830 = stablehlo.reshape %3829 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4453)
    %3831 = stablehlo.reshape %arg529 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4454)
    %3832 = stablehlo.custom_call @tt.mark_argument(%3831) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_attn_to_q_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4455)
    %3833 = stablehlo.reshape %3832 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4456)
    %3834 = stablehlo.transpose %3833, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4457)
    %3835 = stablehlo.dot_general %3830, %3834, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4458)
    %3836 = stablehlo.reshape %3835 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc4459)
    %3837 = stablehlo.transpose %3836, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc4460)
    %3838 = stablehlo.convert %3837 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc4461)
    %3839 = stablehlo.multiply %3838, %cst_7 : tensor<1x20x16x64xf32> loc(#loc4462)
    %3840 = stablehlo.reshape %arg527 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4463)
    %3841 = stablehlo.custom_call @tt.mark_argument(%3840) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ln0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4464)
    %3842 = stablehlo.reshape %3841 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4465)
    %3843 = stablehlo.reshape %arg526 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4466)
    %3844 = stablehlo.custom_call @tt.mark_argument(%3843) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ln0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4467)
    %3845 = stablehlo.reshape %3844 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4468)
    %3846 = stablehlo.composite "tenstorrent.layer_norm" %3743, %3842, %3845 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_3} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4469)
    %3847 = stablehlo.concatenate %3846, %3829, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc4470)
    %3848 = stablehlo.reshape %3847 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4471)
    %3849 = stablehlo.reshape %arg528 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4472)
    %3850 = stablehlo.custom_call @tt.mark_argument(%3849) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_attn_to_k_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4473)
    %3851 = stablehlo.reshape %3850 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4474)
    %3852 = stablehlo.transpose %3851, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4475)
    %3853 = stablehlo.dot_general %3848, %3852, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4476)
    %3854 = stablehlo.reshape %3853 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc4477)
    %3855 = stablehlo.transpose %3854, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc4478)
    %3856 = stablehlo.convert %3855 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc4479)
    %3857 = stablehlo.transpose %3856, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc4480)
    %3858 = stablehlo.multiply %3857, %cst_2 : tensor<1x20x64x273xf32> loc(#loc4481)
    %3859 = stablehlo.dot_general %3839, %3858, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc4482)
    %3860 = stablehlo.convert %3859 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc4483)
    %3861 = stablehlo.compare  EQ, %3860, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc4484)
    %3862 = stablehlo.not %3861 : tensor<1x20x16x273xi1> loc(#loc4485)
    %3863 = stablehlo.reduce(%3862 init: %c_10) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("reduce.12578"), %arg559: tensor<i1> loc("reduce.12578"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc4487)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc4488)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc4486)
    %3864 = stablehlo.reshape %3863 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc4489)
    %3865 = stablehlo.not %3864 : tensor<1x20x16x1xi1> loc(#loc4490)
    %3866 = stablehlo.reshape %3865 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc4491)
    %3867 = stablehlo.broadcast_in_dim %3866, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc4492)
    %3868 = stablehlo.reduce(%3859 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc4493)
    %3869 = stablehlo.broadcast_in_dim %3868, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc4494)
    %3870 = stablehlo.subtract %3859, %3869 : tensor<1x20x16x273xf32> loc(#loc4495)
    %3871 = stablehlo.exponential %3870 : tensor<1x20x16x273xf32> loc(#loc4496)
    %3872 = stablehlo.reduce(%3871 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc4497)
    %3873 = stablehlo.broadcast_in_dim %3872, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc4498)
    %3874 = stablehlo.divide %3871, %3873 : tensor<1x20x16x273xf32> loc(#loc4499)
    %3875 = stablehlo.select %3867, %cst_0, %3874 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc4500)
    %3876 = stablehlo.reshape %arg523 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4501)
    %3877 = stablehlo.custom_call @tt.mark_argument(%3876) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_attn_to_v_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4502)
    %3878 = stablehlo.reshape %3877 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4503)
    %3879 = stablehlo.transpose %3878, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4504)
    %3880 = stablehlo.dot_general %3848, %3879, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4505)
    %3881 = stablehlo.reshape %3880 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc4506)
    %3882 = stablehlo.transpose %3881, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc4507)
    %3883 = stablehlo.convert %3882 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc4508)
    %3884 = stablehlo.dot_general %3875, %3883, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc4509)
    %3885 = stablehlo.convert %3884 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc4510)
    %3886 = stablehlo.transpose %3885, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc4511)
    %3887 = stablehlo.reshape %3886 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc4512)
    %3888 = stablehlo.reshape %arg522 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4513)
    %3889 = stablehlo.custom_call @tt.mark_argument(%3888) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_attn_to_out_0_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4514)
    %3890 = stablehlo.reshape %3889 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4515)
    %3891 = stablehlo.transpose %3890, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4516)
    %3892 = stablehlo.dot_general %3887, %3891, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4517)
    %3893 = stablehlo.reshape %3892 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4518)
    %3894 = stablehlo.divide %3893, %cst : tensor<1x16x1280xbf16> loc(#loc4519)
    %3895 = stablehlo.add %3894, %3822 : tensor<1x16x1280xbf16> loc(#loc4520)
    %3896 = stablehlo.reshape %arg533 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4521)
    %3897 = stablehlo.custom_call @tt.mark_argument(%3896) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ff_0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4522)
    %3898 = stablehlo.reshape %3897 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4523)
    %3899 = stablehlo.reshape %arg532 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4524)
    %3900 = stablehlo.custom_call @tt.mark_argument(%3899) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ff_0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4525)
    %3901 = stablehlo.reshape %3900 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4526)
    %3902 = stablehlo.composite "tenstorrent.layer_norm" %3895, %3898, %3901 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_2} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4527)
    %3903 = stablehlo.reshape %3902 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4528)
    %3904 = stablehlo.reshape %arg531 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4529)
    %3905 = stablehlo.custom_call @tt.mark_argument(%3904) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_0_proj_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4530)
    %3906 = stablehlo.reshape %3905 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc4531)
    %3907 = stablehlo.transpose %3906, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc4532)
    %3908 = stablehlo.dot_general %3903, %3907, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc4533)
    %3909 = stablehlo.reshape %3908 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc4534)
    %3910 = stablehlo.composite "tenstorrent.gelu" %3909 {decomposition = @tenstorrent.gelu.impl_9} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc4535)
    %3911 = stablehlo.reshape %3910 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc4536)
    %3912 = stablehlo.reshape %arg530 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4537)
    %3913 = stablehlo.custom_call @tt.mark_argument(%3912) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4538)
    %3914 = stablehlo.reshape %3913 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc4539)
    %3915 = stablehlo.transpose %3914, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc4540)
    %3916 = stablehlo.dot_general %3911, %3915, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4541)
    %3917 = stablehlo.reshape %3916 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4542)
    %3918 = stablehlo.add %3917, %3895 : tensor<1x16x1280xbf16> loc(#loc4543)
    %3919 = stablehlo.reshape %arg537 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4544)
    %3920 = stablehlo.custom_call @tt.mark_argument(%3919) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ln1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4545)
    %3921 = stablehlo.reshape %3920 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4546)
    %3922 = stablehlo.reshape %arg536 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4547)
    %3923 = stablehlo.custom_call @tt.mark_argument(%3922) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ln1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4548)
    %3924 = stablehlo.reshape %3923 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4549)
    %3925 = stablehlo.composite "tenstorrent.layer_norm" %3918, %3921, %3924 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_1} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4550)
    %3926 = stablehlo.reshape %3925 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4551)
    %3927 = stablehlo.reshape %arg541 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4552)
    %3928 = stablehlo.custom_call @tt.mark_argument(%3927) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_attn_to_q_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4553)
    %3929 = stablehlo.reshape %3928 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4554)
    %3930 = stablehlo.transpose %3929, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4555)
    %3931 = stablehlo.dot_general %3926, %3930, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4556)
    %3932 = stablehlo.reshape %3931 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc4557)
    %3933 = stablehlo.transpose %3932, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc4558)
    %3934 = stablehlo.convert %3933 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc4559)
    %3935 = stablehlo.multiply %3934, %cst_7 : tensor<1x20x16x64xf32> loc(#loc4560)
    %3936 = stablehlo.reshape %arg539 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4561)
    %3937 = stablehlo.custom_call @tt.mark_argument(%3936) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ln0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4562)
    %3938 = stablehlo.reshape %3937 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4563)
    %3939 = stablehlo.reshape %arg538 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4564)
    %3940 = stablehlo.custom_call @tt.mark_argument(%3939) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ln0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4565)
    %3941 = stablehlo.reshape %3940 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4566)
    %3942 = stablehlo.composite "tenstorrent.layer_norm" %3743, %3938, %3941 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_14} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4567)
    %3943 = stablehlo.concatenate %3942, %3925, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc4568)
    %3944 = stablehlo.reshape %3943 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4569)
    %3945 = stablehlo.reshape %arg540 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4570)
    %3946 = stablehlo.custom_call @tt.mark_argument(%3945) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_attn_to_k_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4571)
    %3947 = stablehlo.reshape %3946 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4572)
    %3948 = stablehlo.transpose %3947, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4573)
    %3949 = stablehlo.dot_general %3944, %3948, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4574)
    %3950 = stablehlo.reshape %3949 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc4575)
    %3951 = stablehlo.transpose %3950, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc4576)
    %3952 = stablehlo.convert %3951 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc4577)
    %3953 = stablehlo.transpose %3952, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc4578)
    %3954 = stablehlo.multiply %3953, %cst_2 : tensor<1x20x64x273xf32> loc(#loc4579)
    %3955 = stablehlo.dot_general %3935, %3954, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc4580)
    %3956 = stablehlo.convert %3955 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc4581)
    %3957 = stablehlo.compare  EQ, %3956, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc4582)
    %3958 = stablehlo.not %3957 : tensor<1x20x16x273xi1> loc(#loc4583)
    %3959 = stablehlo.reduce(%3958 init: %c_10) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("reduce.12994"), %arg559: tensor<i1> loc("reduce.12994"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc4585)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc4586)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc4584)
    %3960 = stablehlo.reshape %3959 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc4587)
    %3961 = stablehlo.not %3960 : tensor<1x20x16x1xi1> loc(#loc4588)
    %3962 = stablehlo.reshape %3961 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc4589)
    %3963 = stablehlo.broadcast_in_dim %3962, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc4590)
    %3964 = stablehlo.reduce(%3955 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc4591)
    %3965 = stablehlo.broadcast_in_dim %3964, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc4592)
    %3966 = stablehlo.subtract %3955, %3965 : tensor<1x20x16x273xf32> loc(#loc4593)
    %3967 = stablehlo.exponential %3966 : tensor<1x20x16x273xf32> loc(#loc4594)
    %3968 = stablehlo.reduce(%3967 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc4595)
    %3969 = stablehlo.broadcast_in_dim %3968, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc4596)
    %3970 = stablehlo.divide %3967, %3969 : tensor<1x20x16x273xf32> loc(#loc4597)
    %3971 = stablehlo.select %3963, %cst_0, %3970 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc4598)
    %3972 = stablehlo.reshape %arg535 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4599)
    %3973 = stablehlo.custom_call @tt.mark_argument(%3972) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_attn_to_v_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4600)
    %3974 = stablehlo.reshape %3973 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4601)
    %3975 = stablehlo.transpose %3974, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4602)
    %3976 = stablehlo.dot_general %3944, %3975, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4603)
    %3977 = stablehlo.reshape %3976 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc4604)
    %3978 = stablehlo.transpose %3977, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc4605)
    %3979 = stablehlo.convert %3978 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc4606)
    %3980 = stablehlo.dot_general %3971, %3979, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc4607)
    %3981 = stablehlo.convert %3980 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc4608)
    %3982 = stablehlo.transpose %3981, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc4609)
    %3983 = stablehlo.reshape %3982 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc4610)
    %3984 = stablehlo.reshape %arg534 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4611)
    %3985 = stablehlo.custom_call @tt.mark_argument(%3984) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_attn_to_out_0_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4612)
    %3986 = stablehlo.reshape %3985 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4613)
    %3987 = stablehlo.transpose %3986, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4614)
    %3988 = stablehlo.dot_general %3983, %3987, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4615)
    %3989 = stablehlo.reshape %3988 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4616)
    %3990 = stablehlo.divide %3989, %cst : tensor<1x16x1280xbf16> loc(#loc4617)
    %3991 = stablehlo.add %3990, %3918 : tensor<1x16x1280xbf16> loc(#loc4618)
    %3992 = stablehlo.reshape %arg545 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4619)
    %3993 = stablehlo.custom_call @tt.mark_argument(%3992) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ff_0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4620)
    %3994 = stablehlo.reshape %3993 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4621)
    %3995 = stablehlo.reshape %arg544 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4622)
    %3996 = stablehlo.custom_call @tt.mark_argument(%3995) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ff_0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4623)
    %3997 = stablehlo.reshape %3996 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4624)
    %3998 = stablehlo.composite "tenstorrent.layer_norm" %3991, %3994, %3997 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_11} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4625)
    %3999 = stablehlo.reshape %3998 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4626)
    %4000 = stablehlo.reshape %arg543 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4627)
    %4001 = stablehlo.custom_call @tt.mark_argument(%4000) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_0_proj_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4628)
    %4002 = stablehlo.reshape %4001 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc4629)
    %4003 = stablehlo.transpose %4002, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc4630)
    %4004 = stablehlo.dot_general %3999, %4003, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc4631)
    %4005 = stablehlo.reshape %4004 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc4632)
    %4006 = stablehlo.composite "tenstorrent.gelu" %4005 {decomposition = @tenstorrent.gelu.impl_11} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc4633)
    %4007 = stablehlo.reshape %4006 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc4634)
    %4008 = stablehlo.reshape %arg542 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4635)
    %4009 = stablehlo.custom_call @tt.mark_argument(%4008) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4636)
    %4010 = stablehlo.reshape %4009 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc4637)
    %4011 = stablehlo.transpose %4010, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc4638)
    %4012 = stablehlo.dot_general %4007, %4011, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4639)
    %4013 = stablehlo.reshape %4012 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4640)
    %4014 = stablehlo.add %4013, %3991 : tensor<1x16x1280xbf16> loc(#loc4641)
    %4015 = stablehlo.reshape %arg549 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4642)
    %4016 = stablehlo.custom_call @tt.mark_argument(%4015) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ln1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4643)
    %4017 = stablehlo.reshape %4016 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4644)
    %4018 = stablehlo.reshape %arg548 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4645)
    %4019 = stablehlo.custom_call @tt.mark_argument(%4018) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ln1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4646)
    %4020 = stablehlo.reshape %4019 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4647)
    %4021 = stablehlo.composite "tenstorrent.layer_norm" %4014, %4017, %4020 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_0} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4648)
    %4022 = stablehlo.reshape %4021 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4649)
    %4023 = stablehlo.reshape %arg553 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4650)
    %4024 = stablehlo.custom_call @tt.mark_argument(%4023) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_attn_to_q_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4651)
    %4025 = stablehlo.reshape %4024 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4652)
    %4026 = stablehlo.transpose %4025, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4653)
    %4027 = stablehlo.dot_general %4022, %4026, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4654)
    %4028 = stablehlo.reshape %4027 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc4655)
    %4029 = stablehlo.transpose %4028, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc4656)
    %4030 = stablehlo.convert %4029 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc4657)
    %4031 = stablehlo.multiply %4030, %cst_7 : tensor<1x20x16x64xf32> loc(#loc4658)
    %4032 = stablehlo.reshape %arg551 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4659)
    %4033 = stablehlo.custom_call @tt.mark_argument(%4032) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ln0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4660)
    %4034 = stablehlo.reshape %4033 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4661)
    %4035 = stablehlo.reshape %arg550 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4662)
    %4036 = stablehlo.custom_call @tt.mark_argument(%4035) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ln0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4663)
    %4037 = stablehlo.reshape %4036 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4664)
    %4038 = stablehlo.composite "tenstorrent.layer_norm" %3743, %4034, %4037 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_4} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4665)
    %4039 = stablehlo.concatenate %4038, %4021, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc4666)
    %4040 = stablehlo.reshape %4039 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4667)
    %4041 = stablehlo.reshape %arg552 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4668)
    %4042 = stablehlo.custom_call @tt.mark_argument(%4041) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_attn_to_k_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4669)
    %4043 = stablehlo.reshape %4042 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4670)
    %4044 = stablehlo.transpose %4043, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4671)
    %4045 = stablehlo.dot_general %4040, %4044, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4672)
    %4046 = stablehlo.reshape %4045 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc4673)
    %4047 = stablehlo.transpose %4046, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc4674)
    %4048 = stablehlo.convert %4047 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc4675)
    %4049 = stablehlo.transpose %4048, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc4676)
    %4050 = stablehlo.multiply %4049, %cst_2 : tensor<1x20x64x273xf32> loc(#loc4677)
    %4051 = stablehlo.dot_general %4031, %4050, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc4678)
    %4052 = stablehlo.convert %4051 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc4679)
    %4053 = stablehlo.compare  EQ, %4052, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc4680)
    %4054 = stablehlo.not %4053 : tensor<1x20x16x273xi1> loc(#loc4681)
    %4055 = stablehlo.reduce(%4054 init: %c_10) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("reduce.13410"), %arg559: tensor<i1> loc("reduce.13410"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc4683)
      %4131 = stablehlo.select %4130, %c, %c_10 : tensor<i1>, tensor<i1> loc(#loc4684)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc4682)
    %4056 = stablehlo.reshape %4055 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc4685)
    %4057 = stablehlo.not %4056 : tensor<1x20x16x1xi1> loc(#loc4686)
    %4058 = stablehlo.reshape %4057 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc4687)
    %4059 = stablehlo.broadcast_in_dim %4058, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc4688)
    %4060 = stablehlo.reduce(%4051 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc4689)
    %4061 = stablehlo.broadcast_in_dim %4060, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc4690)
    %4062 = stablehlo.subtract %4051, %4061 : tensor<1x20x16x273xf32> loc(#loc4691)
    %4063 = stablehlo.exponential %4062 : tensor<1x20x16x273xf32> loc(#loc4692)
    %4064 = stablehlo.reduce(%4063 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc4693)
    %4065 = stablehlo.broadcast_in_dim %4064, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc4694)
    %4066 = stablehlo.divide %4063, %4065 : tensor<1x20x16x273xf32> loc(#loc4695)
    %4067 = stablehlo.select %4059, %cst_0, %4066 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc4696)
    %4068 = stablehlo.reshape %arg547 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4697)
    %4069 = stablehlo.custom_call @tt.mark_argument(%4068) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_attn_to_v_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4698)
    %4070 = stablehlo.reshape %4069 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4699)
    %4071 = stablehlo.transpose %4070, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4700)
    %4072 = stablehlo.dot_general %4040, %4071, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc4701)
    %4073 = stablehlo.reshape %4072 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc4702)
    %4074 = stablehlo.transpose %4073, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc4703)
    %4075 = stablehlo.convert %4074 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc4704)
    %4076 = stablehlo.dot_general %4067, %4075, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc4705)
    %4077 = stablehlo.convert %4076 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc4706)
    %4078 = stablehlo.transpose %4077, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc4707)
    %4079 = stablehlo.reshape %4078 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc4708)
    %4080 = stablehlo.reshape %arg546 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4709)
    %4081 = stablehlo.custom_call @tt.mark_argument(%4080) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_attn_to_out_0_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc4710)
    %4082 = stablehlo.reshape %4081 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4711)
    %4083 = stablehlo.transpose %4082, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc4712)
    %4084 = stablehlo.dot_general %4079, %4083, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4713)
    %4085 = stablehlo.reshape %4084 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4714)
    %4086 = stablehlo.divide %4085, %cst : tensor<1x16x1280xbf16> loc(#loc4715)
    %4087 = stablehlo.add %4086, %4014 : tensor<1x16x1280xbf16> loc(#loc4716)
    %4088 = stablehlo.reshape %arg557 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4717)
    %4089 = stablehlo.custom_call @tt.mark_argument(%4088) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ff_0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4718)
    %4090 = stablehlo.reshape %4089 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4719)
    %4091 = stablehlo.reshape %arg556 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4720)
    %4092 = stablehlo.custom_call @tt.mark_argument(%4091) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ff_0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc4721)
    %4093 = stablehlo.reshape %4092 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc4722)
    %4094 = stablehlo.composite "tenstorrent.layer_norm" %4087, %4090, %4093 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4723)
    %4095 = stablehlo.reshape %4094 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4724)
    %4096 = stablehlo.reshape %arg555 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4725)
    %4097 = stablehlo.custom_call @tt.mark_argument(%4096) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_0_proj_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc4726)
    %4098 = stablehlo.reshape %4097 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc4727)
    %4099 = stablehlo.transpose %4098, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc4728)
    %4100 = stablehlo.dot_general %4095, %4099, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc4729)
    %4101 = stablehlo.reshape %4100 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc4730)
    %4102 = stablehlo.composite "tenstorrent.gelu" %4101 {decomposition = @tenstorrent.gelu.impl_14} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc4731)
    %4103 = stablehlo.reshape %4102 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc4732)
    %4104 = stablehlo.reshape %arg554 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4733)
    %4105 = stablehlo.custom_call @tt.mark_argument(%4104) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc4734)
    %4106 = stablehlo.reshape %4105 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc4735)
    %4107 = stablehlo.transpose %4106, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc4736)
    %4108 = stablehlo.dot_general %4103, %4107, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4737)
    %4109 = stablehlo.reshape %4108 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4738)
    %4110 = stablehlo.add %4109, %4087 : tensor<1x16x1280xbf16> loc(#loc4739)
    %4111 = stablehlo.reshape %4110 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4740)
    %4112 = stablehlo.reshape %arg3 : (tensor<2048x1280xbf16>) -> tensor<1x2048x1280xbf16> loc(#loc4741)
    %4113 = stablehlo.custom_call @tt.mark_argument(%4112) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_proj_out_weight"}} : (tensor<1x2048x1280xbf16>) -> tensor<1x2048x1280xbf16> loc(#loc4742)
    %4114 = stablehlo.reshape %4113 : (tensor<1x2048x1280xbf16>) -> tensor<2048x1280xbf16> loc(#loc4743)
    %4115 = stablehlo.transpose %4114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,2048]{0,1}"} : (tensor<2048x1280xbf16>) -> tensor<1280x2048xbf16> loc(#loc4744)
    %4116 = stablehlo.dot_general %4111, %4115, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc4745)
    %4117 = stablehlo.reshape %4116 : (tensor<16x2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc4746)
    %4118 = stablehlo.reshape %arg2 : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc4747)
    %4119 = stablehlo.custom_call @tt.mark_argument(%4118) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_proj_out_bias"}} : (tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc4748)
    %4120 = stablehlo.reshape %4119 : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc4749)
    %4121 = stablehlo.broadcast_in_dim %4120, dims = [2] : (tensor<2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc4750)
    %4122 = stablehlo.add %4117, %4121 : tensor<1x16x2048xbf16> loc(#loc4751)
    %4123 = stablehlo.reshape %arg1 : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc4752)
    %4124 = stablehlo.custom_call @tt.mark_argument(%4123) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_norm_out_weight"}} : (tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc4753)
    %4125 = stablehlo.reshape %4124 : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc4754)
    %4126 = stablehlo.reshape %arg0 : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc4755)
    %4127 = stablehlo.custom_call @tt.mark_argument(%4126) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_norm_out_bias"}} : (tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc4756)
    %4128 = stablehlo.reshape %4127 : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc4757)
    %4129 = stablehlo.composite "tenstorrent.layer_norm" %4122, %4125, %4128 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<2048> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_19} : (tensor<1x16x2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc4758)
    return %4129 : tensor<1x16x2048xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl(%arg0: tensor<1x16x1280xbf16> loc("custom-call.13457"), %arg1: tensor<1280xbf16> loc("custom-call.13454"), %arg2: tensor<1280xbf16> loc("custom-call.13449")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x16x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x16xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4762)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x16xbf16> loc(#loc4763)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4764)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x16x1280xbf16> loc(#loc4765)
    %4 = stablehlo.multiply %3, %3 : tensor<1x16x1280xbf16> loc(#loc4766)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4767)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x16xbf16> loc(#loc4768)
    %7 = stablehlo.reshape %6 : (tensor<1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4769)
    %8 = stablehlo.add %7, %cst : tensor<1x16x1xbf16> loc(#loc4770)
    %9 = stablehlo.rsqrt %8 : tensor<1x16x1xbf16> loc(#loc4771)
    %10 = stablehlo.reshape %9 : (tensor<1x16x1xbf16>) -> tensor<1x16xbf16> loc(#loc4772)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4773)
    %12 = stablehlo.multiply %3, %11 : tensor<1x16x1280xbf16> loc(#loc4774)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4775)
    %14 = stablehlo.multiply %12, %13 : tensor<1x16x1280xbf16> loc(#loc4776)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4777)
    %16 = stablehlo.add %14, %15 : tensor<1x16x1280xbf16> loc(#loc4778)
    return %16 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_0(%arg0: tensor<1x16x1280xbf16> loc("custom-call.13164"), %arg1: tensor<1280xbf16> loc("custom-call.13161"), %arg2: tensor<1280xbf16> loc("custom-call.13156")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x16x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x16xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4782)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x16xbf16> loc(#loc4783)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4784)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x16x1280xbf16> loc(#loc4785)
    %4 = stablehlo.multiply %3, %3 : tensor<1x16x1280xbf16> loc(#loc4786)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4787)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x16xbf16> loc(#loc4788)
    %7 = stablehlo.reshape %6 : (tensor<1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4789)
    %8 = stablehlo.add %7, %cst : tensor<1x16x1xbf16> loc(#loc4790)
    %9 = stablehlo.rsqrt %8 : tensor<1x16x1xbf16> loc(#loc4791)
    %10 = stablehlo.reshape %9 : (tensor<1x16x1xbf16>) -> tensor<1x16xbf16> loc(#loc4792)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4793)
    %12 = stablehlo.multiply %3, %11 : tensor<1x16x1280xbf16> loc(#loc4794)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4795)
    %14 = stablehlo.multiply %12, %13 : tensor<1x16x1280xbf16> loc(#loc4796)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4797)
    %16 = stablehlo.add %14, %15 : tensor<1x16x1280xbf16> loc(#loc4798)
    return %16 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_1(%arg0: tensor<1x16x1280xbf16> loc("custom-call.12748"), %arg1: tensor<1280xbf16> loc("custom-call.12745"), %arg2: tensor<1280xbf16> loc("custom-call.12740")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x16x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x16xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4802)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x16xbf16> loc(#loc4803)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4804)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x16x1280xbf16> loc(#loc4805)
    %4 = stablehlo.multiply %3, %3 : tensor<1x16x1280xbf16> loc(#loc4806)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4807)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x16xbf16> loc(#loc4808)
    %7 = stablehlo.reshape %6 : (tensor<1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4809)
    %8 = stablehlo.add %7, %cst : tensor<1x16x1xbf16> loc(#loc4810)
    %9 = stablehlo.rsqrt %8 : tensor<1x16x1xbf16> loc(#loc4811)
    %10 = stablehlo.reshape %9 : (tensor<1x16x1xbf16>) -> tensor<1x16xbf16> loc(#loc4812)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4813)
    %12 = stablehlo.multiply %3, %11 : tensor<1x16x1280xbf16> loc(#loc4814)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4815)
    %14 = stablehlo.multiply %12, %13 : tensor<1x16x1280xbf16> loc(#loc4816)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4817)
    %16 = stablehlo.add %14, %15 : tensor<1x16x1280xbf16> loc(#loc4818)
    return %16 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_2(%arg0: tensor<1x16x1280xbf16> loc("custom-call.12625"), %arg1: tensor<1280xbf16> loc("custom-call.12622"), %arg2: tensor<1280xbf16> loc("custom-call.12617")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x16x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x16xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4822)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x16xbf16> loc(#loc4823)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4824)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x16x1280xbf16> loc(#loc4825)
    %4 = stablehlo.multiply %3, %3 : tensor<1x16x1280xbf16> loc(#loc4826)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4827)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x16xbf16> loc(#loc4828)
    %7 = stablehlo.reshape %6 : (tensor<1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4829)
    %8 = stablehlo.add %7, %cst : tensor<1x16x1xbf16> loc(#loc4830)
    %9 = stablehlo.rsqrt %8 : tensor<1x16x1xbf16> loc(#loc4831)
    %10 = stablehlo.reshape %9 : (tensor<1x16x1xbf16>) -> tensor<1x16xbf16> loc(#loc4832)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4833)
    %12 = stablehlo.multiply %3, %11 : tensor<1x16x1280xbf16> loc(#loc4834)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4835)
    %14 = stablehlo.multiply %12, %13 : tensor<1x16x1280xbf16> loc(#loc4836)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4837)
    %16 = stablehlo.add %14, %15 : tensor<1x16x1280xbf16> loc(#loc4838)
    return %16 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_3(%arg0: tensor<1x257x1280xbf16> loc("custom-call.12422"), %arg1: tensor<1280xbf16> loc("custom-call.12419"), %arg2: tensor<1280xbf16> loc("custom-call.12414")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4842)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc4843)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4844)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc4845)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc4846)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4847)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc4848)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc4849)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc4850)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc4851)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc4852)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4853)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc4854)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4855)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc4856)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4857)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc4858)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_4(%arg0: tensor<1x257x1280xbf16> loc("custom-call.13254"), %arg1: tensor<1280xbf16> loc("custom-call.13251"), %arg2: tensor<1280xbf16> loc("custom-call.13246")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4862)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc4863)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4864)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc4865)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc4866)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4867)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc4868)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc4869)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc4870)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc4871)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc4872)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4873)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc4874)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4875)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc4876)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4877)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc4878)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_5(%arg0: tensor<1x16x1280xbf16> loc("custom-call.12332"), %arg1: tensor<1280xbf16> loc("custom-call.12329"), %arg2: tensor<1280xbf16> loc("custom-call.12324")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x16x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x16xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4882)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x16xbf16> loc(#loc4883)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4884)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x16x1280xbf16> loc(#loc4885)
    %4 = stablehlo.multiply %3, %3 : tensor<1x16x1280xbf16> loc(#loc4886)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc4887)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x16xbf16> loc(#loc4888)
    %7 = stablehlo.reshape %6 : (tensor<1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4889)
    %8 = stablehlo.add %7, %cst : tensor<1x16x1xbf16> loc(#loc4890)
    %9 = stablehlo.rsqrt %8 : tensor<1x16x1xbf16> loc(#loc4891)
    %10 = stablehlo.reshape %9 : (tensor<1x16x1xbf16>) -> tensor<1x16xbf16> loc(#loc4892)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4893)
    %12 = stablehlo.multiply %3, %11 : tensor<1x16x1280xbf16> loc(#loc4894)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4895)
    %14 = stablehlo.multiply %12, %13 : tensor<1x16x1280xbf16> loc(#loc4896)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4897)
    %16 = stablehlo.add %14, %15 : tensor<1x16x1280xbf16> loc(#loc4898)
    return %16 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl(%arg0: tensor<1x16x5120xbf16> loc("custom-call.12289")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc4900)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc4901)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc4902)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc4903)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc4904)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_6(%arg0: tensor<1x257x1280xbf16> loc("custom-call.12006"), %arg1: tensor<1280xbf16> loc("custom-call.148"), %arg2: tensor<1280xbf16> loc("custom-call.143")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4908)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc4909)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4910)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc4911)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc4912)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4913)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc4914)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc4915)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc4916)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc4917)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc4918)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4919)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc4920)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4921)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc4922)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4923)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc4924)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_0(%arg0: tensor<1x257x5120xbf16> loc("custom-call.11976")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc4926)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc4927)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4928)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc4929)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc4930)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_1(%arg0: tensor<1x257x5120xbf16> loc("custom-call.11666")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc4932)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc4933)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4934)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc4935)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc4936)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_7(%arg0: tensor<1x257x1280xbf16> loc("custom-call.11582"), %arg1: tensor<1280xbf16> loc("custom-call.260"), %arg2: tensor<1280xbf16> loc("custom-call.255")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4940)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc4941)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4942)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc4943)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc4944)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4945)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc4946)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc4947)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc4948)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc4949)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc4950)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4951)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc4952)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4953)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc4954)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4955)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc4956)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_8(%arg0: tensor<1x257x1280xbf16> loc("custom-call.11379"), %arg1: tensor<1280xbf16> loc("custom-call.294"), %arg2: tensor<1280xbf16> loc("custom-call.289")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4960)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc4961)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4962)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc4963)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc4964)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4965)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc4966)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc4967)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc4968)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc4969)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc4970)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4971)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc4972)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4973)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc4974)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4975)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc4976)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_2(%arg0: tensor<1x257x5120xbf16> loc("custom-call.11356")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc4978)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc4979)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4980)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc4981)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc4982)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_3(%arg0: tensor<1x257x5120xbf16> loc("custom-call.11046")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc4984)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc4985)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc4986)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc4987)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc4988)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_9(%arg0: tensor<1x257x1280xbf16> loc("custom-call.10962"), %arg1: tensor<1280xbf16> loc("custom-call.396"), %arg2: tensor<1280xbf16> loc("custom-call.391")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4992)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc4993)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc4994)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc4995)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc4996)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc4997)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc4998)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc4999)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5000)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5001)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5002)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5003)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5004)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5005)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5006)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5007)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5008)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_4(%arg0: tensor<1x257x5120xbf16> loc("custom-call.10736")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5010)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5011)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5012)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5013)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5014)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_10(%arg0: tensor<1x257x1280xbf16> loc("custom-call.10652"), %arg1: tensor<1280xbf16> loc("custom-call.464"), %arg2: tensor<1280xbf16> loc("custom-call.459")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5018)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5019)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5020)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5021)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5022)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5023)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5024)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5025)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5026)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5027)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5028)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5029)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5030)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5031)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5032)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5033)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5034)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_11(%arg0: tensor<1x16x1280xbf16> loc("custom-call.13041"), %arg1: tensor<1280xbf16> loc("custom-call.13038"), %arg2: tensor<1280xbf16> loc("custom-call.13033")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x16x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x16xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc5038)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x16xbf16> loc(#loc5039)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5040)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x16x1280xbf16> loc(#loc5041)
    %4 = stablehlo.multiply %3, %3 : tensor<1x16x1280xbf16> loc(#loc5042)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc5043)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x16xbf16> loc(#loc5044)
    %7 = stablehlo.reshape %6 : (tensor<1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5045)
    %8 = stablehlo.add %7, %cst : tensor<1x16x1xbf16> loc(#loc5046)
    %9 = stablehlo.rsqrt %8 : tensor<1x16x1xbf16> loc(#loc5047)
    %10 = stablehlo.reshape %9 : (tensor<1x16x1xbf16>) -> tensor<1x16xbf16> loc(#loc5048)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5049)
    %12 = stablehlo.multiply %3, %11 : tensor<1x16x1280xbf16> loc(#loc5050)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5051)
    %14 = stablehlo.multiply %12, %13 : tensor<1x16x1280xbf16> loc(#loc5052)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5053)
    %16 = stablehlo.add %14, %15 : tensor<1x16x1280xbf16> loc(#loc5054)
    return %16 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_12(%arg0: tensor<1x257x1280xbf16> loc("custom-call.10449"), %arg1: tensor<1280xbf16> loc("custom-call.498"), %arg2: tensor<1280xbf16> loc("custom-call.493")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5058)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5059)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5060)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5061)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5062)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5063)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5064)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5065)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5066)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5067)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5068)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5069)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5070)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5071)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5072)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5073)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5074)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_5(%arg0: tensor<1x257x5120xbf16> loc("custom-call.10426")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5076)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5077)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5078)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5079)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5080)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_13(%arg0: tensor<1x257x1280xbf16> loc("custom-call.10342"), %arg1: tensor<1280xbf16> loc("custom-call.532"), %arg2: tensor<1280xbf16> loc("custom-call.527")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5084)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5085)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5086)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5087)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5088)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5089)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5090)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5091)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5092)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5093)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5094)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5095)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5096)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5097)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5098)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5099)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5100)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_14(%arg0: tensor<1x257x1280xbf16> loc("custom-call.12838"), %arg1: tensor<1280xbf16> loc("custom-call.12835"), %arg2: tensor<1280xbf16> loc("custom-call.12830")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5104)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5105)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5106)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5107)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5108)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5109)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5110)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5111)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5112)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5113)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5114)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5115)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5116)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5117)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5118)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5119)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5120)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_15(%arg0: tensor<1x257x1280xbf16> loc("custom-call.10139"), %arg1: tensor<1280xbf16> loc("custom-call.566"), %arg2: tensor<1280xbf16> loc("custom-call.561")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5124)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5125)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5126)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5127)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5128)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5129)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5130)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5131)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5132)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5133)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5134)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5135)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5136)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5137)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5138)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5139)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5140)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_16(%arg0: tensor<1x257x1280xbf16> loc("custom-call.10032"), %arg1: tensor<1280xbf16> loc("custom-call.600"), %arg2: tensor<1280xbf16> loc("custom-call.595")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5144)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5145)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5146)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5147)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5148)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5149)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5150)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5151)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5152)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5153)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5154)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5155)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5156)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5157)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5158)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5159)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5160)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_17(%arg0: tensor<1x257x1280xbf16> loc("custom-call.9722"), %arg1: tensor<1280xbf16> loc("custom-call.668"), %arg2: tensor<1280xbf16> loc("custom-call.663")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5164)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5165)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5166)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5167)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5168)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5169)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5170)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5171)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5172)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5173)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5174)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5175)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5176)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5177)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5178)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5179)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5180)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_6(%arg0: tensor<1x257x5120xbf16> loc("custom-call.9496")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5182)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5183)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5184)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5185)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5186)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_18(%arg0: tensor<1x257x1280xbf16> loc("custom-call.8899"), %arg1: tensor<1280xbf16> loc("custom-call.838"), %arg2: tensor<1280xbf16> loc("custom-call.833")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5190)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5191)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5192)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5193)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5194)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5195)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5196)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5197)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5198)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5199)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5200)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5201)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5202)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5203)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5204)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5205)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5206)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_7(%arg0: tensor<1x257x5120xbf16> loc("custom-call.8876")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5208)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5209)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5210)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5211)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5212)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_19(%arg0: tensor<1x16x2048xbf16> loc("custom-call.13563"), %arg1: tensor<2048xbf16> loc("custom-call.11"), %arg2: tensor<2048xbf16> loc("custom-call.6")) -> tensor<1x16x2048xbf16> {
    %cst = stablehlo.constant dense<4.882810e-04> : tensor<1x16xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<1.001360e-05> : tensor<1x16x1xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x2048xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc5216)
    %1 = stablehlo.multiply %0, %cst : tensor<1x16xbf16> loc(#loc5217)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x2048xbf16> loc(#loc5218)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x16x2048xbf16> loc(#loc5219)
    %4 = stablehlo.multiply %3, %3 : tensor<1x16x2048xbf16> loc(#loc5220)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x2048xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc5221)
    %6 = stablehlo.multiply %5, %cst : tensor<1x16xbf16> loc(#loc5222)
    %7 = stablehlo.reshape %6 : (tensor<1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5223)
    %8 = stablehlo.add %7, %cst_0 : tensor<1x16x1xbf16> loc(#loc5224)
    %9 = stablehlo.rsqrt %8 : tensor<1x16x1xbf16> loc(#loc5225)
    %10 = stablehlo.reshape %9 : (tensor<1x16x1xbf16>) -> tensor<1x16xbf16> loc(#loc5226)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x2048xbf16> loc(#loc5227)
    %12 = stablehlo.multiply %3, %11 : tensor<1x16x2048xbf16> loc(#loc5228)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc5229)
    %14 = stablehlo.multiply %12, %13 : tensor<1x16x2048xbf16> loc(#loc5230)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc5231)
    %16 = stablehlo.add %14, %15 : tensor<1x16x2048xbf16> loc(#loc5232)
    return %16 : tensor<1x16x2048xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_20(%arg0: tensor<1x257x1280xbf16> loc("custom-call.8589"), %arg1: tensor<1280xbf16> loc("custom-call.906"), %arg2: tensor<1280xbf16> loc("custom-call.901")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5236)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5237)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5238)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5239)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5240)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5241)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5242)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5243)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5244)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5245)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5246)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5247)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5248)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5249)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5250)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5251)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5252)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_21(%arg0: tensor<1x257x1280xbf16> loc("custom-call.8482"), %arg1: tensor<1280xbf16> loc("custom-call.940"), %arg2: tensor<1280xbf16> loc("custom-call.935")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5256)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5257)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5258)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5259)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5260)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5261)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5262)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5263)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5264)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5265)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5266)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5267)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5268)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5269)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5270)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5271)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5272)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_22(%arg0: tensor<1x257x1280xbf16> loc("custom-call.4762"), %arg1: tensor<1280xbf16> loc("custom-call.1756"), %arg2: tensor<1280xbf16> loc("custom-call.1751")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5276)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5277)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5278)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5279)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5280)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5281)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5282)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5283)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5284)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5285)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5286)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5287)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5288)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5289)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5290)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5291)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5292)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_23(%arg0: tensor<1x257x1280xbf16> loc("custom-call.2312"), %arg1: tensor<1280xbf16> loc("custom-call.2279"), %arg2: tensor<1280xbf16> loc("custom-call.2274")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5296)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5297)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5298)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5299)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5300)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5301)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5302)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5303)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5304)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5305)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5306)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5307)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5308)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5309)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5310)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5311)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5312)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_8(%arg0: tensor<1x257x5120xbf16> loc("custom-call.4536")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5314)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5315)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5316)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5317)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5318)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_24(%arg0: tensor<1x257x1280xbf16> loc("custom-call.3522"), %arg1: tensor<1280xbf16> loc("custom-call.2028"), %arg2: tensor<1280xbf16> loc("custom-call.2023")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5322)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5323)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5324)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5325)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5326)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5327)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5328)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5329)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5330)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5331)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5332)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5333)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5334)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5335)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5336)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5337)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5338)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_25(%arg0: tensor<1x257x1280xbf16> loc("custom-call.2592"), %arg1: tensor<1280xbf16> loc("custom-call.2232"), %arg2: tensor<1280xbf16> loc("custom-call.2227")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5342)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5343)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5344)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5345)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5346)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5347)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5348)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5349)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5350)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5351)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5352)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5353)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5354)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5355)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5356)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5357)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5358)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_26(%arg0: tensor<1x257x1280xbf16> loc("custom-call.10759"), %arg1: tensor<1280xbf16> loc("custom-call.430"), %arg2: tensor<1280xbf16> loc("custom-call.425")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5362)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5363)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5364)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5365)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5366)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5367)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5368)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5369)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5370)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5371)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5372)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5373)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5374)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5375)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5376)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5377)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5378)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_27(%arg0: tensor<1x16x1280xbf16> loc("custom-call.61"), %arg1: tensor<1280xbf16> loc("custom-call.58"), %arg2: tensor<1280xbf16> loc("custom-call.53")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x16x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x16xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc5382)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x16xbf16> loc(#loc5383)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5384)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x16x1280xbf16> loc(#loc5385)
    %4 = stablehlo.multiply %3, %3 : tensor<1x16x1280xbf16> loc(#loc5386)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc5387)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x16xbf16> loc(#loc5388)
    %7 = stablehlo.reshape %6 : (tensor<1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5389)
    %8 = stablehlo.add %7, %cst : tensor<1x16x1xbf16> loc(#loc5390)
    %9 = stablehlo.rsqrt %8 : tensor<1x16x1xbf16> loc(#loc5391)
    %10 = stablehlo.reshape %9 : (tensor<1x16x1xbf16>) -> tensor<1x16xbf16> loc(#loc5392)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5393)
    %12 = stablehlo.multiply %3, %11 : tensor<1x16x1280xbf16> loc(#loc5394)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5395)
    %14 = stablehlo.multiply %12, %13 : tensor<1x16x1280xbf16> loc(#loc5396)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5397)
    %16 = stablehlo.add %14, %15 : tensor<1x16x1280xbf16> loc(#loc5398)
    return %16 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_28(%arg0: tensor<1x257x1280xbf16> loc("custom-call.9209"), %arg1: tensor<1280xbf16> loc("custom-call.770"), %arg2: tensor<1280xbf16> loc("custom-call.765")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5402)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5403)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5404)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5405)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5406)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5407)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5408)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5409)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5410)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5411)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5412)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5413)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5414)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5415)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5416)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5417)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5418)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_29(%arg0: tensor<1x257x1280xbf16> loc("custom-call.4249"), %arg1: tensor<1280xbf16> loc("custom-call.1858"), %arg2: tensor<1280xbf16> loc("custom-call.1853")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5422)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5423)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5424)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5425)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5426)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5427)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5428)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5429)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5430)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5431)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5432)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5433)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5434)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5435)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5436)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5437)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5438)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_9(%arg0: tensor<1x16x5120xbf16> loc("custom-call.12705")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc5440)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc5441)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc5442)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc5443)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc5444)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_30(%arg0: tensor<1x257x1280xbf16> loc("custom-call.6419"), %arg1: tensor<1280xbf16> loc("custom-call.1382"), %arg2: tensor<1280xbf16> loc("custom-call.1377")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5448)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5449)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5450)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5451)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5452)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5453)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5454)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5455)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5456)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5457)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5458)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5459)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5460)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5461)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5462)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5463)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5464)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_31(%arg0: tensor<1x257x1280xbf16> loc("custom-call.3939"), %arg1: tensor<1280xbf16> loc("custom-call.1926"), %arg2: tensor<1280xbf16> loc("custom-call.1921")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5468)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5469)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5470)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5471)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5472)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5473)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5474)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5475)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5476)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5477)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5478)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5479)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5480)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5481)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5482)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5483)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5484)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_32(%arg0: tensor<1x257x1280xbf16> loc("custom-call.9102"), %arg1: tensor<1280xbf16> loc("custom-call.804"), %arg2: tensor<1280xbf16> loc("custom-call.799")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5488)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5489)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5490)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5491)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5492)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5493)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5494)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5495)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5496)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5497)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5498)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5499)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5500)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5501)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5502)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5503)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5504)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_10(%arg0: tensor<1x257x5120xbf16> loc("custom-call.3916")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5506)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5507)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5508)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5509)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5510)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_33(%arg0: tensor<1x257x1280xbf16> loc("custom-call.5489"), %arg1: tensor<1280xbf16> loc("custom-call.1586"), %arg2: tensor<1280xbf16> loc("custom-call.1581")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5514)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5515)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5516)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5517)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5518)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5519)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5520)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5521)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5522)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5523)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5524)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5525)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5526)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5527)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5528)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5529)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5530)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_11(%arg0: tensor<1x16x5120xbf16> loc("custom-call.13121")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc5532)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc5533)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc5534)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc5535)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc5536)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_34(%arg0: tensor<1x257x1280xbf16> loc("custom-call.6002"), %arg1: tensor<1280xbf16> loc("custom-call.1484"), %arg2: tensor<1280xbf16> loc("custom-call.1479")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5540)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5541)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5542)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5543)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5544)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5545)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5546)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5547)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5548)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5549)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5550)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5551)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5552)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5553)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5554)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5555)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5556)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_35(%arg0: tensor<1x257x1280xbf16> loc("custom-call.6312"), %arg1: tensor<1280xbf16> loc("custom-call.1416"), %arg2: tensor<1280xbf16> loc("custom-call.1411")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5560)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5561)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5562)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5563)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5564)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5565)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5566)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5567)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5568)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5569)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5570)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5571)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5572)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5573)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5574)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5575)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5576)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_36(%arg0: tensor<1x257x1280xbf16> loc("custom-call.4452"), %arg1: tensor<1280xbf16> loc("custom-call.1824"), %arg2: tensor<1280xbf16> loc("custom-call.1819")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5580)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5581)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5582)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5583)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5584)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5585)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5586)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5587)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5588)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5589)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5590)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5591)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5592)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5593)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5594)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5595)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5596)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_37(%arg0: tensor<1x257x1280xbf16> loc("custom-call.3629"), %arg1: tensor<1280xbf16> loc("custom-call.1994"), %arg2: tensor<1280xbf16> loc("custom-call.1989")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5600)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5601)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5602)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5603)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5604)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5605)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5606)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5607)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5608)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5609)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5610)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5611)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5612)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5613)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5614)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5615)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5616)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_12(%arg0: tensor<1x257x5120xbf16> loc("custom-call.7326")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5618)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5619)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5620)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5621)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5622)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_38(%arg0: tensor<1x257x1280xbf16> loc("custom-call.2389"), %arg1: tensor<1280xbf16> loc("custom-call.2266"), %arg2: tensor<1280xbf16> loc("custom-call.2261")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5626)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5627)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5628)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5629)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5630)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5631)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5632)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5633)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5634)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5635)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5636)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5637)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5638)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5639)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5640)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5641)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5642)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_13(%arg0: tensor<1x257x5120xbf16> loc("custom-call.5156")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5644)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5645)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5646)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5647)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5648)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_39(%arg0: tensor<1x257x1280xbf16> loc("custom-call.11272"), %arg1: tensor<1280xbf16> loc("custom-call.328"), %arg2: tensor<1280xbf16> loc("custom-call.323")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5652)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5653)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5654)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5655)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5656)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5657)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5658)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5659)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5660)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5661)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5662)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5663)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5664)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5665)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5666)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5667)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5668)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_40(%arg0: tensor<1x257x1280xbf16> loc("custom-call.4869"), %arg1: tensor<1280xbf16> loc("custom-call.1722"), %arg2: tensor<1280xbf16> loc("custom-call.1717")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5672)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5673)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5674)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5675)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5676)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5677)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5678)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5679)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5680)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5681)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5682)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5683)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5684)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5685)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5686)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5687)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5688)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_41(%arg0: tensor<1x257x1280xbf16> loc("custom-call.5799"), %arg1: tensor<1280xbf16> loc("custom-call.1518"), %arg2: tensor<1280xbf16> loc("custom-call.1513")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5692)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5693)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5694)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5695)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5696)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5697)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5698)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5699)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5700)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5701)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5702)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5703)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5704)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5705)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5706)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5707)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5708)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_42(%arg0: tensor<1x257x1280xbf16> loc("custom-call.4142"), %arg1: tensor<1280xbf16> loc("custom-call.1892"), %arg2: tensor<1280xbf16> loc("custom-call.1887")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5712)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5713)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5714)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5715)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5716)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5717)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5718)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5719)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5720)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5721)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5722)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5723)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5724)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5725)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5726)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5727)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5728)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_43(%arg0: tensor<1x257x1280xbf16> loc("custom-call.3319"), %arg1: tensor<1280xbf16> loc("custom-call.2062"), %arg2: tensor<1280xbf16> loc("custom-call.2057")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5732)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5733)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5734)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5735)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5736)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5737)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5738)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5739)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5740)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5741)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5742)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5743)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5744)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5745)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5746)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5747)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5748)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_14(%arg0: tensor<1x16x5120xbf16> loc("custom-call.13537")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc5750)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc5751)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc5752)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc5753)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc5754)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_44(%arg0: tensor<1x257x1280xbf16> loc("custom-call.9829"), %arg1: tensor<1280xbf16> loc("custom-call.634"), %arg2: tensor<1280xbf16> loc("custom-call.629")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5758)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5759)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5760)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5761)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5762)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5763)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5764)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5765)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5766)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5767)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5768)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5769)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5770)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5771)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5772)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5773)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5774)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_15(%arg0: tensor<1x257x5120xbf16> loc("custom-call.8566")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5776)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5777)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5778)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5779)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5780)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_45(%arg0: tensor<1x257x1280xbf16> loc("custom-call.2902"), %arg1: tensor<1280xbf16> loc("custom-call.2164"), %arg2: tensor<1280xbf16> loc("custom-call.2159")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5784)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5785)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5786)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5787)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5788)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5789)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5790)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5791)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5792)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5793)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5794)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5795)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5796)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5797)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5798)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5799)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5800)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_16(%arg0: tensor<1x257x5120xbf16> loc("custom-call.2986")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5802)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5803)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5804)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5805)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5806)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_17(%arg0: tensor<1x257x5120xbf16> loc("custom-call.2676")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5808)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5809)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5810)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5811)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5812)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_46(%arg0: tensor<1x257x1280xbf16> loc("custom-call.6729"), %arg1: tensor<1280xbf16> loc("custom-call.1314"), %arg2: tensor<1280xbf16> loc("custom-call.1309")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5816)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5817)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5818)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5819)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5820)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5821)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5822)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5823)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5824)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5825)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5826)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5827)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5828)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5829)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5830)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5831)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5832)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_47(%arg0: tensor<1x257x1280xbf16> loc("custom-call.11069"), %arg1: tensor<1280xbf16> loc("custom-call.362"), %arg2: tensor<1280xbf16> loc("custom-call.357")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5836)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5837)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5838)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5839)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5840)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5841)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5842)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5843)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5844)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5845)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5846)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5847)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5848)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5849)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5850)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5851)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5852)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_18(%arg0: tensor<1x257x5120xbf16> loc("custom-call.4846")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5854)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5855)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5856)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5857)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5858)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_48(%arg0: tensor<1x16x1280xbf16> loc("custom-call.12209"), %arg1: tensor<1280xbf16> loc("custom-call.12206"), %arg2: tensor<1280xbf16> loc("custom-call.12201")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x16x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x16xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc5862)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x16xbf16> loc(#loc5863)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5864)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x16x1280xbf16> loc(#loc5865)
    %4 = stablehlo.multiply %3, %3 : tensor<1x16x1280xbf16> loc(#loc5866)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xbf16>, tensor<bf16>) -> tensor<1x16xbf16> loc(#loc5867)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x16xbf16> loc(#loc5868)
    %7 = stablehlo.reshape %6 : (tensor<1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5869)
    %8 = stablehlo.add %7, %cst : tensor<1x16x1xbf16> loc(#loc5870)
    %9 = stablehlo.rsqrt %8 : tensor<1x16x1xbf16> loc(#loc5871)
    %10 = stablehlo.reshape %9 : (tensor<1x16x1xbf16>) -> tensor<1x16xbf16> loc(#loc5872)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x16xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5873)
    %12 = stablehlo.multiply %3, %11 : tensor<1x16x1280xbf16> loc(#loc5874)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5875)
    %14 = stablehlo.multiply %12, %13 : tensor<1x16x1280xbf16> loc(#loc5876)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc5877)
    %16 = stablehlo.add %14, %15 : tensor<1x16x1280xbf16> loc(#loc5878)
    return %16 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_19(%arg0: tensor<1x257x5120xbf16> loc("custom-call.9806")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5880)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5881)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5882)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5883)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5884)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_49(%arg0: tensor<1x257x1280xbf16> loc("custom-call.3832"), %arg1: tensor<1280xbf16> loc("custom-call.1960"), %arg2: tensor<1280xbf16> loc("custom-call.1955")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5888)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5889)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5890)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5891)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5892)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5893)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5894)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5895)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5896)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5897)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5898)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5899)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5900)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5901)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5902)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5903)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5904)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_20(%arg0: tensor<1x257x5120xbf16> loc("custom-call.3296")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5906)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5907)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5908)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5909)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5910)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_21(%arg0: tensor<1x257x5120xbf16> loc("custom-call.7016")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5912)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5913)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5914)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5915)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5916)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_50(%arg0: tensor<1x257x1280xbf16> loc("custom-call.7862"), %arg1: tensor<1280xbf16> loc("custom-call.1076"), %arg2: tensor<1280xbf16> loc("custom-call.1071")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5920)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5921)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5922)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5923)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5924)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5925)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5926)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5927)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5928)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5929)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5930)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5931)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5932)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5933)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5934)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5935)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5936)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_51(%arg0: tensor<1x257x1280xbf16> loc("custom-call.5179"), %arg1: tensor<1280xbf16> loc("custom-call.1654"), %arg2: tensor<1280xbf16> loc("custom-call.1649")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5940)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5941)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5942)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5943)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5944)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5945)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5946)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5947)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5948)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5949)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5950)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5951)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5952)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5953)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5954)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5955)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5956)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_52(%arg0: tensor<1x257x1280xbf16> loc("custom-call.5382"), %arg1: tensor<1280xbf16> loc("custom-call.1620"), %arg2: tensor<1280xbf16> loc("custom-call.1615")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5960)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5961)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5962)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5963)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5964)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5965)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5966)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5967)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5968)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5969)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5970)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5971)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5972)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5973)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc5974)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5975)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc5976)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_22(%arg0: tensor<1x257x5120xbf16> loc("custom-call.5466")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc5978)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc5979)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc5980)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc5981)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc5982)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_53(%arg0: tensor<1x257x1280xbf16> loc("custom-call.5072"), %arg1: tensor<1280xbf16> loc("custom-call.1688"), %arg2: tensor<1280xbf16> loc("custom-call.1683")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5986)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc5987)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5988)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc5989)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc5990)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc5991)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc5992)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc5993)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc5994)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc5995)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc5996)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5997)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc5998)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc5999)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6000)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6001)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6002)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_54(%arg0: tensor<1x257x1280xbf16> loc("custom-call.3212"), %arg1: tensor<1280xbf16> loc("custom-call.2096"), %arg2: tensor<1280xbf16> loc("custom-call.2091")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6006)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6007)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6008)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6009)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6010)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6011)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6012)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6013)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6014)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6015)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6016)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6017)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6018)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6019)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6020)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6021)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6022)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_55(%arg0: tensor<1x257x1280xbf16> loc("custom-call.5692"), %arg1: tensor<1280xbf16> loc("custom-call.1552"), %arg2: tensor<1280xbf16> loc("custom-call.1547")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6026)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6027)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6028)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6029)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6030)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6031)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6032)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6033)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6034)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6035)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6036)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6037)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6038)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6039)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6040)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6041)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6042)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_56(%arg0: tensor<1x257x1280xbf16> loc("custom-call.11892"), %arg1: tensor<1280xbf16> loc("custom-call.192"), %arg2: tensor<1280xbf16> loc("custom-call.187")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6046)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6047)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6048)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6049)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6050)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6051)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6052)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6053)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6054)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6055)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6056)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6057)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6058)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6059)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6060)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6061)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6062)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_57(%arg0: tensor<1x257x1280xbf16> loc("custom-call.9519"), %arg1: tensor<1280xbf16> loc("custom-call.702"), %arg2: tensor<1280xbf16> loc("custom-call.697")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6066)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6067)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6068)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6069)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6070)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6071)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6072)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6073)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6074)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6075)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6076)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6077)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6078)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6079)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6080)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6081)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6082)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_23(%arg0: tensor<1x257x5120xbf16> loc("custom-call.4226")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6084)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6085)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6086)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6087)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6088)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_24(%arg0: tensor<1x257x5120xbf16> loc("custom-call.5776")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6090)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6091)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6092)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6093)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6094)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_58(%arg0: tensor<1x257x1280xbf16> loc("custom-call.3009"), %arg1: tensor<1280xbf16> loc("custom-call.2130"), %arg2: tensor<1280xbf16> loc("custom-call.2125")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6098)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6099)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6100)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6101)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6102)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6103)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6104)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6105)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6106)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6107)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6108)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6109)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6110)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6111)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6112)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6113)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6114)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_25(%arg0: tensor<1x257x5120xbf16> loc("custom-call.6086")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6116)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6117)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6118)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6119)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6120)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_26(%arg0: tensor<1x257x5120xbf16> loc("custom-call.6396")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6122)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6123)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6124)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6125)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6126)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_59(%arg0: tensor<1x257x1280xbf16> loc("custom-call.8279"), %arg1: tensor<1280xbf16> loc("custom-call.974"), %arg2: tensor<1280xbf16> loc("custom-call.969")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6130)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6131)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6132)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6133)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6134)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6135)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6136)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6137)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6138)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6139)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6140)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6141)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6142)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6143)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6144)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6145)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6146)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_60(%arg0: tensor<1x257x1280xbf16> loc("custom-call.7039"), %arg1: tensor<1280xbf16> loc("custom-call.1246"), %arg2: tensor<1280xbf16> loc("custom-call.1241")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6150)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6151)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6152)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6153)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6154)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6155)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6156)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6157)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6158)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6159)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6160)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6161)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6162)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6163)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6164)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6165)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6166)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_27(%arg0: tensor<1x257x5120xbf16> loc("custom-call.3606")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6168)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6169)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6170)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6171)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6172)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_61(%arg0: tensor<1x257x1280xbf16> loc("custom-call.6622"), %arg1: tensor<1280xbf16> loc("custom-call.1348"), %arg2: tensor<1280xbf16> loc("custom-call.1343")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6176)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6177)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6178)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6179)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6180)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6181)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6182)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6183)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6184)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6185)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6186)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6187)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6188)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6189)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6190)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6191)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6192)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_62(%arg0: tensor<1x257x1280xbf16> loc("custom-call.9412"), %arg1: tensor<1280xbf16> loc("custom-call.736"), %arg2: tensor<1280xbf16> loc("custom-call.731")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6196)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6197)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6198)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6199)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6200)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6201)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6202)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6203)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6204)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6205)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6206)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6207)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6208)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6209)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6210)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6211)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6212)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_63(%arg0: tensor<1x257x1280xbf16> loc("custom-call.8792"), %arg1: tensor<1280xbf16> loc("custom-call.872"), %arg2: tensor<1280xbf16> loc("custom-call.867")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6216)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6217)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6218)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6219)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6220)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6221)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6222)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6223)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6224)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6225)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6226)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6227)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6228)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6229)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6230)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6231)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6232)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_28(%arg0: tensor<1x257x5120xbf16> loc("custom-call.6706")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6234)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6235)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6236)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6237)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6238)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_64(%arg0: tensor<1x257x1280xbf16> loc("custom-call.6109"), %arg1: tensor<1280xbf16> loc("custom-call.1450"), %arg2: tensor<1280xbf16> loc("custom-call.1445")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6242)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6243)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6244)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6245)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6246)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6247)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6248)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6249)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6250)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6251)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6252)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6253)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6254)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6255)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6256)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6257)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6258)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_65(%arg0: tensor<1x257x1280xbf16> loc("custom-call.6932"), %arg1: tensor<1280xbf16> loc("custom-call.1280"), %arg2: tensor<1280xbf16> loc("custom-call.1275")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6262)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6263)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6264)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6265)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6266)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6267)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6268)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6269)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6270)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6271)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6272)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6273)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6274)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6275)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6276)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6277)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6278)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_66(%arg0: tensor<1x257x1280xbf16> loc("custom-call.7242"), %arg1: tensor<1280xbf16> loc("custom-call.1212"), %arg2: tensor<1280xbf16> loc("custom-call.1207")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6282)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6283)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6284)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6285)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6286)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6287)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6288)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6289)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6290)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6291)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6292)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6293)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6294)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6295)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6296)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6297)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6298)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_67(%arg0: tensor<1x257x1280xbf16> loc("custom-call.7969"), %arg1: tensor<1280xbf16> loc("custom-call.1042"), %arg2: tensor<1280xbf16> loc("custom-call.1037")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6302)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6303)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6304)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6305)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6306)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6307)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6308)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6309)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6310)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6311)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6312)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6313)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6314)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6315)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6316)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6317)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6318)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_68(%arg0: tensor<1x257x1280xbf16> loc("custom-call.7349"), %arg1: tensor<1280xbf16> loc("custom-call.1178"), %arg2: tensor<1280xbf16> loc("custom-call.1173")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6322)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6323)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6324)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6325)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6326)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6327)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6328)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6329)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6330)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6331)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6332)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6333)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6334)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6335)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6336)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6337)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6338)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_29(%arg0: tensor<1x257x5120xbf16> loc("custom-call.10116")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6340)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6341)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6342)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6343)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6344)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_30(%arg0: tensor<1x257x5120xbf16> loc("custom-call.9186")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6346)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6347)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6348)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6349)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6350)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_69(%arg0: tensor<1x257x1280xbf16> loc("custom-call.7552"), %arg1: tensor<1280xbf16> loc("custom-call.1144"), %arg2: tensor<1280xbf16> loc("custom-call.1139")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6354)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6355)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6356)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6357)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6358)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6359)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6360)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6361)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6362)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6363)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6364)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6365)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6366)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6367)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6368)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6369)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6370)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_70(%arg0: tensor<1x257x1280xbf16> loc("custom-call.2699"), %arg1: tensor<1280xbf16> loc("custom-call.2198"), %arg2: tensor<1280xbf16> loc("custom-call.2193")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6374)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6375)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6376)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6377)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6378)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6379)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6380)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6381)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6382)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6383)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6384)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6385)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6386)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6387)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6388)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6389)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6390)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_31(%arg0: tensor<1x257x5120xbf16> loc("custom-call.7636")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6392)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6393)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6394)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6395)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6396)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_71(%arg0: tensor<1x257x1280xbf16> loc("custom-call.7659"), %arg1: tensor<1280xbf16> loc("custom-call.1110"), %arg2: tensor<1280xbf16> loc("custom-call.1105")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6400)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6401)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6402)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6403)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6404)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6405)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6406)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6407)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6408)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6409)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6410)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6411)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6412)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6413)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6414)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6415)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6416)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_72(%arg0: tensor<1x257x1280xbf16> loc("custom-call.4559"), %arg1: tensor<1280xbf16> loc("custom-call.1790"), %arg2: tensor<1280xbf16> loc("custom-call.1785")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6420)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6421)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6422)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6423)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6424)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6425)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6426)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6427)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6428)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6429)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6430)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6431)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6432)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6433)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6434)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6435)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6436)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_32(%arg0: tensor<1x257x5120xbf16> loc("custom-call.7946")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6438)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6439)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6440)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6441)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6442)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_73(%arg0: tensor<1x257x1280xbf16> loc("custom-call.11689"), %arg1: tensor<1280xbf16> loc("custom-call.226"), %arg2: tensor<1280xbf16> loc("custom-call.221")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6446)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6447)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6448)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6449)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6450)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6451)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6452)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6453)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6454)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6455)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6456)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6457)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6458)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6459)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6460)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6461)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6462)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_74(%arg0: tensor<1x257x1280xbf16> loc("custom-call.8172"), %arg1: tensor<1280xbf16> loc("custom-call.1008"), %arg2: tensor<1280xbf16> loc("custom-call.1003")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<1.001360e-05> : tensor<1x257x1xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.820120e-04> : tensor<1x257xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.reduce(%arg0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6466)
    %1 = stablehlo.multiply %0, %cst_0 : tensor<1x257xbf16> loc(#loc6467)
    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6468)
    %3 = stablehlo.subtract %arg0, %2 : tensor<1x257x1280xbf16> loc(#loc6469)
    %4 = stablehlo.multiply %3, %3 : tensor<1x257x1280xbf16> loc(#loc6470)
    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xbf16>, tensor<bf16>) -> tensor<1x257xbf16> loc(#loc6471)
    %6 = stablehlo.multiply %5, %cst_0 : tensor<1x257xbf16> loc(#loc6472)
    %7 = stablehlo.reshape %6 : (tensor<1x257xbf16>) -> tensor<1x257x1xbf16> loc(#loc6473)
    %8 = stablehlo.add %7, %cst : tensor<1x257x1xbf16> loc(#loc6474)
    %9 = stablehlo.rsqrt %8 : tensor<1x257x1xbf16> loc(#loc6475)
    %10 = stablehlo.reshape %9 : (tensor<1x257x1xbf16>) -> tensor<1x257xbf16> loc(#loc6476)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<1x257xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6477)
    %12 = stablehlo.multiply %3, %11 : tensor<1x257x1280xbf16> loc(#loc6478)
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6479)
    %14 = stablehlo.multiply %12, %13 : tensor<1x257x1280xbf16> loc(#loc6480)
    %15 = stablehlo.broadcast_in_dim %arg2, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc6481)
    %16 = stablehlo.add %14, %15 : tensor<1x257x1280xbf16> loc(#loc6482)
    return %16 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_33(%arg0: tensor<1x257x5120xbf16> loc("custom-call.8256")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc6484)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc6485)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc6486)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc6487)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc6488)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc559 = loc("custom-call.33")
#loc560 = loc("reshape.55")
#loc561 = loc("custom-call.56")
#loc562 = loc("reshape.57")
#loc563 = loc("reshape.50")
#loc564 = loc("custom-call.51")
#loc565 = loc("reshape.52")
#loc566 = loc("custom-call.137")
#loc567 = loc("reshape.12111")
#loc568 = loc("reshape.12107")
#loc569 = loc("custom-call.12108")
#loc570 = loc("reshape.12109")
#loc571 = loc("transpose.12110")
#loc572 = loc("dot.12112")
#loc573 = loc("reshape.12114")
#loc574 = loc("transpose.12115")
#loc575 = loc("convert.12116")
#loc576 = loc("multiply.12118")
#loc577 = loc("reshape.2303")
#loc578 = loc("custom-call.2304")
#loc579 = loc("custom-call.2298")
#loc580 = loc("custom-call.2296")
#loc581 = loc("convolution.2299")
#loc582 = loc("reshape.2300")
#loc583 = loc("transpose.2301")
#loc584 = loc("concatenate.2308")
#loc585 = loc("reshape.2289")
#loc586 = loc("custom-call.2290")
#loc587 = loc("reshape.2291")
#loc588 = loc("reshape.2284")
#loc589 = loc("custom-call.2285")
#loc590 = loc("reshape.2287")
#loc591 = loc("convert.2292")
#loc592 = loc("gather.2293")
#loc593 = loc("reshape.2294")
#loc594 = loc("add.2311")
#loc595 = loc("reshape.2276")
#loc596 = loc("custom-call.2277")
#loc597 = loc("reshape.2278")
#loc598 = loc("reshape.2271")
#loc599 = loc("custom-call.2272")
#loc600 = loc("reshape.2273")
#loc601 = loc("custom-call.2388")
#loc602 = loc("reshape.2263")
#loc603 = loc("custom-call.2264")
#loc604 = loc("reshape.2265")
#loc605 = loc("reshape.2258")
#loc606 = loc("custom-call.2259")
#loc607 = loc("reshape.2260")
#loc608 = loc("custom-call.2465")
#loc609 = loc("reshape.2511")
#loc610 = loc("reshape.2507")
#loc611 = loc("custom-call.2508")
#loc612 = loc("reshape.2509")
#loc613 = loc("transpose.2510")
#loc614 = loc("dot.2512")
#loc615 = loc("reshape.2513")
#loc616 = loc("reshape.2503")
#loc617 = loc("custom-call.2504")
#loc618 = loc("reshape.2505")
#loc619 = loc("broadcast.2516")
#loc620 = loc("add.2517")
#loc621 = loc("reshape.2518")
#loc622 = loc("transpose.2519")
#loc623 = loc("convert.2520")
#loc624 = loc("multiply.2522")
#loc625 = loc("reshape.2483")
#loc626 = loc("custom-call.2484")
#loc627 = loc("reshape.2485")
#loc628 = loc("transpose.2486")
#loc629 = loc("dot.2488")
#loc630 = loc("reshape.2489")
#loc631 = loc("reshape.2479")
#loc632 = loc("custom-call.2480")
#loc633 = loc("reshape.2481")
#loc634 = loc("broadcast.2492")
#loc635 = loc("add.2493")
#loc636 = loc("reshape.2494")
#loc637 = loc("transpose.2495")
#loc638 = loc("convert.2496")
#loc639 = loc("transpose.2497")
#loc640 = loc("multiply.2499")
#loc641 = loc("dot.2523")
#loc642 = loc("convert.2550")
#loc643 = loc("compare.2552")
#loc644 = loc("not.2554")
#loc646 = loc("or.2564")
#loc647 = loc("select.2565")
#loc648 = loc("reshape.2570")
#loc649 = loc("not.2572")
#loc650 = loc("reshape.2574")
#loc651 = loc("broadcast.2575")
#loc652 = loc("reduce.2530")
#loc653 = loc("broadcast.2531")
#loc654 = loc("subtract.2532")
#loc655 = loc("exponential.2533")
#loc656 = loc("reduce.2539")
#loc657 = loc("broadcast.2540")
#loc658 = loc("divide.2541")
#loc659 = loc("select.2576")
#loc660 = loc("reshape.2252")
#loc661 = loc("custom-call.2253")
#loc662 = loc("reshape.2254")
#loc663 = loc("transpose.2255")
#loc664 = loc("dot.2467")
#loc665 = loc("reshape.2468")
#loc666 = loc("reshape.2248")
#loc667 = loc("custom-call.2249")
#loc668 = loc("reshape.2250")
#loc669 = loc("broadcast.2471")
#loc670 = loc("add.2472")
#loc671 = loc("reshape.2473")
#loc672 = loc("transpose.2474")
#loc673 = loc("convert.2475")
#loc674 = loc("dot.2577")
#loc675 = loc("convert.2579")
#loc676 = loc("transpose.2580")
#loc677 = loc("reshape.2582")
#loc678 = loc("reshape.2242")
#loc679 = loc("custom-call.2243")
#loc680 = loc("reshape.2244")
#loc681 = loc("transpose.2245")
#loc682 = loc("dot.2583")
#loc683 = loc("reshape.2584")
#loc684 = loc("reshape.2238")
#loc685 = loc("custom-call.2239")
#loc686 = loc("reshape.2240")
#loc687 = loc("broadcast.2587")
#loc688 = loc("add.2588")
#loc689 = loc("add.2591")
#loc690 = loc("reshape.2229")
#loc691 = loc("custom-call.2230")
#loc692 = loc("reshape.2231")
#loc693 = loc("reshape.2224")
#loc694 = loc("custom-call.2225")
#loc695 = loc("reshape.2226")
#loc696 = loc("custom-call.2668")
#loc697 = loc("reshape.2669")
#loc698 = loc("reshape.2218")
#loc699 = loc("custom-call.2219")
#loc700 = loc("reshape.2220")
#loc701 = loc("transpose.2221")
#loc702 = loc("dot.2670")
#loc703 = loc("reshape.2671")
#loc704 = loc("reshape.2214")
#loc705 = loc("custom-call.2215")
#loc706 = loc("reshape.2216")
#loc707 = loc("broadcast.2674")
#loc708 = loc("add.2675")
#loc709 = loc("custom-call.2688")
#loc710 = loc("reshape.2689")
#loc711 = loc("reshape.2208")
#loc712 = loc("custom-call.2209")
#loc713 = loc("reshape.2210")
#loc714 = loc("transpose.2211")
#loc715 = loc("dot.2690")
#loc716 = loc("reshape.2691")
#loc717 = loc("reshape.2204")
#loc718 = loc("custom-call.2205")
#loc719 = loc("reshape.2206")
#loc720 = loc("broadcast.2694")
#loc721 = loc("add.2695")
#loc722 = loc("add.2698")
#loc723 = loc("reshape.2195")
#loc724 = loc("custom-call.2196")
#loc725 = loc("reshape.2197")
#loc726 = loc("reshape.2190")
#loc727 = loc("custom-call.2191")
#loc728 = loc("reshape.2192")
#loc729 = loc("custom-call.2775")
#loc730 = loc("reshape.2821")
#loc731 = loc("reshape.2817")
#loc732 = loc("custom-call.2818")
#loc733 = loc("reshape.2819")
#loc734 = loc("transpose.2820")
#loc735 = loc("dot.2822")
#loc736 = loc("reshape.2823")
#loc737 = loc("reshape.2813")
#loc738 = loc("custom-call.2814")
#loc739 = loc("reshape.2815")
#loc740 = loc("broadcast.2826")
#loc741 = loc("add.2827")
#loc742 = loc("reshape.2828")
#loc743 = loc("transpose.2829")
#loc744 = loc("convert.2830")
#loc745 = loc("multiply.2832")
#loc746 = loc("reshape.2793")
#loc747 = loc("custom-call.2794")
#loc748 = loc("reshape.2795")
#loc749 = loc("transpose.2796")
#loc750 = loc("dot.2798")
#loc751 = loc("reshape.2799")
#loc752 = loc("reshape.2789")
#loc753 = loc("custom-call.2790")
#loc754 = loc("reshape.2791")
#loc755 = loc("broadcast.2802")
#loc756 = loc("add.2803")
#loc757 = loc("reshape.2804")
#loc758 = loc("transpose.2805")
#loc759 = loc("convert.2806")
#loc760 = loc("transpose.2807")
#loc761 = loc("multiply.2809")
#loc762 = loc("dot.2833")
#loc763 = loc("convert.2860")
#loc764 = loc("compare.2862")
#loc765 = loc("not.2864")
#loc767 = loc("or.2874")
#loc768 = loc("select.2875")
#loc769 = loc("reshape.2880")
#loc770 = loc("not.2882")
#loc771 = loc("reshape.2884")
#loc772 = loc("broadcast.2885")
#loc773 = loc("reduce.2840")
#loc774 = loc("broadcast.2841")
#loc775 = loc("subtract.2842")
#loc776 = loc("exponential.2843")
#loc777 = loc("reduce.2849")
#loc778 = loc("broadcast.2850")
#loc779 = loc("divide.2851")
#loc780 = loc("select.2886")
#loc781 = loc("reshape.2184")
#loc782 = loc("custom-call.2185")
#loc783 = loc("reshape.2186")
#loc784 = loc("transpose.2187")
#loc785 = loc("dot.2777")
#loc786 = loc("reshape.2778")
#loc787 = loc("reshape.2180")
#loc788 = loc("custom-call.2181")
#loc789 = loc("reshape.2182")
#loc790 = loc("broadcast.2781")
#loc791 = loc("add.2782")
#loc792 = loc("reshape.2783")
#loc793 = loc("transpose.2784")
#loc794 = loc("convert.2785")
#loc795 = loc("dot.2887")
#loc796 = loc("convert.2889")
#loc797 = loc("transpose.2890")
#loc798 = loc("reshape.2892")
#loc799 = loc("reshape.2174")
#loc800 = loc("custom-call.2175")
#loc801 = loc("reshape.2176")
#loc802 = loc("transpose.2177")
#loc803 = loc("dot.2893")
#loc804 = loc("reshape.2894")
#loc805 = loc("reshape.2170")
#loc806 = loc("custom-call.2171")
#loc807 = loc("reshape.2172")
#loc808 = loc("broadcast.2897")
#loc809 = loc("add.2898")
#loc810 = loc("add.2901")
#loc811 = loc("reshape.2161")
#loc812 = loc("custom-call.2162")
#loc813 = loc("reshape.2163")
#loc814 = loc("reshape.2156")
#loc815 = loc("custom-call.2157")
#loc816 = loc("reshape.2158")
#loc817 = loc("custom-call.2978")
#loc818 = loc("reshape.2979")
#loc819 = loc("reshape.2150")
#loc820 = loc("custom-call.2151")
#loc821 = loc("reshape.2152")
#loc822 = loc("transpose.2153")
#loc823 = loc("dot.2980")
#loc824 = loc("reshape.2981")
#loc825 = loc("reshape.2146")
#loc826 = loc("custom-call.2147")
#loc827 = loc("reshape.2148")
#loc828 = loc("broadcast.2984")
#loc829 = loc("add.2985")
#loc830 = loc("custom-call.2998")
#loc831 = loc("reshape.2999")
#loc832 = loc("reshape.2140")
#loc833 = loc("custom-call.2141")
#loc834 = loc("reshape.2142")
#loc835 = loc("transpose.2143")
#loc836 = loc("dot.3000")
#loc837 = loc("reshape.3001")
#loc838 = loc("reshape.2136")
#loc839 = loc("custom-call.2137")
#loc840 = loc("reshape.2138")
#loc841 = loc("broadcast.3004")
#loc842 = loc("add.3005")
#loc843 = loc("add.3008")
#loc844 = loc("reshape.2127")
#loc845 = loc("custom-call.2128")
#loc846 = loc("reshape.2129")
#loc847 = loc("reshape.2122")
#loc848 = loc("custom-call.2123")
#loc849 = loc("reshape.2124")
#loc850 = loc("custom-call.3085")
#loc851 = loc("reshape.3131")
#loc852 = loc("reshape.3127")
#loc853 = loc("custom-call.3128")
#loc854 = loc("reshape.3129")
#loc855 = loc("transpose.3130")
#loc856 = loc("dot.3132")
#loc857 = loc("reshape.3133")
#loc858 = loc("reshape.3123")
#loc859 = loc("custom-call.3124")
#loc860 = loc("reshape.3125")
#loc861 = loc("broadcast.3136")
#loc862 = loc("add.3137")
#loc863 = loc("reshape.3138")
#loc864 = loc("transpose.3139")
#loc865 = loc("convert.3140")
#loc866 = loc("multiply.3142")
#loc867 = loc("reshape.3103")
#loc868 = loc("custom-call.3104")
#loc869 = loc("reshape.3105")
#loc870 = loc("transpose.3106")
#loc871 = loc("dot.3108")
#loc872 = loc("reshape.3109")
#loc873 = loc("reshape.3099")
#loc874 = loc("custom-call.3100")
#loc875 = loc("reshape.3101")
#loc876 = loc("broadcast.3112")
#loc877 = loc("add.3113")
#loc878 = loc("reshape.3114")
#loc879 = loc("transpose.3115")
#loc880 = loc("convert.3116")
#loc881 = loc("transpose.3117")
#loc882 = loc("multiply.3119")
#loc883 = loc("dot.3143")
#loc884 = loc("convert.3170")
#loc885 = loc("compare.3172")
#loc886 = loc("not.3174")
#loc888 = loc("or.3184")
#loc889 = loc("select.3185")
#loc890 = loc("reshape.3190")
#loc891 = loc("not.3192")
#loc892 = loc("reshape.3194")
#loc893 = loc("broadcast.3195")
#loc894 = loc("reduce.3150")
#loc895 = loc("broadcast.3151")
#loc896 = loc("subtract.3152")
#loc897 = loc("exponential.3153")
#loc898 = loc("reduce.3159")
#loc899 = loc("broadcast.3160")
#loc900 = loc("divide.3161")
#loc901 = loc("select.3196")
#loc902 = loc("reshape.2116")
#loc903 = loc("custom-call.2117")
#loc904 = loc("reshape.2118")
#loc905 = loc("transpose.2119")
#loc906 = loc("dot.3087")
#loc907 = loc("reshape.3088")
#loc908 = loc("reshape.2112")
#loc909 = loc("custom-call.2113")
#loc910 = loc("reshape.2114")
#loc911 = loc("broadcast.3091")
#loc912 = loc("add.3092")
#loc913 = loc("reshape.3093")
#loc914 = loc("transpose.3094")
#loc915 = loc("convert.3095")
#loc916 = loc("dot.3197")
#loc917 = loc("convert.3199")
#loc918 = loc("transpose.3200")
#loc919 = loc("reshape.3202")
#loc920 = loc("reshape.2106")
#loc921 = loc("custom-call.2107")
#loc922 = loc("reshape.2108")
#loc923 = loc("transpose.2109")
#loc924 = loc("dot.3203")
#loc925 = loc("reshape.3204")
#loc926 = loc("reshape.2102")
#loc927 = loc("custom-call.2103")
#loc928 = loc("reshape.2104")
#loc929 = loc("broadcast.3207")
#loc930 = loc("add.3208")
#loc931 = loc("add.3211")
#loc932 = loc("reshape.2093")
#loc933 = loc("custom-call.2094")
#loc934 = loc("reshape.2095")
#loc935 = loc("reshape.2088")
#loc936 = loc("custom-call.2089")
#loc937 = loc("reshape.2090")
#loc938 = loc("custom-call.3288")
#loc939 = loc("reshape.3289")
#loc940 = loc("reshape.2082")
#loc941 = loc("custom-call.2083")
#loc942 = loc("reshape.2084")
#loc943 = loc("transpose.2085")
#loc944 = loc("dot.3290")
#loc945 = loc("reshape.3291")
#loc946 = loc("reshape.2078")
#loc947 = loc("custom-call.2079")
#loc948 = loc("reshape.2080")
#loc949 = loc("broadcast.3294")
#loc950 = loc("add.3295")
#loc951 = loc("custom-call.3308")
#loc952 = loc("reshape.3309")
#loc953 = loc("reshape.2072")
#loc954 = loc("custom-call.2073")
#loc955 = loc("reshape.2074")
#loc956 = loc("transpose.2075")
#loc957 = loc("dot.3310")
#loc958 = loc("reshape.3311")
#loc959 = loc("reshape.2068")
#loc960 = loc("custom-call.2069")
#loc961 = loc("reshape.2070")
#loc962 = loc("broadcast.3314")
#loc963 = loc("add.3315")
#loc964 = loc("add.3318")
#loc965 = loc("reshape.2059")
#loc966 = loc("custom-call.2060")
#loc967 = loc("reshape.2061")
#loc968 = loc("reshape.2054")
#loc969 = loc("custom-call.2055")
#loc970 = loc("reshape.2056")
#loc971 = loc("custom-call.3395")
#loc972 = loc("reshape.3441")
#loc973 = loc("reshape.3437")
#loc974 = loc("custom-call.3438")
#loc975 = loc("reshape.3439")
#loc976 = loc("transpose.3440")
#loc977 = loc("dot.3442")
#loc978 = loc("reshape.3443")
#loc979 = loc("reshape.3433")
#loc980 = loc("custom-call.3434")
#loc981 = loc("reshape.3435")
#loc982 = loc("broadcast.3446")
#loc983 = loc("add.3447")
#loc984 = loc("reshape.3448")
#loc985 = loc("transpose.3449")
#loc986 = loc("convert.3450")
#loc987 = loc("multiply.3452")
#loc988 = loc("reshape.3413")
#loc989 = loc("custom-call.3414")
#loc990 = loc("reshape.3415")
#loc991 = loc("transpose.3416")
#loc992 = loc("dot.3418")
#loc993 = loc("reshape.3419")
#loc994 = loc("reshape.3409")
#loc995 = loc("custom-call.3410")
#loc996 = loc("reshape.3411")
#loc997 = loc("broadcast.3422")
#loc998 = loc("add.3423")
#loc999 = loc("reshape.3424")
#loc1000 = loc("transpose.3425")
#loc1001 = loc("convert.3426")
#loc1002 = loc("transpose.3427")
#loc1003 = loc("multiply.3429")
#loc1004 = loc("dot.3453")
#loc1005 = loc("convert.3480")
#loc1006 = loc("compare.3482")
#loc1007 = loc("not.3484")
#loc1009 = loc("or.3494")
#loc1010 = loc("select.3495")
#loc1011 = loc("reshape.3500")
#loc1012 = loc("not.3502")
#loc1013 = loc("reshape.3504")
#loc1014 = loc("broadcast.3505")
#loc1015 = loc("reduce.3460")
#loc1016 = loc("broadcast.3461")
#loc1017 = loc("subtract.3462")
#loc1018 = loc("exponential.3463")
#loc1019 = loc("reduce.3469")
#loc1020 = loc("broadcast.3470")
#loc1021 = loc("divide.3471")
#loc1022 = loc("select.3506")
#loc1023 = loc("reshape.2048")
#loc1024 = loc("custom-call.2049")
#loc1025 = loc("reshape.2050")
#loc1026 = loc("transpose.2051")
#loc1027 = loc("dot.3397")
#loc1028 = loc("reshape.3398")
#loc1029 = loc("reshape.2044")
#loc1030 = loc("custom-call.2045")
#loc1031 = loc("reshape.2046")
#loc1032 = loc("broadcast.3401")
#loc1033 = loc("add.3402")
#loc1034 = loc("reshape.3403")
#loc1035 = loc("transpose.3404")
#loc1036 = loc("convert.3405")
#loc1037 = loc("dot.3507")
#loc1038 = loc("convert.3509")
#loc1039 = loc("transpose.3510")
#loc1040 = loc("reshape.3512")
#loc1041 = loc("reshape.2038")
#loc1042 = loc("custom-call.2039")
#loc1043 = loc("reshape.2040")
#loc1044 = loc("transpose.2041")
#loc1045 = loc("dot.3513")
#loc1046 = loc("reshape.3514")
#loc1047 = loc("reshape.2034")
#loc1048 = loc("custom-call.2035")
#loc1049 = loc("reshape.2036")
#loc1050 = loc("broadcast.3517")
#loc1051 = loc("add.3518")
#loc1052 = loc("add.3521")
#loc1053 = loc("reshape.2025")
#loc1054 = loc("custom-call.2026")
#loc1055 = loc("reshape.2027")
#loc1056 = loc("reshape.2020")
#loc1057 = loc("custom-call.2021")
#loc1058 = loc("reshape.2022")
#loc1059 = loc("custom-call.3598")
#loc1060 = loc("reshape.3599")
#loc1061 = loc("reshape.2014")
#loc1062 = loc("custom-call.2015")
#loc1063 = loc("reshape.2016")
#loc1064 = loc("transpose.2017")
#loc1065 = loc("dot.3600")
#loc1066 = loc("reshape.3601")
#loc1067 = loc("reshape.2010")
#loc1068 = loc("custom-call.2011")
#loc1069 = loc("reshape.2012")
#loc1070 = loc("broadcast.3604")
#loc1071 = loc("add.3605")
#loc1072 = loc("custom-call.3618")
#loc1073 = loc("reshape.3619")
#loc1074 = loc("reshape.2004")
#loc1075 = loc("custom-call.2005")
#loc1076 = loc("reshape.2006")
#loc1077 = loc("transpose.2007")
#loc1078 = loc("dot.3620")
#loc1079 = loc("reshape.3621")
#loc1080 = loc("reshape.2000")
#loc1081 = loc("custom-call.2001")
#loc1082 = loc("reshape.2002")
#loc1083 = loc("broadcast.3624")
#loc1084 = loc("add.3625")
#loc1085 = loc("add.3628")
#loc1086 = loc("reshape.1991")
#loc1087 = loc("custom-call.1992")
#loc1088 = loc("reshape.1993")
#loc1089 = loc("reshape.1986")
#loc1090 = loc("custom-call.1987")
#loc1091 = loc("reshape.1988")
#loc1092 = loc("custom-call.3705")
#loc1093 = loc("reshape.3751")
#loc1094 = loc("reshape.3747")
#loc1095 = loc("custom-call.3748")
#loc1096 = loc("reshape.3749")
#loc1097 = loc("transpose.3750")
#loc1098 = loc("dot.3752")
#loc1099 = loc("reshape.3753")
#loc1100 = loc("reshape.3743")
#loc1101 = loc("custom-call.3744")
#loc1102 = loc("reshape.3745")
#loc1103 = loc("broadcast.3756")
#loc1104 = loc("add.3757")
#loc1105 = loc("reshape.3758")
#loc1106 = loc("transpose.3759")
#loc1107 = loc("convert.3760")
#loc1108 = loc("multiply.3762")
#loc1109 = loc("reshape.3723")
#loc1110 = loc("custom-call.3724")
#loc1111 = loc("reshape.3725")
#loc1112 = loc("transpose.3726")
#loc1113 = loc("dot.3728")
#loc1114 = loc("reshape.3729")
#loc1115 = loc("reshape.3719")
#loc1116 = loc("custom-call.3720")
#loc1117 = loc("reshape.3721")
#loc1118 = loc("broadcast.3732")
#loc1119 = loc("add.3733")
#loc1120 = loc("reshape.3734")
#loc1121 = loc("transpose.3735")
#loc1122 = loc("convert.3736")
#loc1123 = loc("transpose.3737")
#loc1124 = loc("multiply.3739")
#loc1125 = loc("dot.3763")
#loc1126 = loc("convert.3790")
#loc1127 = loc("compare.3792")
#loc1128 = loc("not.3794")
#loc1130 = loc("or.3804")
#loc1131 = loc("select.3805")
#loc1132 = loc("reshape.3810")
#loc1133 = loc("not.3812")
#loc1134 = loc("reshape.3814")
#loc1135 = loc("broadcast.3815")
#loc1136 = loc("reduce.3770")
#loc1137 = loc("broadcast.3771")
#loc1138 = loc("subtract.3772")
#loc1139 = loc("exponential.3773")
#loc1140 = loc("reduce.3779")
#loc1141 = loc("broadcast.3780")
#loc1142 = loc("divide.3781")
#loc1143 = loc("select.3816")
#loc1144 = loc("reshape.1980")
#loc1145 = loc("custom-call.1981")
#loc1146 = loc("reshape.1982")
#loc1147 = loc("transpose.1983")
#loc1148 = loc("dot.3707")
#loc1149 = loc("reshape.3708")
#loc1150 = loc("reshape.1976")
#loc1151 = loc("custom-call.1977")
#loc1152 = loc("reshape.1978")
#loc1153 = loc("broadcast.3711")
#loc1154 = loc("add.3712")
#loc1155 = loc("reshape.3713")
#loc1156 = loc("transpose.3714")
#loc1157 = loc("convert.3715")
#loc1158 = loc("dot.3817")
#loc1159 = loc("convert.3819")
#loc1160 = loc("transpose.3820")
#loc1161 = loc("reshape.3822")
#loc1162 = loc("reshape.1970")
#loc1163 = loc("custom-call.1971")
#loc1164 = loc("reshape.1972")
#loc1165 = loc("transpose.1973")
#loc1166 = loc("dot.3823")
#loc1167 = loc("reshape.3824")
#loc1168 = loc("reshape.1966")
#loc1169 = loc("custom-call.1967")
#loc1170 = loc("reshape.1968")
#loc1171 = loc("broadcast.3827")
#loc1172 = loc("add.3828")
#loc1173 = loc("add.3831")
#loc1174 = loc("reshape.1957")
#loc1175 = loc("custom-call.1958")
#loc1176 = loc("reshape.1959")
#loc1177 = loc("reshape.1952")
#loc1178 = loc("custom-call.1953")
#loc1179 = loc("reshape.1954")
#loc1180 = loc("custom-call.3908")
#loc1181 = loc("reshape.3909")
#loc1182 = loc("reshape.1946")
#loc1183 = loc("custom-call.1947")
#loc1184 = loc("reshape.1948")
#loc1185 = loc("transpose.1949")
#loc1186 = loc("dot.3910")
#loc1187 = loc("reshape.3911")
#loc1188 = loc("reshape.1942")
#loc1189 = loc("custom-call.1943")
#loc1190 = loc("reshape.1944")
#loc1191 = loc("broadcast.3914")
#loc1192 = loc("add.3915")
#loc1193 = loc("custom-call.3928")
#loc1194 = loc("reshape.3929")
#loc1195 = loc("reshape.1936")
#loc1196 = loc("custom-call.1937")
#loc1197 = loc("reshape.1938")
#loc1198 = loc("transpose.1939")
#loc1199 = loc("dot.3930")
#loc1200 = loc("reshape.3931")
#loc1201 = loc("reshape.1932")
#loc1202 = loc("custom-call.1933")
#loc1203 = loc("reshape.1934")
#loc1204 = loc("broadcast.3934")
#loc1205 = loc("add.3935")
#loc1206 = loc("add.3938")
#loc1207 = loc("reshape.1923")
#loc1208 = loc("custom-call.1924")
#loc1209 = loc("reshape.1925")
#loc1210 = loc("reshape.1918")
#loc1211 = loc("custom-call.1919")
#loc1212 = loc("reshape.1920")
#loc1213 = loc("custom-call.4015")
#loc1214 = loc("reshape.4061")
#loc1215 = loc("reshape.4057")
#loc1216 = loc("custom-call.4058")
#loc1217 = loc("reshape.4059")
#loc1218 = loc("transpose.4060")
#loc1219 = loc("dot.4062")
#loc1220 = loc("reshape.4063")
#loc1221 = loc("reshape.4053")
#loc1222 = loc("custom-call.4054")
#loc1223 = loc("reshape.4055")
#loc1224 = loc("broadcast.4066")
#loc1225 = loc("add.4067")
#loc1226 = loc("reshape.4068")
#loc1227 = loc("transpose.4069")
#loc1228 = loc("convert.4070")
#loc1229 = loc("multiply.4072")
#loc1230 = loc("reshape.4033")
#loc1231 = loc("custom-call.4034")
#loc1232 = loc("reshape.4035")
#loc1233 = loc("transpose.4036")
#loc1234 = loc("dot.4038")
#loc1235 = loc("reshape.4039")
#loc1236 = loc("reshape.4029")
#loc1237 = loc("custom-call.4030")
#loc1238 = loc("reshape.4031")
#loc1239 = loc("broadcast.4042")
#loc1240 = loc("add.4043")
#loc1241 = loc("reshape.4044")
#loc1242 = loc("transpose.4045")
#loc1243 = loc("convert.4046")
#loc1244 = loc("transpose.4047")
#loc1245 = loc("multiply.4049")
#loc1246 = loc("dot.4073")
#loc1247 = loc("convert.4100")
#loc1248 = loc("compare.4102")
#loc1249 = loc("not.4104")
#loc1251 = loc("or.4114")
#loc1252 = loc("select.4115")
#loc1253 = loc("reshape.4120")
#loc1254 = loc("not.4122")
#loc1255 = loc("reshape.4124")
#loc1256 = loc("broadcast.4125")
#loc1257 = loc("reduce.4080")
#loc1258 = loc("broadcast.4081")
#loc1259 = loc("subtract.4082")
#loc1260 = loc("exponential.4083")
#loc1261 = loc("reduce.4089")
#loc1262 = loc("broadcast.4090")
#loc1263 = loc("divide.4091")
#loc1264 = loc("select.4126")
#loc1265 = loc("reshape.1912")
#loc1266 = loc("custom-call.1913")
#loc1267 = loc("reshape.1914")
#loc1268 = loc("transpose.1915")
#loc1269 = loc("dot.4017")
#loc1270 = loc("reshape.4018")
#loc1271 = loc("reshape.1908")
#loc1272 = loc("custom-call.1909")
#loc1273 = loc("reshape.1910")
#loc1274 = loc("broadcast.4021")
#loc1275 = loc("add.4022")
#loc1276 = loc("reshape.4023")
#loc1277 = loc("transpose.4024")
#loc1278 = loc("convert.4025")
#loc1279 = loc("dot.4127")
#loc1280 = loc("convert.4129")
#loc1281 = loc("transpose.4130")
#loc1282 = loc("reshape.4132")
#loc1283 = loc("reshape.1902")
#loc1284 = loc("custom-call.1903")
#loc1285 = loc("reshape.1904")
#loc1286 = loc("transpose.1905")
#loc1287 = loc("dot.4133")
#loc1288 = loc("reshape.4134")
#loc1289 = loc("reshape.1898")
#loc1290 = loc("custom-call.1899")
#loc1291 = loc("reshape.1900")
#loc1292 = loc("broadcast.4137")
#loc1293 = loc("add.4138")
#loc1294 = loc("add.4141")
#loc1295 = loc("reshape.1889")
#loc1296 = loc("custom-call.1890")
#loc1297 = loc("reshape.1891")
#loc1298 = loc("reshape.1884")
#loc1299 = loc("custom-call.1885")
#loc1300 = loc("reshape.1886")
#loc1301 = loc("custom-call.4218")
#loc1302 = loc("reshape.4219")
#loc1303 = loc("reshape.1878")
#loc1304 = loc("custom-call.1879")
#loc1305 = loc("reshape.1880")
#loc1306 = loc("transpose.1881")
#loc1307 = loc("dot.4220")
#loc1308 = loc("reshape.4221")
#loc1309 = loc("reshape.1874")
#loc1310 = loc("custom-call.1875")
#loc1311 = loc("reshape.1876")
#loc1312 = loc("broadcast.4224")
#loc1313 = loc("add.4225")
#loc1314 = loc("custom-call.4238")
#loc1315 = loc("reshape.4239")
#loc1316 = loc("reshape.1868")
#loc1317 = loc("custom-call.1869")
#loc1318 = loc("reshape.1870")
#loc1319 = loc("transpose.1871")
#loc1320 = loc("dot.4240")
#loc1321 = loc("reshape.4241")
#loc1322 = loc("reshape.1864")
#loc1323 = loc("custom-call.1865")
#loc1324 = loc("reshape.1866")
#loc1325 = loc("broadcast.4244")
#loc1326 = loc("add.4245")
#loc1327 = loc("add.4248")
#loc1328 = loc("reshape.1855")
#loc1329 = loc("custom-call.1856")
#loc1330 = loc("reshape.1857")
#loc1331 = loc("reshape.1850")
#loc1332 = loc("custom-call.1851")
#loc1333 = loc("reshape.1852")
#loc1334 = loc("custom-call.4325")
#loc1335 = loc("reshape.4371")
#loc1336 = loc("reshape.4367")
#loc1337 = loc("custom-call.4368")
#loc1338 = loc("reshape.4369")
#loc1339 = loc("transpose.4370")
#loc1340 = loc("dot.4372")
#loc1341 = loc("reshape.4373")
#loc1342 = loc("reshape.4363")
#loc1343 = loc("custom-call.4364")
#loc1344 = loc("reshape.4365")
#loc1345 = loc("broadcast.4376")
#loc1346 = loc("add.4377")
#loc1347 = loc("reshape.4378")
#loc1348 = loc("transpose.4379")
#loc1349 = loc("convert.4380")
#loc1350 = loc("multiply.4382")
#loc1351 = loc("reshape.4343")
#loc1352 = loc("custom-call.4344")
#loc1353 = loc("reshape.4345")
#loc1354 = loc("transpose.4346")
#loc1355 = loc("dot.4348")
#loc1356 = loc("reshape.4349")
#loc1357 = loc("reshape.4339")
#loc1358 = loc("custom-call.4340")
#loc1359 = loc("reshape.4341")
#loc1360 = loc("broadcast.4352")
#loc1361 = loc("add.4353")
#loc1362 = loc("reshape.4354")
#loc1363 = loc("transpose.4355")
#loc1364 = loc("convert.4356")
#loc1365 = loc("transpose.4357")
#loc1366 = loc("multiply.4359")
#loc1367 = loc("dot.4383")
#loc1368 = loc("convert.4410")
#loc1369 = loc("compare.4412")
#loc1370 = loc("not.4414")
#loc1372 = loc("or.4424")
#loc1373 = loc("select.4425")
#loc1374 = loc("reshape.4430")
#loc1375 = loc("not.4432")
#loc1376 = loc("reshape.4434")
#loc1377 = loc("broadcast.4435")
#loc1378 = loc("reduce.4390")
#loc1379 = loc("broadcast.4391")
#loc1380 = loc("subtract.4392")
#loc1381 = loc("exponential.4393")
#loc1382 = loc("reduce.4399")
#loc1383 = loc("broadcast.4400")
#loc1384 = loc("divide.4401")
#loc1385 = loc("select.4436")
#loc1386 = loc("reshape.1844")
#loc1387 = loc("custom-call.1845")
#loc1388 = loc("reshape.1846")
#loc1389 = loc("transpose.1847")
#loc1390 = loc("dot.4327")
#loc1391 = loc("reshape.4328")
#loc1392 = loc("reshape.1840")
#loc1393 = loc("custom-call.1841")
#loc1394 = loc("reshape.1842")
#loc1395 = loc("broadcast.4331")
#loc1396 = loc("add.4332")
#loc1397 = loc("reshape.4333")
#loc1398 = loc("transpose.4334")
#loc1399 = loc("convert.4335")
#loc1400 = loc("dot.4437")
#loc1401 = loc("convert.4439")
#loc1402 = loc("transpose.4440")
#loc1403 = loc("reshape.4442")
#loc1404 = loc("reshape.1834")
#loc1405 = loc("custom-call.1835")
#loc1406 = loc("reshape.1836")
#loc1407 = loc("transpose.1837")
#loc1408 = loc("dot.4443")
#loc1409 = loc("reshape.4444")
#loc1410 = loc("reshape.1830")
#loc1411 = loc("custom-call.1831")
#loc1412 = loc("reshape.1832")
#loc1413 = loc("broadcast.4447")
#loc1414 = loc("add.4448")
#loc1415 = loc("add.4451")
#loc1416 = loc("reshape.1821")
#loc1417 = loc("custom-call.1822")
#loc1418 = loc("reshape.1823")
#loc1419 = loc("reshape.1816")
#loc1420 = loc("custom-call.1817")
#loc1421 = loc("reshape.1818")
#loc1422 = loc("custom-call.4528")
#loc1423 = loc("reshape.4529")
#loc1424 = loc("reshape.1810")
#loc1425 = loc("custom-call.1811")
#loc1426 = loc("reshape.1812")
#loc1427 = loc("transpose.1813")
#loc1428 = loc("dot.4530")
#loc1429 = loc("reshape.4531")
#loc1430 = loc("reshape.1806")
#loc1431 = loc("custom-call.1807")
#loc1432 = loc("reshape.1808")
#loc1433 = loc("broadcast.4534")
#loc1434 = loc("add.4535")
#loc1435 = loc("custom-call.4548")
#loc1436 = loc("reshape.4549")
#loc1437 = loc("reshape.1800")
#loc1438 = loc("custom-call.1801")
#loc1439 = loc("reshape.1802")
#loc1440 = loc("transpose.1803")
#loc1441 = loc("dot.4550")
#loc1442 = loc("reshape.4551")
#loc1443 = loc("reshape.1796")
#loc1444 = loc("custom-call.1797")
#loc1445 = loc("reshape.1798")
#loc1446 = loc("broadcast.4554")
#loc1447 = loc("add.4555")
#loc1448 = loc("add.4558")
#loc1449 = loc("reshape.1787")
#loc1450 = loc("custom-call.1788")
#loc1451 = loc("reshape.1789")
#loc1452 = loc("reshape.1782")
#loc1453 = loc("custom-call.1783")
#loc1454 = loc("reshape.1784")
#loc1455 = loc("custom-call.4635")
#loc1456 = loc("reshape.4681")
#loc1457 = loc("reshape.4677")
#loc1458 = loc("custom-call.4678")
#loc1459 = loc("reshape.4679")
#loc1460 = loc("transpose.4680")
#loc1461 = loc("dot.4682")
#loc1462 = loc("reshape.4683")
#loc1463 = loc("reshape.4673")
#loc1464 = loc("custom-call.4674")
#loc1465 = loc("reshape.4675")
#loc1466 = loc("broadcast.4686")
#loc1467 = loc("add.4687")
#loc1468 = loc("reshape.4688")
#loc1469 = loc("transpose.4689")
#loc1470 = loc("convert.4690")
#loc1471 = loc("multiply.4692")
#loc1472 = loc("reshape.4653")
#loc1473 = loc("custom-call.4654")
#loc1474 = loc("reshape.4655")
#loc1475 = loc("transpose.4656")
#loc1476 = loc("dot.4658")
#loc1477 = loc("reshape.4659")
#loc1478 = loc("reshape.4649")
#loc1479 = loc("custom-call.4650")
#loc1480 = loc("reshape.4651")
#loc1481 = loc("broadcast.4662")
#loc1482 = loc("add.4663")
#loc1483 = loc("reshape.4664")
#loc1484 = loc("transpose.4665")
#loc1485 = loc("convert.4666")
#loc1486 = loc("transpose.4667")
#loc1487 = loc("multiply.4669")
#loc1488 = loc("dot.4693")
#loc1489 = loc("convert.4720")
#loc1490 = loc("compare.4722")
#loc1491 = loc("not.4724")
#loc1493 = loc("or.4734")
#loc1494 = loc("select.4735")
#loc1495 = loc("reshape.4740")
#loc1496 = loc("not.4742")
#loc1497 = loc("reshape.4744")
#loc1498 = loc("broadcast.4745")
#loc1499 = loc("reduce.4700")
#loc1500 = loc("broadcast.4701")
#loc1501 = loc("subtract.4702")
#loc1502 = loc("exponential.4703")
#loc1503 = loc("reduce.4709")
#loc1504 = loc("broadcast.4710")
#loc1505 = loc("divide.4711")
#loc1506 = loc("select.4746")
#loc1507 = loc("reshape.1776")
#loc1508 = loc("custom-call.1777")
#loc1509 = loc("reshape.1778")
#loc1510 = loc("transpose.1779")
#loc1511 = loc("dot.4637")
#loc1512 = loc("reshape.4638")
#loc1513 = loc("reshape.1772")
#loc1514 = loc("custom-call.1773")
#loc1515 = loc("reshape.1774")
#loc1516 = loc("broadcast.4641")
#loc1517 = loc("add.4642")
#loc1518 = loc("reshape.4643")
#loc1519 = loc("transpose.4644")
#loc1520 = loc("convert.4645")
#loc1521 = loc("dot.4747")
#loc1522 = loc("convert.4749")
#loc1523 = loc("transpose.4750")
#loc1524 = loc("reshape.4752")
#loc1525 = loc("reshape.1766")
#loc1526 = loc("custom-call.1767")
#loc1527 = loc("reshape.1768")
#loc1528 = loc("transpose.1769")
#loc1529 = loc("dot.4753")
#loc1530 = loc("reshape.4754")
#loc1531 = loc("reshape.1762")
#loc1532 = loc("custom-call.1763")
#loc1533 = loc("reshape.1764")
#loc1534 = loc("broadcast.4757")
#loc1535 = loc("add.4758")
#loc1536 = loc("add.4761")
#loc1537 = loc("reshape.1753")
#loc1538 = loc("custom-call.1754")
#loc1539 = loc("reshape.1755")
#loc1540 = loc("reshape.1748")
#loc1541 = loc("custom-call.1749")
#loc1542 = loc("reshape.1750")
#loc1543 = loc("custom-call.4838")
#loc1544 = loc("reshape.4839")
#loc1545 = loc("reshape.1742")
#loc1546 = loc("custom-call.1743")
#loc1547 = loc("reshape.1744")
#loc1548 = loc("transpose.1745")
#loc1549 = loc("dot.4840")
#loc1550 = loc("reshape.4841")
#loc1551 = loc("reshape.1738")
#loc1552 = loc("custom-call.1739")
#loc1553 = loc("reshape.1740")
#loc1554 = loc("broadcast.4844")
#loc1555 = loc("add.4845")
#loc1556 = loc("custom-call.4858")
#loc1557 = loc("reshape.4859")
#loc1558 = loc("reshape.1732")
#loc1559 = loc("custom-call.1733")
#loc1560 = loc("reshape.1734")
#loc1561 = loc("transpose.1735")
#loc1562 = loc("dot.4860")
#loc1563 = loc("reshape.4861")
#loc1564 = loc("reshape.1728")
#loc1565 = loc("custom-call.1729")
#loc1566 = loc("reshape.1730")
#loc1567 = loc("broadcast.4864")
#loc1568 = loc("add.4865")
#loc1569 = loc("add.4868")
#loc1570 = loc("reshape.1719")
#loc1571 = loc("custom-call.1720")
#loc1572 = loc("reshape.1721")
#loc1573 = loc("reshape.1714")
#loc1574 = loc("custom-call.1715")
#loc1575 = loc("reshape.1716")
#loc1576 = loc("custom-call.4945")
#loc1577 = loc("reshape.4991")
#loc1578 = loc("reshape.4987")
#loc1579 = loc("custom-call.4988")
#loc1580 = loc("reshape.4989")
#loc1581 = loc("transpose.4990")
#loc1582 = loc("dot.4992")
#loc1583 = loc("reshape.4993")
#loc1584 = loc("reshape.4983")
#loc1585 = loc("custom-call.4984")
#loc1586 = loc("reshape.4985")
#loc1587 = loc("broadcast.4996")
#loc1588 = loc("add.4997")
#loc1589 = loc("reshape.4998")
#loc1590 = loc("transpose.4999")
#loc1591 = loc("convert.5000")
#loc1592 = loc("multiply.5002")
#loc1593 = loc("reshape.4963")
#loc1594 = loc("custom-call.4964")
#loc1595 = loc("reshape.4965")
#loc1596 = loc("transpose.4966")
#loc1597 = loc("dot.4968")
#loc1598 = loc("reshape.4969")
#loc1599 = loc("reshape.4959")
#loc1600 = loc("custom-call.4960")
#loc1601 = loc("reshape.4961")
#loc1602 = loc("broadcast.4972")
#loc1603 = loc("add.4973")
#loc1604 = loc("reshape.4974")
#loc1605 = loc("transpose.4975")
#loc1606 = loc("convert.4976")
#loc1607 = loc("transpose.4977")
#loc1608 = loc("multiply.4979")
#loc1609 = loc("dot.5003")
#loc1610 = loc("convert.5030")
#loc1611 = loc("compare.5032")
#loc1612 = loc("not.5034")
#loc1614 = loc("or.5044")
#loc1615 = loc("select.5045")
#loc1616 = loc("reshape.5050")
#loc1617 = loc("not.5052")
#loc1618 = loc("reshape.5054")
#loc1619 = loc("broadcast.5055")
#loc1620 = loc("reduce.5010")
#loc1621 = loc("broadcast.5011")
#loc1622 = loc("subtract.5012")
#loc1623 = loc("exponential.5013")
#loc1624 = loc("reduce.5019")
#loc1625 = loc("broadcast.5020")
#loc1626 = loc("divide.5021")
#loc1627 = loc("select.5056")
#loc1628 = loc("reshape.1708")
#loc1629 = loc("custom-call.1709")
#loc1630 = loc("reshape.1710")
#loc1631 = loc("transpose.1711")
#loc1632 = loc("dot.4947")
#loc1633 = loc("reshape.4948")
#loc1634 = loc("reshape.1704")
#loc1635 = loc("custom-call.1705")
#loc1636 = loc("reshape.1706")
#loc1637 = loc("broadcast.4951")
#loc1638 = loc("add.4952")
#loc1639 = loc("reshape.4953")
#loc1640 = loc("transpose.4954")
#loc1641 = loc("convert.4955")
#loc1642 = loc("dot.5057")
#loc1643 = loc("convert.5059")
#loc1644 = loc("transpose.5060")
#loc1645 = loc("reshape.5062")
#loc1646 = loc("reshape.1698")
#loc1647 = loc("custom-call.1699")
#loc1648 = loc("reshape.1700")
#loc1649 = loc("transpose.1701")
#loc1650 = loc("dot.5063")
#loc1651 = loc("reshape.5064")
#loc1652 = loc("reshape.1694")
#loc1653 = loc("custom-call.1695")
#loc1654 = loc("reshape.1696")
#loc1655 = loc("broadcast.5067")
#loc1656 = loc("add.5068")
#loc1657 = loc("add.5071")
#loc1658 = loc("reshape.1685")
#loc1659 = loc("custom-call.1686")
#loc1660 = loc("reshape.1687")
#loc1661 = loc("reshape.1680")
#loc1662 = loc("custom-call.1681")
#loc1663 = loc("reshape.1682")
#loc1664 = loc("custom-call.5148")
#loc1665 = loc("reshape.5149")
#loc1666 = loc("reshape.1674")
#loc1667 = loc("custom-call.1675")
#loc1668 = loc("reshape.1676")
#loc1669 = loc("transpose.1677")
#loc1670 = loc("dot.5150")
#loc1671 = loc("reshape.5151")
#loc1672 = loc("reshape.1670")
#loc1673 = loc("custom-call.1671")
#loc1674 = loc("reshape.1672")
#loc1675 = loc("broadcast.5154")
#loc1676 = loc("add.5155")
#loc1677 = loc("custom-call.5168")
#loc1678 = loc("reshape.5169")
#loc1679 = loc("reshape.1664")
#loc1680 = loc("custom-call.1665")
#loc1681 = loc("reshape.1666")
#loc1682 = loc("transpose.1667")
#loc1683 = loc("dot.5170")
#loc1684 = loc("reshape.5171")
#loc1685 = loc("reshape.1660")
#loc1686 = loc("custom-call.1661")
#loc1687 = loc("reshape.1662")
#loc1688 = loc("broadcast.5174")
#loc1689 = loc("add.5175")
#loc1690 = loc("add.5178")
#loc1691 = loc("reshape.1651")
#loc1692 = loc("custom-call.1652")
#loc1693 = loc("reshape.1653")
#loc1694 = loc("reshape.1646")
#loc1695 = loc("custom-call.1647")
#loc1696 = loc("reshape.1648")
#loc1697 = loc("custom-call.5255")
#loc1698 = loc("reshape.5301")
#loc1699 = loc("reshape.5297")
#loc1700 = loc("custom-call.5298")
#loc1701 = loc("reshape.5299")
#loc1702 = loc("transpose.5300")
#loc1703 = loc("dot.5302")
#loc1704 = loc("reshape.5303")
#loc1705 = loc("reshape.5293")
#loc1706 = loc("custom-call.5294")
#loc1707 = loc("reshape.5295")
#loc1708 = loc("broadcast.5306")
#loc1709 = loc("add.5307")
#loc1710 = loc("reshape.5308")
#loc1711 = loc("transpose.5309")
#loc1712 = loc("convert.5310")
#loc1713 = loc("multiply.5312")
#loc1714 = loc("reshape.5273")
#loc1715 = loc("custom-call.5274")
#loc1716 = loc("reshape.5275")
#loc1717 = loc("transpose.5276")
#loc1718 = loc("dot.5278")
#loc1719 = loc("reshape.5279")
#loc1720 = loc("reshape.5269")
#loc1721 = loc("custom-call.5270")
#loc1722 = loc("reshape.5271")
#loc1723 = loc("broadcast.5282")
#loc1724 = loc("add.5283")
#loc1725 = loc("reshape.5284")
#loc1726 = loc("transpose.5285")
#loc1727 = loc("convert.5286")
#loc1728 = loc("transpose.5287")
#loc1729 = loc("multiply.5289")
#loc1730 = loc("dot.5313")
#loc1731 = loc("convert.5340")
#loc1732 = loc("compare.5342")
#loc1733 = loc("not.5344")
#loc1735 = loc("or.5354")
#loc1736 = loc("select.5355")
#loc1737 = loc("reshape.5360")
#loc1738 = loc("not.5362")
#loc1739 = loc("reshape.5364")
#loc1740 = loc("broadcast.5365")
#loc1741 = loc("reduce.5320")
#loc1742 = loc("broadcast.5321")
#loc1743 = loc("subtract.5322")
#loc1744 = loc("exponential.5323")
#loc1745 = loc("reduce.5329")
#loc1746 = loc("broadcast.5330")
#loc1747 = loc("divide.5331")
#loc1748 = loc("select.5366")
#loc1749 = loc("reshape.1640")
#loc1750 = loc("custom-call.1641")
#loc1751 = loc("reshape.1642")
#loc1752 = loc("transpose.1643")
#loc1753 = loc("dot.5257")
#loc1754 = loc("reshape.5258")
#loc1755 = loc("reshape.1636")
#loc1756 = loc("custom-call.1637")
#loc1757 = loc("reshape.1638")
#loc1758 = loc("broadcast.5261")
#loc1759 = loc("add.5262")
#loc1760 = loc("reshape.5263")
#loc1761 = loc("transpose.5264")
#loc1762 = loc("convert.5265")
#loc1763 = loc("dot.5367")
#loc1764 = loc("convert.5369")
#loc1765 = loc("transpose.5370")
#loc1766 = loc("reshape.5372")
#loc1767 = loc("reshape.1630")
#loc1768 = loc("custom-call.1631")
#loc1769 = loc("reshape.1632")
#loc1770 = loc("transpose.1633")
#loc1771 = loc("dot.5373")
#loc1772 = loc("reshape.5374")
#loc1773 = loc("reshape.1626")
#loc1774 = loc("custom-call.1627")
#loc1775 = loc("reshape.1628")
#loc1776 = loc("broadcast.5377")
#loc1777 = loc("add.5378")
#loc1778 = loc("add.5381")
#loc1779 = loc("reshape.1617")
#loc1780 = loc("custom-call.1618")
#loc1781 = loc("reshape.1619")
#loc1782 = loc("reshape.1612")
#loc1783 = loc("custom-call.1613")
#loc1784 = loc("reshape.1614")
#loc1785 = loc("custom-call.5458")
#loc1786 = loc("reshape.5459")
#loc1787 = loc("reshape.1606")
#loc1788 = loc("custom-call.1607")
#loc1789 = loc("reshape.1608")
#loc1790 = loc("transpose.1609")
#loc1791 = loc("dot.5460")
#loc1792 = loc("reshape.5461")
#loc1793 = loc("reshape.1602")
#loc1794 = loc("custom-call.1603")
#loc1795 = loc("reshape.1604")
#loc1796 = loc("broadcast.5464")
#loc1797 = loc("add.5465")
#loc1798 = loc("custom-call.5478")
#loc1799 = loc("reshape.5479")
#loc1800 = loc("reshape.1596")
#loc1801 = loc("custom-call.1597")
#loc1802 = loc("reshape.1598")
#loc1803 = loc("transpose.1599")
#loc1804 = loc("dot.5480")
#loc1805 = loc("reshape.5481")
#loc1806 = loc("reshape.1592")
#loc1807 = loc("custom-call.1593")
#loc1808 = loc("reshape.1594")
#loc1809 = loc("broadcast.5484")
#loc1810 = loc("add.5485")
#loc1811 = loc("add.5488")
#loc1812 = loc("reshape.1583")
#loc1813 = loc("custom-call.1584")
#loc1814 = loc("reshape.1585")
#loc1815 = loc("reshape.1578")
#loc1816 = loc("custom-call.1579")
#loc1817 = loc("reshape.1580")
#loc1818 = loc("custom-call.5565")
#loc1819 = loc("reshape.5611")
#loc1820 = loc("reshape.5607")
#loc1821 = loc("custom-call.5608")
#loc1822 = loc("reshape.5609")
#loc1823 = loc("transpose.5610")
#loc1824 = loc("dot.5612")
#loc1825 = loc("reshape.5613")
#loc1826 = loc("reshape.5603")
#loc1827 = loc("custom-call.5604")
#loc1828 = loc("reshape.5605")
#loc1829 = loc("broadcast.5616")
#loc1830 = loc("add.5617")
#loc1831 = loc("reshape.5618")
#loc1832 = loc("transpose.5619")
#loc1833 = loc("convert.5620")
#loc1834 = loc("multiply.5622")
#loc1835 = loc("reshape.5583")
#loc1836 = loc("custom-call.5584")
#loc1837 = loc("reshape.5585")
#loc1838 = loc("transpose.5586")
#loc1839 = loc("dot.5588")
#loc1840 = loc("reshape.5589")
#loc1841 = loc("reshape.5579")
#loc1842 = loc("custom-call.5580")
#loc1843 = loc("reshape.5581")
#loc1844 = loc("broadcast.5592")
#loc1845 = loc("add.5593")
#loc1846 = loc("reshape.5594")
#loc1847 = loc("transpose.5595")
#loc1848 = loc("convert.5596")
#loc1849 = loc("transpose.5597")
#loc1850 = loc("multiply.5599")
#loc1851 = loc("dot.5623")
#loc1852 = loc("convert.5650")
#loc1853 = loc("compare.5652")
#loc1854 = loc("not.5654")
#loc1856 = loc("or.5664")
#loc1857 = loc("select.5665")
#loc1858 = loc("reshape.5670")
#loc1859 = loc("not.5672")
#loc1860 = loc("reshape.5674")
#loc1861 = loc("broadcast.5675")
#loc1862 = loc("reduce.5630")
#loc1863 = loc("broadcast.5631")
#loc1864 = loc("subtract.5632")
#loc1865 = loc("exponential.5633")
#loc1866 = loc("reduce.5639")
#loc1867 = loc("broadcast.5640")
#loc1868 = loc("divide.5641")
#loc1869 = loc("select.5676")
#loc1870 = loc("reshape.1572")
#loc1871 = loc("custom-call.1573")
#loc1872 = loc("reshape.1574")
#loc1873 = loc("transpose.1575")
#loc1874 = loc("dot.5567")
#loc1875 = loc("reshape.5568")
#loc1876 = loc("reshape.1568")
#loc1877 = loc("custom-call.1569")
#loc1878 = loc("reshape.1570")
#loc1879 = loc("broadcast.5571")
#loc1880 = loc("add.5572")
#loc1881 = loc("reshape.5573")
#loc1882 = loc("transpose.5574")
#loc1883 = loc("convert.5575")
#loc1884 = loc("dot.5677")
#loc1885 = loc("convert.5679")
#loc1886 = loc("transpose.5680")
#loc1887 = loc("reshape.5682")
#loc1888 = loc("reshape.1562")
#loc1889 = loc("custom-call.1563")
#loc1890 = loc("reshape.1564")
#loc1891 = loc("transpose.1565")
#loc1892 = loc("dot.5683")
#loc1893 = loc("reshape.5684")
#loc1894 = loc("reshape.1558")
#loc1895 = loc("custom-call.1559")
#loc1896 = loc("reshape.1560")
#loc1897 = loc("broadcast.5687")
#loc1898 = loc("add.5688")
#loc1899 = loc("add.5691")
#loc1900 = loc("reshape.1549")
#loc1901 = loc("custom-call.1550")
#loc1902 = loc("reshape.1551")
#loc1903 = loc("reshape.1544")
#loc1904 = loc("custom-call.1545")
#loc1905 = loc("reshape.1546")
#loc1906 = loc("custom-call.5768")
#loc1907 = loc("reshape.5769")
#loc1908 = loc("reshape.1538")
#loc1909 = loc("custom-call.1539")
#loc1910 = loc("reshape.1540")
#loc1911 = loc("transpose.1541")
#loc1912 = loc("dot.5770")
#loc1913 = loc("reshape.5771")
#loc1914 = loc("reshape.1534")
#loc1915 = loc("custom-call.1535")
#loc1916 = loc("reshape.1536")
#loc1917 = loc("broadcast.5774")
#loc1918 = loc("add.5775")
#loc1919 = loc("custom-call.5788")
#loc1920 = loc("reshape.5789")
#loc1921 = loc("reshape.1528")
#loc1922 = loc("custom-call.1529")
#loc1923 = loc("reshape.1530")
#loc1924 = loc("transpose.1531")
#loc1925 = loc("dot.5790")
#loc1926 = loc("reshape.5791")
#loc1927 = loc("reshape.1524")
#loc1928 = loc("custom-call.1525")
#loc1929 = loc("reshape.1526")
#loc1930 = loc("broadcast.5794")
#loc1931 = loc("add.5795")
#loc1932 = loc("add.5798")
#loc1933 = loc("reshape.1515")
#loc1934 = loc("custom-call.1516")
#loc1935 = loc("reshape.1517")
#loc1936 = loc("reshape.1510")
#loc1937 = loc("custom-call.1511")
#loc1938 = loc("reshape.1512")
#loc1939 = loc("custom-call.5875")
#loc1940 = loc("reshape.5921")
#loc1941 = loc("reshape.5917")
#loc1942 = loc("custom-call.5918")
#loc1943 = loc("reshape.5919")
#loc1944 = loc("transpose.5920")
#loc1945 = loc("dot.5922")
#loc1946 = loc("reshape.5923")
#loc1947 = loc("reshape.5913")
#loc1948 = loc("custom-call.5914")
#loc1949 = loc("reshape.5915")
#loc1950 = loc("broadcast.5926")
#loc1951 = loc("add.5927")
#loc1952 = loc("reshape.5928")
#loc1953 = loc("transpose.5929")
#loc1954 = loc("convert.5930")
#loc1955 = loc("multiply.5932")
#loc1956 = loc("reshape.5893")
#loc1957 = loc("custom-call.5894")
#loc1958 = loc("reshape.5895")
#loc1959 = loc("transpose.5896")
#loc1960 = loc("dot.5898")
#loc1961 = loc("reshape.5899")
#loc1962 = loc("reshape.5889")
#loc1963 = loc("custom-call.5890")
#loc1964 = loc("reshape.5891")
#loc1965 = loc("broadcast.5902")
#loc1966 = loc("add.5903")
#loc1967 = loc("reshape.5904")
#loc1968 = loc("transpose.5905")
#loc1969 = loc("convert.5906")
#loc1970 = loc("transpose.5907")
#loc1971 = loc("multiply.5909")
#loc1972 = loc("dot.5933")
#loc1973 = loc("convert.5960")
#loc1974 = loc("compare.5962")
#loc1975 = loc("not.5964")
#loc1977 = loc("or.5974")
#loc1978 = loc("select.5975")
#loc1979 = loc("reshape.5980")
#loc1980 = loc("not.5982")
#loc1981 = loc("reshape.5984")
#loc1982 = loc("broadcast.5985")
#loc1983 = loc("reduce.5940")
#loc1984 = loc("broadcast.5941")
#loc1985 = loc("subtract.5942")
#loc1986 = loc("exponential.5943")
#loc1987 = loc("reduce.5949")
#loc1988 = loc("broadcast.5950")
#loc1989 = loc("divide.5951")
#loc1990 = loc("select.5986")
#loc1991 = loc("reshape.1504")
#loc1992 = loc("custom-call.1505")
#loc1993 = loc("reshape.1506")
#loc1994 = loc("transpose.1507")
#loc1995 = loc("dot.5877")
#loc1996 = loc("reshape.5878")
#loc1997 = loc("reshape.1500")
#loc1998 = loc("custom-call.1501")
#loc1999 = loc("reshape.1502")
#loc2000 = loc("broadcast.5881")
#loc2001 = loc("add.5882")
#loc2002 = loc("reshape.5883")
#loc2003 = loc("transpose.5884")
#loc2004 = loc("convert.5885")
#loc2005 = loc("dot.5987")
#loc2006 = loc("convert.5989")
#loc2007 = loc("transpose.5990")
#loc2008 = loc("reshape.5992")
#loc2009 = loc("reshape.1494")
#loc2010 = loc("custom-call.1495")
#loc2011 = loc("reshape.1496")
#loc2012 = loc("transpose.1497")
#loc2013 = loc("dot.5993")
#loc2014 = loc("reshape.5994")
#loc2015 = loc("reshape.1490")
#loc2016 = loc("custom-call.1491")
#loc2017 = loc("reshape.1492")
#loc2018 = loc("broadcast.5997")
#loc2019 = loc("add.5998")
#loc2020 = loc("add.6001")
#loc2021 = loc("reshape.1481")
#loc2022 = loc("custom-call.1482")
#loc2023 = loc("reshape.1483")
#loc2024 = loc("reshape.1476")
#loc2025 = loc("custom-call.1477")
#loc2026 = loc("reshape.1478")
#loc2027 = loc("custom-call.6078")
#loc2028 = loc("reshape.6079")
#loc2029 = loc("reshape.1470")
#loc2030 = loc("custom-call.1471")
#loc2031 = loc("reshape.1472")
#loc2032 = loc("transpose.1473")
#loc2033 = loc("dot.6080")
#loc2034 = loc("reshape.6081")
#loc2035 = loc("reshape.1466")
#loc2036 = loc("custom-call.1467")
#loc2037 = loc("reshape.1468")
#loc2038 = loc("broadcast.6084")
#loc2039 = loc("add.6085")
#loc2040 = loc("custom-call.6098")
#loc2041 = loc("reshape.6099")
#loc2042 = loc("reshape.1460")
#loc2043 = loc("custom-call.1461")
#loc2044 = loc("reshape.1462")
#loc2045 = loc("transpose.1463")
#loc2046 = loc("dot.6100")
#loc2047 = loc("reshape.6101")
#loc2048 = loc("reshape.1456")
#loc2049 = loc("custom-call.1457")
#loc2050 = loc("reshape.1458")
#loc2051 = loc("broadcast.6104")
#loc2052 = loc("add.6105")
#loc2053 = loc("add.6108")
#loc2054 = loc("reshape.1447")
#loc2055 = loc("custom-call.1448")
#loc2056 = loc("reshape.1449")
#loc2057 = loc("reshape.1442")
#loc2058 = loc("custom-call.1443")
#loc2059 = loc("reshape.1444")
#loc2060 = loc("custom-call.6185")
#loc2061 = loc("reshape.6231")
#loc2062 = loc("reshape.6227")
#loc2063 = loc("custom-call.6228")
#loc2064 = loc("reshape.6229")
#loc2065 = loc("transpose.6230")
#loc2066 = loc("dot.6232")
#loc2067 = loc("reshape.6233")
#loc2068 = loc("reshape.6223")
#loc2069 = loc("custom-call.6224")
#loc2070 = loc("reshape.6225")
#loc2071 = loc("broadcast.6236")
#loc2072 = loc("add.6237")
#loc2073 = loc("reshape.6238")
#loc2074 = loc("transpose.6239")
#loc2075 = loc("convert.6240")
#loc2076 = loc("multiply.6242")
#loc2077 = loc("reshape.6203")
#loc2078 = loc("custom-call.6204")
#loc2079 = loc("reshape.6205")
#loc2080 = loc("transpose.6206")
#loc2081 = loc("dot.6208")
#loc2082 = loc("reshape.6209")
#loc2083 = loc("reshape.6199")
#loc2084 = loc("custom-call.6200")
#loc2085 = loc("reshape.6201")
#loc2086 = loc("broadcast.6212")
#loc2087 = loc("add.6213")
#loc2088 = loc("reshape.6214")
#loc2089 = loc("transpose.6215")
#loc2090 = loc("convert.6216")
#loc2091 = loc("transpose.6217")
#loc2092 = loc("multiply.6219")
#loc2093 = loc("dot.6243")
#loc2094 = loc("convert.6270")
#loc2095 = loc("compare.6272")
#loc2096 = loc("not.6274")
#loc2098 = loc("or.6284")
#loc2099 = loc("select.6285")
#loc2100 = loc("reshape.6290")
#loc2101 = loc("not.6292")
#loc2102 = loc("reshape.6294")
#loc2103 = loc("broadcast.6295")
#loc2104 = loc("reduce.6250")
#loc2105 = loc("broadcast.6251")
#loc2106 = loc("subtract.6252")
#loc2107 = loc("exponential.6253")
#loc2108 = loc("reduce.6259")
#loc2109 = loc("broadcast.6260")
#loc2110 = loc("divide.6261")
#loc2111 = loc("select.6296")
#loc2112 = loc("reshape.1436")
#loc2113 = loc("custom-call.1437")
#loc2114 = loc("reshape.1438")
#loc2115 = loc("transpose.1439")
#loc2116 = loc("dot.6187")
#loc2117 = loc("reshape.6188")
#loc2118 = loc("reshape.1432")
#loc2119 = loc("custom-call.1433")
#loc2120 = loc("reshape.1434")
#loc2121 = loc("broadcast.6191")
#loc2122 = loc("add.6192")
#loc2123 = loc("reshape.6193")
#loc2124 = loc("transpose.6194")
#loc2125 = loc("convert.6195")
#loc2126 = loc("dot.6297")
#loc2127 = loc("convert.6299")
#loc2128 = loc("transpose.6300")
#loc2129 = loc("reshape.6302")
#loc2130 = loc("reshape.1426")
#loc2131 = loc("custom-call.1427")
#loc2132 = loc("reshape.1428")
#loc2133 = loc("transpose.1429")
#loc2134 = loc("dot.6303")
#loc2135 = loc("reshape.6304")
#loc2136 = loc("reshape.1422")
#loc2137 = loc("custom-call.1423")
#loc2138 = loc("reshape.1424")
#loc2139 = loc("broadcast.6307")
#loc2140 = loc("add.6308")
#loc2141 = loc("add.6311")
#loc2142 = loc("reshape.1413")
#loc2143 = loc("custom-call.1414")
#loc2144 = loc("reshape.1415")
#loc2145 = loc("reshape.1408")
#loc2146 = loc("custom-call.1409")
#loc2147 = loc("reshape.1410")
#loc2148 = loc("custom-call.6388")
#loc2149 = loc("reshape.6389")
#loc2150 = loc("reshape.1402")
#loc2151 = loc("custom-call.1403")
#loc2152 = loc("reshape.1404")
#loc2153 = loc("transpose.1405")
#loc2154 = loc("dot.6390")
#loc2155 = loc("reshape.6391")
#loc2156 = loc("reshape.1398")
#loc2157 = loc("custom-call.1399")
#loc2158 = loc("reshape.1400")
#loc2159 = loc("broadcast.6394")
#loc2160 = loc("add.6395")
#loc2161 = loc("custom-call.6408")
#loc2162 = loc("reshape.6409")
#loc2163 = loc("reshape.1392")
#loc2164 = loc("custom-call.1393")
#loc2165 = loc("reshape.1394")
#loc2166 = loc("transpose.1395")
#loc2167 = loc("dot.6410")
#loc2168 = loc("reshape.6411")
#loc2169 = loc("reshape.1388")
#loc2170 = loc("custom-call.1389")
#loc2171 = loc("reshape.1390")
#loc2172 = loc("broadcast.6414")
#loc2173 = loc("add.6415")
#loc2174 = loc("add.6418")
#loc2175 = loc("reshape.1379")
#loc2176 = loc("custom-call.1380")
#loc2177 = loc("reshape.1381")
#loc2178 = loc("reshape.1374")
#loc2179 = loc("custom-call.1375")
#loc2180 = loc("reshape.1376")
#loc2181 = loc("custom-call.6495")
#loc2182 = loc("reshape.6541")
#loc2183 = loc("reshape.6537")
#loc2184 = loc("custom-call.6538")
#loc2185 = loc("reshape.6539")
#loc2186 = loc("transpose.6540")
#loc2187 = loc("dot.6542")
#loc2188 = loc("reshape.6543")
#loc2189 = loc("reshape.6533")
#loc2190 = loc("custom-call.6534")
#loc2191 = loc("reshape.6535")
#loc2192 = loc("broadcast.6546")
#loc2193 = loc("add.6547")
#loc2194 = loc("reshape.6548")
#loc2195 = loc("transpose.6549")
#loc2196 = loc("convert.6550")
#loc2197 = loc("multiply.6552")
#loc2198 = loc("reshape.6513")
#loc2199 = loc("custom-call.6514")
#loc2200 = loc("reshape.6515")
#loc2201 = loc("transpose.6516")
#loc2202 = loc("dot.6518")
#loc2203 = loc("reshape.6519")
#loc2204 = loc("reshape.6509")
#loc2205 = loc("custom-call.6510")
#loc2206 = loc("reshape.6511")
#loc2207 = loc("broadcast.6522")
#loc2208 = loc("add.6523")
#loc2209 = loc("reshape.6524")
#loc2210 = loc("transpose.6525")
#loc2211 = loc("convert.6526")
#loc2212 = loc("transpose.6527")
#loc2213 = loc("multiply.6529")
#loc2214 = loc("dot.6553")
#loc2215 = loc("convert.6580")
#loc2216 = loc("compare.6582")
#loc2217 = loc("not.6584")
#loc2219 = loc("or.6594")
#loc2220 = loc("select.6595")
#loc2221 = loc("reshape.6600")
#loc2222 = loc("not.6602")
#loc2223 = loc("reshape.6604")
#loc2224 = loc("broadcast.6605")
#loc2225 = loc("reduce.6560")
#loc2226 = loc("broadcast.6561")
#loc2227 = loc("subtract.6562")
#loc2228 = loc("exponential.6563")
#loc2229 = loc("reduce.6569")
#loc2230 = loc("broadcast.6570")
#loc2231 = loc("divide.6571")
#loc2232 = loc("select.6606")
#loc2233 = loc("reshape.1368")
#loc2234 = loc("custom-call.1369")
#loc2235 = loc("reshape.1370")
#loc2236 = loc("transpose.1371")
#loc2237 = loc("dot.6497")
#loc2238 = loc("reshape.6498")
#loc2239 = loc("reshape.1364")
#loc2240 = loc("custom-call.1365")
#loc2241 = loc("reshape.1366")
#loc2242 = loc("broadcast.6501")
#loc2243 = loc("add.6502")
#loc2244 = loc("reshape.6503")
#loc2245 = loc("transpose.6504")
#loc2246 = loc("convert.6505")
#loc2247 = loc("dot.6607")
#loc2248 = loc("convert.6609")
#loc2249 = loc("transpose.6610")
#loc2250 = loc("reshape.6612")
#loc2251 = loc("reshape.1358")
#loc2252 = loc("custom-call.1359")
#loc2253 = loc("reshape.1360")
#loc2254 = loc("transpose.1361")
#loc2255 = loc("dot.6613")
#loc2256 = loc("reshape.6614")
#loc2257 = loc("reshape.1354")
#loc2258 = loc("custom-call.1355")
#loc2259 = loc("reshape.1356")
#loc2260 = loc("broadcast.6617")
#loc2261 = loc("add.6618")
#loc2262 = loc("add.6621")
#loc2263 = loc("reshape.1345")
#loc2264 = loc("custom-call.1346")
#loc2265 = loc("reshape.1347")
#loc2266 = loc("reshape.1340")
#loc2267 = loc("custom-call.1341")
#loc2268 = loc("reshape.1342")
#loc2269 = loc("custom-call.6698")
#loc2270 = loc("reshape.6699")
#loc2271 = loc("reshape.1334")
#loc2272 = loc("custom-call.1335")
#loc2273 = loc("reshape.1336")
#loc2274 = loc("transpose.1337")
#loc2275 = loc("dot.6700")
#loc2276 = loc("reshape.6701")
#loc2277 = loc("reshape.1330")
#loc2278 = loc("custom-call.1331")
#loc2279 = loc("reshape.1332")
#loc2280 = loc("broadcast.6704")
#loc2281 = loc("add.6705")
#loc2282 = loc("custom-call.6718")
#loc2283 = loc("reshape.6719")
#loc2284 = loc("reshape.1324")
#loc2285 = loc("custom-call.1325")
#loc2286 = loc("reshape.1326")
#loc2287 = loc("transpose.1327")
#loc2288 = loc("dot.6720")
#loc2289 = loc("reshape.6721")
#loc2290 = loc("reshape.1320")
#loc2291 = loc("custom-call.1321")
#loc2292 = loc("reshape.1322")
#loc2293 = loc("broadcast.6724")
#loc2294 = loc("add.6725")
#loc2295 = loc("add.6728")
#loc2296 = loc("reshape.1311")
#loc2297 = loc("custom-call.1312")
#loc2298 = loc("reshape.1313")
#loc2299 = loc("reshape.1306")
#loc2300 = loc("custom-call.1307")
#loc2301 = loc("reshape.1308")
#loc2302 = loc("custom-call.6805")
#loc2303 = loc("reshape.6851")
#loc2304 = loc("reshape.6847")
#loc2305 = loc("custom-call.6848")
#loc2306 = loc("reshape.6849")
#loc2307 = loc("transpose.6850")
#loc2308 = loc("dot.6852")
#loc2309 = loc("reshape.6853")
#loc2310 = loc("reshape.6843")
#loc2311 = loc("custom-call.6844")
#loc2312 = loc("reshape.6845")
#loc2313 = loc("broadcast.6856")
#loc2314 = loc("add.6857")
#loc2315 = loc("reshape.6858")
#loc2316 = loc("transpose.6859")
#loc2317 = loc("convert.6860")
#loc2318 = loc("multiply.6862")
#loc2319 = loc("reshape.6823")
#loc2320 = loc("custom-call.6824")
#loc2321 = loc("reshape.6825")
#loc2322 = loc("transpose.6826")
#loc2323 = loc("dot.6828")
#loc2324 = loc("reshape.6829")
#loc2325 = loc("reshape.6819")
#loc2326 = loc("custom-call.6820")
#loc2327 = loc("reshape.6821")
#loc2328 = loc("broadcast.6832")
#loc2329 = loc("add.6833")
#loc2330 = loc("reshape.6834")
#loc2331 = loc("transpose.6835")
#loc2332 = loc("convert.6836")
#loc2333 = loc("transpose.6837")
#loc2334 = loc("multiply.6839")
#loc2335 = loc("dot.6863")
#loc2336 = loc("convert.6890")
#loc2337 = loc("compare.6892")
#loc2338 = loc("not.6894")
#loc2340 = loc("or.6904")
#loc2341 = loc("select.6905")
#loc2342 = loc("reshape.6910")
#loc2343 = loc("not.6912")
#loc2344 = loc("reshape.6914")
#loc2345 = loc("broadcast.6915")
#loc2346 = loc("reduce.6870")
#loc2347 = loc("broadcast.6871")
#loc2348 = loc("subtract.6872")
#loc2349 = loc("exponential.6873")
#loc2350 = loc("reduce.6879")
#loc2351 = loc("broadcast.6880")
#loc2352 = loc("divide.6881")
#loc2353 = loc("select.6916")
#loc2354 = loc("reshape.1300")
#loc2355 = loc("custom-call.1301")
#loc2356 = loc("reshape.1302")
#loc2357 = loc("transpose.1303")
#loc2358 = loc("dot.6807")
#loc2359 = loc("reshape.6808")
#loc2360 = loc("reshape.1296")
#loc2361 = loc("custom-call.1297")
#loc2362 = loc("reshape.1298")
#loc2363 = loc("broadcast.6811")
#loc2364 = loc("add.6812")
#loc2365 = loc("reshape.6813")
#loc2366 = loc("transpose.6814")
#loc2367 = loc("convert.6815")
#loc2368 = loc("dot.6917")
#loc2369 = loc("convert.6919")
#loc2370 = loc("transpose.6920")
#loc2371 = loc("reshape.6922")
#loc2372 = loc("reshape.1290")
#loc2373 = loc("custom-call.1291")
#loc2374 = loc("reshape.1292")
#loc2375 = loc("transpose.1293")
#loc2376 = loc("dot.6923")
#loc2377 = loc("reshape.6924")
#loc2378 = loc("reshape.1286")
#loc2379 = loc("custom-call.1287")
#loc2380 = loc("reshape.1288")
#loc2381 = loc("broadcast.6927")
#loc2382 = loc("add.6928")
#loc2383 = loc("add.6931")
#loc2384 = loc("reshape.1277")
#loc2385 = loc("custom-call.1278")
#loc2386 = loc("reshape.1279")
#loc2387 = loc("reshape.1272")
#loc2388 = loc("custom-call.1273")
#loc2389 = loc("reshape.1274")
#loc2390 = loc("custom-call.7008")
#loc2391 = loc("reshape.7009")
#loc2392 = loc("reshape.1266")
#loc2393 = loc("custom-call.1267")
#loc2394 = loc("reshape.1268")
#loc2395 = loc("transpose.1269")
#loc2396 = loc("dot.7010")
#loc2397 = loc("reshape.7011")
#loc2398 = loc("reshape.1262")
#loc2399 = loc("custom-call.1263")
#loc2400 = loc("reshape.1264")
#loc2401 = loc("broadcast.7014")
#loc2402 = loc("add.7015")
#loc2403 = loc("custom-call.7028")
#loc2404 = loc("reshape.7029")
#loc2405 = loc("reshape.1256")
#loc2406 = loc("custom-call.1257")
#loc2407 = loc("reshape.1258")
#loc2408 = loc("transpose.1259")
#loc2409 = loc("dot.7030")
#loc2410 = loc("reshape.7031")
#loc2411 = loc("reshape.1252")
#loc2412 = loc("custom-call.1253")
#loc2413 = loc("reshape.1254")
#loc2414 = loc("broadcast.7034")
#loc2415 = loc("add.7035")
#loc2416 = loc("add.7038")
#loc2417 = loc("reshape.1243")
#loc2418 = loc("custom-call.1244")
#loc2419 = loc("reshape.1245")
#loc2420 = loc("reshape.1238")
#loc2421 = loc("custom-call.1239")
#loc2422 = loc("reshape.1240")
#loc2423 = loc("custom-call.7115")
#loc2424 = loc("reshape.7161")
#loc2425 = loc("reshape.7157")
#loc2426 = loc("custom-call.7158")
#loc2427 = loc("reshape.7159")
#loc2428 = loc("transpose.7160")
#loc2429 = loc("dot.7162")
#loc2430 = loc("reshape.7163")
#loc2431 = loc("reshape.7153")
#loc2432 = loc("custom-call.7154")
#loc2433 = loc("reshape.7155")
#loc2434 = loc("broadcast.7166")
#loc2435 = loc("add.7167")
#loc2436 = loc("reshape.7168")
#loc2437 = loc("transpose.7169")
#loc2438 = loc("convert.7170")
#loc2439 = loc("multiply.7172")
#loc2440 = loc("reshape.7133")
#loc2441 = loc("custom-call.7134")
#loc2442 = loc("reshape.7135")
#loc2443 = loc("transpose.7136")
#loc2444 = loc("dot.7138")
#loc2445 = loc("reshape.7139")
#loc2446 = loc("reshape.7129")
#loc2447 = loc("custom-call.7130")
#loc2448 = loc("reshape.7131")
#loc2449 = loc("broadcast.7142")
#loc2450 = loc("add.7143")
#loc2451 = loc("reshape.7144")
#loc2452 = loc("transpose.7145")
#loc2453 = loc("convert.7146")
#loc2454 = loc("transpose.7147")
#loc2455 = loc("multiply.7149")
#loc2456 = loc("dot.7173")
#loc2457 = loc("convert.7200")
#loc2458 = loc("compare.7202")
#loc2459 = loc("not.7204")
#loc2461 = loc("or.7214")
#loc2462 = loc("select.7215")
#loc2463 = loc("reshape.7220")
#loc2464 = loc("not.7222")
#loc2465 = loc("reshape.7224")
#loc2466 = loc("broadcast.7225")
#loc2467 = loc("reduce.7180")
#loc2468 = loc("broadcast.7181")
#loc2469 = loc("subtract.7182")
#loc2470 = loc("exponential.7183")
#loc2471 = loc("reduce.7189")
#loc2472 = loc("broadcast.7190")
#loc2473 = loc("divide.7191")
#loc2474 = loc("select.7226")
#loc2475 = loc("reshape.1232")
#loc2476 = loc("custom-call.1233")
#loc2477 = loc("reshape.1234")
#loc2478 = loc("transpose.1235")
#loc2479 = loc("dot.7117")
#loc2480 = loc("reshape.7118")
#loc2481 = loc("reshape.1228")
#loc2482 = loc("custom-call.1229")
#loc2483 = loc("reshape.1230")
#loc2484 = loc("broadcast.7121")
#loc2485 = loc("add.7122")
#loc2486 = loc("reshape.7123")
#loc2487 = loc("transpose.7124")
#loc2488 = loc("convert.7125")
#loc2489 = loc("dot.7227")
#loc2490 = loc("convert.7229")
#loc2491 = loc("transpose.7230")
#loc2492 = loc("reshape.7232")
#loc2493 = loc("reshape.1222")
#loc2494 = loc("custom-call.1223")
#loc2495 = loc("reshape.1224")
#loc2496 = loc("transpose.1225")
#loc2497 = loc("dot.7233")
#loc2498 = loc("reshape.7234")
#loc2499 = loc("reshape.1218")
#loc2500 = loc("custom-call.1219")
#loc2501 = loc("reshape.1220")
#loc2502 = loc("broadcast.7237")
#loc2503 = loc("add.7238")
#loc2504 = loc("add.7241")
#loc2505 = loc("reshape.1209")
#loc2506 = loc("custom-call.1210")
#loc2507 = loc("reshape.1211")
#loc2508 = loc("reshape.1204")
#loc2509 = loc("custom-call.1205")
#loc2510 = loc("reshape.1206")
#loc2511 = loc("custom-call.7318")
#loc2512 = loc("reshape.7319")
#loc2513 = loc("reshape.1198")
#loc2514 = loc("custom-call.1199")
#loc2515 = loc("reshape.1200")
#loc2516 = loc("transpose.1201")
#loc2517 = loc("dot.7320")
#loc2518 = loc("reshape.7321")
#loc2519 = loc("reshape.1194")
#loc2520 = loc("custom-call.1195")
#loc2521 = loc("reshape.1196")
#loc2522 = loc("broadcast.7324")
#loc2523 = loc("add.7325")
#loc2524 = loc("custom-call.7338")
#loc2525 = loc("reshape.7339")
#loc2526 = loc("reshape.1188")
#loc2527 = loc("custom-call.1189")
#loc2528 = loc("reshape.1190")
#loc2529 = loc("transpose.1191")
#loc2530 = loc("dot.7340")
#loc2531 = loc("reshape.7341")
#loc2532 = loc("reshape.1184")
#loc2533 = loc("custom-call.1185")
#loc2534 = loc("reshape.1186")
#loc2535 = loc("broadcast.7344")
#loc2536 = loc("add.7345")
#loc2537 = loc("add.7348")
#loc2538 = loc("reshape.1175")
#loc2539 = loc("custom-call.1176")
#loc2540 = loc("reshape.1177")
#loc2541 = loc("reshape.1170")
#loc2542 = loc("custom-call.1171")
#loc2543 = loc("reshape.1172")
#loc2544 = loc("custom-call.7425")
#loc2545 = loc("reshape.7471")
#loc2546 = loc("reshape.7467")
#loc2547 = loc("custom-call.7468")
#loc2548 = loc("reshape.7469")
#loc2549 = loc("transpose.7470")
#loc2550 = loc("dot.7472")
#loc2551 = loc("reshape.7473")
#loc2552 = loc("reshape.7463")
#loc2553 = loc("custom-call.7464")
#loc2554 = loc("reshape.7465")
#loc2555 = loc("broadcast.7476")
#loc2556 = loc("add.7477")
#loc2557 = loc("reshape.7478")
#loc2558 = loc("transpose.7479")
#loc2559 = loc("convert.7480")
#loc2560 = loc("multiply.7482")
#loc2561 = loc("reshape.7443")
#loc2562 = loc("custom-call.7444")
#loc2563 = loc("reshape.7445")
#loc2564 = loc("transpose.7446")
#loc2565 = loc("dot.7448")
#loc2566 = loc("reshape.7449")
#loc2567 = loc("reshape.7439")
#loc2568 = loc("custom-call.7440")
#loc2569 = loc("reshape.7441")
#loc2570 = loc("broadcast.7452")
#loc2571 = loc("add.7453")
#loc2572 = loc("reshape.7454")
#loc2573 = loc("transpose.7455")
#loc2574 = loc("convert.7456")
#loc2575 = loc("transpose.7457")
#loc2576 = loc("multiply.7459")
#loc2577 = loc("dot.7483")
#loc2578 = loc("convert.7510")
#loc2579 = loc("compare.7512")
#loc2580 = loc("not.7514")
#loc2582 = loc("or.7524")
#loc2583 = loc("select.7525")
#loc2584 = loc("reshape.7530")
#loc2585 = loc("not.7532")
#loc2586 = loc("reshape.7534")
#loc2587 = loc("broadcast.7535")
#loc2588 = loc("reduce.7490")
#loc2589 = loc("broadcast.7491")
#loc2590 = loc("subtract.7492")
#loc2591 = loc("exponential.7493")
#loc2592 = loc("reduce.7499")
#loc2593 = loc("broadcast.7500")
#loc2594 = loc("divide.7501")
#loc2595 = loc("select.7536")
#loc2596 = loc("reshape.1164")
#loc2597 = loc("custom-call.1165")
#loc2598 = loc("reshape.1166")
#loc2599 = loc("transpose.1167")
#loc2600 = loc("dot.7427")
#loc2601 = loc("reshape.7428")
#loc2602 = loc("reshape.1160")
#loc2603 = loc("custom-call.1161")
#loc2604 = loc("reshape.1162")
#loc2605 = loc("broadcast.7431")
#loc2606 = loc("add.7432")
#loc2607 = loc("reshape.7433")
#loc2608 = loc("transpose.7434")
#loc2609 = loc("convert.7435")
#loc2610 = loc("dot.7537")
#loc2611 = loc("convert.7539")
#loc2612 = loc("transpose.7540")
#loc2613 = loc("reshape.7542")
#loc2614 = loc("reshape.1154")
#loc2615 = loc("custom-call.1155")
#loc2616 = loc("reshape.1156")
#loc2617 = loc("transpose.1157")
#loc2618 = loc("dot.7543")
#loc2619 = loc("reshape.7544")
#loc2620 = loc("reshape.1150")
#loc2621 = loc("custom-call.1151")
#loc2622 = loc("reshape.1152")
#loc2623 = loc("broadcast.7547")
#loc2624 = loc("add.7548")
#loc2625 = loc("add.7551")
#loc2626 = loc("reshape.1141")
#loc2627 = loc("custom-call.1142")
#loc2628 = loc("reshape.1143")
#loc2629 = loc("reshape.1136")
#loc2630 = loc("custom-call.1137")
#loc2631 = loc("reshape.1138")
#loc2632 = loc("custom-call.7628")
#loc2633 = loc("reshape.7629")
#loc2634 = loc("reshape.1130")
#loc2635 = loc("custom-call.1131")
#loc2636 = loc("reshape.1132")
#loc2637 = loc("transpose.1133")
#loc2638 = loc("dot.7630")
#loc2639 = loc("reshape.7631")
#loc2640 = loc("reshape.1126")
#loc2641 = loc("custom-call.1127")
#loc2642 = loc("reshape.1128")
#loc2643 = loc("broadcast.7634")
#loc2644 = loc("add.7635")
#loc2645 = loc("custom-call.7648")
#loc2646 = loc("reshape.7649")
#loc2647 = loc("reshape.1120")
#loc2648 = loc("custom-call.1121")
#loc2649 = loc("reshape.1122")
#loc2650 = loc("transpose.1123")
#loc2651 = loc("dot.7650")
#loc2652 = loc("reshape.7651")
#loc2653 = loc("reshape.1116")
#loc2654 = loc("custom-call.1117")
#loc2655 = loc("reshape.1118")
#loc2656 = loc("broadcast.7654")
#loc2657 = loc("add.7655")
#loc2658 = loc("add.7658")
#loc2659 = loc("reshape.1107")
#loc2660 = loc("custom-call.1108")
#loc2661 = loc("reshape.1109")
#loc2662 = loc("reshape.1102")
#loc2663 = loc("custom-call.1103")
#loc2664 = loc("reshape.1104")
#loc2665 = loc("custom-call.7735")
#loc2666 = loc("reshape.7781")
#loc2667 = loc("reshape.7777")
#loc2668 = loc("custom-call.7778")
#loc2669 = loc("reshape.7779")
#loc2670 = loc("transpose.7780")
#loc2671 = loc("dot.7782")
#loc2672 = loc("reshape.7783")
#loc2673 = loc("reshape.7773")
#loc2674 = loc("custom-call.7774")
#loc2675 = loc("reshape.7775")
#loc2676 = loc("broadcast.7786")
#loc2677 = loc("add.7787")
#loc2678 = loc("reshape.7788")
#loc2679 = loc("transpose.7789")
#loc2680 = loc("convert.7790")
#loc2681 = loc("multiply.7792")
#loc2682 = loc("reshape.7753")
#loc2683 = loc("custom-call.7754")
#loc2684 = loc("reshape.7755")
#loc2685 = loc("transpose.7756")
#loc2686 = loc("dot.7758")
#loc2687 = loc("reshape.7759")
#loc2688 = loc("reshape.7749")
#loc2689 = loc("custom-call.7750")
#loc2690 = loc("reshape.7751")
#loc2691 = loc("broadcast.7762")
#loc2692 = loc("add.7763")
#loc2693 = loc("reshape.7764")
#loc2694 = loc("transpose.7765")
#loc2695 = loc("convert.7766")
#loc2696 = loc("transpose.7767")
#loc2697 = loc("multiply.7769")
#loc2698 = loc("dot.7793")
#loc2699 = loc("convert.7820")
#loc2700 = loc("compare.7822")
#loc2701 = loc("not.7824")
#loc2703 = loc("or.7834")
#loc2704 = loc("select.7835")
#loc2705 = loc("reshape.7840")
#loc2706 = loc("not.7842")
#loc2707 = loc("reshape.7844")
#loc2708 = loc("broadcast.7845")
#loc2709 = loc("reduce.7800")
#loc2710 = loc("broadcast.7801")
#loc2711 = loc("subtract.7802")
#loc2712 = loc("exponential.7803")
#loc2713 = loc("reduce.7809")
#loc2714 = loc("broadcast.7810")
#loc2715 = loc("divide.7811")
#loc2716 = loc("select.7846")
#loc2717 = loc("reshape.1096")
#loc2718 = loc("custom-call.1097")
#loc2719 = loc("reshape.1098")
#loc2720 = loc("transpose.1099")
#loc2721 = loc("dot.7737")
#loc2722 = loc("reshape.7738")
#loc2723 = loc("reshape.1092")
#loc2724 = loc("custom-call.1093")
#loc2725 = loc("reshape.1094")
#loc2726 = loc("broadcast.7741")
#loc2727 = loc("add.7742")
#loc2728 = loc("reshape.7743")
#loc2729 = loc("transpose.7744")
#loc2730 = loc("convert.7745")
#loc2731 = loc("dot.7847")
#loc2732 = loc("convert.7849")
#loc2733 = loc("transpose.7850")
#loc2734 = loc("reshape.7852")
#loc2735 = loc("reshape.1086")
#loc2736 = loc("custom-call.1087")
#loc2737 = loc("reshape.1088")
#loc2738 = loc("transpose.1089")
#loc2739 = loc("dot.7853")
#loc2740 = loc("reshape.7854")
#loc2741 = loc("reshape.1082")
#loc2742 = loc("custom-call.1083")
#loc2743 = loc("reshape.1084")
#loc2744 = loc("broadcast.7857")
#loc2745 = loc("add.7858")
#loc2746 = loc("add.7861")
#loc2747 = loc("reshape.1073")
#loc2748 = loc("custom-call.1074")
#loc2749 = loc("reshape.1075")
#loc2750 = loc("reshape.1068")
#loc2751 = loc("custom-call.1069")
#loc2752 = loc("reshape.1070")
#loc2753 = loc("custom-call.7938")
#loc2754 = loc("reshape.7939")
#loc2755 = loc("reshape.1062")
#loc2756 = loc("custom-call.1063")
#loc2757 = loc("reshape.1064")
#loc2758 = loc("transpose.1065")
#loc2759 = loc("dot.7940")
#loc2760 = loc("reshape.7941")
#loc2761 = loc("reshape.1058")
#loc2762 = loc("custom-call.1059")
#loc2763 = loc("reshape.1060")
#loc2764 = loc("broadcast.7944")
#loc2765 = loc("add.7945")
#loc2766 = loc("custom-call.7958")
#loc2767 = loc("reshape.7959")
#loc2768 = loc("reshape.1052")
#loc2769 = loc("custom-call.1053")
#loc2770 = loc("reshape.1054")
#loc2771 = loc("transpose.1055")
#loc2772 = loc("dot.7960")
#loc2773 = loc("reshape.7961")
#loc2774 = loc("reshape.1048")
#loc2775 = loc("custom-call.1049")
#loc2776 = loc("reshape.1050")
#loc2777 = loc("broadcast.7964")
#loc2778 = loc("add.7965")
#loc2779 = loc("add.7968")
#loc2780 = loc("reshape.1039")
#loc2781 = loc("custom-call.1040")
#loc2782 = loc("reshape.1041")
#loc2783 = loc("reshape.1034")
#loc2784 = loc("custom-call.1035")
#loc2785 = loc("reshape.1036")
#loc2786 = loc("custom-call.8045")
#loc2787 = loc("reshape.8091")
#loc2788 = loc("reshape.8087")
#loc2789 = loc("custom-call.8088")
#loc2790 = loc("reshape.8089")
#loc2791 = loc("transpose.8090")
#loc2792 = loc("dot.8092")
#loc2793 = loc("reshape.8093")
#loc2794 = loc("reshape.8083")
#loc2795 = loc("custom-call.8084")
#loc2796 = loc("reshape.8085")
#loc2797 = loc("broadcast.8096")
#loc2798 = loc("add.8097")
#loc2799 = loc("reshape.8098")
#loc2800 = loc("transpose.8099")
#loc2801 = loc("convert.8100")
#loc2802 = loc("multiply.8102")
#loc2803 = loc("reshape.8063")
#loc2804 = loc("custom-call.8064")
#loc2805 = loc("reshape.8065")
#loc2806 = loc("transpose.8066")
#loc2807 = loc("dot.8068")
#loc2808 = loc("reshape.8069")
#loc2809 = loc("reshape.8059")
#loc2810 = loc("custom-call.8060")
#loc2811 = loc("reshape.8061")
#loc2812 = loc("broadcast.8072")
#loc2813 = loc("add.8073")
#loc2814 = loc("reshape.8074")
#loc2815 = loc("transpose.8075")
#loc2816 = loc("convert.8076")
#loc2817 = loc("transpose.8077")
#loc2818 = loc("multiply.8079")
#loc2819 = loc("dot.8103")
#loc2820 = loc("convert.8130")
#loc2821 = loc("compare.8132")
#loc2822 = loc("not.8134")
#loc2824 = loc("or.8144")
#loc2825 = loc("select.8145")
#loc2826 = loc("reshape.8150")
#loc2827 = loc("not.8152")
#loc2828 = loc("reshape.8154")
#loc2829 = loc("broadcast.8155")
#loc2830 = loc("reduce.8110")
#loc2831 = loc("broadcast.8111")
#loc2832 = loc("subtract.8112")
#loc2833 = loc("exponential.8113")
#loc2834 = loc("reduce.8119")
#loc2835 = loc("broadcast.8120")
#loc2836 = loc("divide.8121")
#loc2837 = loc("select.8156")
#loc2838 = loc("reshape.1028")
#loc2839 = loc("custom-call.1029")
#loc2840 = loc("reshape.1030")
#loc2841 = loc("transpose.1031")
#loc2842 = loc("dot.8047")
#loc2843 = loc("reshape.8048")
#loc2844 = loc("reshape.1024")
#loc2845 = loc("custom-call.1025")
#loc2846 = loc("reshape.1026")
#loc2847 = loc("broadcast.8051")
#loc2848 = loc("add.8052")
#loc2849 = loc("reshape.8053")
#loc2850 = loc("transpose.8054")
#loc2851 = loc("convert.8055")
#loc2852 = loc("dot.8157")
#loc2853 = loc("convert.8159")
#loc2854 = loc("transpose.8160")
#loc2855 = loc("reshape.8162")
#loc2856 = loc("reshape.1018")
#loc2857 = loc("custom-call.1019")
#loc2858 = loc("reshape.1020")
#loc2859 = loc("transpose.1021")
#loc2860 = loc("dot.8163")
#loc2861 = loc("reshape.8164")
#loc2862 = loc("reshape.1014")
#loc2863 = loc("custom-call.1015")
#loc2864 = loc("reshape.1016")
#loc2865 = loc("broadcast.8167")
#loc2866 = loc("add.8168")
#loc2867 = loc("add.8171")
#loc2868 = loc("reshape.1005")
#loc2869 = loc("custom-call.1006")
#loc2870 = loc("reshape.1007")
#loc2871 = loc("reshape.1000")
#loc2872 = loc("custom-call.1001")
#loc2873 = loc("reshape.1002")
#loc2874 = loc("custom-call.8248")
#loc2875 = loc("reshape.8249")
#loc2876 = loc("reshape.994")
#loc2877 = loc("custom-call.995")
#loc2878 = loc("reshape.996")
#loc2879 = loc("transpose.997")
#loc2880 = loc("dot.8250")
#loc2881 = loc("reshape.8251")
#loc2882 = loc("reshape.990")
#loc2883 = loc("custom-call.991")
#loc2884 = loc("reshape.992")
#loc2885 = loc("broadcast.8254")
#loc2886 = loc("add.8255")
#loc2887 = loc("custom-call.8268")
#loc2888 = loc("reshape.8269")
#loc2889 = loc("reshape.984")
#loc2890 = loc("custom-call.985")
#loc2891 = loc("reshape.986")
#loc2892 = loc("transpose.987")
#loc2893 = loc("dot.8270")
#loc2894 = loc("reshape.8271")
#loc2895 = loc("reshape.980")
#loc2896 = loc("custom-call.981")
#loc2897 = loc("reshape.982")
#loc2898 = loc("broadcast.8274")
#loc2899 = loc("add.8275")
#loc2900 = loc("add.8278")
#loc2901 = loc("reshape.971")
#loc2902 = loc("custom-call.972")
#loc2903 = loc("reshape.973")
#loc2904 = loc("reshape.966")
#loc2905 = loc("custom-call.967")
#loc2906 = loc("reshape.968")
#loc2907 = loc("custom-call.8355")
#loc2908 = loc("reshape.8401")
#loc2909 = loc("reshape.8397")
#loc2910 = loc("custom-call.8398")
#loc2911 = loc("reshape.8399")
#loc2912 = loc("transpose.8400")
#loc2913 = loc("dot.8402")
#loc2914 = loc("reshape.8403")
#loc2915 = loc("reshape.8393")
#loc2916 = loc("custom-call.8394")
#loc2917 = loc("reshape.8395")
#loc2918 = loc("broadcast.8406")
#loc2919 = loc("add.8407")
#loc2920 = loc("reshape.8408")
#loc2921 = loc("transpose.8409")
#loc2922 = loc("convert.8410")
#loc2923 = loc("multiply.8412")
#loc2924 = loc("reshape.8373")
#loc2925 = loc("custom-call.8374")
#loc2926 = loc("reshape.8375")
#loc2927 = loc("transpose.8376")
#loc2928 = loc("dot.8378")
#loc2929 = loc("reshape.8379")
#loc2930 = loc("reshape.8369")
#loc2931 = loc("custom-call.8370")
#loc2932 = loc("reshape.8371")
#loc2933 = loc("broadcast.8382")
#loc2934 = loc("add.8383")
#loc2935 = loc("reshape.8384")
#loc2936 = loc("transpose.8385")
#loc2937 = loc("convert.8386")
#loc2938 = loc("transpose.8387")
#loc2939 = loc("multiply.8389")
#loc2940 = loc("dot.8413")
#loc2941 = loc("convert.8440")
#loc2942 = loc("compare.8442")
#loc2943 = loc("not.8444")
#loc2945 = loc("or.8454")
#loc2946 = loc("select.8455")
#loc2947 = loc("reshape.8460")
#loc2948 = loc("not.8462")
#loc2949 = loc("reshape.8464")
#loc2950 = loc("broadcast.8465")
#loc2951 = loc("reduce.8420")
#loc2952 = loc("broadcast.8421")
#loc2953 = loc("subtract.8422")
#loc2954 = loc("exponential.8423")
#loc2955 = loc("reduce.8429")
#loc2956 = loc("broadcast.8430")
#loc2957 = loc("divide.8431")
#loc2958 = loc("select.8466")
#loc2959 = loc("reshape.960")
#loc2960 = loc("custom-call.961")
#loc2961 = loc("reshape.962")
#loc2962 = loc("transpose.963")
#loc2963 = loc("dot.8357")
#loc2964 = loc("reshape.8358")
#loc2965 = loc("reshape.956")
#loc2966 = loc("custom-call.957")
#loc2967 = loc("reshape.958")
#loc2968 = loc("broadcast.8361")
#loc2969 = loc("add.8362")
#loc2970 = loc("reshape.8363")
#loc2971 = loc("transpose.8364")
#loc2972 = loc("convert.8365")
#loc2973 = loc("dot.8467")
#loc2974 = loc("convert.8469")
#loc2975 = loc("transpose.8470")
#loc2976 = loc("reshape.8472")
#loc2977 = loc("reshape.950")
#loc2978 = loc("custom-call.951")
#loc2979 = loc("reshape.952")
#loc2980 = loc("transpose.953")
#loc2981 = loc("dot.8473")
#loc2982 = loc("reshape.8474")
#loc2983 = loc("reshape.946")
#loc2984 = loc("custom-call.947")
#loc2985 = loc("reshape.948")
#loc2986 = loc("broadcast.8477")
#loc2987 = loc("add.8478")
#loc2988 = loc("add.8481")
#loc2989 = loc("reshape.937")
#loc2990 = loc("custom-call.938")
#loc2991 = loc("reshape.939")
#loc2992 = loc("reshape.932")
#loc2993 = loc("custom-call.933")
#loc2994 = loc("reshape.934")
#loc2995 = loc("custom-call.8558")
#loc2996 = loc("reshape.8559")
#loc2997 = loc("reshape.926")
#loc2998 = loc("custom-call.927")
#loc2999 = loc("reshape.928")
#loc3000 = loc("transpose.929")
#loc3001 = loc("dot.8560")
#loc3002 = loc("reshape.8561")
#loc3003 = loc("reshape.922")
#loc3004 = loc("custom-call.923")
#loc3005 = loc("reshape.924")
#loc3006 = loc("broadcast.8564")
#loc3007 = loc("add.8565")
#loc3008 = loc("custom-call.8578")
#loc3009 = loc("reshape.8579")
#loc3010 = loc("reshape.916")
#loc3011 = loc("custom-call.917")
#loc3012 = loc("reshape.918")
#loc3013 = loc("transpose.919")
#loc3014 = loc("dot.8580")
#loc3015 = loc("reshape.8581")
#loc3016 = loc("reshape.912")
#loc3017 = loc("custom-call.913")
#loc3018 = loc("reshape.914")
#loc3019 = loc("broadcast.8584")
#loc3020 = loc("add.8585")
#loc3021 = loc("add.8588")
#loc3022 = loc("reshape.903")
#loc3023 = loc("custom-call.904")
#loc3024 = loc("reshape.905")
#loc3025 = loc("reshape.898")
#loc3026 = loc("custom-call.899")
#loc3027 = loc("reshape.900")
#loc3028 = loc("custom-call.8665")
#loc3029 = loc("reshape.8711")
#loc3030 = loc("reshape.8707")
#loc3031 = loc("custom-call.8708")
#loc3032 = loc("reshape.8709")
#loc3033 = loc("transpose.8710")
#loc3034 = loc("dot.8712")
#loc3035 = loc("reshape.8713")
#loc3036 = loc("reshape.8703")
#loc3037 = loc("custom-call.8704")
#loc3038 = loc("reshape.8705")
#loc3039 = loc("broadcast.8716")
#loc3040 = loc("add.8717")
#loc3041 = loc("reshape.8718")
#loc3042 = loc("transpose.8719")
#loc3043 = loc("convert.8720")
#loc3044 = loc("multiply.8722")
#loc3045 = loc("reshape.8683")
#loc3046 = loc("custom-call.8684")
#loc3047 = loc("reshape.8685")
#loc3048 = loc("transpose.8686")
#loc3049 = loc("dot.8688")
#loc3050 = loc("reshape.8689")
#loc3051 = loc("reshape.8679")
#loc3052 = loc("custom-call.8680")
#loc3053 = loc("reshape.8681")
#loc3054 = loc("broadcast.8692")
#loc3055 = loc("add.8693")
#loc3056 = loc("reshape.8694")
#loc3057 = loc("transpose.8695")
#loc3058 = loc("convert.8696")
#loc3059 = loc("transpose.8697")
#loc3060 = loc("multiply.8699")
#loc3061 = loc("dot.8723")
#loc3062 = loc("convert.8750")
#loc3063 = loc("compare.8752")
#loc3064 = loc("not.8754")
#loc3066 = loc("or.8764")
#loc3067 = loc("select.8765")
#loc3068 = loc("reshape.8770")
#loc3069 = loc("not.8772")
#loc3070 = loc("reshape.8774")
#loc3071 = loc("broadcast.8775")
#loc3072 = loc("reduce.8730")
#loc3073 = loc("broadcast.8731")
#loc3074 = loc("subtract.8732")
#loc3075 = loc("exponential.8733")
#loc3076 = loc("reduce.8739")
#loc3077 = loc("broadcast.8740")
#loc3078 = loc("divide.8741")
#loc3079 = loc("select.8776")
#loc3080 = loc("reshape.892")
#loc3081 = loc("custom-call.893")
#loc3082 = loc("reshape.894")
#loc3083 = loc("transpose.895")
#loc3084 = loc("dot.8667")
#loc3085 = loc("reshape.8668")
#loc3086 = loc("reshape.888")
#loc3087 = loc("custom-call.889")
#loc3088 = loc("reshape.890")
#loc3089 = loc("broadcast.8671")
#loc3090 = loc("add.8672")
#loc3091 = loc("reshape.8673")
#loc3092 = loc("transpose.8674")
#loc3093 = loc("convert.8675")
#loc3094 = loc("dot.8777")
#loc3095 = loc("convert.8779")
#loc3096 = loc("transpose.8780")
#loc3097 = loc("reshape.8782")
#loc3098 = loc("reshape.882")
#loc3099 = loc("custom-call.883")
#loc3100 = loc("reshape.884")
#loc3101 = loc("transpose.885")
#loc3102 = loc("dot.8783")
#loc3103 = loc("reshape.8784")
#loc3104 = loc("reshape.878")
#loc3105 = loc("custom-call.879")
#loc3106 = loc("reshape.880")
#loc3107 = loc("broadcast.8787")
#loc3108 = loc("add.8788")
#loc3109 = loc("add.8791")
#loc3110 = loc("reshape.869")
#loc3111 = loc("custom-call.870")
#loc3112 = loc("reshape.871")
#loc3113 = loc("reshape.864")
#loc3114 = loc("custom-call.865")
#loc3115 = loc("reshape.866")
#loc3116 = loc("custom-call.8868")
#loc3117 = loc("reshape.8869")
#loc3118 = loc("reshape.858")
#loc3119 = loc("custom-call.859")
#loc3120 = loc("reshape.860")
#loc3121 = loc("transpose.861")
#loc3122 = loc("dot.8870")
#loc3123 = loc("reshape.8871")
#loc3124 = loc("reshape.854")
#loc3125 = loc("custom-call.855")
#loc3126 = loc("reshape.856")
#loc3127 = loc("broadcast.8874")
#loc3128 = loc("add.8875")
#loc3129 = loc("custom-call.8888")
#loc3130 = loc("reshape.8889")
#loc3131 = loc("reshape.848")
#loc3132 = loc("custom-call.849")
#loc3133 = loc("reshape.850")
#loc3134 = loc("transpose.851")
#loc3135 = loc("dot.8890")
#loc3136 = loc("reshape.8891")
#loc3137 = loc("reshape.844")
#loc3138 = loc("custom-call.845")
#loc3139 = loc("reshape.846")
#loc3140 = loc("broadcast.8894")
#loc3141 = loc("add.8895")
#loc3142 = loc("add.8898")
#loc3143 = loc("reshape.835")
#loc3144 = loc("custom-call.836")
#loc3145 = loc("reshape.837")
#loc3146 = loc("reshape.830")
#loc3147 = loc("custom-call.831")
#loc3148 = loc("reshape.832")
#loc3149 = loc("custom-call.8975")
#loc3150 = loc("reshape.9021")
#loc3151 = loc("reshape.9017")
#loc3152 = loc("custom-call.9018")
#loc3153 = loc("reshape.9019")
#loc3154 = loc("transpose.9020")
#loc3155 = loc("dot.9022")
#loc3156 = loc("reshape.9023")
#loc3157 = loc("reshape.9013")
#loc3158 = loc("custom-call.9014")
#loc3159 = loc("reshape.9015")
#loc3160 = loc("broadcast.9026")
#loc3161 = loc("add.9027")
#loc3162 = loc("reshape.9028")
#loc3163 = loc("transpose.9029")
#loc3164 = loc("convert.9030")
#loc3165 = loc("multiply.9032")
#loc3166 = loc("reshape.8993")
#loc3167 = loc("custom-call.8994")
#loc3168 = loc("reshape.8995")
#loc3169 = loc("transpose.8996")
#loc3170 = loc("dot.8998")
#loc3171 = loc("reshape.8999")
#loc3172 = loc("reshape.8989")
#loc3173 = loc("custom-call.8990")
#loc3174 = loc("reshape.8991")
#loc3175 = loc("broadcast.9002")
#loc3176 = loc("add.9003")
#loc3177 = loc("reshape.9004")
#loc3178 = loc("transpose.9005")
#loc3179 = loc("convert.9006")
#loc3180 = loc("transpose.9007")
#loc3181 = loc("multiply.9009")
#loc3182 = loc("dot.9033")
#loc3183 = loc("convert.9060")
#loc3184 = loc("compare.9062")
#loc3185 = loc("not.9064")
#loc3187 = loc("or.9074")
#loc3188 = loc("select.9075")
#loc3189 = loc("reshape.9080")
#loc3190 = loc("not.9082")
#loc3191 = loc("reshape.9084")
#loc3192 = loc("broadcast.9085")
#loc3193 = loc("reduce.9040")
#loc3194 = loc("broadcast.9041")
#loc3195 = loc("subtract.9042")
#loc3196 = loc("exponential.9043")
#loc3197 = loc("reduce.9049")
#loc3198 = loc("broadcast.9050")
#loc3199 = loc("divide.9051")
#loc3200 = loc("select.9086")
#loc3201 = loc("reshape.824")
#loc3202 = loc("custom-call.825")
#loc3203 = loc("reshape.826")
#loc3204 = loc("transpose.827")
#loc3205 = loc("dot.8977")
#loc3206 = loc("reshape.8978")
#loc3207 = loc("reshape.820")
#loc3208 = loc("custom-call.821")
#loc3209 = loc("reshape.822")
#loc3210 = loc("broadcast.8981")
#loc3211 = loc("add.8982")
#loc3212 = loc("reshape.8983")
#loc3213 = loc("transpose.8984")
#loc3214 = loc("convert.8985")
#loc3215 = loc("dot.9087")
#loc3216 = loc("convert.9089")
#loc3217 = loc("transpose.9090")
#loc3218 = loc("reshape.9092")
#loc3219 = loc("reshape.814")
#loc3220 = loc("custom-call.815")
#loc3221 = loc("reshape.816")
#loc3222 = loc("transpose.817")
#loc3223 = loc("dot.9093")
#loc3224 = loc("reshape.9094")
#loc3225 = loc("reshape.810")
#loc3226 = loc("custom-call.811")
#loc3227 = loc("reshape.812")
#loc3228 = loc("broadcast.9097")
#loc3229 = loc("add.9098")
#loc3230 = loc("add.9101")
#loc3231 = loc("reshape.801")
#loc3232 = loc("custom-call.802")
#loc3233 = loc("reshape.803")
#loc3234 = loc("reshape.796")
#loc3235 = loc("custom-call.797")
#loc3236 = loc("reshape.798")
#loc3237 = loc("custom-call.9178")
#loc3238 = loc("reshape.9179")
#loc3239 = loc("reshape.790")
#loc3240 = loc("custom-call.791")
#loc3241 = loc("reshape.792")
#loc3242 = loc("transpose.793")
#loc3243 = loc("dot.9180")
#loc3244 = loc("reshape.9181")
#loc3245 = loc("reshape.786")
#loc3246 = loc("custom-call.787")
#loc3247 = loc("reshape.788")
#loc3248 = loc("broadcast.9184")
#loc3249 = loc("add.9185")
#loc3250 = loc("custom-call.9198")
#loc3251 = loc("reshape.9199")
#loc3252 = loc("reshape.780")
#loc3253 = loc("custom-call.781")
#loc3254 = loc("reshape.782")
#loc3255 = loc("transpose.783")
#loc3256 = loc("dot.9200")
#loc3257 = loc("reshape.9201")
#loc3258 = loc("reshape.776")
#loc3259 = loc("custom-call.777")
#loc3260 = loc("reshape.778")
#loc3261 = loc("broadcast.9204")
#loc3262 = loc("add.9205")
#loc3263 = loc("add.9208")
#loc3264 = loc("reshape.767")
#loc3265 = loc("custom-call.768")
#loc3266 = loc("reshape.769")
#loc3267 = loc("reshape.762")
#loc3268 = loc("custom-call.763")
#loc3269 = loc("reshape.764")
#loc3270 = loc("custom-call.9285")
#loc3271 = loc("reshape.9331")
#loc3272 = loc("reshape.9327")
#loc3273 = loc("custom-call.9328")
#loc3274 = loc("reshape.9329")
#loc3275 = loc("transpose.9330")
#loc3276 = loc("dot.9332")
#loc3277 = loc("reshape.9333")
#loc3278 = loc("reshape.9323")
#loc3279 = loc("custom-call.9324")
#loc3280 = loc("reshape.9325")
#loc3281 = loc("broadcast.9336")
#loc3282 = loc("add.9337")
#loc3283 = loc("reshape.9338")
#loc3284 = loc("transpose.9339")
#loc3285 = loc("convert.9340")
#loc3286 = loc("multiply.9342")
#loc3287 = loc("reshape.9303")
#loc3288 = loc("custom-call.9304")
#loc3289 = loc("reshape.9305")
#loc3290 = loc("transpose.9306")
#loc3291 = loc("dot.9308")
#loc3292 = loc("reshape.9309")
#loc3293 = loc("reshape.9299")
#loc3294 = loc("custom-call.9300")
#loc3295 = loc("reshape.9301")
#loc3296 = loc("broadcast.9312")
#loc3297 = loc("add.9313")
#loc3298 = loc("reshape.9314")
#loc3299 = loc("transpose.9315")
#loc3300 = loc("convert.9316")
#loc3301 = loc("transpose.9317")
#loc3302 = loc("multiply.9319")
#loc3303 = loc("dot.9343")
#loc3304 = loc("convert.9370")
#loc3305 = loc("compare.9372")
#loc3306 = loc("not.9374")
#loc3308 = loc("or.9384")
#loc3309 = loc("select.9385")
#loc3310 = loc("reshape.9390")
#loc3311 = loc("not.9392")
#loc3312 = loc("reshape.9394")
#loc3313 = loc("broadcast.9395")
#loc3314 = loc("reduce.9350")
#loc3315 = loc("broadcast.9351")
#loc3316 = loc("subtract.9352")
#loc3317 = loc("exponential.9353")
#loc3318 = loc("reduce.9359")
#loc3319 = loc("broadcast.9360")
#loc3320 = loc("divide.9361")
#loc3321 = loc("select.9396")
#loc3322 = loc("reshape.756")
#loc3323 = loc("custom-call.757")
#loc3324 = loc("reshape.758")
#loc3325 = loc("transpose.759")
#loc3326 = loc("dot.9287")
#loc3327 = loc("reshape.9288")
#loc3328 = loc("reshape.752")
#loc3329 = loc("custom-call.753")
#loc3330 = loc("reshape.754")
#loc3331 = loc("broadcast.9291")
#loc3332 = loc("add.9292")
#loc3333 = loc("reshape.9293")
#loc3334 = loc("transpose.9294")
#loc3335 = loc("convert.9295")
#loc3336 = loc("dot.9397")
#loc3337 = loc("convert.9399")
#loc3338 = loc("transpose.9400")
#loc3339 = loc("reshape.9402")
#loc3340 = loc("reshape.746")
#loc3341 = loc("custom-call.747")
#loc3342 = loc("reshape.748")
#loc3343 = loc("transpose.749")
#loc3344 = loc("dot.9403")
#loc3345 = loc("reshape.9404")
#loc3346 = loc("reshape.742")
#loc3347 = loc("custom-call.743")
#loc3348 = loc("reshape.744")
#loc3349 = loc("broadcast.9407")
#loc3350 = loc("add.9408")
#loc3351 = loc("add.9411")
#loc3352 = loc("reshape.733")
#loc3353 = loc("custom-call.734")
#loc3354 = loc("reshape.735")
#loc3355 = loc("reshape.728")
#loc3356 = loc("custom-call.729")
#loc3357 = loc("reshape.730")
#loc3358 = loc("custom-call.9488")
#loc3359 = loc("reshape.9489")
#loc3360 = loc("reshape.722")
#loc3361 = loc("custom-call.723")
#loc3362 = loc("reshape.724")
#loc3363 = loc("transpose.725")
#loc3364 = loc("dot.9490")
#loc3365 = loc("reshape.9491")
#loc3366 = loc("reshape.718")
#loc3367 = loc("custom-call.719")
#loc3368 = loc("reshape.720")
#loc3369 = loc("broadcast.9494")
#loc3370 = loc("add.9495")
#loc3371 = loc("custom-call.9508")
#loc3372 = loc("reshape.9509")
#loc3373 = loc("reshape.712")
#loc3374 = loc("custom-call.713")
#loc3375 = loc("reshape.714")
#loc3376 = loc("transpose.715")
#loc3377 = loc("dot.9510")
#loc3378 = loc("reshape.9511")
#loc3379 = loc("reshape.708")
#loc3380 = loc("custom-call.709")
#loc3381 = loc("reshape.710")
#loc3382 = loc("broadcast.9514")
#loc3383 = loc("add.9515")
#loc3384 = loc("add.9518")
#loc3385 = loc("reshape.699")
#loc3386 = loc("custom-call.700")
#loc3387 = loc("reshape.701")
#loc3388 = loc("reshape.694")
#loc3389 = loc("custom-call.695")
#loc3390 = loc("reshape.696")
#loc3391 = loc("custom-call.9595")
#loc3392 = loc("reshape.9641")
#loc3393 = loc("reshape.9637")
#loc3394 = loc("custom-call.9638")
#loc3395 = loc("reshape.9639")
#loc3396 = loc("transpose.9640")
#loc3397 = loc("dot.9642")
#loc3398 = loc("reshape.9643")
#loc3399 = loc("reshape.9633")
#loc3400 = loc("custom-call.9634")
#loc3401 = loc("reshape.9635")
#loc3402 = loc("broadcast.9646")
#loc3403 = loc("add.9647")
#loc3404 = loc("reshape.9648")
#loc3405 = loc("transpose.9649")
#loc3406 = loc("convert.9650")
#loc3407 = loc("multiply.9652")
#loc3408 = loc("reshape.9613")
#loc3409 = loc("custom-call.9614")
#loc3410 = loc("reshape.9615")
#loc3411 = loc("transpose.9616")
#loc3412 = loc("dot.9618")
#loc3413 = loc("reshape.9619")
#loc3414 = loc("reshape.9609")
#loc3415 = loc("custom-call.9610")
#loc3416 = loc("reshape.9611")
#loc3417 = loc("broadcast.9622")
#loc3418 = loc("add.9623")
#loc3419 = loc("reshape.9624")
#loc3420 = loc("transpose.9625")
#loc3421 = loc("convert.9626")
#loc3422 = loc("transpose.9627")
#loc3423 = loc("multiply.9629")
#loc3424 = loc("dot.9653")
#loc3425 = loc("convert.9680")
#loc3426 = loc("compare.9682")
#loc3427 = loc("not.9684")
#loc3429 = loc("or.9694")
#loc3430 = loc("select.9695")
#loc3431 = loc("reshape.9700")
#loc3432 = loc("not.9702")
#loc3433 = loc("reshape.9704")
#loc3434 = loc("broadcast.9705")
#loc3435 = loc("reduce.9660")
#loc3436 = loc("broadcast.9661")
#loc3437 = loc("subtract.9662")
#loc3438 = loc("exponential.9663")
#loc3439 = loc("reduce.9669")
#loc3440 = loc("broadcast.9670")
#loc3441 = loc("divide.9671")
#loc3442 = loc("select.9706")
#loc3443 = loc("reshape.688")
#loc3444 = loc("custom-call.689")
#loc3445 = loc("reshape.690")
#loc3446 = loc("transpose.691")
#loc3447 = loc("dot.9597")
#loc3448 = loc("reshape.9598")
#loc3449 = loc("reshape.684")
#loc3450 = loc("custom-call.685")
#loc3451 = loc("reshape.686")
#loc3452 = loc("broadcast.9601")
#loc3453 = loc("add.9602")
#loc3454 = loc("reshape.9603")
#loc3455 = loc("transpose.9604")
#loc3456 = loc("convert.9605")
#loc3457 = loc("dot.9707")
#loc3458 = loc("convert.9709")
#loc3459 = loc("transpose.9710")
#loc3460 = loc("reshape.9712")
#loc3461 = loc("reshape.678")
#loc3462 = loc("custom-call.679")
#loc3463 = loc("reshape.680")
#loc3464 = loc("transpose.681")
#loc3465 = loc("dot.9713")
#loc3466 = loc("reshape.9714")
#loc3467 = loc("reshape.674")
#loc3468 = loc("custom-call.675")
#loc3469 = loc("reshape.676")
#loc3470 = loc("broadcast.9717")
#loc3471 = loc("add.9718")
#loc3472 = loc("add.9721")
#loc3473 = loc("reshape.665")
#loc3474 = loc("custom-call.666")
#loc3475 = loc("reshape.667")
#loc3476 = loc("reshape.660")
#loc3477 = loc("custom-call.661")
#loc3478 = loc("reshape.662")
#loc3479 = loc("custom-call.9798")
#loc3480 = loc("reshape.9799")
#loc3481 = loc("reshape.654")
#loc3482 = loc("custom-call.655")
#loc3483 = loc("reshape.656")
#loc3484 = loc("transpose.657")
#loc3485 = loc("dot.9800")
#loc3486 = loc("reshape.9801")
#loc3487 = loc("reshape.650")
#loc3488 = loc("custom-call.651")
#loc3489 = loc("reshape.652")
#loc3490 = loc("broadcast.9804")
#loc3491 = loc("add.9805")
#loc3492 = loc("custom-call.9818")
#loc3493 = loc("reshape.9819")
#loc3494 = loc("reshape.644")
#loc3495 = loc("custom-call.645")
#loc3496 = loc("reshape.646")
#loc3497 = loc("transpose.647")
#loc3498 = loc("dot.9820")
#loc3499 = loc("reshape.9821")
#loc3500 = loc("reshape.640")
#loc3501 = loc("custom-call.641")
#loc3502 = loc("reshape.642")
#loc3503 = loc("broadcast.9824")
#loc3504 = loc("add.9825")
#loc3505 = loc("add.9828")
#loc3506 = loc("reshape.631")
#loc3507 = loc("custom-call.632")
#loc3508 = loc("reshape.633")
#loc3509 = loc("reshape.626")
#loc3510 = loc("custom-call.627")
#loc3511 = loc("reshape.628")
#loc3512 = loc("custom-call.9905")
#loc3513 = loc("reshape.9951")
#loc3514 = loc("reshape.9947")
#loc3515 = loc("custom-call.9948")
#loc3516 = loc("reshape.9949")
#loc3517 = loc("transpose.9950")
#loc3518 = loc("dot.9952")
#loc3519 = loc("reshape.9953")
#loc3520 = loc("reshape.9943")
#loc3521 = loc("custom-call.9944")
#loc3522 = loc("reshape.9945")
#loc3523 = loc("broadcast.9956")
#loc3524 = loc("add.9957")
#loc3525 = loc("reshape.9958")
#loc3526 = loc("transpose.9959")
#loc3527 = loc("convert.9960")
#loc3528 = loc("multiply.9962")
#loc3529 = loc("reshape.9923")
#loc3530 = loc("custom-call.9924")
#loc3531 = loc("reshape.9925")
#loc3532 = loc("transpose.9926")
#loc3533 = loc("dot.9928")
#loc3534 = loc("reshape.9929")
#loc3535 = loc("reshape.9919")
#loc3536 = loc("custom-call.9920")
#loc3537 = loc("reshape.9921")
#loc3538 = loc("broadcast.9932")
#loc3539 = loc("add.9933")
#loc3540 = loc("reshape.9934")
#loc3541 = loc("transpose.9935")
#loc3542 = loc("convert.9936")
#loc3543 = loc("transpose.9937")
#loc3544 = loc("multiply.9939")
#loc3545 = loc("dot.9963")
#loc3546 = loc("convert.9990")
#loc3547 = loc("compare.9992")
#loc3548 = loc("not.9994")
#loc3550 = loc("or.10004")
#loc3551 = loc("select.10005")
#loc3552 = loc("reshape.10010")
#loc3553 = loc("not.10012")
#loc3554 = loc("reshape.10014")
#loc3555 = loc("broadcast.10015")
#loc3556 = loc("reduce.9970")
#loc3557 = loc("broadcast.9971")
#loc3558 = loc("subtract.9972")
#loc3559 = loc("exponential.9973")
#loc3560 = loc("reduce.9979")
#loc3561 = loc("broadcast.9980")
#loc3562 = loc("divide.9981")
#loc3563 = loc("select.10016")
#loc3564 = loc("reshape.620")
#loc3565 = loc("custom-call.621")
#loc3566 = loc("reshape.622")
#loc3567 = loc("transpose.623")
#loc3568 = loc("dot.9907")
#loc3569 = loc("reshape.9908")
#loc3570 = loc("reshape.616")
#loc3571 = loc("custom-call.617")
#loc3572 = loc("reshape.618")
#loc3573 = loc("broadcast.9911")
#loc3574 = loc("add.9912")
#loc3575 = loc("reshape.9913")
#loc3576 = loc("transpose.9914")
#loc3577 = loc("convert.9915")
#loc3578 = loc("dot.10017")
#loc3579 = loc("convert.10019")
#loc3580 = loc("transpose.10020")
#loc3581 = loc("reshape.10022")
#loc3582 = loc("reshape.610")
#loc3583 = loc("custom-call.611")
#loc3584 = loc("reshape.612")
#loc3585 = loc("transpose.613")
#loc3586 = loc("dot.10023")
#loc3587 = loc("reshape.10024")
#loc3588 = loc("reshape.606")
#loc3589 = loc("custom-call.607")
#loc3590 = loc("reshape.608")
#loc3591 = loc("broadcast.10027")
#loc3592 = loc("add.10028")
#loc3593 = loc("add.10031")
#loc3594 = loc("reshape.597")
#loc3595 = loc("custom-call.598")
#loc3596 = loc("reshape.599")
#loc3597 = loc("reshape.592")
#loc3598 = loc("custom-call.593")
#loc3599 = loc("reshape.594")
#loc3600 = loc("custom-call.10108")
#loc3601 = loc("reshape.10109")
#loc3602 = loc("reshape.586")
#loc3603 = loc("custom-call.587")
#loc3604 = loc("reshape.588")
#loc3605 = loc("transpose.589")
#loc3606 = loc("dot.10110")
#loc3607 = loc("reshape.10111")
#loc3608 = loc("reshape.582")
#loc3609 = loc("custom-call.583")
#loc3610 = loc("reshape.584")
#loc3611 = loc("broadcast.10114")
#loc3612 = loc("add.10115")
#loc3613 = loc("custom-call.10128")
#loc3614 = loc("reshape.10129")
#loc3615 = loc("reshape.576")
#loc3616 = loc("custom-call.577")
#loc3617 = loc("reshape.578")
#loc3618 = loc("transpose.579")
#loc3619 = loc("dot.10130")
#loc3620 = loc("reshape.10131")
#loc3621 = loc("reshape.572")
#loc3622 = loc("custom-call.573")
#loc3623 = loc("reshape.574")
#loc3624 = loc("broadcast.10134")
#loc3625 = loc("add.10135")
#loc3626 = loc("add.10138")
#loc3627 = loc("reshape.563")
#loc3628 = loc("custom-call.564")
#loc3629 = loc("reshape.565")
#loc3630 = loc("reshape.558")
#loc3631 = loc("custom-call.559")
#loc3632 = loc("reshape.560")
#loc3633 = loc("custom-call.10215")
#loc3634 = loc("reshape.10261")
#loc3635 = loc("reshape.10257")
#loc3636 = loc("custom-call.10258")
#loc3637 = loc("reshape.10259")
#loc3638 = loc("transpose.10260")
#loc3639 = loc("dot.10262")
#loc3640 = loc("reshape.10263")
#loc3641 = loc("reshape.10253")
#loc3642 = loc("custom-call.10254")
#loc3643 = loc("reshape.10255")
#loc3644 = loc("broadcast.10266")
#loc3645 = loc("add.10267")
#loc3646 = loc("reshape.10268")
#loc3647 = loc("transpose.10269")
#loc3648 = loc("convert.10270")
#loc3649 = loc("multiply.10272")
#loc3650 = loc("reshape.10233")
#loc3651 = loc("custom-call.10234")
#loc3652 = loc("reshape.10235")
#loc3653 = loc("transpose.10236")
#loc3654 = loc("dot.10238")
#loc3655 = loc("reshape.10239")
#loc3656 = loc("reshape.10229")
#loc3657 = loc("custom-call.10230")
#loc3658 = loc("reshape.10231")
#loc3659 = loc("broadcast.10242")
#loc3660 = loc("add.10243")
#loc3661 = loc("reshape.10244")
#loc3662 = loc("transpose.10245")
#loc3663 = loc("convert.10246")
#loc3664 = loc("transpose.10247")
#loc3665 = loc("multiply.10249")
#loc3666 = loc("dot.10273")
#loc3667 = loc("convert.10300")
#loc3668 = loc("compare.10302")
#loc3669 = loc("not.10304")
#loc3671 = loc("or.10314")
#loc3672 = loc("select.10315")
#loc3673 = loc("reshape.10320")
#loc3674 = loc("not.10322")
#loc3675 = loc("reshape.10324")
#loc3676 = loc("broadcast.10325")
#loc3677 = loc("reduce.10280")
#loc3678 = loc("broadcast.10281")
#loc3679 = loc("subtract.10282")
#loc3680 = loc("exponential.10283")
#loc3681 = loc("reduce.10289")
#loc3682 = loc("broadcast.10290")
#loc3683 = loc("divide.10291")
#loc3684 = loc("select.10326")
#loc3685 = loc("reshape.552")
#loc3686 = loc("custom-call.553")
#loc3687 = loc("reshape.554")
#loc3688 = loc("transpose.555")
#loc3689 = loc("dot.10217")
#loc3690 = loc("reshape.10218")
#loc3691 = loc("reshape.548")
#loc3692 = loc("custom-call.549")
#loc3693 = loc("reshape.550")
#loc3694 = loc("broadcast.10221")
#loc3695 = loc("add.10222")
#loc3696 = loc("reshape.10223")
#loc3697 = loc("transpose.10224")
#loc3698 = loc("convert.10225")
#loc3699 = loc("dot.10327")
#loc3700 = loc("convert.10329")
#loc3701 = loc("transpose.10330")
#loc3702 = loc("reshape.10332")
#loc3703 = loc("reshape.542")
#loc3704 = loc("custom-call.543")
#loc3705 = loc("reshape.544")
#loc3706 = loc("transpose.545")
#loc3707 = loc("dot.10333")
#loc3708 = loc("reshape.10334")
#loc3709 = loc("reshape.538")
#loc3710 = loc("custom-call.539")
#loc3711 = loc("reshape.540")
#loc3712 = loc("broadcast.10337")
#loc3713 = loc("add.10338")
#loc3714 = loc("add.10341")
#loc3715 = loc("reshape.529")
#loc3716 = loc("custom-call.530")
#loc3717 = loc("reshape.531")
#loc3718 = loc("reshape.524")
#loc3719 = loc("custom-call.525")
#loc3720 = loc("reshape.526")
#loc3721 = loc("custom-call.10418")
#loc3722 = loc("reshape.10419")
#loc3723 = loc("reshape.518")
#loc3724 = loc("custom-call.519")
#loc3725 = loc("reshape.520")
#loc3726 = loc("transpose.521")
#loc3727 = loc("dot.10420")
#loc3728 = loc("reshape.10421")
#loc3729 = loc("reshape.514")
#loc3730 = loc("custom-call.515")
#loc3731 = loc("reshape.516")
#loc3732 = loc("broadcast.10424")
#loc3733 = loc("add.10425")
#loc3734 = loc("custom-call.10438")
#loc3735 = loc("reshape.10439")
#loc3736 = loc("reshape.508")
#loc3737 = loc("custom-call.509")
#loc3738 = loc("reshape.510")
#loc3739 = loc("transpose.511")
#loc3740 = loc("dot.10440")
#loc3741 = loc("reshape.10441")
#loc3742 = loc("reshape.504")
#loc3743 = loc("custom-call.505")
#loc3744 = loc("reshape.506")
#loc3745 = loc("broadcast.10444")
#loc3746 = loc("add.10445")
#loc3747 = loc("add.10448")
#loc3748 = loc("reshape.495")
#loc3749 = loc("custom-call.496")
#loc3750 = loc("reshape.497")
#loc3751 = loc("reshape.490")
#loc3752 = loc("custom-call.491")
#loc3753 = loc("reshape.492")
#loc3754 = loc("custom-call.10525")
#loc3755 = loc("reshape.10571")
#loc3756 = loc("reshape.10567")
#loc3757 = loc("custom-call.10568")
#loc3758 = loc("reshape.10569")
#loc3759 = loc("transpose.10570")
#loc3760 = loc("dot.10572")
#loc3761 = loc("reshape.10573")
#loc3762 = loc("reshape.10563")
#loc3763 = loc("custom-call.10564")
#loc3764 = loc("reshape.10565")
#loc3765 = loc("broadcast.10576")
#loc3766 = loc("add.10577")
#loc3767 = loc("reshape.10578")
#loc3768 = loc("transpose.10579")
#loc3769 = loc("convert.10580")
#loc3770 = loc("multiply.10582")
#loc3771 = loc("reshape.10543")
#loc3772 = loc("custom-call.10544")
#loc3773 = loc("reshape.10545")
#loc3774 = loc("transpose.10546")
#loc3775 = loc("dot.10548")
#loc3776 = loc("reshape.10549")
#loc3777 = loc("reshape.10539")
#loc3778 = loc("custom-call.10540")
#loc3779 = loc("reshape.10541")
#loc3780 = loc("broadcast.10552")
#loc3781 = loc("add.10553")
#loc3782 = loc("reshape.10554")
#loc3783 = loc("transpose.10555")
#loc3784 = loc("convert.10556")
#loc3785 = loc("transpose.10557")
#loc3786 = loc("multiply.10559")
#loc3787 = loc("dot.10583")
#loc3788 = loc("convert.10610")
#loc3789 = loc("compare.10612")
#loc3790 = loc("not.10614")
#loc3792 = loc("or.10624")
#loc3793 = loc("select.10625")
#loc3794 = loc("reshape.10630")
#loc3795 = loc("not.10632")
#loc3796 = loc("reshape.10634")
#loc3797 = loc("broadcast.10635")
#loc3798 = loc("reduce.10590")
#loc3799 = loc("broadcast.10591")
#loc3800 = loc("subtract.10592")
#loc3801 = loc("exponential.10593")
#loc3802 = loc("reduce.10599")
#loc3803 = loc("broadcast.10600")
#loc3804 = loc("divide.10601")
#loc3805 = loc("select.10636")
#loc3806 = loc("reshape.484")
#loc3807 = loc("custom-call.485")
#loc3808 = loc("reshape.486")
#loc3809 = loc("transpose.487")
#loc3810 = loc("dot.10527")
#loc3811 = loc("reshape.10528")
#loc3812 = loc("reshape.480")
#loc3813 = loc("custom-call.481")
#loc3814 = loc("reshape.482")
#loc3815 = loc("broadcast.10531")
#loc3816 = loc("add.10532")
#loc3817 = loc("reshape.10533")
#loc3818 = loc("transpose.10534")
#loc3819 = loc("convert.10535")
#loc3820 = loc("dot.10637")
#loc3821 = loc("convert.10639")
#loc3822 = loc("transpose.10640")
#loc3823 = loc("reshape.10642")
#loc3824 = loc("reshape.474")
#loc3825 = loc("custom-call.475")
#loc3826 = loc("reshape.476")
#loc3827 = loc("transpose.477")
#loc3828 = loc("dot.10643")
#loc3829 = loc("reshape.10644")
#loc3830 = loc("reshape.470")
#loc3831 = loc("custom-call.471")
#loc3832 = loc("reshape.472")
#loc3833 = loc("broadcast.10647")
#loc3834 = loc("add.10648")
#loc3835 = loc("add.10651")
#loc3836 = loc("reshape.461")
#loc3837 = loc("custom-call.462")
#loc3838 = loc("reshape.463")
#loc3839 = loc("reshape.456")
#loc3840 = loc("custom-call.457")
#loc3841 = loc("reshape.458")
#loc3842 = loc("custom-call.10728")
#loc3843 = loc("reshape.10729")
#loc3844 = loc("reshape.450")
#loc3845 = loc("custom-call.451")
#loc3846 = loc("reshape.452")
#loc3847 = loc("transpose.453")
#loc3848 = loc("dot.10730")
#loc3849 = loc("reshape.10731")
#loc3850 = loc("reshape.446")
#loc3851 = loc("custom-call.447")
#loc3852 = loc("reshape.448")
#loc3853 = loc("broadcast.10734")
#loc3854 = loc("add.10735")
#loc3855 = loc("custom-call.10748")
#loc3856 = loc("reshape.10749")
#loc3857 = loc("reshape.440")
#loc3858 = loc("custom-call.441")
#loc3859 = loc("reshape.442")
#loc3860 = loc("transpose.443")
#loc3861 = loc("dot.10750")
#loc3862 = loc("reshape.10751")
#loc3863 = loc("reshape.436")
#loc3864 = loc("custom-call.437")
#loc3865 = loc("reshape.438")
#loc3866 = loc("broadcast.10754")
#loc3867 = loc("add.10755")
#loc3868 = loc("add.10758")
#loc3869 = loc("reshape.427")
#loc3870 = loc("custom-call.428")
#loc3871 = loc("reshape.429")
#loc3872 = loc("reshape.422")
#loc3873 = loc("custom-call.423")
#loc3874 = loc("reshape.424")
#loc3875 = loc("custom-call.10835")
#loc3876 = loc("reshape.10881")
#loc3877 = loc("reshape.10877")
#loc3878 = loc("custom-call.10878")
#loc3879 = loc("reshape.10879")
#loc3880 = loc("transpose.10880")
#loc3881 = loc("dot.10882")
#loc3882 = loc("reshape.10883")
#loc3883 = loc("reshape.10873")
#loc3884 = loc("custom-call.10874")
#loc3885 = loc("reshape.10875")
#loc3886 = loc("broadcast.10886")
#loc3887 = loc("add.10887")
#loc3888 = loc("reshape.10888")
#loc3889 = loc("transpose.10889")
#loc3890 = loc("convert.10890")
#loc3891 = loc("multiply.10892")
#loc3892 = loc("reshape.10853")
#loc3893 = loc("custom-call.10854")
#loc3894 = loc("reshape.10855")
#loc3895 = loc("transpose.10856")
#loc3896 = loc("dot.10858")
#loc3897 = loc("reshape.10859")
#loc3898 = loc("reshape.10849")
#loc3899 = loc("custom-call.10850")
#loc3900 = loc("reshape.10851")
#loc3901 = loc("broadcast.10862")
#loc3902 = loc("add.10863")
#loc3903 = loc("reshape.10864")
#loc3904 = loc("transpose.10865")
#loc3905 = loc("convert.10866")
#loc3906 = loc("transpose.10867")
#loc3907 = loc("multiply.10869")
#loc3908 = loc("dot.10893")
#loc3909 = loc("convert.10920")
#loc3910 = loc("compare.10922")
#loc3911 = loc("not.10924")
#loc3913 = loc("or.10934")
#loc3914 = loc("select.10935")
#loc3915 = loc("reshape.10940")
#loc3916 = loc("not.10942")
#loc3917 = loc("reshape.10944")
#loc3918 = loc("broadcast.10945")
#loc3919 = loc("reduce.10900")
#loc3920 = loc("broadcast.10901")
#loc3921 = loc("subtract.10902")
#loc3922 = loc("exponential.10903")
#loc3923 = loc("reduce.10909")
#loc3924 = loc("broadcast.10910")
#loc3925 = loc("divide.10911")
#loc3926 = loc("select.10946")
#loc3927 = loc("reshape.416")
#loc3928 = loc("custom-call.417")
#loc3929 = loc("reshape.418")
#loc3930 = loc("transpose.419")
#loc3931 = loc("dot.10837")
#loc3932 = loc("reshape.10838")
#loc3933 = loc("reshape.412")
#loc3934 = loc("custom-call.413")
#loc3935 = loc("reshape.414")
#loc3936 = loc("broadcast.10841")
#loc3937 = loc("add.10842")
#loc3938 = loc("reshape.10843")
#loc3939 = loc("transpose.10844")
#loc3940 = loc("convert.10845")
#loc3941 = loc("dot.10947")
#loc3942 = loc("convert.10949")
#loc3943 = loc("transpose.10950")
#loc3944 = loc("reshape.10952")
#loc3945 = loc("reshape.406")
#loc3946 = loc("custom-call.407")
#loc3947 = loc("reshape.408")
#loc3948 = loc("transpose.409")
#loc3949 = loc("dot.10953")
#loc3950 = loc("reshape.10954")
#loc3951 = loc("reshape.402")
#loc3952 = loc("custom-call.403")
#loc3953 = loc("reshape.404")
#loc3954 = loc("broadcast.10957")
#loc3955 = loc("add.10958")
#loc3956 = loc("add.10961")
#loc3957 = loc("reshape.393")
#loc3958 = loc("custom-call.394")
#loc3959 = loc("reshape.395")
#loc3960 = loc("reshape.388")
#loc3961 = loc("custom-call.389")
#loc3962 = loc("reshape.390")
#loc3963 = loc("custom-call.11038")
#loc3964 = loc("reshape.11039")
#loc3965 = loc("reshape.382")
#loc3966 = loc("custom-call.383")
#loc3967 = loc("reshape.384")
#loc3968 = loc("transpose.385")
#loc3969 = loc("dot.11040")
#loc3970 = loc("reshape.11041")
#loc3971 = loc("reshape.378")
#loc3972 = loc("custom-call.379")
#loc3973 = loc("reshape.380")
#loc3974 = loc("broadcast.11044")
#loc3975 = loc("add.11045")
#loc3976 = loc("custom-call.11058")
#loc3977 = loc("reshape.11059")
#loc3978 = loc("reshape.372")
#loc3979 = loc("custom-call.373")
#loc3980 = loc("reshape.374")
#loc3981 = loc("transpose.375")
#loc3982 = loc("dot.11060")
#loc3983 = loc("reshape.11061")
#loc3984 = loc("reshape.368")
#loc3985 = loc("custom-call.369")
#loc3986 = loc("reshape.370")
#loc3987 = loc("broadcast.11064")
#loc3988 = loc("add.11065")
#loc3989 = loc("add.11068")
#loc3990 = loc("reshape.359")
#loc3991 = loc("custom-call.360")
#loc3992 = loc("reshape.361")
#loc3993 = loc("reshape.354")
#loc3994 = loc("custom-call.355")
#loc3995 = loc("reshape.356")
#loc3996 = loc("custom-call.11145")
#loc3997 = loc("reshape.11191")
#loc3998 = loc("reshape.11187")
#loc3999 = loc("custom-call.11188")
#loc4000 = loc("reshape.11189")
#loc4001 = loc("transpose.11190")
#loc4002 = loc("dot.11192")
#loc4003 = loc("reshape.11193")
#loc4004 = loc("reshape.11183")
#loc4005 = loc("custom-call.11184")
#loc4006 = loc("reshape.11185")
#loc4007 = loc("broadcast.11196")
#loc4008 = loc("add.11197")
#loc4009 = loc("reshape.11198")
#loc4010 = loc("transpose.11199")
#loc4011 = loc("convert.11200")
#loc4012 = loc("multiply.11202")
#loc4013 = loc("reshape.11163")
#loc4014 = loc("custom-call.11164")
#loc4015 = loc("reshape.11165")
#loc4016 = loc("transpose.11166")
#loc4017 = loc("dot.11168")
#loc4018 = loc("reshape.11169")
#loc4019 = loc("reshape.11159")
#loc4020 = loc("custom-call.11160")
#loc4021 = loc("reshape.11161")
#loc4022 = loc("broadcast.11172")
#loc4023 = loc("add.11173")
#loc4024 = loc("reshape.11174")
#loc4025 = loc("transpose.11175")
#loc4026 = loc("convert.11176")
#loc4027 = loc("transpose.11177")
#loc4028 = loc("multiply.11179")
#loc4029 = loc("dot.11203")
#loc4030 = loc("convert.11230")
#loc4031 = loc("compare.11232")
#loc4032 = loc("not.11234")
#loc4034 = loc("or.11244")
#loc4035 = loc("select.11245")
#loc4036 = loc("reshape.11250")
#loc4037 = loc("not.11252")
#loc4038 = loc("reshape.11254")
#loc4039 = loc("broadcast.11255")
#loc4040 = loc("reduce.11210")
#loc4041 = loc("broadcast.11211")
#loc4042 = loc("subtract.11212")
#loc4043 = loc("exponential.11213")
#loc4044 = loc("reduce.11219")
#loc4045 = loc("broadcast.11220")
#loc4046 = loc("divide.11221")
#loc4047 = loc("select.11256")
#loc4048 = loc("reshape.348")
#loc4049 = loc("custom-call.349")
#loc4050 = loc("reshape.350")
#loc4051 = loc("transpose.351")
#loc4052 = loc("dot.11147")
#loc4053 = loc("reshape.11148")
#loc4054 = loc("reshape.344")
#loc4055 = loc("custom-call.345")
#loc4056 = loc("reshape.346")
#loc4057 = loc("broadcast.11151")
#loc4058 = loc("add.11152")
#loc4059 = loc("reshape.11153")
#loc4060 = loc("transpose.11154")
#loc4061 = loc("convert.11155")
#loc4062 = loc("dot.11257")
#loc4063 = loc("convert.11259")
#loc4064 = loc("transpose.11260")
#loc4065 = loc("reshape.11262")
#loc4066 = loc("reshape.338")
#loc4067 = loc("custom-call.339")
#loc4068 = loc("reshape.340")
#loc4069 = loc("transpose.341")
#loc4070 = loc("dot.11263")
#loc4071 = loc("reshape.11264")
#loc4072 = loc("reshape.334")
#loc4073 = loc("custom-call.335")
#loc4074 = loc("reshape.336")
#loc4075 = loc("broadcast.11267")
#loc4076 = loc("add.11268")
#loc4077 = loc("add.11271")
#loc4078 = loc("reshape.325")
#loc4079 = loc("custom-call.326")
#loc4080 = loc("reshape.327")
#loc4081 = loc("reshape.320")
#loc4082 = loc("custom-call.321")
#loc4083 = loc("reshape.322")
#loc4084 = loc("custom-call.11348")
#loc4085 = loc("reshape.11349")
#loc4086 = loc("reshape.314")
#loc4087 = loc("custom-call.315")
#loc4088 = loc("reshape.316")
#loc4089 = loc("transpose.317")
#loc4090 = loc("dot.11350")
#loc4091 = loc("reshape.11351")
#loc4092 = loc("reshape.310")
#loc4093 = loc("custom-call.311")
#loc4094 = loc("reshape.312")
#loc4095 = loc("broadcast.11354")
#loc4096 = loc("add.11355")
#loc4097 = loc("custom-call.11368")
#loc4098 = loc("reshape.11369")
#loc4099 = loc("reshape.304")
#loc4100 = loc("custom-call.305")
#loc4101 = loc("reshape.306")
#loc4102 = loc("transpose.307")
#loc4103 = loc("dot.11370")
#loc4104 = loc("reshape.11371")
#loc4105 = loc("reshape.300")
#loc4106 = loc("custom-call.301")
#loc4107 = loc("reshape.302")
#loc4108 = loc("broadcast.11374")
#loc4109 = loc("add.11375")
#loc4110 = loc("add.11378")
#loc4111 = loc("reshape.291")
#loc4112 = loc("custom-call.292")
#loc4113 = loc("reshape.293")
#loc4114 = loc("reshape.286")
#loc4115 = loc("custom-call.287")
#loc4116 = loc("reshape.288")
#loc4117 = loc("custom-call.11455")
#loc4118 = loc("reshape.11501")
#loc4119 = loc("reshape.11497")
#loc4120 = loc("custom-call.11498")
#loc4121 = loc("reshape.11499")
#loc4122 = loc("transpose.11500")
#loc4123 = loc("dot.11502")
#loc4124 = loc("reshape.11503")
#loc4125 = loc("reshape.11493")
#loc4126 = loc("custom-call.11494")
#loc4127 = loc("reshape.11495")
#loc4128 = loc("broadcast.11506")
#loc4129 = loc("add.11507")
#loc4130 = loc("reshape.11508")
#loc4131 = loc("transpose.11509")
#loc4132 = loc("convert.11510")
#loc4133 = loc("multiply.11512")
#loc4134 = loc("reshape.11473")
#loc4135 = loc("custom-call.11474")
#loc4136 = loc("reshape.11475")
#loc4137 = loc("transpose.11476")
#loc4138 = loc("dot.11478")
#loc4139 = loc("reshape.11479")
#loc4140 = loc("reshape.11469")
#loc4141 = loc("custom-call.11470")
#loc4142 = loc("reshape.11471")
#loc4143 = loc("broadcast.11482")
#loc4144 = loc("add.11483")
#loc4145 = loc("reshape.11484")
#loc4146 = loc("transpose.11485")
#loc4147 = loc("convert.11486")
#loc4148 = loc("transpose.11487")
#loc4149 = loc("multiply.11489")
#loc4150 = loc("dot.11513")
#loc4151 = loc("convert.11540")
#loc4152 = loc("compare.11542")
#loc4153 = loc("not.11544")
#loc4155 = loc("or.11554")
#loc4156 = loc("select.11555")
#loc4157 = loc("reshape.11560")
#loc4158 = loc("not.11562")
#loc4159 = loc("reshape.11564")
#loc4160 = loc("broadcast.11565")
#loc4161 = loc("reduce.11520")
#loc4162 = loc("broadcast.11521")
#loc4163 = loc("subtract.11522")
#loc4164 = loc("exponential.11523")
#loc4165 = loc("reduce.11529")
#loc4166 = loc("broadcast.11530")
#loc4167 = loc("divide.11531")
#loc4168 = loc("select.11566")
#loc4169 = loc("reshape.280")
#loc4170 = loc("custom-call.281")
#loc4171 = loc("reshape.282")
#loc4172 = loc("transpose.283")
#loc4173 = loc("dot.11457")
#loc4174 = loc("reshape.11458")
#loc4175 = loc("reshape.276")
#loc4176 = loc("custom-call.277")
#loc4177 = loc("reshape.278")
#loc4178 = loc("broadcast.11461")
#loc4179 = loc("add.11462")
#loc4180 = loc("reshape.11463")
#loc4181 = loc("transpose.11464")
#loc4182 = loc("convert.11465")
#loc4183 = loc("dot.11567")
#loc4184 = loc("convert.11569")
#loc4185 = loc("transpose.11570")
#loc4186 = loc("reshape.11572")
#loc4187 = loc("reshape.270")
#loc4188 = loc("custom-call.271")
#loc4189 = loc("reshape.272")
#loc4190 = loc("transpose.273")
#loc4191 = loc("dot.11573")
#loc4192 = loc("reshape.11574")
#loc4193 = loc("reshape.266")
#loc4194 = loc("custom-call.267")
#loc4195 = loc("reshape.268")
#loc4196 = loc("broadcast.11577")
#loc4197 = loc("add.11578")
#loc4198 = loc("add.11581")
#loc4199 = loc("reshape.257")
#loc4200 = loc("custom-call.258")
#loc4201 = loc("reshape.259")
#loc4202 = loc("reshape.252")
#loc4203 = loc("custom-call.253")
#loc4204 = loc("reshape.254")
#loc4205 = loc("custom-call.11658")
#loc4206 = loc("reshape.11659")
#loc4207 = loc("reshape.246")
#loc4208 = loc("custom-call.247")
#loc4209 = loc("reshape.248")
#loc4210 = loc("transpose.249")
#loc4211 = loc("dot.11660")
#loc4212 = loc("reshape.11661")
#loc4213 = loc("reshape.242")
#loc4214 = loc("custom-call.243")
#loc4215 = loc("reshape.244")
#loc4216 = loc("broadcast.11664")
#loc4217 = loc("add.11665")
#loc4218 = loc("custom-call.11678")
#loc4219 = loc("reshape.11679")
#loc4220 = loc("reshape.236")
#loc4221 = loc("custom-call.237")
#loc4222 = loc("reshape.238")
#loc4223 = loc("transpose.239")
#loc4224 = loc("dot.11680")
#loc4225 = loc("reshape.11681")
#loc4226 = loc("reshape.232")
#loc4227 = loc("custom-call.233")
#loc4228 = loc("reshape.234")
#loc4229 = loc("broadcast.11684")
#loc4230 = loc("add.11685")
#loc4231 = loc("add.11688")
#loc4232 = loc("reshape.223")
#loc4233 = loc("custom-call.224")
#loc4234 = loc("reshape.225")
#loc4235 = loc("reshape.218")
#loc4236 = loc("custom-call.219")
#loc4237 = loc("reshape.220")
#loc4238 = loc("custom-call.11765")
#loc4239 = loc("reshape.11811")
#loc4240 = loc("reshape.11807")
#loc4241 = loc("custom-call.11808")
#loc4242 = loc("reshape.11809")
#loc4243 = loc("transpose.11810")
#loc4244 = loc("dot.11812")
#loc4245 = loc("reshape.11813")
#loc4246 = loc("reshape.11803")
#loc4247 = loc("custom-call.11804")
#loc4248 = loc("reshape.11805")
#loc4249 = loc("broadcast.11816")
#loc4250 = loc("add.11817")
#loc4251 = loc("reshape.11818")
#loc4252 = loc("transpose.11819")
#loc4253 = loc("convert.11820")
#loc4254 = loc("multiply.11822")
#loc4255 = loc("reshape.11783")
#loc4256 = loc("custom-call.11784")
#loc4257 = loc("reshape.11785")
#loc4258 = loc("transpose.11786")
#loc4259 = loc("dot.11788")
#loc4260 = loc("reshape.11789")
#loc4261 = loc("reshape.11779")
#loc4262 = loc("custom-call.11780")
#loc4263 = loc("reshape.11781")
#loc4264 = loc("broadcast.11792")
#loc4265 = loc("add.11793")
#loc4266 = loc("reshape.11794")
#loc4267 = loc("transpose.11795")
#loc4268 = loc("convert.11796")
#loc4269 = loc("transpose.11797")
#loc4270 = loc("multiply.11799")
#loc4271 = loc("dot.11823")
#loc4272 = loc("convert.11850")
#loc4273 = loc("compare.11852")
#loc4274 = loc("not.11854")
#loc4276 = loc("or.11864")
#loc4277 = loc("select.11865")
#loc4278 = loc("reshape.11870")
#loc4279 = loc("not.11872")
#loc4280 = loc("reshape.11874")
#loc4281 = loc("broadcast.11875")
#loc4282 = loc("reduce.11830")
#loc4283 = loc("broadcast.11831")
#loc4284 = loc("subtract.11832")
#loc4285 = loc("exponential.11833")
#loc4286 = loc("reduce.11839")
#loc4287 = loc("broadcast.11840")
#loc4288 = loc("divide.11841")
#loc4289 = loc("select.11876")
#loc4290 = loc("reshape.212")
#loc4291 = loc("custom-call.213")
#loc4292 = loc("reshape.214")
#loc4293 = loc("transpose.215")
#loc4294 = loc("dot.11767")
#loc4295 = loc("reshape.11768")
#loc4296 = loc("reshape.208")
#loc4297 = loc("custom-call.209")
#loc4298 = loc("reshape.210")
#loc4299 = loc("broadcast.11771")
#loc4300 = loc("add.11772")
#loc4301 = loc("reshape.11773")
#loc4302 = loc("transpose.11774")
#loc4303 = loc("convert.11775")
#loc4304 = loc("dot.11877")
#loc4305 = loc("convert.11879")
#loc4306 = loc("transpose.11880")
#loc4307 = loc("reshape.11882")
#loc4308 = loc("reshape.202")
#loc4309 = loc("custom-call.203")
#loc4310 = loc("reshape.204")
#loc4311 = loc("transpose.205")
#loc4312 = loc("dot.11883")
#loc4313 = loc("reshape.11884")
#loc4314 = loc("reshape.198")
#loc4315 = loc("custom-call.199")
#loc4316 = loc("reshape.200")
#loc4317 = loc("broadcast.11887")
#loc4318 = loc("add.11888")
#loc4319 = loc("add.11891")
#loc4320 = loc("reshape.189")
#loc4321 = loc("custom-call.190")
#loc4322 = loc("reshape.191")
#loc4323 = loc("reshape.184")
#loc4324 = loc("custom-call.185")
#loc4325 = loc("reshape.186")
#loc4326 = loc("custom-call.11968")
#loc4327 = loc("reshape.11969")
#loc4328 = loc("reshape.178")
#loc4329 = loc("custom-call.179")
#loc4330 = loc("reshape.180")
#loc4331 = loc("transpose.181")
#loc4332 = loc("dot.11970")
#loc4333 = loc("reshape.11971")
#loc4334 = loc("reshape.174")
#loc4335 = loc("custom-call.175")
#loc4336 = loc("reshape.176")
#loc4337 = loc("broadcast.11974")
#loc4338 = loc("add.11975")
#loc4339 = loc("custom-call.11988")
#loc4340 = loc("reshape.11989")
#loc4341 = loc("reshape.168")
#loc4342 = loc("custom-call.169")
#loc4343 = loc("reshape.170")
#loc4344 = loc("transpose.171")
#loc4345 = loc("dot.11990")
#loc4346 = loc("reshape.11991")
#loc4347 = loc("reshape.164")
#loc4348 = loc("custom-call.165")
#loc4349 = loc("reshape.166")
#loc4350 = loc("broadcast.11994")
#loc4351 = loc("add.11995")
#loc4352 = loc("add.11998")
#loc4353 = loc("reshape.11999")
#loc4354 = loc("reshape.157")
#loc4355 = loc("custom-call.158")
#loc4356 = loc("reshape.159")
#loc4357 = loc("transpose.160")
#loc4358 = loc("dot.12000")
#loc4359 = loc("reshape.12001")
#loc4360 = loc("reshape.153")
#loc4361 = loc("custom-call.154")
#loc4362 = loc("reshape.155")
#loc4363 = loc("broadcast.12004")
#loc4364 = loc("add.12005")
#loc4365 = loc("reshape.145")
#loc4366 = loc("custom-call.146")
#loc4367 = loc("reshape.147")
#loc4368 = loc("reshape.140")
#loc4369 = loc("custom-call.141")
#loc4370 = loc("reshape.142")
#loc4371 = loc("custom-call.12082")
#loc4372 = loc("concatenate.12083")
#loc4373 = loc("reshape.12096")
#loc4374 = loc("reshape.12092")
#loc4375 = loc("custom-call.12093")
#loc4376 = loc("reshape.12094")
#loc4377 = loc("transpose.12095")
#loc4378 = loc("dot.12097")
#loc4379 = loc("reshape.12099")
#loc4380 = loc("transpose.12100")
#loc4381 = loc("convert.12101")
#loc4382 = loc("transpose.12102")
#loc4383 = loc("multiply.12104")
#loc4384 = loc("dot.12119")
#loc4385 = loc("convert.12146")
#loc4386 = loc("compare.12148")
#loc4387 = loc("not.12150")
#loc4389 = loc("or.12160")
#loc4390 = loc("select.12161")
#loc4391 = loc("reshape.12166")
#loc4392 = loc("not.12168")
#loc4393 = loc("reshape.12170")
#loc4394 = loc("broadcast.12171")
#loc4395 = loc("reduce.12126")
#loc4396 = loc("broadcast.12127")
#loc4397 = loc("subtract.12128")
#loc4398 = loc("exponential.12129")
#loc4399 = loc("reduce.12135")
#loc4400 = loc("broadcast.12136")
#loc4401 = loc("divide.12137")
#loc4402 = loc("select.12172")
#loc4403 = loc("reshape.44")
#loc4404 = loc("custom-call.45")
#loc4405 = loc("reshape.46")
#loc4406 = loc("transpose.47")
#loc4407 = loc("dot.12085")
#loc4408 = loc("reshape.12087")
#loc4409 = loc("transpose.12088")
#loc4410 = loc("convert.12089")
#loc4411 = loc("dot.12173")
#loc4412 = loc("convert.12175")
#loc4413 = loc("transpose.12176")
#loc4414 = loc("reshape.12178")
#loc4415 = loc("reshape.39")
#loc4416 = loc("custom-call.40")
#loc4417 = loc("reshape.41")
#loc4418 = loc("transpose.42")
#loc4419 = loc("dot.12179")
#loc4420 = loc("reshape.12180")
#loc4421 = loc("divide.12182")
#loc4422 = loc("add.12185")
#loc4423 = loc("reshape.12203")
#loc4424 = loc("custom-call.12204")
#loc4425 = loc("reshape.12205")
#loc4426 = loc("reshape.12198")
#loc4427 = loc("custom-call.12199")
#loc4428 = loc("reshape.12200")
#loc4429 = loc("custom-call.12285")
#loc4430 = loc("reshape.12286")
#loc4431 = loc("reshape.12192")
#loc4432 = loc("custom-call.12193")
#loc4433 = loc("reshape.12194")
#loc4434 = loc("transpose.12195")
#loc4435 = loc("dot.12287")
#loc4436 = loc("reshape.12288")
#loc4437 = loc("custom-call.12301")
#loc4438 = loc("reshape.12302")
#loc4439 = loc("reshape.12187")
#loc4440 = loc("custom-call.12188")
#loc4441 = loc("reshape.12189")
#loc4442 = loc("transpose.12190")
#loc4443 = loc("dot.12303")
#loc4444 = loc("reshape.12304")
#loc4445 = loc("add.12307")
#loc4446 = loc("reshape.12326")
#loc4447 = loc("custom-call.12327")
#loc4448 = loc("reshape.12328")
#loc4449 = loc("reshape.12321")
#loc4450 = loc("custom-call.12322")
#loc4451 = loc("reshape.12323")
#loc4452 = loc("custom-call.12408")
#loc4453 = loc("reshape.12527")
#loc4454 = loc("reshape.12523")
#loc4455 = loc("custom-call.12524")
#loc4456 = loc("reshape.12525")
#loc4457 = loc("transpose.12526")
#loc4458 = loc("dot.12528")
#loc4459 = loc("reshape.12530")
#loc4460 = loc("transpose.12531")
#loc4461 = loc("convert.12532")
#loc4462 = loc("multiply.12534")
#loc4463 = loc("reshape.12416")
#loc4464 = loc("custom-call.12417")
#loc4465 = loc("reshape.12418")
#loc4466 = loc("reshape.12411")
#loc4467 = loc("custom-call.12412")
#loc4468 = loc("reshape.12413")
#loc4469 = loc("custom-call.12498")
#loc4470 = loc("concatenate.12499")
#loc4471 = loc("reshape.12512")
#loc4472 = loc("reshape.12508")
#loc4473 = loc("custom-call.12509")
#loc4474 = loc("reshape.12510")
#loc4475 = loc("transpose.12511")
#loc4476 = loc("dot.12513")
#loc4477 = loc("reshape.12515")
#loc4478 = loc("transpose.12516")
#loc4479 = loc("convert.12517")
#loc4480 = loc("transpose.12518")
#loc4481 = loc("multiply.12520")
#loc4482 = loc("dot.12535")
#loc4483 = loc("convert.12562")
#loc4484 = loc("compare.12564")
#loc4485 = loc("not.12566")
#loc4487 = loc("or.12576")
#loc4488 = loc("select.12577")
#loc4489 = loc("reshape.12582")
#loc4490 = loc("not.12584")
#loc4491 = loc("reshape.12586")
#loc4492 = loc("broadcast.12587")
#loc4493 = loc("reduce.12542")
#loc4494 = loc("broadcast.12543")
#loc4495 = loc("subtract.12544")
#loc4496 = loc("exponential.12545")
#loc4497 = loc("reduce.12551")
#loc4498 = loc("broadcast.12552")
#loc4499 = loc("divide.12553")
#loc4500 = loc("select.12588")
#loc4501 = loc("reshape.12315")
#loc4502 = loc("custom-call.12316")
#loc4503 = loc("reshape.12317")
#loc4504 = loc("transpose.12318")
#loc4505 = loc("dot.12501")
#loc4506 = loc("reshape.12503")
#loc4507 = loc("transpose.12504")
#loc4508 = loc("convert.12505")
#loc4509 = loc("dot.12589")
#loc4510 = loc("convert.12591")
#loc4511 = loc("transpose.12592")
#loc4512 = loc("reshape.12594")
#loc4513 = loc("reshape.12310")
#loc4514 = loc("custom-call.12311")
#loc4515 = loc("reshape.12312")
#loc4516 = loc("transpose.12313")
#loc4517 = loc("dot.12595")
#loc4518 = loc("reshape.12596")
#loc4519 = loc("divide.12598")
#loc4520 = loc("add.12601")
#loc4521 = loc("reshape.12619")
#loc4522 = loc("custom-call.12620")
#loc4523 = loc("reshape.12621")
#loc4524 = loc("reshape.12614")
#loc4525 = loc("custom-call.12615")
#loc4526 = loc("reshape.12616")
#loc4527 = loc("custom-call.12701")
#loc4528 = loc("reshape.12702")
#loc4529 = loc("reshape.12608")
#loc4530 = loc("custom-call.12609")
#loc4531 = loc("reshape.12610")
#loc4532 = loc("transpose.12611")
#loc4533 = loc("dot.12703")
#loc4534 = loc("reshape.12704")
#loc4535 = loc("custom-call.12717")
#loc4536 = loc("reshape.12718")
#loc4537 = loc("reshape.12603")
#loc4538 = loc("custom-call.12604")
#loc4539 = loc("reshape.12605")
#loc4540 = loc("transpose.12606")
#loc4541 = loc("dot.12719")
#loc4542 = loc("reshape.12720")
#loc4543 = loc("add.12723")
#loc4544 = loc("reshape.12742")
#loc4545 = loc("custom-call.12743")
#loc4546 = loc("reshape.12744")
#loc4547 = loc("reshape.12737")
#loc4548 = loc("custom-call.12738")
#loc4549 = loc("reshape.12739")
#loc4550 = loc("custom-call.12824")
#loc4551 = loc("reshape.12943")
#loc4552 = loc("reshape.12939")
#loc4553 = loc("custom-call.12940")
#loc4554 = loc("reshape.12941")
#loc4555 = loc("transpose.12942")
#loc4556 = loc("dot.12944")
#loc4557 = loc("reshape.12946")
#loc4558 = loc("transpose.12947")
#loc4559 = loc("convert.12948")
#loc4560 = loc("multiply.12950")
#loc4561 = loc("reshape.12832")
#loc4562 = loc("custom-call.12833")
#loc4563 = loc("reshape.12834")
#loc4564 = loc("reshape.12827")
#loc4565 = loc("custom-call.12828")
#loc4566 = loc("reshape.12829")
#loc4567 = loc("custom-call.12914")
#loc4568 = loc("concatenate.12915")
#loc4569 = loc("reshape.12928")
#loc4570 = loc("reshape.12924")
#loc4571 = loc("custom-call.12925")
#loc4572 = loc("reshape.12926")
#loc4573 = loc("transpose.12927")
#loc4574 = loc("dot.12929")
#loc4575 = loc("reshape.12931")
#loc4576 = loc("transpose.12932")
#loc4577 = loc("convert.12933")
#loc4578 = loc("transpose.12934")
#loc4579 = loc("multiply.12936")
#loc4580 = loc("dot.12951")
#loc4581 = loc("convert.12978")
#loc4582 = loc("compare.12980")
#loc4583 = loc("not.12982")
#loc4585 = loc("or.12992")
#loc4586 = loc("select.12993")
#loc4587 = loc("reshape.12998")
#loc4588 = loc("not.13000")
#loc4589 = loc("reshape.13002")
#loc4590 = loc("broadcast.13003")
#loc4591 = loc("reduce.12958")
#loc4592 = loc("broadcast.12959")
#loc4593 = loc("subtract.12960")
#loc4594 = loc("exponential.12961")
#loc4595 = loc("reduce.12967")
#loc4596 = loc("broadcast.12968")
#loc4597 = loc("divide.12969")
#loc4598 = loc("select.13004")
#loc4599 = loc("reshape.12731")
#loc4600 = loc("custom-call.12732")
#loc4601 = loc("reshape.12733")
#loc4602 = loc("transpose.12734")
#loc4603 = loc("dot.12917")
#loc4604 = loc("reshape.12919")
#loc4605 = loc("transpose.12920")
#loc4606 = loc("convert.12921")
#loc4607 = loc("dot.13005")
#loc4608 = loc("convert.13007")
#loc4609 = loc("transpose.13008")
#loc4610 = loc("reshape.13010")
#loc4611 = loc("reshape.12726")
#loc4612 = loc("custom-call.12727")
#loc4613 = loc("reshape.12728")
#loc4614 = loc("transpose.12729")
#loc4615 = loc("dot.13011")
#loc4616 = loc("reshape.13012")
#loc4617 = loc("divide.13014")
#loc4618 = loc("add.13017")
#loc4619 = loc("reshape.13035")
#loc4620 = loc("custom-call.13036")
#loc4621 = loc("reshape.13037")
#loc4622 = loc("reshape.13030")
#loc4623 = loc("custom-call.13031")
#loc4624 = loc("reshape.13032")
#loc4625 = loc("custom-call.13117")
#loc4626 = loc("reshape.13118")
#loc4627 = loc("reshape.13024")
#loc4628 = loc("custom-call.13025")
#loc4629 = loc("reshape.13026")
#loc4630 = loc("transpose.13027")
#loc4631 = loc("dot.13119")
#loc4632 = loc("reshape.13120")
#loc4633 = loc("custom-call.13133")
#loc4634 = loc("reshape.13134")
#loc4635 = loc("reshape.13019")
#loc4636 = loc("custom-call.13020")
#loc4637 = loc("reshape.13021")
#loc4638 = loc("transpose.13022")
#loc4639 = loc("dot.13135")
#loc4640 = loc("reshape.13136")
#loc4641 = loc("add.13139")
#loc4642 = loc("reshape.13158")
#loc4643 = loc("custom-call.13159")
#loc4644 = loc("reshape.13160")
#loc4645 = loc("reshape.13153")
#loc4646 = loc("custom-call.13154")
#loc4647 = loc("reshape.13155")
#loc4648 = loc("custom-call.13240")
#loc4649 = loc("reshape.13359")
#loc4650 = loc("reshape.13355")
#loc4651 = loc("custom-call.13356")
#loc4652 = loc("reshape.13357")
#loc4653 = loc("transpose.13358")
#loc4654 = loc("dot.13360")
#loc4655 = loc("reshape.13362")
#loc4656 = loc("transpose.13363")
#loc4657 = loc("convert.13364")
#loc4658 = loc("multiply.13366")
#loc4659 = loc("reshape.13248")
#loc4660 = loc("custom-call.13249")
#loc4661 = loc("reshape.13250")
#loc4662 = loc("reshape.13243")
#loc4663 = loc("custom-call.13244")
#loc4664 = loc("reshape.13245")
#loc4665 = loc("custom-call.13330")
#loc4666 = loc("concatenate.13331")
#loc4667 = loc("reshape.13344")
#loc4668 = loc("reshape.13340")
#loc4669 = loc("custom-call.13341")
#loc4670 = loc("reshape.13342")
#loc4671 = loc("transpose.13343")
#loc4672 = loc("dot.13345")
#loc4673 = loc("reshape.13347")
#loc4674 = loc("transpose.13348")
#loc4675 = loc("convert.13349")
#loc4676 = loc("transpose.13350")
#loc4677 = loc("multiply.13352")
#loc4678 = loc("dot.13367")
#loc4679 = loc("convert.13394")
#loc4680 = loc("compare.13396")
#loc4681 = loc("not.13398")
#loc4683 = loc("or.13408")
#loc4684 = loc("select.13409")
#loc4685 = loc("reshape.13414")
#loc4686 = loc("not.13416")
#loc4687 = loc("reshape.13418")
#loc4688 = loc("broadcast.13419")
#loc4689 = loc("reduce.13374")
#loc4690 = loc("broadcast.13375")
#loc4691 = loc("subtract.13376")
#loc4692 = loc("exponential.13377")
#loc4693 = loc("reduce.13383")
#loc4694 = loc("broadcast.13384")
#loc4695 = loc("divide.13385")
#loc4696 = loc("select.13420")
#loc4697 = loc("reshape.13147")
#loc4698 = loc("custom-call.13148")
#loc4699 = loc("reshape.13149")
#loc4700 = loc("transpose.13150")
#loc4701 = loc("dot.13333")
#loc4702 = loc("reshape.13335")
#loc4703 = loc("transpose.13336")
#loc4704 = loc("convert.13337")
#loc4705 = loc("dot.13421")
#loc4706 = loc("convert.13423")
#loc4707 = loc("transpose.13424")
#loc4708 = loc("reshape.13426")
#loc4709 = loc("reshape.13142")
#loc4710 = loc("custom-call.13143")
#loc4711 = loc("reshape.13144")
#loc4712 = loc("transpose.13145")
#loc4713 = loc("dot.13427")
#loc4714 = loc("reshape.13428")
#loc4715 = loc("divide.13430")
#loc4716 = loc("add.13433")
#loc4717 = loc("reshape.13451")
#loc4718 = loc("custom-call.13452")
#loc4719 = loc("reshape.13453")
#loc4720 = loc("reshape.13446")
#loc4721 = loc("custom-call.13447")
#loc4722 = loc("reshape.13448")
#loc4723 = loc("custom-call.13533")
#loc4724 = loc("reshape.13534")
#loc4725 = loc("reshape.13440")
#loc4726 = loc("custom-call.13441")
#loc4727 = loc("reshape.13442")
#loc4728 = loc("transpose.13443")
#loc4729 = loc("dot.13535")
#loc4730 = loc("reshape.13536")
#loc4731 = loc("custom-call.13549")
#loc4732 = loc("reshape.13550")
#loc4733 = loc("reshape.13435")
#loc4734 = loc("custom-call.13436")
#loc4735 = loc("reshape.13437")
#loc4736 = loc("transpose.13438")
#loc4737 = loc("dot.13551")
#loc4738 = loc("reshape.13552")
#loc4739 = loc("add.13555")
#loc4740 = loc("reshape.13556")
#loc4741 = loc("reshape.20")
#loc4742 = loc("custom-call.21")
#loc4743 = loc("reshape.22")
#loc4744 = loc("transpose.23")
#loc4745 = loc("dot.13557")
#loc4746 = loc("reshape.13558")
#loc4747 = loc("reshape.16")
#loc4748 = loc("custom-call.17")
#loc4749 = loc("reshape.18")
#loc4750 = loc("broadcast.13561")
#loc4751 = loc("add.13562")
#loc4752 = loc("reshape.8")
#loc4753 = loc("custom-call.9")
#loc4754 = loc("reshape.10")
#loc4755 = loc("reshape.3")
#loc4756 = loc("custom-call.4")
#loc4757 = loc("reshape.5")
#loc4758 = loc("custom-call.13639")
#loc4762 = loc("reduce.13503")
#loc4763 = loc("multiply.13512")
#loc4764 = loc("broadcast.13522")
#loc4765 = loc("subtract.13523")
#loc4766 = loc("multiply.13479")
#loc4767 = loc("reduce.13486")
#loc4768 = loc("multiply.13495")
#loc4769 = loc("reshape.13496")
#loc4770 = loc("add.13516")
#loc4771 = loc("rsqrt.13517")
#loc4772 = loc("reshape.13524")
#loc4773 = loc("broadcast.13525")
#loc4774 = loc("multiply.13526")
#loc4775 = loc("broadcast.13527")
#loc4776 = loc("multiply.13528")
#loc4777 = loc("broadcast.13531")
#loc4778 = loc("add.13532")
#loc4782 = loc("reduce.13210")
#loc4783 = loc("multiply.13219")
#loc4784 = loc("broadcast.13229")
#loc4785 = loc("subtract.13230")
#loc4786 = loc("multiply.13186")
#loc4787 = loc("reduce.13193")
#loc4788 = loc("multiply.13202")
#loc4789 = loc("reshape.13203")
#loc4790 = loc("add.13223")
#loc4791 = loc("rsqrt.13224")
#loc4792 = loc("reshape.13231")
#loc4793 = loc("broadcast.13232")
#loc4794 = loc("multiply.13233")
#loc4795 = loc("broadcast.13234")
#loc4796 = loc("multiply.13235")
#loc4797 = loc("broadcast.13238")
#loc4798 = loc("add.13239")
#loc4802 = loc("reduce.12794")
#loc4803 = loc("multiply.12803")
#loc4804 = loc("broadcast.12813")
#loc4805 = loc("subtract.12814")
#loc4806 = loc("multiply.12770")
#loc4807 = loc("reduce.12777")
#loc4808 = loc("multiply.12786")
#loc4809 = loc("reshape.12787")
#loc4810 = loc("add.12807")
#loc4811 = loc("rsqrt.12808")
#loc4812 = loc("reshape.12815")
#loc4813 = loc("broadcast.12816")
#loc4814 = loc("multiply.12817")
#loc4815 = loc("broadcast.12818")
#loc4816 = loc("multiply.12819")
#loc4817 = loc("broadcast.12822")
#loc4818 = loc("add.12823")
#loc4822 = loc("reduce.12671")
#loc4823 = loc("multiply.12680")
#loc4824 = loc("broadcast.12690")
#loc4825 = loc("subtract.12691")
#loc4826 = loc("multiply.12647")
#loc4827 = loc("reduce.12654")
#loc4828 = loc("multiply.12663")
#loc4829 = loc("reshape.12664")
#loc4830 = loc("add.12684")
#loc4831 = loc("rsqrt.12685")
#loc4832 = loc("reshape.12692")
#loc4833 = loc("broadcast.12693")
#loc4834 = loc("multiply.12694")
#loc4835 = loc("broadcast.12695")
#loc4836 = loc("multiply.12696")
#loc4837 = loc("broadcast.12699")
#loc4838 = loc("add.12700")
#loc4842 = loc("reduce.12468")
#loc4843 = loc("multiply.12477")
#loc4844 = loc("broadcast.12487")
#loc4845 = loc("subtract.12488")
#loc4846 = loc("multiply.12444")
#loc4847 = loc("reduce.12451")
#loc4848 = loc("multiply.12460")
#loc4849 = loc("reshape.12461")
#loc4850 = loc("add.12481")
#loc4851 = loc("rsqrt.12482")
#loc4852 = loc("reshape.12489")
#loc4853 = loc("broadcast.12490")
#loc4854 = loc("multiply.12491")
#loc4855 = loc("broadcast.12492")
#loc4856 = loc("multiply.12493")
#loc4857 = loc("broadcast.12496")
#loc4858 = loc("add.12497")
#loc4862 = loc("reduce.13300")
#loc4863 = loc("multiply.13309")
#loc4864 = loc("broadcast.13319")
#loc4865 = loc("subtract.13320")
#loc4866 = loc("multiply.13276")
#loc4867 = loc("reduce.13283")
#loc4868 = loc("multiply.13292")
#loc4869 = loc("reshape.13293")
#loc4870 = loc("add.13313")
#loc4871 = loc("rsqrt.13314")
#loc4872 = loc("reshape.13321")
#loc4873 = loc("broadcast.13322")
#loc4874 = loc("multiply.13323")
#loc4875 = loc("broadcast.13324")
#loc4876 = loc("multiply.13325")
#loc4877 = loc("broadcast.13328")
#loc4878 = loc("add.13329")
#loc4882 = loc("reduce.12378")
#loc4883 = loc("multiply.12387")
#loc4884 = loc("broadcast.12397")
#loc4885 = loc("subtract.12398")
#loc4886 = loc("multiply.12354")
#loc4887 = loc("reduce.12361")
#loc4888 = loc("multiply.12370")
#loc4889 = loc("reshape.12371")
#loc4890 = loc("add.12391")
#loc4891 = loc("rsqrt.12392")
#loc4892 = loc("reshape.12399")
#loc4893 = loc("broadcast.12400")
#loc4894 = loc("multiply.12401")
#loc4895 = loc("broadcast.12402")
#loc4896 = loc("multiply.12403")
#loc4897 = loc("broadcast.12406")
#loc4898 = loc("add.12407")
#loc4900 = loc("multiply.12299")
#loc4901 = loc("multiply.12294")
#loc4902 = loc("erf.12295")
#loc4903 = loc("add.12297")
#loc4904 = loc("multiply.12300")
#loc4908 = loc("reduce.12052")
#loc4909 = loc("multiply.12061")
#loc4910 = loc("broadcast.12071")
#loc4911 = loc("subtract.12072")
#loc4912 = loc("multiply.12028")
#loc4913 = loc("reduce.12035")
#loc4914 = loc("multiply.12044")
#loc4915 = loc("reshape.12045")
#loc4916 = loc("add.12065")
#loc4917 = loc("rsqrt.12066")
#loc4918 = loc("reshape.12073")
#loc4919 = loc("broadcast.12074")
#loc4920 = loc("multiply.12075")
#loc4921 = loc("broadcast.12076")
#loc4922 = loc("multiply.12077")
#loc4923 = loc("broadcast.12080")
#loc4924 = loc("add.12081")
#loc4926 = loc("multiply.11986")
#loc4927 = loc("multiply.11981")
#loc4928 = loc("erf.11982")
#loc4929 = loc("add.11984")
#loc4930 = loc("multiply.11987")
#loc4932 = loc("multiply.11676")
#loc4933 = loc("multiply.11671")
#loc4934 = loc("erf.11672")
#loc4935 = loc("add.11674")
#loc4936 = loc("multiply.11677")
#loc4940 = loc("reduce.11628")
#loc4941 = loc("multiply.11637")
#loc4942 = loc("broadcast.11647")
#loc4943 = loc("subtract.11648")
#loc4944 = loc("multiply.11604")
#loc4945 = loc("reduce.11611")
#loc4946 = loc("multiply.11620")
#loc4947 = loc("reshape.11621")
#loc4948 = loc("add.11641")
#loc4949 = loc("rsqrt.11642")
#loc4950 = loc("reshape.11649")
#loc4951 = loc("broadcast.11650")
#loc4952 = loc("multiply.11651")
#loc4953 = loc("broadcast.11652")
#loc4954 = loc("multiply.11653")
#loc4955 = loc("broadcast.11656")
#loc4956 = loc("add.11657")
#loc4960 = loc("reduce.11425")
#loc4961 = loc("multiply.11434")
#loc4962 = loc("broadcast.11444")
#loc4963 = loc("subtract.11445")
#loc4964 = loc("multiply.11401")
#loc4965 = loc("reduce.11408")
#loc4966 = loc("multiply.11417")
#loc4967 = loc("reshape.11418")
#loc4968 = loc("add.11438")
#loc4969 = loc("rsqrt.11439")
#loc4970 = loc("reshape.11446")
#loc4971 = loc("broadcast.11447")
#loc4972 = loc("multiply.11448")
#loc4973 = loc("broadcast.11449")
#loc4974 = loc("multiply.11450")
#loc4975 = loc("broadcast.11453")
#loc4976 = loc("add.11454")
#loc4978 = loc("multiply.11366")
#loc4979 = loc("multiply.11361")
#loc4980 = loc("erf.11362")
#loc4981 = loc("add.11364")
#loc4982 = loc("multiply.11367")
#loc4984 = loc("multiply.11056")
#loc4985 = loc("multiply.11051")
#loc4986 = loc("erf.11052")
#loc4987 = loc("add.11054")
#loc4988 = loc("multiply.11057")
#loc4992 = loc("reduce.11008")
#loc4993 = loc("multiply.11017")
#loc4994 = loc("broadcast.11027")
#loc4995 = loc("subtract.11028")
#loc4996 = loc("multiply.10984")
#loc4997 = loc("reduce.10991")
#loc4998 = loc("multiply.11000")
#loc4999 = loc("reshape.11001")
#loc5000 = loc("add.11021")
#loc5001 = loc("rsqrt.11022")
#loc5002 = loc("reshape.11029")
#loc5003 = loc("broadcast.11030")
#loc5004 = loc("multiply.11031")
#loc5005 = loc("broadcast.11032")
#loc5006 = loc("multiply.11033")
#loc5007 = loc("broadcast.11036")
#loc5008 = loc("add.11037")
#loc5010 = loc("multiply.10746")
#loc5011 = loc("multiply.10741")
#loc5012 = loc("erf.10742")
#loc5013 = loc("add.10744")
#loc5014 = loc("multiply.10747")
#loc5018 = loc("reduce.10698")
#loc5019 = loc("multiply.10707")
#loc5020 = loc("broadcast.10717")
#loc5021 = loc("subtract.10718")
#loc5022 = loc("multiply.10674")
#loc5023 = loc("reduce.10681")
#loc5024 = loc("multiply.10690")
#loc5025 = loc("reshape.10691")
#loc5026 = loc("add.10711")
#loc5027 = loc("rsqrt.10712")
#loc5028 = loc("reshape.10719")
#loc5029 = loc("broadcast.10720")
#loc5030 = loc("multiply.10721")
#loc5031 = loc("broadcast.10722")
#loc5032 = loc("multiply.10723")
#loc5033 = loc("broadcast.10726")
#loc5034 = loc("add.10727")
#loc5038 = loc("reduce.13087")
#loc5039 = loc("multiply.13096")
#loc5040 = loc("broadcast.13106")
#loc5041 = loc("subtract.13107")
#loc5042 = loc("multiply.13063")
#loc5043 = loc("reduce.13070")
#loc5044 = loc("multiply.13079")
#loc5045 = loc("reshape.13080")
#loc5046 = loc("add.13100")
#loc5047 = loc("rsqrt.13101")
#loc5048 = loc("reshape.13108")
#loc5049 = loc("broadcast.13109")
#loc5050 = loc("multiply.13110")
#loc5051 = loc("broadcast.13111")
#loc5052 = loc("multiply.13112")
#loc5053 = loc("broadcast.13115")
#loc5054 = loc("add.13116")
#loc5058 = loc("reduce.10495")
#loc5059 = loc("multiply.10504")
#loc5060 = loc("broadcast.10514")
#loc5061 = loc("subtract.10515")
#loc5062 = loc("multiply.10471")
#loc5063 = loc("reduce.10478")
#loc5064 = loc("multiply.10487")
#loc5065 = loc("reshape.10488")
#loc5066 = loc("add.10508")
#loc5067 = loc("rsqrt.10509")
#loc5068 = loc("reshape.10516")
#loc5069 = loc("broadcast.10517")
#loc5070 = loc("multiply.10518")
#loc5071 = loc("broadcast.10519")
#loc5072 = loc("multiply.10520")
#loc5073 = loc("broadcast.10523")
#loc5074 = loc("add.10524")
#loc5076 = loc("multiply.10436")
#loc5077 = loc("multiply.10431")
#loc5078 = loc("erf.10432")
#loc5079 = loc("add.10434")
#loc5080 = loc("multiply.10437")
#loc5084 = loc("reduce.10388")
#loc5085 = loc("multiply.10397")
#loc5086 = loc("broadcast.10407")
#loc5087 = loc("subtract.10408")
#loc5088 = loc("multiply.10364")
#loc5089 = loc("reduce.10371")
#loc5090 = loc("multiply.10380")
#loc5091 = loc("reshape.10381")
#loc5092 = loc("add.10401")
#loc5093 = loc("rsqrt.10402")
#loc5094 = loc("reshape.10409")
#loc5095 = loc("broadcast.10410")
#loc5096 = loc("multiply.10411")
#loc5097 = loc("broadcast.10412")
#loc5098 = loc("multiply.10413")
#loc5099 = loc("broadcast.10416")
#loc5100 = loc("add.10417")
#loc5104 = loc("reduce.12884")
#loc5105 = loc("multiply.12893")
#loc5106 = loc("broadcast.12903")
#loc5107 = loc("subtract.12904")
#loc5108 = loc("multiply.12860")
#loc5109 = loc("reduce.12867")
#loc5110 = loc("multiply.12876")
#loc5111 = loc("reshape.12877")
#loc5112 = loc("add.12897")
#loc5113 = loc("rsqrt.12898")
#loc5114 = loc("reshape.12905")
#loc5115 = loc("broadcast.12906")
#loc5116 = loc("multiply.12907")
#loc5117 = loc("broadcast.12908")
#loc5118 = loc("multiply.12909")
#loc5119 = loc("broadcast.12912")
#loc5120 = loc("add.12913")
#loc5124 = loc("reduce.10185")
#loc5125 = loc("multiply.10194")
#loc5126 = loc("broadcast.10204")
#loc5127 = loc("subtract.10205")
#loc5128 = loc("multiply.10161")
#loc5129 = loc("reduce.10168")
#loc5130 = loc("multiply.10177")
#loc5131 = loc("reshape.10178")
#loc5132 = loc("add.10198")
#loc5133 = loc("rsqrt.10199")
#loc5134 = loc("reshape.10206")
#loc5135 = loc("broadcast.10207")
#loc5136 = loc("multiply.10208")
#loc5137 = loc("broadcast.10209")
#loc5138 = loc("multiply.10210")
#loc5139 = loc("broadcast.10213")
#loc5140 = loc("add.10214")
#loc5144 = loc("reduce.10078")
#loc5145 = loc("multiply.10087")
#loc5146 = loc("broadcast.10097")
#loc5147 = loc("subtract.10098")
#loc5148 = loc("multiply.10054")
#loc5149 = loc("reduce.10061")
#loc5150 = loc("multiply.10070")
#loc5151 = loc("reshape.10071")
#loc5152 = loc("add.10091")
#loc5153 = loc("rsqrt.10092")
#loc5154 = loc("reshape.10099")
#loc5155 = loc("broadcast.10100")
#loc5156 = loc("multiply.10101")
#loc5157 = loc("broadcast.10102")
#loc5158 = loc("multiply.10103")
#loc5159 = loc("broadcast.10106")
#loc5160 = loc("add.10107")
#loc5164 = loc("reduce.9768")
#loc5165 = loc("multiply.9777")
#loc5166 = loc("broadcast.9787")
#loc5167 = loc("subtract.9788")
#loc5168 = loc("multiply.9744")
#loc5169 = loc("reduce.9751")
#loc5170 = loc("multiply.9760")
#loc5171 = loc("reshape.9761")
#loc5172 = loc("add.9781")
#loc5173 = loc("rsqrt.9782")
#loc5174 = loc("reshape.9789")
#loc5175 = loc("broadcast.9790")
#loc5176 = loc("multiply.9791")
#loc5177 = loc("broadcast.9792")
#loc5178 = loc("multiply.9793")
#loc5179 = loc("broadcast.9796")
#loc5180 = loc("add.9797")
#loc5182 = loc("multiply.9506")
#loc5183 = loc("multiply.9501")
#loc5184 = loc("erf.9502")
#loc5185 = loc("add.9504")
#loc5186 = loc("multiply.9507")
#loc5190 = loc("reduce.8945")
#loc5191 = loc("multiply.8954")
#loc5192 = loc("broadcast.8964")
#loc5193 = loc("subtract.8965")
#loc5194 = loc("multiply.8921")
#loc5195 = loc("reduce.8928")
#loc5196 = loc("multiply.8937")
#loc5197 = loc("reshape.8938")
#loc5198 = loc("add.8958")
#loc5199 = loc("rsqrt.8959")
#loc5200 = loc("reshape.8966")
#loc5201 = loc("broadcast.8967")
#loc5202 = loc("multiply.8968")
#loc5203 = loc("broadcast.8969")
#loc5204 = loc("multiply.8970")
#loc5205 = loc("broadcast.8973")
#loc5206 = loc("add.8974")
#loc5208 = loc("multiply.8886")
#loc5209 = loc("multiply.8881")
#loc5210 = loc("erf.8882")
#loc5211 = loc("add.8884")
#loc5212 = loc("multiply.8887")
#loc5216 = loc("reduce.13609")
#loc5217 = loc("multiply.13618")
#loc5218 = loc("broadcast.13628")
#loc5219 = loc("subtract.13629")
#loc5220 = loc("multiply.13585")
#loc5221 = loc("reduce.13592")
#loc5222 = loc("multiply.13601")
#loc5223 = loc("reshape.13602")
#loc5224 = loc("add.13622")
#loc5225 = loc("rsqrt.13623")
#loc5226 = loc("reshape.13630")
#loc5227 = loc("broadcast.13631")
#loc5228 = loc("multiply.13632")
#loc5229 = loc("broadcast.13633")
#loc5230 = loc("multiply.13634")
#loc5231 = loc("broadcast.13637")
#loc5232 = loc("add.13638")
#loc5236 = loc("reduce.8635")
#loc5237 = loc("multiply.8644")
#loc5238 = loc("broadcast.8654")
#loc5239 = loc("subtract.8655")
#loc5240 = loc("multiply.8611")
#loc5241 = loc("reduce.8618")
#loc5242 = loc("multiply.8627")
#loc5243 = loc("reshape.8628")
#loc5244 = loc("add.8648")
#loc5245 = loc("rsqrt.8649")
#loc5246 = loc("reshape.8656")
#loc5247 = loc("broadcast.8657")
#loc5248 = loc("multiply.8658")
#loc5249 = loc("broadcast.8659")
#loc5250 = loc("multiply.8660")
#loc5251 = loc("broadcast.8663")
#loc5252 = loc("add.8664")
#loc5256 = loc("reduce.8528")
#loc5257 = loc("multiply.8537")
#loc5258 = loc("broadcast.8547")
#loc5259 = loc("subtract.8548")
#loc5260 = loc("multiply.8504")
#loc5261 = loc("reduce.8511")
#loc5262 = loc("multiply.8520")
#loc5263 = loc("reshape.8521")
#loc5264 = loc("add.8541")
#loc5265 = loc("rsqrt.8542")
#loc5266 = loc("reshape.8549")
#loc5267 = loc("broadcast.8550")
#loc5268 = loc("multiply.8551")
#loc5269 = loc("broadcast.8552")
#loc5270 = loc("multiply.8553")
#loc5271 = loc("broadcast.8556")
#loc5272 = loc("add.8557")
#loc5276 = loc("reduce.4808")
#loc5277 = loc("multiply.4817")
#loc5278 = loc("broadcast.4827")
#loc5279 = loc("subtract.4828")
#loc5280 = loc("multiply.4784")
#loc5281 = loc("reduce.4791")
#loc5282 = loc("multiply.4800")
#loc5283 = loc("reshape.4801")
#loc5284 = loc("add.4821")
#loc5285 = loc("rsqrt.4822")
#loc5286 = loc("reshape.4829")
#loc5287 = loc("broadcast.4830")
#loc5288 = loc("multiply.4831")
#loc5289 = loc("broadcast.4832")
#loc5290 = loc("multiply.4833")
#loc5291 = loc("broadcast.4836")
#loc5292 = loc("add.4837")
#loc5296 = loc("reduce.2358")
#loc5297 = loc("multiply.2367")
#loc5298 = loc("broadcast.2377")
#loc5299 = loc("subtract.2378")
#loc5300 = loc("multiply.2334")
#loc5301 = loc("reduce.2341")
#loc5302 = loc("multiply.2350")
#loc5303 = loc("reshape.2351")
#loc5304 = loc("add.2371")
#loc5305 = loc("rsqrt.2372")
#loc5306 = loc("reshape.2379")
#loc5307 = loc("broadcast.2380")
#loc5308 = loc("multiply.2381")
#loc5309 = loc("broadcast.2382")
#loc5310 = loc("multiply.2383")
#loc5311 = loc("broadcast.2386")
#loc5312 = loc("add.2387")
#loc5314 = loc("multiply.4546")
#loc5315 = loc("multiply.4541")
#loc5316 = loc("erf.4542")
#loc5317 = loc("add.4544")
#loc5318 = loc("multiply.4547")
#loc5322 = loc("reduce.3568")
#loc5323 = loc("multiply.3577")
#loc5324 = loc("broadcast.3587")
#loc5325 = loc("subtract.3588")
#loc5326 = loc("multiply.3544")
#loc5327 = loc("reduce.3551")
#loc5328 = loc("multiply.3560")
#loc5329 = loc("reshape.3561")
#loc5330 = loc("add.3581")
#loc5331 = loc("rsqrt.3582")
#loc5332 = loc("reshape.3589")
#loc5333 = loc("broadcast.3590")
#loc5334 = loc("multiply.3591")
#loc5335 = loc("broadcast.3592")
#loc5336 = loc("multiply.3593")
#loc5337 = loc("broadcast.3596")
#loc5338 = loc("add.3597")
#loc5342 = loc("reduce.2638")
#loc5343 = loc("multiply.2647")
#loc5344 = loc("broadcast.2657")
#loc5345 = loc("subtract.2658")
#loc5346 = loc("multiply.2614")
#loc5347 = loc("reduce.2621")
#loc5348 = loc("multiply.2630")
#loc5349 = loc("reshape.2631")
#loc5350 = loc("add.2651")
#loc5351 = loc("rsqrt.2652")
#loc5352 = loc("reshape.2659")
#loc5353 = loc("broadcast.2660")
#loc5354 = loc("multiply.2661")
#loc5355 = loc("broadcast.2662")
#loc5356 = loc("multiply.2663")
#loc5357 = loc("broadcast.2666")
#loc5358 = loc("add.2667")
#loc5362 = loc("reduce.10805")
#loc5363 = loc("multiply.10814")
#loc5364 = loc("broadcast.10824")
#loc5365 = loc("subtract.10825")
#loc5366 = loc("multiply.10781")
#loc5367 = loc("reduce.10788")
#loc5368 = loc("multiply.10797")
#loc5369 = loc("reshape.10798")
#loc5370 = loc("add.10818")
#loc5371 = loc("rsqrt.10819")
#loc5372 = loc("reshape.10826")
#loc5373 = loc("broadcast.10827")
#loc5374 = loc("multiply.10828")
#loc5375 = loc("broadcast.10829")
#loc5376 = loc("multiply.10830")
#loc5377 = loc("broadcast.10833")
#loc5378 = loc("add.10834")
#loc5382 = loc("reduce.107")
#loc5383 = loc("multiply.116")
#loc5384 = loc("broadcast.126")
#loc5385 = loc("subtract.127")
#loc5386 = loc("multiply.83")
#loc5387 = loc("reduce.90")
#loc5388 = loc("multiply.99")
#loc5389 = loc("reshape.100")
#loc5390 = loc("add.120")
#loc5391 = loc("rsqrt.121")
#loc5392 = loc("reshape.128")
#loc5393 = loc("broadcast.129")
#loc5394 = loc("multiply.130")
#loc5395 = loc("broadcast.131")
#loc5396 = loc("multiply.132")
#loc5397 = loc("broadcast.135")
#loc5398 = loc("add.136")
#loc5402 = loc("reduce.9255")
#loc5403 = loc("multiply.9264")
#loc5404 = loc("broadcast.9274")
#loc5405 = loc("subtract.9275")
#loc5406 = loc("multiply.9231")
#loc5407 = loc("reduce.9238")
#loc5408 = loc("multiply.9247")
#loc5409 = loc("reshape.9248")
#loc5410 = loc("add.9268")
#loc5411 = loc("rsqrt.9269")
#loc5412 = loc("reshape.9276")
#loc5413 = loc("broadcast.9277")
#loc5414 = loc("multiply.9278")
#loc5415 = loc("broadcast.9279")
#loc5416 = loc("multiply.9280")
#loc5417 = loc("broadcast.9283")
#loc5418 = loc("add.9284")
#loc5422 = loc("reduce.4295")
#loc5423 = loc("multiply.4304")
#loc5424 = loc("broadcast.4314")
#loc5425 = loc("subtract.4315")
#loc5426 = loc("multiply.4271")
#loc5427 = loc("reduce.4278")
#loc5428 = loc("multiply.4287")
#loc5429 = loc("reshape.4288")
#loc5430 = loc("add.4308")
#loc5431 = loc("rsqrt.4309")
#loc5432 = loc("reshape.4316")
#loc5433 = loc("broadcast.4317")
#loc5434 = loc("multiply.4318")
#loc5435 = loc("broadcast.4319")
#loc5436 = loc("multiply.4320")
#loc5437 = loc("broadcast.4323")
#loc5438 = loc("add.4324")
#loc5440 = loc("multiply.12715")
#loc5441 = loc("multiply.12710")
#loc5442 = loc("erf.12711")
#loc5443 = loc("add.12713")
#loc5444 = loc("multiply.12716")
#loc5448 = loc("reduce.6465")
#loc5449 = loc("multiply.6474")
#loc5450 = loc("broadcast.6484")
#loc5451 = loc("subtract.6485")
#loc5452 = loc("multiply.6441")
#loc5453 = loc("reduce.6448")
#loc5454 = loc("multiply.6457")
#loc5455 = loc("reshape.6458")
#loc5456 = loc("add.6478")
#loc5457 = loc("rsqrt.6479")
#loc5458 = loc("reshape.6486")
#loc5459 = loc("broadcast.6487")
#loc5460 = loc("multiply.6488")
#loc5461 = loc("broadcast.6489")
#loc5462 = loc("multiply.6490")
#loc5463 = loc("broadcast.6493")
#loc5464 = loc("add.6494")
#loc5468 = loc("reduce.3985")
#loc5469 = loc("multiply.3994")
#loc5470 = loc("broadcast.4004")
#loc5471 = loc("subtract.4005")
#loc5472 = loc("multiply.3961")
#loc5473 = loc("reduce.3968")
#loc5474 = loc("multiply.3977")
#loc5475 = loc("reshape.3978")
#loc5476 = loc("add.3998")
#loc5477 = loc("rsqrt.3999")
#loc5478 = loc("reshape.4006")
#loc5479 = loc("broadcast.4007")
#loc5480 = loc("multiply.4008")
#loc5481 = loc("broadcast.4009")
#loc5482 = loc("multiply.4010")
#loc5483 = loc("broadcast.4013")
#loc5484 = loc("add.4014")
#loc5488 = loc("reduce.9148")
#loc5489 = loc("multiply.9157")
#loc5490 = loc("broadcast.9167")
#loc5491 = loc("subtract.9168")
#loc5492 = loc("multiply.9124")
#loc5493 = loc("reduce.9131")
#loc5494 = loc("multiply.9140")
#loc5495 = loc("reshape.9141")
#loc5496 = loc("add.9161")
#loc5497 = loc("rsqrt.9162")
#loc5498 = loc("reshape.9169")
#loc5499 = loc("broadcast.9170")
#loc5500 = loc("multiply.9171")
#loc5501 = loc("broadcast.9172")
#loc5502 = loc("multiply.9173")
#loc5503 = loc("broadcast.9176")
#loc5504 = loc("add.9177")
#loc5506 = loc("multiply.3926")
#loc5507 = loc("multiply.3921")
#loc5508 = loc("erf.3922")
#loc5509 = loc("add.3924")
#loc5510 = loc("multiply.3927")
#loc5514 = loc("reduce.5535")
#loc5515 = loc("multiply.5544")
#loc5516 = loc("broadcast.5554")
#loc5517 = loc("subtract.5555")
#loc5518 = loc("multiply.5511")
#loc5519 = loc("reduce.5518")
#loc5520 = loc("multiply.5527")
#loc5521 = loc("reshape.5528")
#loc5522 = loc("add.5548")
#loc5523 = loc("rsqrt.5549")
#loc5524 = loc("reshape.5556")
#loc5525 = loc("broadcast.5557")
#loc5526 = loc("multiply.5558")
#loc5527 = loc("broadcast.5559")
#loc5528 = loc("multiply.5560")
#loc5529 = loc("broadcast.5563")
#loc5530 = loc("add.5564")
#loc5532 = loc("multiply.13131")
#loc5533 = loc("multiply.13126")
#loc5534 = loc("erf.13127")
#loc5535 = loc("add.13129")
#loc5536 = loc("multiply.13132")
#loc5540 = loc("reduce.6048")
#loc5541 = loc("multiply.6057")
#loc5542 = loc("broadcast.6067")
#loc5543 = loc("subtract.6068")
#loc5544 = loc("multiply.6024")
#loc5545 = loc("reduce.6031")
#loc5546 = loc("multiply.6040")
#loc5547 = loc("reshape.6041")
#loc5548 = loc("add.6061")
#loc5549 = loc("rsqrt.6062")
#loc5550 = loc("reshape.6069")
#loc5551 = loc("broadcast.6070")
#loc5552 = loc("multiply.6071")
#loc5553 = loc("broadcast.6072")
#loc5554 = loc("multiply.6073")
#loc5555 = loc("broadcast.6076")
#loc5556 = loc("add.6077")
#loc5560 = loc("reduce.6358")
#loc5561 = loc("multiply.6367")
#loc5562 = loc("broadcast.6377")
#loc5563 = loc("subtract.6378")
#loc5564 = loc("multiply.6334")
#loc5565 = loc("reduce.6341")
#loc5566 = loc("multiply.6350")
#loc5567 = loc("reshape.6351")
#loc5568 = loc("add.6371")
#loc5569 = loc("rsqrt.6372")
#loc5570 = loc("reshape.6379")
#loc5571 = loc("broadcast.6380")
#loc5572 = loc("multiply.6381")
#loc5573 = loc("broadcast.6382")
#loc5574 = loc("multiply.6383")
#loc5575 = loc("broadcast.6386")
#loc5576 = loc("add.6387")
#loc5580 = loc("reduce.4498")
#loc5581 = loc("multiply.4507")
#loc5582 = loc("broadcast.4517")
#loc5583 = loc("subtract.4518")
#loc5584 = loc("multiply.4474")
#loc5585 = loc("reduce.4481")
#loc5586 = loc("multiply.4490")
#loc5587 = loc("reshape.4491")
#loc5588 = loc("add.4511")
#loc5589 = loc("rsqrt.4512")
#loc5590 = loc("reshape.4519")
#loc5591 = loc("broadcast.4520")
#loc5592 = loc("multiply.4521")
#loc5593 = loc("broadcast.4522")
#loc5594 = loc("multiply.4523")
#loc5595 = loc("broadcast.4526")
#loc5596 = loc("add.4527")
#loc5600 = loc("reduce.3675")
#loc5601 = loc("multiply.3684")
#loc5602 = loc("broadcast.3694")
#loc5603 = loc("subtract.3695")
#loc5604 = loc("multiply.3651")
#loc5605 = loc("reduce.3658")
#loc5606 = loc("multiply.3667")
#loc5607 = loc("reshape.3668")
#loc5608 = loc("add.3688")
#loc5609 = loc("rsqrt.3689")
#loc5610 = loc("reshape.3696")
#loc5611 = loc("broadcast.3697")
#loc5612 = loc("multiply.3698")
#loc5613 = loc("broadcast.3699")
#loc5614 = loc("multiply.3700")
#loc5615 = loc("broadcast.3703")
#loc5616 = loc("add.3704")
#loc5618 = loc("multiply.7336")
#loc5619 = loc("multiply.7331")
#loc5620 = loc("erf.7332")
#loc5621 = loc("add.7334")
#loc5622 = loc("multiply.7337")
#loc5626 = loc("reduce.2435")
#loc5627 = loc("multiply.2444")
#loc5628 = loc("broadcast.2454")
#loc5629 = loc("subtract.2455")
#loc5630 = loc("multiply.2411")
#loc5631 = loc("reduce.2418")
#loc5632 = loc("multiply.2427")
#loc5633 = loc("reshape.2428")
#loc5634 = loc("add.2448")
#loc5635 = loc("rsqrt.2449")
#loc5636 = loc("reshape.2456")
#loc5637 = loc("broadcast.2457")
#loc5638 = loc("multiply.2458")
#loc5639 = loc("broadcast.2459")
#loc5640 = loc("multiply.2460")
#loc5641 = loc("broadcast.2463")
#loc5642 = loc("add.2464")
#loc5644 = loc("multiply.5166")
#loc5645 = loc("multiply.5161")
#loc5646 = loc("erf.5162")
#loc5647 = loc("add.5164")
#loc5648 = loc("multiply.5167")
#loc5652 = loc("reduce.11318")
#loc5653 = loc("multiply.11327")
#loc5654 = loc("broadcast.11337")
#loc5655 = loc("subtract.11338")
#loc5656 = loc("multiply.11294")
#loc5657 = loc("reduce.11301")
#loc5658 = loc("multiply.11310")
#loc5659 = loc("reshape.11311")
#loc5660 = loc("add.11331")
#loc5661 = loc("rsqrt.11332")
#loc5662 = loc("reshape.11339")
#loc5663 = loc("broadcast.11340")
#loc5664 = loc("multiply.11341")
#loc5665 = loc("broadcast.11342")
#loc5666 = loc("multiply.11343")
#loc5667 = loc("broadcast.11346")
#loc5668 = loc("add.11347")
#loc5672 = loc("reduce.4915")
#loc5673 = loc("multiply.4924")
#loc5674 = loc("broadcast.4934")
#loc5675 = loc("subtract.4935")
#loc5676 = loc("multiply.4891")
#loc5677 = loc("reduce.4898")
#loc5678 = loc("multiply.4907")
#loc5679 = loc("reshape.4908")
#loc5680 = loc("add.4928")
#loc5681 = loc("rsqrt.4929")
#loc5682 = loc("reshape.4936")
#loc5683 = loc("broadcast.4937")
#loc5684 = loc("multiply.4938")
#loc5685 = loc("broadcast.4939")
#loc5686 = loc("multiply.4940")
#loc5687 = loc("broadcast.4943")
#loc5688 = loc("add.4944")
#loc5692 = loc("reduce.5845")
#loc5693 = loc("multiply.5854")
#loc5694 = loc("broadcast.5864")
#loc5695 = loc("subtract.5865")
#loc5696 = loc("multiply.5821")
#loc5697 = loc("reduce.5828")
#loc5698 = loc("multiply.5837")
#loc5699 = loc("reshape.5838")
#loc5700 = loc("add.5858")
#loc5701 = loc("rsqrt.5859")
#loc5702 = loc("reshape.5866")
#loc5703 = loc("broadcast.5867")
#loc5704 = loc("multiply.5868")
#loc5705 = loc("broadcast.5869")
#loc5706 = loc("multiply.5870")
#loc5707 = loc("broadcast.5873")
#loc5708 = loc("add.5874")
#loc5712 = loc("reduce.4188")
#loc5713 = loc("multiply.4197")
#loc5714 = loc("broadcast.4207")
#loc5715 = loc("subtract.4208")
#loc5716 = loc("multiply.4164")
#loc5717 = loc("reduce.4171")
#loc5718 = loc("multiply.4180")
#loc5719 = loc("reshape.4181")
#loc5720 = loc("add.4201")
#loc5721 = loc("rsqrt.4202")
#loc5722 = loc("reshape.4209")
#loc5723 = loc("broadcast.4210")
#loc5724 = loc("multiply.4211")
#loc5725 = loc("broadcast.4212")
#loc5726 = loc("multiply.4213")
#loc5727 = loc("broadcast.4216")
#loc5728 = loc("add.4217")
#loc5732 = loc("reduce.3365")
#loc5733 = loc("multiply.3374")
#loc5734 = loc("broadcast.3384")
#loc5735 = loc("subtract.3385")
#loc5736 = loc("multiply.3341")
#loc5737 = loc("reduce.3348")
#loc5738 = loc("multiply.3357")
#loc5739 = loc("reshape.3358")
#loc5740 = loc("add.3378")
#loc5741 = loc("rsqrt.3379")
#loc5742 = loc("reshape.3386")
#loc5743 = loc("broadcast.3387")
#loc5744 = loc("multiply.3388")
#loc5745 = loc("broadcast.3389")
#loc5746 = loc("multiply.3390")
#loc5747 = loc("broadcast.3393")
#loc5748 = loc("add.3394")
#loc5750 = loc("multiply.13547")
#loc5751 = loc("multiply.13542")
#loc5752 = loc("erf.13543")
#loc5753 = loc("add.13545")
#loc5754 = loc("multiply.13548")
#loc5758 = loc("reduce.9875")
#loc5759 = loc("multiply.9884")
#loc5760 = loc("broadcast.9894")
#loc5761 = loc("subtract.9895")
#loc5762 = loc("multiply.9851")
#loc5763 = loc("reduce.9858")
#loc5764 = loc("multiply.9867")
#loc5765 = loc("reshape.9868")
#loc5766 = loc("add.9888")
#loc5767 = loc("rsqrt.9889")
#loc5768 = loc("reshape.9896")
#loc5769 = loc("broadcast.9897")
#loc5770 = loc("multiply.9898")
#loc5771 = loc("broadcast.9899")
#loc5772 = loc("multiply.9900")
#loc5773 = loc("broadcast.9903")
#loc5774 = loc("add.9904")
#loc5776 = loc("multiply.8576")
#loc5777 = loc("multiply.8571")
#loc5778 = loc("erf.8572")
#loc5779 = loc("add.8574")
#loc5780 = loc("multiply.8577")
#loc5784 = loc("reduce.2948")
#loc5785 = loc("multiply.2957")
#loc5786 = loc("broadcast.2967")
#loc5787 = loc("subtract.2968")
#loc5788 = loc("multiply.2924")
#loc5789 = loc("reduce.2931")
#loc5790 = loc("multiply.2940")
#loc5791 = loc("reshape.2941")
#loc5792 = loc("add.2961")
#loc5793 = loc("rsqrt.2962")
#loc5794 = loc("reshape.2969")
#loc5795 = loc("broadcast.2970")
#loc5796 = loc("multiply.2971")
#loc5797 = loc("broadcast.2972")
#loc5798 = loc("multiply.2973")
#loc5799 = loc("broadcast.2976")
#loc5800 = loc("add.2977")
#loc5802 = loc("multiply.2996")
#loc5803 = loc("multiply.2991")
#loc5804 = loc("erf.2992")
#loc5805 = loc("add.2994")
#loc5806 = loc("multiply.2997")
#loc5808 = loc("multiply.2686")
#loc5809 = loc("multiply.2681")
#loc5810 = loc("erf.2682")
#loc5811 = loc("add.2684")
#loc5812 = loc("multiply.2687")
#loc5816 = loc("reduce.6775")
#loc5817 = loc("multiply.6784")
#loc5818 = loc("broadcast.6794")
#loc5819 = loc("subtract.6795")
#loc5820 = loc("multiply.6751")
#loc5821 = loc("reduce.6758")
#loc5822 = loc("multiply.6767")
#loc5823 = loc("reshape.6768")
#loc5824 = loc("add.6788")
#loc5825 = loc("rsqrt.6789")
#loc5826 = loc("reshape.6796")
#loc5827 = loc("broadcast.6797")
#loc5828 = loc("multiply.6798")
#loc5829 = loc("broadcast.6799")
#loc5830 = loc("multiply.6800")
#loc5831 = loc("broadcast.6803")
#loc5832 = loc("add.6804")
#loc5836 = loc("reduce.11115")
#loc5837 = loc("multiply.11124")
#loc5838 = loc("broadcast.11134")
#loc5839 = loc("subtract.11135")
#loc5840 = loc("multiply.11091")
#loc5841 = loc("reduce.11098")
#loc5842 = loc("multiply.11107")
#loc5843 = loc("reshape.11108")
#loc5844 = loc("add.11128")
#loc5845 = loc("rsqrt.11129")
#loc5846 = loc("reshape.11136")
#loc5847 = loc("broadcast.11137")
#loc5848 = loc("multiply.11138")
#loc5849 = loc("broadcast.11139")
#loc5850 = loc("multiply.11140")
#loc5851 = loc("broadcast.11143")
#loc5852 = loc("add.11144")
#loc5854 = loc("multiply.4856")
#loc5855 = loc("multiply.4851")
#loc5856 = loc("erf.4852")
#loc5857 = loc("add.4854")
#loc5858 = loc("multiply.4857")
#loc5862 = loc("reduce.12255")
#loc5863 = loc("multiply.12264")
#loc5864 = loc("broadcast.12274")
#loc5865 = loc("subtract.12275")
#loc5866 = loc("multiply.12231")
#loc5867 = loc("reduce.12238")
#loc5868 = loc("multiply.12247")
#loc5869 = loc("reshape.12248")
#loc5870 = loc("add.12268")
#loc5871 = loc("rsqrt.12269")
#loc5872 = loc("reshape.12276")
#loc5873 = loc("broadcast.12277")
#loc5874 = loc("multiply.12278")
#loc5875 = loc("broadcast.12279")
#loc5876 = loc("multiply.12280")
#loc5877 = loc("broadcast.12283")
#loc5878 = loc("add.12284")
#loc5880 = loc("multiply.9816")
#loc5881 = loc("multiply.9811")
#loc5882 = loc("erf.9812")
#loc5883 = loc("add.9814")
#loc5884 = loc("multiply.9817")
#loc5888 = loc("reduce.3878")
#loc5889 = loc("multiply.3887")
#loc5890 = loc("broadcast.3897")
#loc5891 = loc("subtract.3898")
#loc5892 = loc("multiply.3854")
#loc5893 = loc("reduce.3861")
#loc5894 = loc("multiply.3870")
#loc5895 = loc("reshape.3871")
#loc5896 = loc("add.3891")
#loc5897 = loc("rsqrt.3892")
#loc5898 = loc("reshape.3899")
#loc5899 = loc("broadcast.3900")
#loc5900 = loc("multiply.3901")
#loc5901 = loc("broadcast.3902")
#loc5902 = loc("multiply.3903")
#loc5903 = loc("broadcast.3906")
#loc5904 = loc("add.3907")
#loc5906 = loc("multiply.3306")
#loc5907 = loc("multiply.3301")
#loc5908 = loc("erf.3302")
#loc5909 = loc("add.3304")
#loc5910 = loc("multiply.3307")
#loc5912 = loc("multiply.7026")
#loc5913 = loc("multiply.7021")
#loc5914 = loc("erf.7022")
#loc5915 = loc("add.7024")
#loc5916 = loc("multiply.7027")
#loc5920 = loc("reduce.7908")
#loc5921 = loc("multiply.7917")
#loc5922 = loc("broadcast.7927")
#loc5923 = loc("subtract.7928")
#loc5924 = loc("multiply.7884")
#loc5925 = loc("reduce.7891")
#loc5926 = loc("multiply.7900")
#loc5927 = loc("reshape.7901")
#loc5928 = loc("add.7921")
#loc5929 = loc("rsqrt.7922")
#loc5930 = loc("reshape.7929")
#loc5931 = loc("broadcast.7930")
#loc5932 = loc("multiply.7931")
#loc5933 = loc("broadcast.7932")
#loc5934 = loc("multiply.7933")
#loc5935 = loc("broadcast.7936")
#loc5936 = loc("add.7937")
#loc5940 = loc("reduce.5225")
#loc5941 = loc("multiply.5234")
#loc5942 = loc("broadcast.5244")
#loc5943 = loc("subtract.5245")
#loc5944 = loc("multiply.5201")
#loc5945 = loc("reduce.5208")
#loc5946 = loc("multiply.5217")
#loc5947 = loc("reshape.5218")
#loc5948 = loc("add.5238")
#loc5949 = loc("rsqrt.5239")
#loc5950 = loc("reshape.5246")
#loc5951 = loc("broadcast.5247")
#loc5952 = loc("multiply.5248")
#loc5953 = loc("broadcast.5249")
#loc5954 = loc("multiply.5250")
#loc5955 = loc("broadcast.5253")
#loc5956 = loc("add.5254")
#loc5960 = loc("reduce.5428")
#loc5961 = loc("multiply.5437")
#loc5962 = loc("broadcast.5447")
#loc5963 = loc("subtract.5448")
#loc5964 = loc("multiply.5404")
#loc5965 = loc("reduce.5411")
#loc5966 = loc("multiply.5420")
#loc5967 = loc("reshape.5421")
#loc5968 = loc("add.5441")
#loc5969 = loc("rsqrt.5442")
#loc5970 = loc("reshape.5449")
#loc5971 = loc("broadcast.5450")
#loc5972 = loc("multiply.5451")
#loc5973 = loc("broadcast.5452")
#loc5974 = loc("multiply.5453")
#loc5975 = loc("broadcast.5456")
#loc5976 = loc("add.5457")
#loc5978 = loc("multiply.5476")
#loc5979 = loc("multiply.5471")
#loc5980 = loc("erf.5472")
#loc5981 = loc("add.5474")
#loc5982 = loc("multiply.5477")
#loc5986 = loc("reduce.5118")
#loc5987 = loc("multiply.5127")
#loc5988 = loc("broadcast.5137")
#loc5989 = loc("subtract.5138")
#loc5990 = loc("multiply.5094")
#loc5991 = loc("reduce.5101")
#loc5992 = loc("multiply.5110")
#loc5993 = loc("reshape.5111")
#loc5994 = loc("add.5131")
#loc5995 = loc("rsqrt.5132")
#loc5996 = loc("reshape.5139")
#loc5997 = loc("broadcast.5140")
#loc5998 = loc("multiply.5141")
#loc5999 = loc("broadcast.5142")
#loc6000 = loc("multiply.5143")
#loc6001 = loc("broadcast.5146")
#loc6002 = loc("add.5147")
#loc6006 = loc("reduce.3258")
#loc6007 = loc("multiply.3267")
#loc6008 = loc("broadcast.3277")
#loc6009 = loc("subtract.3278")
#loc6010 = loc("multiply.3234")
#loc6011 = loc("reduce.3241")
#loc6012 = loc("multiply.3250")
#loc6013 = loc("reshape.3251")
#loc6014 = loc("add.3271")
#loc6015 = loc("rsqrt.3272")
#loc6016 = loc("reshape.3279")
#loc6017 = loc("broadcast.3280")
#loc6018 = loc("multiply.3281")
#loc6019 = loc("broadcast.3282")
#loc6020 = loc("multiply.3283")
#loc6021 = loc("broadcast.3286")
#loc6022 = loc("add.3287")
#loc6026 = loc("reduce.5738")
#loc6027 = loc("multiply.5747")
#loc6028 = loc("broadcast.5757")
#loc6029 = loc("subtract.5758")
#loc6030 = loc("multiply.5714")
#loc6031 = loc("reduce.5721")
#loc6032 = loc("multiply.5730")
#loc6033 = loc("reshape.5731")
#loc6034 = loc("add.5751")
#loc6035 = loc("rsqrt.5752")
#loc6036 = loc("reshape.5759")
#loc6037 = loc("broadcast.5760")
#loc6038 = loc("multiply.5761")
#loc6039 = loc("broadcast.5762")
#loc6040 = loc("multiply.5763")
#loc6041 = loc("broadcast.5766")
#loc6042 = loc("add.5767")
#loc6046 = loc("reduce.11938")
#loc6047 = loc("multiply.11947")
#loc6048 = loc("broadcast.11957")
#loc6049 = loc("subtract.11958")
#loc6050 = loc("multiply.11914")
#loc6051 = loc("reduce.11921")
#loc6052 = loc("multiply.11930")
#loc6053 = loc("reshape.11931")
#loc6054 = loc("add.11951")
#loc6055 = loc("rsqrt.11952")
#loc6056 = loc("reshape.11959")
#loc6057 = loc("broadcast.11960")
#loc6058 = loc("multiply.11961")
#loc6059 = loc("broadcast.11962")
#loc6060 = loc("multiply.11963")
#loc6061 = loc("broadcast.11966")
#loc6062 = loc("add.11967")
#loc6066 = loc("reduce.9565")
#loc6067 = loc("multiply.9574")
#loc6068 = loc("broadcast.9584")
#loc6069 = loc("subtract.9585")
#loc6070 = loc("multiply.9541")
#loc6071 = loc("reduce.9548")
#loc6072 = loc("multiply.9557")
#loc6073 = loc("reshape.9558")
#loc6074 = loc("add.9578")
#loc6075 = loc("rsqrt.9579")
#loc6076 = loc("reshape.9586")
#loc6077 = loc("broadcast.9587")
#loc6078 = loc("multiply.9588")
#loc6079 = loc("broadcast.9589")
#loc6080 = loc("multiply.9590")
#loc6081 = loc("broadcast.9593")
#loc6082 = loc("add.9594")
#loc6084 = loc("multiply.4236")
#loc6085 = loc("multiply.4231")
#loc6086 = loc("erf.4232")
#loc6087 = loc("add.4234")
#loc6088 = loc("multiply.4237")
#loc6090 = loc("multiply.5786")
#loc6091 = loc("multiply.5781")
#loc6092 = loc("erf.5782")
#loc6093 = loc("add.5784")
#loc6094 = loc("multiply.5787")
#loc6098 = loc("reduce.3055")
#loc6099 = loc("multiply.3064")
#loc6100 = loc("broadcast.3074")
#loc6101 = loc("subtract.3075")
#loc6102 = loc("multiply.3031")
#loc6103 = loc("reduce.3038")
#loc6104 = loc("multiply.3047")
#loc6105 = loc("reshape.3048")
#loc6106 = loc("add.3068")
#loc6107 = loc("rsqrt.3069")
#loc6108 = loc("reshape.3076")
#loc6109 = loc("broadcast.3077")
#loc6110 = loc("multiply.3078")
#loc6111 = loc("broadcast.3079")
#loc6112 = loc("multiply.3080")
#loc6113 = loc("broadcast.3083")
#loc6114 = loc("add.3084")
#loc6116 = loc("multiply.6096")
#loc6117 = loc("multiply.6091")
#loc6118 = loc("erf.6092")
#loc6119 = loc("add.6094")
#loc6120 = loc("multiply.6097")
#loc6122 = loc("multiply.6406")
#loc6123 = loc("multiply.6401")
#loc6124 = loc("erf.6402")
#loc6125 = loc("add.6404")
#loc6126 = loc("multiply.6407")
#loc6130 = loc("reduce.8325")
#loc6131 = loc("multiply.8334")
#loc6132 = loc("broadcast.8344")
#loc6133 = loc("subtract.8345")
#loc6134 = loc("multiply.8301")
#loc6135 = loc("reduce.8308")
#loc6136 = loc("multiply.8317")
#loc6137 = loc("reshape.8318")
#loc6138 = loc("add.8338")
#loc6139 = loc("rsqrt.8339")
#loc6140 = loc("reshape.8346")
#loc6141 = loc("broadcast.8347")
#loc6142 = loc("multiply.8348")
#loc6143 = loc("broadcast.8349")
#loc6144 = loc("multiply.8350")
#loc6145 = loc("broadcast.8353")
#loc6146 = loc("add.8354")
#loc6150 = loc("reduce.7085")
#loc6151 = loc("multiply.7094")
#loc6152 = loc("broadcast.7104")
#loc6153 = loc("subtract.7105")
#loc6154 = loc("multiply.7061")
#loc6155 = loc("reduce.7068")
#loc6156 = loc("multiply.7077")
#loc6157 = loc("reshape.7078")
#loc6158 = loc("add.7098")
#loc6159 = loc("rsqrt.7099")
#loc6160 = loc("reshape.7106")
#loc6161 = loc("broadcast.7107")
#loc6162 = loc("multiply.7108")
#loc6163 = loc("broadcast.7109")
#loc6164 = loc("multiply.7110")
#loc6165 = loc("broadcast.7113")
#loc6166 = loc("add.7114")
#loc6168 = loc("multiply.3616")
#loc6169 = loc("multiply.3611")
#loc6170 = loc("erf.3612")
#loc6171 = loc("add.3614")
#loc6172 = loc("multiply.3617")
#loc6176 = loc("reduce.6668")
#loc6177 = loc("multiply.6677")
#loc6178 = loc("broadcast.6687")
#loc6179 = loc("subtract.6688")
#loc6180 = loc("multiply.6644")
#loc6181 = loc("reduce.6651")
#loc6182 = loc("multiply.6660")
#loc6183 = loc("reshape.6661")
#loc6184 = loc("add.6681")
#loc6185 = loc("rsqrt.6682")
#loc6186 = loc("reshape.6689")
#loc6187 = loc("broadcast.6690")
#loc6188 = loc("multiply.6691")
#loc6189 = loc("broadcast.6692")
#loc6190 = loc("multiply.6693")
#loc6191 = loc("broadcast.6696")
#loc6192 = loc("add.6697")
#loc6196 = loc("reduce.9458")
#loc6197 = loc("multiply.9467")
#loc6198 = loc("broadcast.9477")
#loc6199 = loc("subtract.9478")
#loc6200 = loc("multiply.9434")
#loc6201 = loc("reduce.9441")
#loc6202 = loc("multiply.9450")
#loc6203 = loc("reshape.9451")
#loc6204 = loc("add.9471")
#loc6205 = loc("rsqrt.9472")
#loc6206 = loc("reshape.9479")
#loc6207 = loc("broadcast.9480")
#loc6208 = loc("multiply.9481")
#loc6209 = loc("broadcast.9482")
#loc6210 = loc("multiply.9483")
#loc6211 = loc("broadcast.9486")
#loc6212 = loc("add.9487")
#loc6216 = loc("reduce.8838")
#loc6217 = loc("multiply.8847")
#loc6218 = loc("broadcast.8857")
#loc6219 = loc("subtract.8858")
#loc6220 = loc("multiply.8814")
#loc6221 = loc("reduce.8821")
#loc6222 = loc("multiply.8830")
#loc6223 = loc("reshape.8831")
#loc6224 = loc("add.8851")
#loc6225 = loc("rsqrt.8852")
#loc6226 = loc("reshape.8859")
#loc6227 = loc("broadcast.8860")
#loc6228 = loc("multiply.8861")
#loc6229 = loc("broadcast.8862")
#loc6230 = loc("multiply.8863")
#loc6231 = loc("broadcast.8866")
#loc6232 = loc("add.8867")
#loc6234 = loc("multiply.6716")
#loc6235 = loc("multiply.6711")
#loc6236 = loc("erf.6712")
#loc6237 = loc("add.6714")
#loc6238 = loc("multiply.6717")
#loc6242 = loc("reduce.6155")
#loc6243 = loc("multiply.6164")
#loc6244 = loc("broadcast.6174")
#loc6245 = loc("subtract.6175")
#loc6246 = loc("multiply.6131")
#loc6247 = loc("reduce.6138")
#loc6248 = loc("multiply.6147")
#loc6249 = loc("reshape.6148")
#loc6250 = loc("add.6168")
#loc6251 = loc("rsqrt.6169")
#loc6252 = loc("reshape.6176")
#loc6253 = loc("broadcast.6177")
#loc6254 = loc("multiply.6178")
#loc6255 = loc("broadcast.6179")
#loc6256 = loc("multiply.6180")
#loc6257 = loc("broadcast.6183")
#loc6258 = loc("add.6184")
#loc6262 = loc("reduce.6978")
#loc6263 = loc("multiply.6987")
#loc6264 = loc("broadcast.6997")
#loc6265 = loc("subtract.6998")
#loc6266 = loc("multiply.6954")
#loc6267 = loc("reduce.6961")
#loc6268 = loc("multiply.6970")
#loc6269 = loc("reshape.6971")
#loc6270 = loc("add.6991")
#loc6271 = loc("rsqrt.6992")
#loc6272 = loc("reshape.6999")
#loc6273 = loc("broadcast.7000")
#loc6274 = loc("multiply.7001")
#loc6275 = loc("broadcast.7002")
#loc6276 = loc("multiply.7003")
#loc6277 = loc("broadcast.7006")
#loc6278 = loc("add.7007")
#loc6282 = loc("reduce.7288")
#loc6283 = loc("multiply.7297")
#loc6284 = loc("broadcast.7307")
#loc6285 = loc("subtract.7308")
#loc6286 = loc("multiply.7264")
#loc6287 = loc("reduce.7271")
#loc6288 = loc("multiply.7280")
#loc6289 = loc("reshape.7281")
#loc6290 = loc("add.7301")
#loc6291 = loc("rsqrt.7302")
#loc6292 = loc("reshape.7309")
#loc6293 = loc("broadcast.7310")
#loc6294 = loc("multiply.7311")
#loc6295 = loc("broadcast.7312")
#loc6296 = loc("multiply.7313")
#loc6297 = loc("broadcast.7316")
#loc6298 = loc("add.7317")
#loc6302 = loc("reduce.8015")
#loc6303 = loc("multiply.8024")
#loc6304 = loc("broadcast.8034")
#loc6305 = loc("subtract.8035")
#loc6306 = loc("multiply.7991")
#loc6307 = loc("reduce.7998")
#loc6308 = loc("multiply.8007")
#loc6309 = loc("reshape.8008")
#loc6310 = loc("add.8028")
#loc6311 = loc("rsqrt.8029")
#loc6312 = loc("reshape.8036")
#loc6313 = loc("broadcast.8037")
#loc6314 = loc("multiply.8038")
#loc6315 = loc("broadcast.8039")
#loc6316 = loc("multiply.8040")
#loc6317 = loc("broadcast.8043")
#loc6318 = loc("add.8044")
#loc6322 = loc("reduce.7395")
#loc6323 = loc("multiply.7404")
#loc6324 = loc("broadcast.7414")
#loc6325 = loc("subtract.7415")
#loc6326 = loc("multiply.7371")
#loc6327 = loc("reduce.7378")
#loc6328 = loc("multiply.7387")
#loc6329 = loc("reshape.7388")
#loc6330 = loc("add.7408")
#loc6331 = loc("rsqrt.7409")
#loc6332 = loc("reshape.7416")
#loc6333 = loc("broadcast.7417")
#loc6334 = loc("multiply.7418")
#loc6335 = loc("broadcast.7419")
#loc6336 = loc("multiply.7420")
#loc6337 = loc("broadcast.7423")
#loc6338 = loc("add.7424")
#loc6340 = loc("multiply.10126")
#loc6341 = loc("multiply.10121")
#loc6342 = loc("erf.10122")
#loc6343 = loc("add.10124")
#loc6344 = loc("multiply.10127")
#loc6346 = loc("multiply.9196")
#loc6347 = loc("multiply.9191")
#loc6348 = loc("erf.9192")
#loc6349 = loc("add.9194")
#loc6350 = loc("multiply.9197")
#loc6354 = loc("reduce.7598")
#loc6355 = loc("multiply.7607")
#loc6356 = loc("broadcast.7617")
#loc6357 = loc("subtract.7618")
#loc6358 = loc("multiply.7574")
#loc6359 = loc("reduce.7581")
#loc6360 = loc("multiply.7590")
#loc6361 = loc("reshape.7591")
#loc6362 = loc("add.7611")
#loc6363 = loc("rsqrt.7612")
#loc6364 = loc("reshape.7619")
#loc6365 = loc("broadcast.7620")
#loc6366 = loc("multiply.7621")
#loc6367 = loc("broadcast.7622")
#loc6368 = loc("multiply.7623")
#loc6369 = loc("broadcast.7626")
#loc6370 = loc("add.7627")
#loc6374 = loc("reduce.2745")
#loc6375 = loc("multiply.2754")
#loc6376 = loc("broadcast.2764")
#loc6377 = loc("subtract.2765")
#loc6378 = loc("multiply.2721")
#loc6379 = loc("reduce.2728")
#loc6380 = loc("multiply.2737")
#loc6381 = loc("reshape.2738")
#loc6382 = loc("add.2758")
#loc6383 = loc("rsqrt.2759")
#loc6384 = loc("reshape.2766")
#loc6385 = loc("broadcast.2767")
#loc6386 = loc("multiply.2768")
#loc6387 = loc("broadcast.2769")
#loc6388 = loc("multiply.2770")
#loc6389 = loc("broadcast.2773")
#loc6390 = loc("add.2774")
#loc6392 = loc("multiply.7646")
#loc6393 = loc("multiply.7641")
#loc6394 = loc("erf.7642")
#loc6395 = loc("add.7644")
#loc6396 = loc("multiply.7647")
#loc6400 = loc("reduce.7705")
#loc6401 = loc("multiply.7714")
#loc6402 = loc("broadcast.7724")
#loc6403 = loc("subtract.7725")
#loc6404 = loc("multiply.7681")
#loc6405 = loc("reduce.7688")
#loc6406 = loc("multiply.7697")
#loc6407 = loc("reshape.7698")
#loc6408 = loc("add.7718")
#loc6409 = loc("rsqrt.7719")
#loc6410 = loc("reshape.7726")
#loc6411 = loc("broadcast.7727")
#loc6412 = loc("multiply.7728")
#loc6413 = loc("broadcast.7729")
#loc6414 = loc("multiply.7730")
#loc6415 = loc("broadcast.7733")
#loc6416 = loc("add.7734")
#loc6420 = loc("reduce.4605")
#loc6421 = loc("multiply.4614")
#loc6422 = loc("broadcast.4624")
#loc6423 = loc("subtract.4625")
#loc6424 = loc("multiply.4581")
#loc6425 = loc("reduce.4588")
#loc6426 = loc("multiply.4597")
#loc6427 = loc("reshape.4598")
#loc6428 = loc("add.4618")
#loc6429 = loc("rsqrt.4619")
#loc6430 = loc("reshape.4626")
#loc6431 = loc("broadcast.4627")
#loc6432 = loc("multiply.4628")
#loc6433 = loc("broadcast.4629")
#loc6434 = loc("multiply.4630")
#loc6435 = loc("broadcast.4633")
#loc6436 = loc("add.4634")
#loc6438 = loc("multiply.7956")
#loc6439 = loc("multiply.7951")
#loc6440 = loc("erf.7952")
#loc6441 = loc("add.7954")
#loc6442 = loc("multiply.7957")
#loc6446 = loc("reduce.11735")
#loc6447 = loc("multiply.11744")
#loc6448 = loc("broadcast.11754")
#loc6449 = loc("subtract.11755")
#loc6450 = loc("multiply.11711")
#loc6451 = loc("reduce.11718")
#loc6452 = loc("multiply.11727")
#loc6453 = loc("reshape.11728")
#loc6454 = loc("add.11748")
#loc6455 = loc("rsqrt.11749")
#loc6456 = loc("reshape.11756")
#loc6457 = loc("broadcast.11757")
#loc6458 = loc("multiply.11758")
#loc6459 = loc("broadcast.11759")
#loc6460 = loc("multiply.11760")
#loc6461 = loc("broadcast.11763")
#loc6462 = loc("add.11764")
#loc6466 = loc("reduce.8218")
#loc6467 = loc("multiply.8227")
#loc6468 = loc("broadcast.8237")
#loc6469 = loc("subtract.8238")
#loc6470 = loc("multiply.8194")
#loc6471 = loc("reduce.8201")
#loc6472 = loc("multiply.8210")
#loc6473 = loc("reshape.8211")
#loc6474 = loc("add.8231")
#loc6475 = loc("rsqrt.8232")
#loc6476 = loc("reshape.8239")
#loc6477 = loc("broadcast.8240")
#loc6478 = loc("multiply.8241")
#loc6479 = loc("broadcast.8242")
#loc6480 = loc("multiply.8243")
#loc6481 = loc("broadcast.8246")
#loc6482 = loc("add.8247")
#loc6484 = loc("multiply.8266")
#loc6485 = loc("multiply.8261")
#loc6486 = loc("erf.8262")
#loc6487 = loc("add.8264")
#loc6488 = loc("multiply.8267")
